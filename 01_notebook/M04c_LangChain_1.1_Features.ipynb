{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üÜï LangChain v1.1.0 - Neue Features (Dezember 2025)\n",
    "\n",
    "**Modul:** M04c - LangChain Advanced  \n",
    "**Version:** 1.0  \n",
    "**Datum:** Dezember 2025  \n",
    "**Autor:** GenAI Projekt Team\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Lernziele\n",
    "\n",
    "In diesem Notebook lernst du die **5 wichtigsten neuen Features** von LangChain v1.1.0 (Release: 2. Dezember 2025) kennen:\n",
    "\n",
    "1. ‚ú® **Model Profiles** - Automatische Capability-Detection\n",
    "2. ‚ú® **Smart Structured Output** - Auto-Inference von `ProviderStrategy`\n",
    "3. ‚ú® **SystemMessage in Agents** - Cache-Control f√ºr Anthropic Claude\n",
    "4. ‚ú® **ModelRetryMiddleware** - Automatische Retries mit exponential backoff\n",
    "5. ‚ú® **ContentModerationMiddleware** - OpenAI Moderation f√ºr Safety-Layer\n",
    "\n",
    "**Dokumentation:**\n",
    "- [LangChain 1.0 Must-Haves](../LangChain_1.0_Must_Haves.md)\n",
    "- [LangChain 1.1 Release Notes](https://changelog.langchain.com/announcements/langchain-1-1)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation (Google Colab)\n",
    "!pip install -q langchain>=1.1.0 langchain-core>=1.1.0 langchain-openai>=0.2.0\n",
    "!pip install -q python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API Keys laden\n",
    "load_dotenv()\n",
    "\n",
    "# Manuelle Eingabe falls .env nicht vorhanden\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    import getpass\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key: \")\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Model Profiles - Automatische Capability-Detection\n",
    "\n",
    "### ‚ùì Was ist neu?\n",
    "\n",
    "**Vor v1.1.0:**\n",
    "```python\n",
    "# Hardcoded Provider-Checks\n",
    "if provider == \"openai\":\n",
    "    use_structured_output = True\n",
    "elif provider == \"anthropic\":\n",
    "    use_structured_output = False\n",
    "```\n",
    "\n",
    "**Ab v1.1.0:**\n",
    "```python\n",
    "# Automatische Detection via Model Profiles\n",
    "if llm.profile.supports_structured_output:\n",
    "    use_structured_output = True\n",
    "```\n",
    "\n",
    "**Quelle:** Model Profiles werden von [models.dev](https://models.dev) (Open-Source Model Index) bezogen.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Model initialisieren\n",
    "llm = init_chat_model(\n",
    "    \"gpt-4o-mini\",\n",
    "    model_provider=\"openai\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(\"üîç Model Profiles von gpt-4o-mini:\\n\")\n",
    "print(f\"‚úÖ Structured Output: {llm.profile.supports_structured_output}\")\n",
    "print(f\"‚úÖ Function Calling: {llm.profile.supports_function_calling}\")\n",
    "print(f\"‚úÖ Vision: {llm.profile.supports_vision}\")\n",
    "print(f\"‚úÖ JSON Mode: {llm.profile.supports_json_mode}\")\n",
    "print(f\"üìä Context Window: {llm.profile.context_window} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamische Feature-Detection in der Praxis\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def analyze_image_if_supported(llm, image_url: str, question: str):\n",
    "    \"\"\"Analysiert Bild nur wenn Model Vision unterst√ºtzt.\"\"\"\n",
    "    \n",
    "    if llm.profile.supports_vision:\n",
    "        print(\"‚úÖ Model unterst√ºtzt Vision - Bild wird analysiert...\")\n",
    "        \n",
    "        response = llm.invoke([HumanMessage(content=[\n",
    "            {\"type\": \"text\", \"text\": question},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n",
    "        ])])\n",
    "        \n",
    "        return response.content\n",
    "    else:\n",
    "        print(\"‚ùå Model unterst√ºtzt keine Vision - Fallback zu Text-only\")\n",
    "        return \"Model unterst√ºtzt keine Bildverarbeitung\"\n",
    "\n",
    "# Beispiel\n",
    "result = analyze_image_if_supported(\n",
    "    llm,\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "    \"Was siehst du auf diesem Bild?\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüìù Antwort:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Smart Structured Output - Auto-Inference\n",
    "\n",
    "### ‚ùì Was ist neu?\n",
    "\n",
    "`ProviderStrategy` wird automatisch aus Model Profiles abgeleitet:\n",
    "\n",
    "- Wenn `llm.profile.supports_structured_output == True` ‚Üí nutzt **native API** (OpenAI Structured Output)\n",
    "- Wenn `llm.profile.supports_json_mode == True` ‚Üí nutzt **JSON mode**\n",
    "- Fallback ‚Üí **function calling**\n",
    "\n",
    "**Vorteil:** Weniger Konfiguration, intelligentere Defaults!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# Pydantic Schema definieren\n",
    "class MovieRecommendation(BaseModel):\n",
    "    \"\"\"Filmempfehlung mit Metadaten.\"\"\"\n",
    "    title: str = Field(description=\"Filmtitel\")\n",
    "    genre: str = Field(description=\"Genre (z.B. Sci-Fi, Drama, Action)\")\n",
    "    year: int = Field(description=\"Erscheinungsjahr\")\n",
    "    rating: float = Field(description=\"Bewertung von 1-10\", ge=1, le=10)\n",
    "    reason: str = Field(description=\"Kurze Begr√ºndung der Empfehlung\")\n",
    "\n",
    "# ‚ú® NEU in v1.1.0: Auto-Inference der besten Strategie\n",
    "structured_llm = llm.with_structured_output(MovieRecommendation)\n",
    "\n",
    "# Query\n",
    "result = structured_llm.invoke(\n",
    "    \"Empfiehl mir einen guten Science-Fiction Film aus den 2010er Jahren\"\n",
    ")\n",
    "\n",
    "print(\"üé¨ Filmempfehlung:\\n\")\n",
    "print(f\"Titel: {result.title}\")\n",
    "print(f\"Genre: {result.genre}\")\n",
    "print(f\"Jahr: {result.year}\")\n",
    "print(f\"Bewertung: {result.rating}/10\")\n",
    "print(f\"Begr√ºndung: {result.reason}\")\n",
    "\n",
    "# Type-Check\n",
    "print(f\"\\n‚úÖ Type-Check: {type(result)} = {MovieRecommendation}\")\n",
    "assert isinstance(result, MovieRecommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Vergleich: ALT vs. NEU\n",
    "\n",
    "**‚ùå ALT (v1.0):**\n",
    "```python\n",
    "# Manuelle Strategie-Auswahl\n",
    "structured_llm = llm.with_structured_output(\n",
    "    MovieRecommendation,\n",
    "    provider_strategy=\"native\"  # Manuell festgelegt!\n",
    ")\n",
    "```\n",
    "\n",
    "**‚úÖ NEU (v1.1.0):**\n",
    "```python\n",
    "# Automatische Strategie-Auswahl via Model Profiles\n",
    "structured_llm = llm.with_structured_output(MovieRecommendation)\n",
    "# Nutzt automatisch \"native\" wenn llm.profile.supports_structured_output == True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ SystemMessage Support in create_agent()\n",
    "\n",
    "### ‚ùì Was ist neu?\n",
    "\n",
    "`system_prompt` akzeptiert jetzt `SystemMessage` mit erweiterten Features:\n",
    "\n",
    "- ‚úÖ **Cache-Control** (Anthropic Claude) - spart Tokens bei wiederholten System-Prompts\n",
    "- ‚úÖ **Strukturierte Content Blocks** - Bilder, Dokumente im System-Prompt\n",
    "- ‚úÖ **Metadata** - Orchestration-Hints f√ºr komplexe Agents\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Tool definieren\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Berechnet mathematische Ausdr√ºcke.\n",
    "    \n",
    "    Args:\n",
    "        expression: Mathematischer Ausdruck (z.B. \"2 + 2\", \"sqrt(16)\")\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import math\n",
    "        result = eval(expression, {\"__builtins__\": None}, {\"math\": math})\n",
    "        return f\"Result: {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# ‚ùå ALT: String-basierter System-Prompt\n",
    "agent_old = create_agent(\n",
    "    model=llm,\n",
    "    tools=[calculate],\n",
    "    system_prompt=\"You are a helpful math assistant\",\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "# ‚úÖ NEU (v1.1.0): SystemMessage mit erweiterten Features\n",
    "agent_new = create_agent(\n",
    "    model=llm,\n",
    "    tools=[calculate],\n",
    "    system_prompt=SystemMessage(\n",
    "        content=\"\"\"You are a helpful math assistant specialized in complex calculations.\n",
    "        \n",
    "Guidelines:\n",
    "- Always explain your reasoning\n",
    "- Use the calculate tool for all mathematical operations\n",
    "- Format results clearly\"\"\",\n",
    "        # Cache-Control f√ºr Anthropic (w√ºrde Tokens sparen bei wiederholten Calls)\n",
    "        # cache_control={\"type\": \"ephemeral\"}  # Nur f√ºr Anthropic relevant\n",
    "    ),\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Agent mit SystemMessage erstellt!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent testen\n",
    "response = agent_new.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Was ist die Quadratwurzel von 144?\"}]\n",
    "})\n",
    "\n",
    "print(\"üìä Agent Response:\")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ ModelRetryMiddleware - Automatische Retries\n",
    "\n",
    "### ‚ùì Was ist neu?\n",
    "\n",
    "**Neue Middleware** f√ºr automatische Retries bei API-Fehlern:\n",
    "\n",
    "- ‚úÖ Exponential Backoff (2s, 4s, 8s, ...)\n",
    "- ‚úÖ Konfigurierbare Retry-Conditions (rate_limit, timeout, server_error)\n",
    "- ‚úÖ Jitter (Randomized delay) zur Vermeidung von Thundering Herd\n",
    "- ‚úÖ Production-Ready Resilience\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import ModelRetryMiddleware\n",
    "\n",
    "# Agent mit Retry-Middleware\n",
    "agent_with_retry = create_agent(\n",
    "    model=llm,\n",
    "    tools=[calculate],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    "    middleware=[\n",
    "        ModelRetryMiddleware(\n",
    "            max_retries=3,\n",
    "            backoff_factor=2.0,  # Exponential Backoff: 2s, 4s, 8s\n",
    "            retry_on=[\"rate_limit_error\", \"timeout\", \"server_error\"],\n",
    "            jitter=True  # Randomized delay\n",
    "        )\n",
    "    ],\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Agent mit ModelRetryMiddleware erstellt!\")\n",
    "print(\"\\nüîÑ Retry-Konfiguration:\")\n",
    "print(\"- Max Retries: 3\")\n",
    "print(\"- Backoff: Exponential (2s, 4s, 8s)\")\n",
    "print(\"- Retry bei: Rate Limit, Timeout, Server Error\")\n",
    "print(\"- Jitter: Aktiviert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel-Query (w√ºrde bei Rate Limit automatisch retries durchf√ºhren)\n",
    "response = agent_with_retry.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Berechne 123 * 456\"}]\n",
    "})\n",
    "\n",
    "print(\"üìä Agent Response:\")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Use Cases f√ºr ModelRetryMiddleware\n",
    "\n",
    "1. **Rate Limiting** - OpenAI/Anthropic Rate Limits\n",
    "2. **Transiente Netzwerkfehler** - Timeouts, DNS-Fehler\n",
    "3. **Server-Timeouts** - API-Provider √ºberlastet\n",
    "4. **Production Resilience** - Robuste Production-Apps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ ContentModerationMiddleware - Safety-Layer\n",
    "\n",
    "### ‚ùì Was ist neu?\n",
    "\n",
    "**Neue Middleware** f√ºr Content-Moderation via OpenAI Moderation API:\n",
    "\n",
    "- ‚úÖ Filter f√ºr User-Input, Model-Output, Tool-Results\n",
    "- ‚úÖ Kategorien: Hate, Violence, Sexual, Self-Harm, etc.\n",
    "- ‚úÖ Block on Violation (Workflow stoppen)\n",
    "- ‚úÖ DSGVO & Brand-Safety f√ºr Production-Apps\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import ContentModerationMiddleware\n",
    "\n",
    "# Agent mit Content-Moderation\n",
    "agent_with_moderation = create_agent(\n",
    "    model=llm,\n",
    "    tools=[calculate],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    "    middleware=[\n",
    "        ContentModerationMiddleware(\n",
    "            provider=\"openai\",  # Nutzt OpenAI Moderation API\n",
    "            check_user_input=True,      # Filter User-Inputs\n",
    "            check_model_output=True,    # Filter Model-Responses\n",
    "            check_tool_output=True,     # Filter Tool-Results\n",
    "            block_on_violation=True,    # Workflow stoppen bei Violation\n",
    "            categories=[\"hate\", \"violence\", \"sexual\", \"self-harm\"]  # Kategorien\n",
    "        )\n",
    "    ],\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Agent mit ContentModerationMiddleware erstellt!\")\n",
    "print(\"\\nüõ°Ô∏è Moderation-Konfiguration:\")\n",
    "print(\"- Provider: OpenAI Moderation API\")\n",
    "print(\"- Check User Input: ‚úÖ\")\n",
    "print(\"- Check Model Output: ‚úÖ\")\n",
    "print(\"- Check Tool Output: ‚úÖ\")\n",
    "print(\"- Block on Violation: ‚úÖ\")\n",
    "print(\"- Kategorien: Hate, Violence, Sexual, Self-Harm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel: Sichere Query\n",
    "safe_response = agent_with_moderation.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Was ist 5 + 3?\"}]\n",
    "})\n",
    "\n",
    "print(\"‚úÖ Sichere Query - Passed Moderation:\")\n",
    "print(safe_response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Use Cases f√ºr ContentModerationMiddleware\n",
    "\n",
    "1. **User-Generated Content** - Filter f√ºr Chat-Apps\n",
    "2. **DSGVO-Compliance** - Datenschutz & Safety\n",
    "3. **Brand-Safety** - Production-Apps mit Public-Facing Content\n",
    "4. **Multi-Layer-Moderation** - Input + Output + Tool-Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Zusammenfassung: Was ist neu in v1.1.0?\n",
    "\n",
    "| Feature | Vorteil | Use Case |\n",
    "|---------|---------|----------|\n",
    "| **Model Profiles** | Automatische Capability-Detection | Dynamische Feature-Detection statt hardcoded Provider-Checks |\n",
    "| **Smart Structured Output** | Auto-Inference von `ProviderStrategy` | Weniger Konfiguration, intelligentere Defaults |\n",
    "| **SystemMessage in Agents** | Cache-Control & Content Blocks | Kostenersparnis bei Anthropic Claude |\n",
    "| **ModelRetryMiddleware** | Automatische Retries | Resilience gegen API-Fehler (Rate Limits, Timeouts) |\n",
    "| **ContentModerationMiddleware** | Safety-Layer | DSGVO-Compliance, Brand-Safety f√ºr Production |\n",
    "\n",
    "### ‚úÖ Keine Breaking Changes!\n",
    "\n",
    "- v1.1.0 ist **100% r√ºckw√§rtskompatibel** mit v1.0\n",
    "- Bestehender Code funktioniert weiterhin\n",
    "- Neue Features sind **optional**, aber empfohlen\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Weitere Ressourcen\n",
    "\n",
    "- **LangChain 1.1 Release Notes:** https://changelog.langchain.com/announcements/langchain-1-1\n",
    "- **LangChain 1.0 Must-Haves:** [../LangChain_1.0_Must_Haves.md](../LangChain_1.0_Must_Haves.md)\n",
    "- **CLAUDE.md:** [../CLAUDE.md](../CLAUDE.md)\n",
    "- **models.dev:** https://models.dev (Model Profiles Quelle)\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Aufgaben\n",
    "\n",
    "### Aufgabe 1: Model Profiles erkunden\n",
    "\n",
    "Teste verschiedene Modelle und vergleiche ihre Profiles:\n",
    "- `gpt-4o-mini`\n",
    "- `gpt-4`\n",
    "- `claude-3-sonnet` (falls Anthropic API-Key vorhanden)\n",
    "\n",
    "Welche Unterschiede gibt es bei den unterst√ºtzten Features?\n",
    "\n",
    "### Aufgabe 2: Strukturierte Ausgabe testen\n",
    "\n",
    "Erstelle ein eigenes Pydantic-Schema (z.B. f√ºr Produkt-Reviews, Rezepte, oder Reiseziele) und teste `with_structured_output()` mit verschiedenen Queries.\n",
    "\n",
    "### Aufgabe 3: Multi-Middleware Agent\n",
    "\n",
    "Erstelle einen Agent, der **alle 3 Middleware** kombiniert:\n",
    "- `ModelRetryMiddleware`\n",
    "- `ContentModerationMiddleware`\n",
    "- `SummarizationMiddleware` (f√ºr lange Sessions)\n",
    "\n",
    "Teste den Agent mit verschiedenen Queries und beobachte das Zusammenspiel der Middleware.\n",
    "\n",
    "---\n",
    "\n",
    "**Ende des Notebooks** üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{"cells":[{"cell_type":"markdown","metadata":{"id":"3S4vrXmPdxYG"},"source":["# üÜï LangChain v1.1.0 - Neue Features (Dezember 2025)\n","\n","**Modul:** M04c - LangChain Advanced  \n","**Version:** 1.0  \n","**Datum:** Dezember 2025  \n","**Autor:** GenAI Projekt Team\n","\n","---\n","\n","## üìö Lernziele\n","\n","In diesem Notebook lernst du die **5 wichtigsten neuen Features** von LangChain v1.1.0 (Release: 2. Dezember 2025) kennen:\n","\n","1. ‚ú® **Model Profiles** - Automatische Capability-Detection\n","2. ‚ú® **Smart Structured Output** - Auto-Inference von `ProviderStrategy`\n","3. ‚ú® **SystemMessage in Agents** - Cache-Control f√ºr Anthropic Claude\n","4. ‚ú® **ModelRetryMiddleware** - Automatische Retries mit exponential backoff\n","5. ‚ú® **ContentModerationMiddleware** - OpenAI Moderation f√ºr Safety-Layer\n","\n","**Dokumentation:**\n","- [LangChain 1.0 Must-Haves](../LangChain_1.0_Must_Haves.md)\n","- [LangChain 1.1 Release Notes](https://changelog.langchain.com/announcements/langchain-1-1)\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"MBr4rdg2dxYM"},"source":["## üõ†Ô∏è Setup & Installation"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"ecgmd6UNdxYN","executionInfo":{"status":"ok","timestamp":1764769638967,"user_tz":-60,"elapsed":23141,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Installation (Google Colab)\n","!pip install -q langchain>=1.1.0 langchain-core>=1.1.0 langchain-openai>=0.2.0\n","!pip install -q python-dotenv"]},{"cell_type":"code","source":["!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul"],"metadata":{"id":"G5BhG-RBd9Zp","executionInfo":{"status":"ok","timestamp":1764769697698,"user_tz":-60,"elapsed":45258,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n","!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul\n","from genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages, load_chat_prompt_template\n","setup_api_keys(['OPENAI_API_KEY'], create_globals=False)\n","print()\n","check_environment()\n","print()\n","get_ipinfo()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jvEZc7Crd_in","executionInfo":{"status":"ok","timestamp":1764769720683,"user_tz":-60,"elapsed":23007,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"7e2d2e7e-7d41-48ab-e758-ba2c08564372"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úì OPENAI_API_KEY erfolgreich gesetzt\n","\n","Python Version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n","\n","Installierte LangChain- und LangGraph-Bibliotheken:\n","langchain                                1.1.0\n","langchain-chroma                         1.0.0\n","langchain-classic                        1.0.0\n","langchain-community                      0.4.1\n","langchain-core                           1.1.0\n","langchain-ollama                         1.0.0\n","langchain-openai                         1.1.0\n","langchain-text-splitters                 1.0.0\n","langgraph                                1.0.3\n","langgraph-checkpoint                     3.0.1\n","langgraph-prebuilt                       1.0.5\n","langgraph-sdk                            0.2.10\n","\n","IP-Adresse: 34.41.49.79\n","Hostname: 79.49.41.34.bc.googleusercontent.com\n","Stadt: Council Bluffs\n","Region: Iowa\n","Land: US\n","Koordinaten: 41.2619,-95.8608\n","Provider: AS396982 Google LLC\n","Postleitzahl: 51502\n","Zeitzone: America/Chicago\n"]}]},{"cell_type":"markdown","metadata":{"id":"bTAYwmt9dxYP"},"source":["---\n","\n","## 1Ô∏è‚É£ Model Profiles - Automatische Capability-Detection\n","\n","### ‚ùì Was ist neu?\n","\n","**Vor v1.1.0:**\n","```python\n","# Hardcoded Provider-Checks\n","if provider == \"openai\":\n","    use_structured_output = True\n","elif provider == \"anthropic\":\n","    use_structured_output = False\n","```\n","\n","**Ab v1.1.0:**\n","```python\n","# Automatische Detection via Model Profiles\n","if llm.profile.supports_structured_output:\n","    use_structured_output = True\n","```\n","\n","**Quelle:** Model Profiles werden von [models.dev](https://models.dev) (Open-Source Model Index) bezogen.\n","\n","---"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3w5rAg9PdxYP","executionInfo":{"status":"ok","timestamp":1764769816495,"user_tz":-60,"elapsed":42,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"cd34f515-195b-4e6e-b2d2-61de7d45ec84"},"outputs":[{"output_type":"stream","name":"stdout","text":["üîç Model Profiles von gpt-4o-mini:\n","\n","Full LLM Profile: {'max_input_tokens': 128000, 'max_output_tokens': 16384, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}\n","\n","‚úÖ Structured Output: True\n","‚úÖ Function Calling: True\n","‚úÖ Vision: True\n","üìä Context Window: 128000 tokens\n"]}],"source":["from langchain.chat_models import init_chat_model\n","\n","# Model initialisieren (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0.0)\n","\n","print(\"üîç Model Profiles von gpt-4o-mini:\\n\")\n","# Print the full profile to debug available keys\n","print(f\"Full LLM Profile: {llm.profile}\\n\")\n","print(f\"‚úÖ Structured Output: {llm.profile['structured_output']}\")\n","print(f\"‚úÖ Function Calling: {llm.profile['tool_calling']}\")\n","print(f\"‚úÖ Vision: {llm.profile['image_inputs']}\")\n","# The profile does not directly contain 'supports_json_mode', but 'structured_output' often implies JSON capability for many models.\n","# For now, we will skip it or use a placeholder if 'json_mode' isn't explicitly listed.\n","# print(f\"‚úÖ JSON Mode: {llm.profile['supports_json_mode']}\") # This key is not present.\n","print(f\"üìä Context Window: {llm.profile['max_input_tokens']} tokens\")"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HtOzao6rdxYQ","executionInfo":{"status":"ok","timestamp":1764769961347,"user_tz":-60,"elapsed":3572,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"99c54fde-1799-4bd9-81dd-3e2a523c679e"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Model unterst√ºtzt Vision - Bild wird analysiert...\n","\n","üìù Antwort:\n","Auf dem Bild ist ein schwarzer Welpe zu sehen, der auf einem Holzboden liegt. Der Welpe hat gro√üe, neugierige Augen und wirkt freundlich und verspielt.\n"]}],"source":["# Dynamische Feature-Detection in der Praxis\n","from langchain_core.messages import HumanMessage\n","\n","def analyze_image_if_supported(llm, image_url: str, question: str):\n","    \"\"\"Analysiert Bild nur wenn Model Vision unterst√ºtzt.\"\"\"\n","\n","    if llm.profile['image_inputs']:\n","        print(\"‚úÖ Model unterst√ºtzt Vision - Bild wird analysiert...\")\n","\n","        response = llm.invoke([HumanMessage(content=[\n","            {\"type\": \"text\", \"text\": question},\n","            {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n","        ])])\n","\n","        return response.content\n","    else:\n","        print(\"‚ùå Model unterst√ºtzt keine Vision - Fallback zu Text-only\")\n","        return \"Model unterst√ºtzt keine Bildverarbeitung\"\n","\n","# Beispiel\n","result = analyze_image_if_supported(\n","    llm,\n","    \"https://picsum.photos/id/237/200/300\", # Changed to a more reliable image URL\n","    \"Was siehst du auf diesem Bild?\"\n",")\n","\n","print(f\"\\nüìù Antwort:\\n{result}\")"]},{"cell_type":"markdown","metadata":{"id":"a86SFM1ydxYR"},"source":["---\n","\n","## 2Ô∏è‚É£ Smart Structured Output - Auto-Inference\n","\n","### ‚ùì Was ist neu?\n","\n","`ProviderStrategy` wird automatisch aus Model Profiles abgeleitet:\n","\n","- Wenn `llm.profile.supports_structured_output == True` ‚Üí nutzt **native API** (OpenAI Structured Output)\n","- Wenn `llm.profile.supports_json_mode == True` ‚Üí nutzt **JSON mode**\n","- Fallback ‚Üí **function calling**\n","\n","**Vorteil:** Weniger Konfiguration, intelligentere Defaults!\n","\n","---"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2OpwWI3UdxYR","executionInfo":{"status":"ok","timestamp":1764769895024,"user_tz":-60,"elapsed":2054,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"8ab6bf7b-8082-4772-c7b5-27309b6e7e6a"},"outputs":[{"output_type":"stream","name":"stdout","text":["üé¨ Filmempfehlung:\n","\n","Titel: Inception\n","Genre: Sci-Fi, Action, Thriller\n","Jahr: 2010\n","Bewertung: 9.0/10\n","Begr√ºndung: Inception ist ein visuell beeindruckender Film, der die Grenzen zwischen Traum und Realit√§t verschwimmen l√§sst. Die komplexe Handlung und die tiefgr√ºndigen Themen √ºber das Unterbewusstsein machen ihn zu einem Meisterwerk des Science-Fiction-Genres.\n","\n","‚úÖ Type-Check: <class '__main__.MovieRecommendation'> = <class '__main__.MovieRecommendation'>\n"]}],"source":["from pydantic import BaseModel, Field\n","from typing import List\n","\n","# Pydantic Schema definieren\n","class MovieRecommendation(BaseModel):\n","    \"\"\"Filmempfehlung mit Metadaten.\"\"\"\n","    title: str = Field(description=\"Filmtitel\")\n","    genre: str = Field(description=\"Genre (z.B. Sci-Fi, Drama, Action)\")\n","    year: int = Field(description=\"Erscheinungsjahr\")\n","    rating: float = Field(description=\"Bewertung von 1-10\", ge=1, le=10)\n","    reason: str = Field(description=\"Kurze Begr√ºndung der Empfehlung\")\n","\n","# ‚ú® NEU in v1.1.0: Auto-Inference der besten Strategie\n","structured_llm = llm.with_structured_output(MovieRecommendation)\n","\n","# Query\n","result = structured_llm.invoke(\n","    \"Empfiehl mir einen guten Science-Fiction Film aus den 2010er Jahren\"\n",")\n","\n","print(\"üé¨ Filmempfehlung:\\n\")\n","print(f\"Titel: {result.title}\")\n","print(f\"Genre: {result.genre}\")\n","print(f\"Jahr: {result.year}\")\n","print(f\"Bewertung: {result.rating}/10\")\n","print(f\"Begr√ºndung: {result.reason}\")\n","\n","# Type-Check\n","print(f\"\\n‚úÖ Type-Check: {type(result)} = {MovieRecommendation}\")\n","assert isinstance(result, MovieRecommendation)"]},{"cell_type":"markdown","metadata":{"id":"jtwlO15MdxYS"},"source":["### üîç Vergleich: ALT vs. NEU\n","\n","**‚ùå ALT (v1.0):**\n","```python\n","# Manuelle Strategie-Auswahl\n","structured_llm = llm.with_structured_output(\n","    MovieRecommendation,\n","    provider_strategy=\"native\"  # Manuell festgelegt!\n",")\n","```\n","\n","**‚úÖ NEU (v1.1.0):**\n","```python\n","# Automatische Strategie-Auswahl via Model Profiles\n","structured_llm = llm.with_structured_output(MovieRecommendation)\n","# Nutzt automatisch \"native\" wenn llm.profile.supports_structured_output == True\n","```"]},{"cell_type":"markdown","metadata":{"id":"4sHI_jkAdxYS"},"source":["---\n","\n","## 3Ô∏è‚É£ SystemMessage Support in create_agent()\n","\n","### ‚ùì Was ist neu?\n","\n","`system_prompt` akzeptiert jetzt `SystemMessage` mit erweiterten Features:\n","\n","- ‚úÖ **Cache-Control** (Anthropic Claude) - spart Tokens bei wiederholten System-Prompts\n","- ‚úÖ **Strukturierte Content Blocks** - Bilder, Dokumente im System-Prompt\n","- ‚úÖ **Metadata** - Orchestration-Hints f√ºr komplexe Agents\n","\n","---"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NeILoKhsdxYS","executionInfo":{"status":"ok","timestamp":1764770007362,"user_tz":-60,"elapsed":1391,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"c3b6705e-5a34-40c6-f0fa-07811ac8cad3"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Agent mit SystemMessage erstellt!\n"]}],"source":["from langchain.agents import create_agent\n","from langchain_core.messages import SystemMessage\n","from langchain_core.tools import tool\n","\n","# Tool definieren\n","@tool\n","def calculate(expression: str) -> str:\n","    \"\"\"Berechnet mathematische Ausdr√ºcke.\n","\n","    Args:\n","        expression: Mathematischer Ausdruck (z.B. \"2 + 2\", \"sqrt(16)\")\n","    \"\"\"\n","    try:\n","        import math\n","        result = eval(expression, {\"__builtins__\": None}, {\"math\": math})\n","        return f\"Result: {result}\"\n","    except Exception as e:\n","        return f\"Error: {str(e)}\"\n","\n","# ‚ùå ALT: String-basierter System-Prompt\n","agent_old = create_agent(\n","    model=llm,\n","    tools=[calculate],\n","    system_prompt=\"You are a helpful math assistant\",\n","    debug=False\n",")\n","\n","# ‚úÖ NEU (v1.1.0): SystemMessage mit erweiterten Features\n","agent_new = create_agent(\n","    model=llm,\n","    tools=[calculate],\n","    system_prompt=SystemMessage(\n","        content=\"\"\"You are a helpful math assistant specialized in complex calculations.\n","\n","Guidelines:\n","- Always explain your reasoning\n","- Use the calculate tool for all mathematical operations\n","- Format results clearly\"\"\",\n","        # Cache-Control f√ºr Anthropic (w√ºrde Tokens sparen bei wiederholten Calls)\n","        # cache_control={\"type\": \"ephemeral\"}  # Nur f√ºr Anthropic relevant\n","    ),\n","    debug=False\n",")\n","\n","print(\"‚úÖ Agent mit SystemMessage erstellt!\")"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MOSpYl2xdxYT","executionInfo":{"status":"ok","timestamp":1764770014589,"user_tz":-60,"elapsed":4576,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"dd435f88-ce41-4ddb-c69d-f5d2bb131ed2"},"outputs":[{"output_type":"stream","name":"stdout","text":["üìä Agent Response:\n","Die Quadratwurzel von 144 ist 12.0.\n"]}],"source":["# Agent testen\n","response = agent_new.invoke({\n","    \"messages\": [{\"role\": \"user\", \"content\": \"Was ist die Quadratwurzel von 144?\"}]\n","})\n","\n","print(\"üìä Agent Response:\")\n","print(response[\"messages\"][-1].content)"]},{"cell_type":"markdown","metadata":{"id":"GS2B2Ci-dxYT"},"source":["---\n","\n","## 4Ô∏è‚É£ ModelRetryMiddleware - Automatische Retries\n","\n","### ‚ùì Was ist neu?\n","\n","**Neue Middleware** f√ºr automatische Retries bei API-Fehlern:\n","\n","- ‚úÖ Exponential Backoff (2s, 4s, 8s, ...)\n","- ‚úÖ Konfigurierbare Retry-Conditions (rate_limit, timeout, server_error)\n","- ‚úÖ Jitter (Randomized delay) zur Vermeidung von Thundering Herd\n","- ‚úÖ Production-Ready Resilience\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hqKs1d_UdxYT"},"outputs":[],"source":["from langchain.agents.middleware import ModelRetryMiddleware\n","\n","# Agent mit Retry-Middleware\n","agent_with_retry = create_agent(\n","    model=llm,\n","    tools=[calculate],\n","    system_prompt=\"You are a helpful assistant\",\n","    middleware=[\n","        ModelRetryMiddleware(\n","            max_retries=3,\n","            backoff_factor=2.0,  # Exponential Backoff: 2s, 4s, 8s\n","            retry_on=[\"rate_limit_error\", \"timeout\", \"server_error\"],\n","            jitter=True  # Randomized delay\n","        )\n","    ],\n","    debug=False\n",")\n","\n","print(\"‚úÖ Agent mit ModelRetryMiddleware erstellt!\")\n","print(\"\\nüîÑ Retry-Konfiguration:\")\n","print(\"- Max Retries: 3\")\n","print(\"- Backoff: Exponential (2s, 4s, 8s)\")\n","print(\"- Retry bei: Rate Limit, Timeout, Server Error\")\n","print(\"- Jitter: Aktiviert\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PEgxfn6-dxYU"},"outputs":[],"source":["# Beispiel-Query (w√ºrde bei Rate Limit automatisch retries durchf√ºhren)\n","response = agent_with_retry.invoke({\n","    \"messages\": [{\"role\": \"user\", \"content\": \"Berechne 123 * 456\"}]\n","})\n","\n","print(\"üìä Agent Response:\")\n","print(response[\"messages\"][-1].content)"]},{"cell_type":"markdown","metadata":{"id":"iYBSiIRYdxYU"},"source":["### üí° Use Cases f√ºr ModelRetryMiddleware\n","\n","1. **Rate Limiting** - OpenAI/Anthropic Rate Limits\n","2. **Transiente Netzwerkfehler** - Timeouts, DNS-Fehler\n","3. **Server-Timeouts** - API-Provider √ºberlastet\n","4. **Production Resilience** - Robuste Production-Apps"]},{"cell_type":"markdown","metadata":{"id":"DC-IkyfZdxYU"},"source":["---\n","\n","## 5Ô∏è‚É£ ContentModerationMiddleware - Safety-Layer\n","\n","### ‚ùì Was ist neu?\n","\n","**Neue Middleware** f√ºr Content-Moderation via OpenAI Moderation API:\n","\n","- ‚úÖ Filter f√ºr User-Input, Model-Output, Tool-Results\n","- ‚úÖ Kategorien: Hate, Violence, Sexual, Self-Harm, etc.\n","- ‚úÖ Block on Violation (Workflow stoppen)\n","- ‚úÖ DSGVO & Brand-Safety f√ºr Production-Apps\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_YnBiK9NdxYU"},"outputs":[],"source":["from langchain.agents.middleware import ContentModerationMiddleware\n","\n","# Agent mit Content-Moderation\n","agent_with_moderation = create_agent(\n","    model=llm,\n","    tools=[calculate],\n","    system_prompt=\"You are a helpful assistant\",\n","    middleware=[\n","        ContentModerationMiddleware(\n","            provider=\"openai\",  # Nutzt OpenAI Moderation API\n","            check_user_input=True,      # Filter User-Inputs\n","            check_model_output=True,    # Filter Model-Responses\n","            check_tool_output=True,     # Filter Tool-Results\n","            block_on_violation=True,    # Workflow stoppen bei Violation\n","            categories=[\"hate\", \"violence\", \"sexual\", \"self-harm\"]  # Kategorien\n","        )\n","    ],\n","    debug=False\n",")\n","\n","print(\"‚úÖ Agent mit ContentModerationMiddleware erstellt!\")\n","print(\"\\nüõ°Ô∏è Moderation-Konfiguration:\")\n","print(\"- Provider: OpenAI Moderation API\")\n","print(\"- Check User Input: ‚úÖ\")\n","print(\"- Check Model Output: ‚úÖ\")\n","print(\"- Check Tool Output: ‚úÖ\")\n","print(\"- Block on Violation: ‚úÖ\")\n","print(\"- Kategorien: Hate, Violence, Sexual, Self-Harm\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1_OlaDJodxYU"},"outputs":[],"source":["# Beispiel: Sichere Query\n","safe_response = agent_with_moderation.invoke({\n","    \"messages\": [{\"role\": \"user\", \"content\": \"Was ist 5 + 3?\"}]\n","})\n","\n","print(\"‚úÖ Sichere Query - Passed Moderation:\")\n","print(safe_response[\"messages\"][-1].content)"]},{"cell_type":"markdown","metadata":{"id":"vms2lw-ydxYV"},"source":["### üí° Use Cases f√ºr ContentModerationMiddleware\n","\n","1. **User-Generated Content** - Filter f√ºr Chat-Apps\n","2. **DSGVO-Compliance** - Datenschutz & Safety\n","3. **Brand-Safety** - Production-Apps mit Public-Facing Content\n","4. **Multi-Layer-Moderation** - Input + Output + Tool-Results"]},{"cell_type":"markdown","metadata":{"id":"xq7K9di0dxYV"},"source":["---\n","\n","## üéØ Zusammenfassung: Was ist neu in v1.1.0?\n","\n","| Feature | Vorteil | Use Case |\n","|---------|---------|----------|\n","| **Model Profiles** | Automatische Capability-Detection | Dynamische Feature-Detection statt hardcoded Provider-Checks |\n","| **Smart Structured Output** | Auto-Inference von `ProviderStrategy` | Weniger Konfiguration, intelligentere Defaults |\n","| **SystemMessage in Agents** | Cache-Control & Content Blocks | Kostenersparnis bei Anthropic Claude |\n","| **ModelRetryMiddleware** | Automatische Retries | Resilience gegen API-Fehler (Rate Limits, Timeouts) |\n","| **ContentModerationMiddleware** | Safety-Layer | DSGVO-Compliance, Brand-Safety f√ºr Production |\n","\n","### ‚úÖ Keine Breaking Changes!\n","\n","- v1.1.0 ist **100% r√ºckw√§rtskompatibel** mit v1.0\n","- Bestehender Code funktioniert weiterhin\n","- Neue Features sind **optional**, aber empfohlen\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"lv_TX8xqdxYV"},"source":["## üìö Weitere Ressourcen\n","\n","- **LangChain 1.1 Release Notes:** https://changelog.langchain.com/announcements/langchain-1-1\n","- **LangChain 1.0 Must-Haves:** [../LangChain_1.0_Must_Haves.md](../LangChain_1.0_Must_Haves.md)\n","- **CLAUDE.md:** [../CLAUDE.md](../CLAUDE.md)\n","- **models.dev:** https://models.dev (Model Profiles Quelle)\n","\n","---\n","\n","## üéì Aufgaben\n","\n","### Aufgabe 1: Model Profiles erkunden\n","\n","Teste verschiedene Modelle und vergleiche ihre Profiles:\n","- `gpt-4o-mini`\n","- `gpt-4`\n","- `claude-3-sonnet` (falls Anthropic API-Key vorhanden)\n","\n","Welche Unterschiede gibt es bei den unterst√ºtzten Features?\n","\n","### Aufgabe 2: Strukturierte Ausgabe testen\n","\n","Erstelle ein eigenes Pydantic-Schema (z.B. f√ºr Produkt-Reviews, Rezepte, oder Reiseziele) und teste `with_structured_output()` mit verschiedenen Queries.\n","\n","### Aufgabe 3: Multi-Middleware Agent\n","\n","Erstelle einen Agent, der **alle 3 Middleware** kombiniert:\n","- `ModelRetryMiddleware`\n","- `ContentModerationMiddleware`\n","- `SummarizationMiddleware` (f√ºr lange Sessions)\n","\n","Teste den Agent mit verschiedenen Queries und beobachte das Zusammenspiel der Middleware.\n","\n","---\n","\n","**Ende des Notebooks** üéâ"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font size=\"5\" color='grey'> <b>\n",
    "Advanced RAG mit LangGraph\n",
    "</b></font> </br></p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul\nfrom genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages\nsetup_api_keys(['OPENAI_API_KEY', 'HF_TOKEN'], create_globals=False)\nprint()\ncheck_environment()\nprint()\nget_ipinfo()\n\n# LangChain Version anzeigen\nimport langchain\nprint(f\"\\n‚úÖ LangChain Version: {langchain.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#@title üõ†Ô∏è Installationen { display-mode: \"form\" }\n",
    "install_packages([\n",
    "    ('markitdown[all]', 'markitdown'),\n",
    "    'langchain_chroma',\n",
    "    'langchain_huggingface',\n",
    "    'langgraph',\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 | Von Basic zu Advanced RAG\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='black' size='5'>\n",
    "Motivation\n",
    "</font></p>\n",
    "\n",
    "Im Modul **M08** haben Sie die Grundlagen von RAG (Retrieval-Augmented Generation) kennengelernt:\n",
    "\n",
    "```\n",
    "User Query ‚Üí Retrieve Documents ‚Üí Generate Answer\n",
    "```\n",
    "\n",
    "**Das funktioniert gut, aber es gibt Probleme:**\n",
    "\n",
    "‚ùå **Problem 1: Irrelevante Dokumente**\n",
    "- Der Retriever findet manchmal unpassende Dokumente\n",
    "- Diese verwirren das LLM und f√ºhren zu schlechten Antworten\n",
    "\n",
    "‚ùå **Problem 2: Halluzinationen**\n",
    "- Das LLM erfindet Fakten, die nicht in den Dokumenten stehen\n",
    "- Keine √úberpr√ºfung der generierten Antwort\n",
    "\n",
    "‚ùå **Problem 3: Unflexibel**\n",
    "- Der Workflow ist statisch: Immer retrieve ‚Üí generate\n",
    "- Keine Anpassung an die Qualit√§t der Zwischenergebnisse\n",
    "\n",
    "**L√∂sung: Advanced RAG mit LangGraph!**\n",
    "\n",
    "‚úÖ **Self-RAG:** Bewertet gefundene Dokumente vor der Generierung\n",
    "‚úÖ **Corrective RAG:** Verbessert Queries bei schlechten Ergebnissen\n",
    "‚úÖ **Adaptive RAG:** Passt den Workflow dynamisch an"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='black' size='5'>\n",
    "√úbersicht der Patterns\n",
    "</font></p>\n",
    "\n",
    "| Pattern | Was macht es? | Wann verwenden? |\n",
    "|---------|---------------|------------------|\n",
    "| **Self-RAG** | Bewertet Relevanz der Dokumente | Wenn Retrieval-Qualit√§t wichtig ist |\n",
    "| **Corrective RAG** | Verbessert Query bei schlechten Retrievals | Wenn Nutzer-Queries ungenau sind |\n",
    "| **Adaptive RAG** | W√§hlt beste Strategie basierend auf Query-Typ | F√ºr vielseitige Anwendungen |\n",
    "\n",
    "**In diesem Modul bauen wir:**\n",
    "1. Self-RAG mit Document Grading\n",
    "2. Corrective RAG mit Query Rewriting\n",
    "3. Adaptive RAG mit Routing Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 | Setup & Vorbereitung\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Imports\n",
    "from typing import TypedDict, List\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Modell und Embeddings\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "print(\"‚úÖ LLM und Embeddings initialisiert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='black' size='5'>\n",
    "Beispiel-Dokumente vorbereiten\n",
    "</font></p>\n",
    "\n",
    "Wir erstellen eine kleine Wissensbasis zu KI-Themen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Beispiel-Dokumente\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"LangGraph ist ein Framework zur Erstellung von zustandsbasierten, mehrstufigen KI-Anwendungen. Es verwendet Graphen mit Nodes und Edges.\",\n",
    "        metadata={\"source\": \"langgraph_docs\", \"topic\": \"framework\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Self-RAG bewertet die Relevanz von abgerufenen Dokumenten, bevor sie zur Generierung verwendet werden. Dies verbessert die Antwortqualit√§t.\",\n",
    "        metadata={\"source\": \"rag_paper\", \"topic\": \"rag\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Corrective RAG erkennt schlechte Retrievals und formuliert die Query neu, um bessere Dokumente zu finden.\",\n",
    "        metadata={\"source\": \"crag_paper\", \"topic\": \"rag\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"LangChain ist eine Bibliothek f√ºr die Entwicklung von Anwendungen mit Large Language Models. Es bietet Tools wie Chains, Agents und RAG.\",\n",
    "        metadata={\"source\": \"langchain_docs\", \"topic\": \"framework\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Embeddings sind numerische Repr√§sentationen von Text, die semantische √Ñhnlichkeit erfassen. Sie werden f√ºr Vektorsuche verwendet.\",\n",
    "        metadata={\"source\": \"embeddings_guide\", \"topic\": \"embeddings\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Halluzinationen treten auf, wenn LLMs Fakten erfinden, die nicht in den Eingabedaten vorhanden sind. Self-RAG hilft, dies zu reduzieren.\",\n",
    "        metadata={\"source\": \"llm_hallucinations\", \"topic\": \"llm\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Vektordatenbank erstellen\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"advanced_rag_demo\"\n",
    ")\n",
    "\n",
    "# Retriever erstellen\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "print(f\"‚úÖ Vektordatenbank mit {len(documents)} Dokumenten erstellt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 | Self-RAG: Document Grading\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='black' size='5'>\n",
    "Konzept\n",
    "</font></p>\n",
    "\n",
    "**Self-RAG f√ºgt einen Bewertungsschritt hinzu:**\n",
    "\n",
    "```\n",
    "Query ‚Üí Retrieve ‚Üí [GRADE DOCS] ‚Üí Generate (nur mit relevanten Docs)\n",
    "```\n",
    "\n",
    "**Warum wichtig?**\n",
    "- Irrelevante Dokumente werden herausgefiltert\n",
    "- Das LLM bekommt nur hochwertige Informationen\n",
    "- Reduziert Halluzinationen und verbessert Genauigkeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='black' size='5'>\n",
    "State Definition\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SelfRAGState(TypedDict):\n",
    "    \"\"\"State f√ºr Self-RAG Workflow\"\"\"\n",
    "    query: str                          # Nutzer-Anfrage\n",
    "    documents: List[Document]           # Abgerufene Dokumente\n",
    "    relevant_documents: List[Document]  # Gefilterte relevante Dokumente\n",
    "    answer: str                         # Generierte Antwort\n",
    "    relevance_scores: List[str]         # \"yes\"/\"no\" f√ºr jedes Dokument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='black' size='5'>\n",
    "Document Grader mit structured output\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Pydantic-Modell f√ºr Bewertung\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Bewertet ob ein Dokument relevant f√ºr die Query ist\"\"\"\n",
    "    score: str = Field(\n",
    "        description=\"'yes' wenn relevant, 'no' wenn nicht relevant\"\n",
    "    )\n",
    "\n",
    "# Strukturiertes LLM f√ºr Grading\n",
    "grader_llm = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Grading Prompt\n",
    "grade_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein Experte f√ºr die Bewertung der Relevanz von Dokumenten.\"),\n",
    "    (\"human\", \"\"\"Bewertet, ob das folgende Dokument relevant f√ºr die Benutzeranfrage ist.\n",
    "\n",
    "Dokument:\n",
    "{document}\n",
    "\n",
    "Benutzeranfrage:\n",
    "{query}\n",
    "\n",
    "Gib 'yes' zur√ºck wenn das Dokument Informationen enth√§lt, die zur Beantwortung der Anfrage relevant sind.\n",
    "Gib 'no' zur√ºck wenn das Dokument nicht relevant ist.\"\"\")\n",
    "])\n",
    "\n",
    "# Grading Chain\n",
    "grader_chain = grade_prompt | grader_llm\n",
    "\n",
    "print(\"‚úÖ Document Grader erstellt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='black' size='5'>\n",
    "Graph Nodes definieren\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def retrieve_node(state: SelfRAGState) -> SelfRAGState:\n",
    "    \"\"\"Ruft Dokumente ab\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    documents = retriever.invoke(query)\n",
    "    print(f\"üîç Retrieved {len(documents)} Dokumente\")\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "def grade_documents_node(state: SelfRAGState) -> SelfRAGState:\n",
    "    \"\"\"Bewertet jedes Dokument auf Relevanz\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    relevant_docs = []\n",
    "    scores = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Dokument bewerten\n",
    "        grade = grader_chain.invoke({\n",
    "            \"document\": doc.page_content,\n",
    "            \"query\": query\n",
    "        })\n",
    "        \n",
    "        scores.append(grade.score)\n",
    "        \n",
    "        if grade.score == \"yes\":\n",
    "            relevant_docs.append(doc)\n",
    "            print(f\"‚úÖ Relevant: {doc.page_content[:50]}...\")\n",
    "        else:\n",
    "            print(f\"‚ùå Nicht relevant: {doc.page_content[:50]}...\")\n",
    "    \n",
    "    print(f\"\\nüìä {len(relevant_docs)}/{len(documents)} Dokumente relevant\")\n",
    "    \n",
    "    return {\n",
    "        \"relevant_documents\": relevant_docs,\n",
    "        \"relevance_scores\": scores\n",
    "    }\n",
    "\n",
    "def generate_node(state: SelfRAGState) -> SelfRAGState:\n",
    "    \"\"\"Generiert Antwort basierend auf relevanten Dokumenten\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    docs = state[\"relevant_documents\"]\n",
    "    \n",
    "    # Kontext zusammenstellen\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # RAG Prompt\n",
    "    rag_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Du bist ein hilfreicher Assistent. Beantworte die Frage basierend auf dem Kontext.\"),\n",
    "        (\"human\", \"\"\"Kontext:\n",
    "{context}\n",
    "\n",
    "Frage: {query}\n",
    "\n",
    "Antwort:\"\"\")\n",
    "    ])\n",
    "    \n",
    "    # Generierung\n",
    "    chain = rag_prompt | llm | StrOutputParser()\n",
    "    answer = chain.invoke({\"context\": context, \"query\": query})\n",
    "    \n",
    "    print(f\"\\nü§ñ Antwort generiert ({len(answer)} Zeichen)\")\n",
    "    \n",
    "    return {\"answer\": answer}\n",
    "\n",
    "print(\"‚úÖ Nodes definiert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='black' size='5'>\n",
    "Graph aufbauen\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Self-RAG Workflow erstellen\n",
    "self_rag_workflow = StateGraph(SelfRAGState)\n",
    "\n",
    "# Nodes hinzuf√ºgen\n",
    "self_rag_workflow.add_node(\"retrieve\", retrieve_node)\n",
    "self_rag_workflow.add_node(\"grade_documents\", grade_documents_node)\n",
    "self_rag_workflow.add_node(\"generate\", generate_node)\n",
    "\n",
    "# Edges definieren\n",
    "self_rag_workflow.add_edge(START, \"retrieve\")\n",
    "self_rag_workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "self_rag_workflow.add_edge(\"grade_documents\", \"generate\")\n",
    "self_rag_workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Graph kompilieren\n",
    "self_rag_app = self_rag_workflow.compile()\n",
    "\n",
    "print(\"‚úÖ Self-RAG Graph kompiliert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='black' size='5'>\n",
    "Graph Visualisierung\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(self_rag_app.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Graph-Visualisierung nicht verf√ºgbar: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='black' size='5'>\n",
    "Self-RAG testen\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test Query\n",
    "test_query = \"Was ist Self-RAG und warum ist es wichtig?\"\n",
    "\n",
    "# Workflow ausf√ºhren\n",
    "result = self_rag_app.invoke({\n",
    "    \"query\": test_query\n",
    "})\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "mprint(\"## üéØ Self-RAG Ergebnis\")\n",
    "mprint(\"---\")\n",
    "mprint(f\"**Query:** {test_query}\")\n",
    "mprint(f\"**Antwort:** {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 | Corrective RAG: Query Rewriting\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='black' size='5'>\n",
    "Konzept\n",
    "</font></p>\n",
    "\n",
    "**Corrective RAG (CRAG) erkennt schlechte Retrievals und reagiert:**\n",
    "\n",
    "```\n",
    "Query ‚Üí Retrieve ‚Üí Grade\n",
    "                      ‚Üì\n",
    "            Gut? ‚Üí Generate\n",
    "                      ‚Üì\n",
    "         Schlecht? ‚Üí Rewrite Query ‚Üí Retrieve wieder\n",
    "```\n",
    "\n",
    "**Warum wichtig?**\n",
    "- Nutzer-Queries sind oft ungenau oder missverst√§ndlich\n",
    "- Durch Umformulierung werden bessere Dokumente gefunden\n",
    "- Erh√∂ht Erfolgsrate drastisch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='black' size='5'>\n",
    "State Definition\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class CRAGState(TypedDict):\n",
    "    \"\"\"State f√ºr Corrective RAG\"\"\"\n",
    "    original_query: str           # Urspr√ºngliche Query\n",
    "    query: str                    # Aktuelle Query (kann umgeschrieben sein)\n",
    "    documents: List[Document]\n",
    "    relevant_documents: List[Document]\n",
    "    answer: str\n",
    "    retry_count: int              # Wie oft wurde umgeschrieben?\n",
    "    has_relevant_docs: bool       # Flag f√ºr Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='black' size='5'>\n",
    "Query Rewriter\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Query Rewriting Prompt\n",
    "rewrite_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein Experte f√ºr das Umformulieren von Suchanfragen.\"),\n",
    "    (\"human\", \"\"\"Die folgende Suchanfrage hat keine guten Ergebnisse geliefert.\n",
    "Formuliere sie um, um bessere Dokumente zu finden.\n",
    "Mache sie spezifischer und nutze Synonyme.\n",
    "\n",
    "Urspr√ºngliche Query: {query}\n",
    "\n",
    "Verbesserte Query:\"\"\")\n",
    "])\n",
    "\n",
    "# Rewriting Chain\n",
    "rewriter_chain = rewrite_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"‚úÖ Query Rewriter erstellt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='black' size='5'>\n",
    "CRAG Nodes\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def crag_retrieve_node(state: CRAGState) -> CRAGState:\n",
    "    \"\"\"Retrieval f√ºr CRAG\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    documents = retriever.invoke(query)\n",
    "    print(f\"üîç Retrieved {len(documents)} Dokumente f√ºr Query: '{query}'\")\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "def crag_grade_node(state: CRAGState) -> CRAGState:\n",
    "    \"\"\"Grading f√ºr CRAG mit Entscheidungslogik\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    relevant_docs = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        grade = grader_chain.invoke({\n",
    "            \"document\": doc.page_content,\n",
    "            \"query\": query\n",
    "        })\n",
    "        \n",
    "        if grade.score == \"yes\":\n",
    "            relevant_docs.append(doc)\n",
    "    \n",
    "    # Entscheidung: Gibt es genug relevante Dokumente?\n",
    "    has_relevant = len(relevant_docs) > 0\n",
    "    \n",
    "    if has_relevant:\n",
    "        print(f\"‚úÖ {len(relevant_docs)} relevante Dokumente gefunden\")\n",
    "    else:\n",
    "        print(f\"‚ùå Keine relevanten Dokumente ‚Üí Query wird umgeschrieben\")\n",
    "    \n",
    "    return {\n",
    "        \"relevant_documents\": relevant_docs,\n",
    "        \"has_relevant_docs\": has_relevant\n",
    "    }\n",
    "\n",
    "def rewrite_query_node(state: CRAGState) -> CRAGState:\n",
    "    \"\"\"Schreibt Query um\"\"\"\n",
    "    original = state[\"original_query\"]\n",
    "    current = state[\"query\"]\n",
    "    retry = state.get(\"retry_count\", 0)\n",
    "    \n",
    "    # Query umschreiben\n",
    "    new_query = rewriter_chain.invoke({\"query\": current})\n",
    "    \n",
    "    print(f\"üîÑ Query umgeschrieben (Versuch {retry + 1})\")\n",
    "    print(f\"   Alt: '{current}'\")\n",
    "    print(f\"   Neu: '{new_query}'\")\n",
    "    \n",
    "    return {\n",
    "        \"query\": new_query,\n",
    "        \"retry_count\": retry + 1\n",
    "    }\n",
    "\n",
    "def crag_generate_node(state: CRAGState) -> CRAGState:\n",
    "    \"\"\"Generierung f√ºr CRAG\"\"\"\n",
    "    query = state[\"original_query\"]  # Verwende urspr√ºngliche Query f√ºr Antwort\n",
    "    docs = state[\"relevant_documents\"]\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    rag_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Du bist ein hilfreicher Assistent.\"),\n",
    "        (\"human\", \"Kontext:\\n{context}\\n\\nFrage: {query}\\n\\nAntwort:\")\n",
    "    ])\n",
    "    \n",
    "    chain = rag_prompt | llm | StrOutputParser()\n",
    "    answer = chain.invoke({\"context\": context, \"query\": query})\n",
    "    \n",
    "    return {\"answer\": answer}\n",
    "\n",
    "print(\"‚úÖ CRAG Nodes definiert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='black' size='5'>\n",
    "CRAG Graph mit Conditional Routing\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Routing Funktion\n",
    "def route_after_grading(state: CRAGState) -> str:\n",
    "    \"\"\"Entscheidet: Generate oder Rewrite?\"\"\"\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "    has_relevant = state[\"has_relevant_docs\"]\n",
    "    \n",
    "    # Maximal 2 Rewrite-Versuche\n",
    "    if has_relevant or retry_count >= 2:\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        return \"rewrite_query\"\n",
    "\n",
    "# CRAG Workflow\n",
    "crag_workflow = StateGraph(CRAGState)\n",
    "\n",
    "# Nodes\n",
    "crag_workflow.add_node(\"retrieve\", crag_retrieve_node)\n",
    "crag_workflow.add_node(\"grade\", crag_grade_node)\n",
    "crag_workflow.add_node(\"rewrite_query\", rewrite_query_node)\n",
    "crag_workflow.add_node(\"generate\", crag_generate_node)\n",
    "\n",
    "# Edges\n",
    "crag_workflow.add_edge(START, \"retrieve\")\n",
    "crag_workflow.add_edge(\"retrieve\", \"grade\")\n",
    "\n",
    "# Conditional Edge: Nach Grading\n",
    "crag_workflow.add_conditional_edges(\n",
    "    \"grade\",\n",
    "    route_after_grading,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"rewrite_query\": \"rewrite_query\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Nach Rewrite wieder zu Retrieve\n",
    "crag_workflow.add_edge(\"rewrite_query\", \"retrieve\")\n",
    "crag_workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Kompilieren\n",
    "crag_app = crag_workflow.compile()\n",
    "\n",
    "print(\"‚úÖ CRAG Graph mit Conditional Routing kompiliert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='black' size='5'>\n",
    "CRAG Graph Visualisierung\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "try:\n",
    "    display(Image(crag_app.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Graph-Visualisierung nicht verf√ºgbar: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='black' size='5'>\n",
    "CRAG testen\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test mit ung√ºnstiger Query\n",
    "test_query = \"Wie funktioniert das Ding mit den Graphen?\"\n",
    "\n",
    "result = crag_app.invoke({\n",
    "    \"original_query\": test_query,\n",
    "    \"query\": test_query,\n",
    "    \"retry_count\": 0\n",
    "})\n",
    "\n",
    "mprint(\"## üéØ CRAG Ergebnis\")\n",
    "mprint(\"---\")\n",
    "mprint(f\"**Urspr√ºngliche Query:** {result['original_query']}\")\n",
    "mprint(f\"**Finale Query:** {result['query']}\")\n",
    "mprint(f\"**Rewrite-Versuche:** {result.get('retry_count', 0)}\")\n",
    "mprint(f\"**Antwort:** {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 | Vergleich: Basic vs. Advanced RAG\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Aspekt | Basic RAG (M08) | Self-RAG | Corrective RAG |\n",
    "|--------|-----------------|----------|----------------|\n",
    "| **Workflow** | Linear | Mit Grading | Mit Retry-Loop |\n",
    "| **Dokument-Qualit√§t** | Nicht gepr√ºft | ‚úÖ Gepr√ºft | ‚úÖ Gepr√ºft |\n",
    "| **Query-Verbesserung** | ‚ùå Nein | ‚ùå Nein | ‚úÖ Ja |\n",
    "| **Anpassungsf√§hig** | ‚ùå Statisch | ‚ö†Ô∏è Teilweise | ‚úÖ Dynamisch |\n",
    "| **Komplexit√§t** | Niedrig | Mittel | Hoch |\n",
    "| **Use Case** | Einfache Q&A | Qualit√§tskritisch | Unklare Queries |\n",
    "\n",
    "**Empfehlung:**\n",
    "- **Basic RAG:** F√ºr Prototyping und klare Use Cases\n",
    "- **Self-RAG:** Wenn Retrieval-Qualit√§t wichtig ist\n",
    "- **Corrective RAG:** Wenn Nutzer-Queries ungenau sind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 | Best Practices & Tipps\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='black' size='5'>\n",
    "Performance-Optimierung\n",
    "</font></p>\n",
    "\n",
    "**Grading ist teuer (LLM-Calls):**\n",
    "- ‚úÖ Cache Grading-Ergebnisse f√ºr identische Dokumente\n",
    "- ‚úÖ Paralleles Grading mit `asyncio` (f√ºr gro√üe Dokumentenmengen)\n",
    "- ‚úÖ Alternative: Lightweight Relevance Scorer (z.B. Cosine-Similarity-Threshold)\n",
    "\n",
    "**Rewriting Limits:**\n",
    "- ‚úÖ Maximal 2-3 Rewrite-Versuche (sonst zu langsam)\n",
    "- ‚úÖ Fallback: Web-Search wenn kein Doc gefunden wird\n",
    "\n",
    "**Monitoring:**\n",
    "- ‚úÖ Logge Rewrite-Rate (zu hoch ‚Üí Retrieval-Problem)\n",
    "- ‚úÖ Tracke Relevance-Scores (zu niedrig ‚Üí Daten-Problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='black' size='5'>\n",
    "Wann welches Pattern?\n",
    "</font></p>\n",
    "\n",
    "| Szenario | Empfohlenes Pattern |\n",
    "|----------|---------------------|\n",
    "| **Hohe Datenqualit√§t, klare Queries** | Basic RAG |\n",
    "| **Viele irrelevante Retrieval-Ergebnisse** | Self-RAG |\n",
    "| **Nutzer mit ungenauen Fragen** | Corrective RAG |\n",
    "| **Kritische Anwendung (Legal, Medical)** | Self-RAG + Hallucination Check |\n",
    "| **Niedrige Latenz wichtig** | Basic RAG (schneller) |\n",
    "| **Hohe Accuracy wichtig** | Corrective RAG (besser) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A | Aufgaben\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='black' size='5'>\n",
    "Aufgabe 1: Hallucination Detection hinzuf√ºgen\n",
    "</font></p>\n",
    "\n",
    "Erweitern Sie Self-RAG um einen **Hallucination Check Node**:\n",
    "\n",
    "1. Nach der Generierung: Pr√ºfe ob die Antwort durch die Dokumente gest√ºtzt wird\n",
    "2. Wenn Halluzinationen erkannt: Generiere Antwort neu mit st√§rkerem Prompt\n",
    "3. Verwende `with_structured_output()` mit Schema: `{\"is_grounded\": bool, \"explanation\": str}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='black' size='5'>\n",
    "Aufgabe 2: Hybrid Retrieval\n",
    "</font></p>\n",
    "\n",
    "Kombiniere Vektor-Suche mit Keyword-Suche:\n",
    "\n",
    "1. Erstelle zwei Retriever: Einer mit Embeddings, einer mit BM25\n",
    "2. Node der beide Retriever parallel aufruft\n",
    "3. Fusion-Node der die Ergebnisse kombiniert (z.B. nach Score sortieren)\n",
    "\n",
    "**Tipp:** Nutze `langchain_community.retrievers.BM25Retriever`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color='black' size='5'>\n",
    "Aufgabe 3: Adaptive RAG\n",
    "</font></p>\n",
    "\n",
    "Implementiere **Adaptive RAG** das basierend auf Query-Typ entscheidet:\n",
    "\n",
    "1. **Classifier Node:** Kategorisiere Query (\"simple\", \"complex\", \"ambiguous\")\n",
    "2. **Routing:**\n",
    "   - `simple` ‚Üí Basic RAG\n",
    "   - `complex` ‚Üí Self-RAG\n",
    "   - `ambiguous` ‚Üí Corrective RAG\n",
    "3. Teste mit verschiedenen Query-Typen\n",
    "\n",
    "**Bonus:** Verwende `with_structured_output()` f√ºr Classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
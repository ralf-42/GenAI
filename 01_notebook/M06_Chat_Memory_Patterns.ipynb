{"cells":[{"cell_type":"markdown","id":"header-1","metadata":{"id":"header-1"},"source":["<p><font size=\"6\" color='grey'> <b>\n","\n","Generative KI. Verstehen. Anwenden. Gestalten.\n","</b></font> </br></p>"]},{"cell_type":"markdown","id":"header-2","metadata":{"id":"header-2"},"source":["<p><font size=\"5\" color='grey'> <b> Chat Memory Patterns</b></font> </br></p>\n","\n","---"]},{"cell_type":"code","execution_count":null,"id":"setup-1","metadata":{"id":"setup-1"},"outputs":[],"source":["#@title ðŸ”§ Umgebung einrichten{ display-mode: \"form\" }\n","!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul\n","from genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages\n","setup_api_keys(['OPENAI_API_KEY'], create_globals=False)\n","print()\n","check_environment()\n","print()\n","get_ipinfo()"]},{"cell_type":"markdown","id":"intro-header","metadata":{"id":"intro-header"},"source":["# 1 | Intro\n","---"]},{"cell_type":"markdown","id":"intro-text","metadata":{"id":"intro-text"},"source":["<p><font color='black' size=\"5\">\n","Zustandslosigkeit von LLMs\n","</font></p>\n","\n","Large Language Models (LLMs) wie GPT sind von Natur aus **zustandslos** - sie verfugen uber kein eingebautes Gedachtnis. Jede Anfrage wird isoliert verarbeitet, ohne Bezug zu vorherigen Interaktionen. Deshalb muss der Chatverlauf (Historie) bei jeder Anfrage neu ubergeben werden.\n","\n","```\n","Ohne Memory:\n","User: \"Mein Name ist Max\"\n","AI: \"Hallo Max!\"\n","User: \"Wie heisse ich?\"\n","AI: \"Das habe ich nicht gespeichert.\"\n","```\n","\n","**Dieses Notebook zeigt Memory-Patterns mit reinem Python (ohne LangGraph):**\n","\n","| Pattern | Beschreibung | Anwendungsfall |\n","|---------|--------------|----------------|\n","| **Python-Liste** | Einfachste Losung | Prototyping, kurze Sessions |\n","| **Trimming** | Nur letzte N Nachrichten | Token-Limit einhalten |\n","| **Summary** | Alte Nachrichten zusammenfassen | Lange Sessions, Kontext erhalten |\n","| **Datenbank** | Persistente Speicherung | Production, Multi-User |"]},{"cell_type":"markdown","id":"shortterm-header","metadata":{"id":"shortterm-header"},"source":["# 2 | Short-term Memory (Python-Liste)\n","---"]},{"cell_type":"markdown","id":"shortterm-intro","metadata":{"id":"shortterm-intro"},"source":["Die einfachste Form von Memory: Eine **Python-Liste**, die alle Nachrichten speichert und bei jedem API-Call mitgesendet wird."]},{"cell_type":"code","execution_count":null,"id":"shortterm-imports","metadata":{"id":"shortterm-imports"},"outputs":[],"source":["# Importe\n","from langchain.chat_models import init_chat_model\n","from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# System-Prompt\n","system_prompt = \"Du bist ein hilfreicher und humorvoller KI-Assistent.\"\n","\n","# Prompt-Template mit Historie (MessagesPlaceholder nimmt die Historie entgegen)\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"human\", \"{user_input}\")\n","])\n","\n","# LLM (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0)\n","\n","# Parser\n","parser = StrOutputParser()\n","\n","# Chain\n","chain = prompt | llm | parser"]},{"cell_type":"code","execution_count":null,"id":"shortterm-function","metadata":{"id":"shortterm-function"},"outputs":[],"source":["# Chat-Funktion mit manueller Historien-Verwaltung\n","def chat(chat_history: list, user_input: str) -> list:\n","    \"\"\"Fuhrt eine Chat-Interaktion mit manueller Historien-Verwaltung durch.\"\"\"\n","\n","    # Chain aufrufen (Historie wird im Prompt mitgeschickt)\n","    response = chain.invoke({\n","        'system_prompt': system_prompt,\n","        'chat_history': chat_history,\n","        'user_input': user_input\n","    })\n","\n","    # Ausgabe\n","    mprint(f\"### Mensch:\\n{user_input}\")\n","    mprint(f\"### KI:\\n{response}\\n\")\n","\n","    # Memory (Liste) MANUELL aktualisieren\n","    chat_history.append(HumanMessage(content=user_input))\n","    chat_history.append(AIMessage(content=response))\n","\n","    return chat_history"]},{"cell_type":"code","execution_count":null,"id":"shortterm-demo","metadata":{"id":"shortterm-demo"},"outputs":[],"source":["# Historie initialisieren\n","chat_history = [SystemMessage(content=system_prompt)]\n","\n","\n","# Konversation\n","chat_history = chat(chat_history, \"Mein Name ist Max\")\n","chat_history = chat(chat_history, \"Ich mag Python-Programmierung\")\n","chat_history = chat(chat_history, \"Weisst du noch, wie ich heisse und was ich mag?\")\n","\n","mprint(\"### Gespeicherte Nachrichten (Liste):\\n---\")\n","for msg in chat_history:\n","    mprint(f\"  **{msg.type}**:   {msg.content}\")"]},{"cell_type":"markdown","id":"shortterm-problem","metadata":{"id":"shortterm-problem"},"source":["<p><font color='darkblue' size=\"4\">\n","Problem:\n","</font></p>\n","\n","- Keine automatische Session-Verwaltung (Multi-User)\n","- Manuelles Memory-Management fehleranfallig\n","- **Bei langen Konversationen: Token-Limit wird uberschritten!**"]},{"cell_type":"markdown","id":"trimming-header","metadata":{"id":"trimming-header"},"source":["# 3 | Trimming (Sliding Window)\n","---"]},{"cell_type":"markdown","id":"trimming-intro","metadata":{"id":"trimming-intro"},"source":["**Strategie:** Behalte nur die letzten *n* Nachrichten. Altere Nachrichten werden entfernt.\n","\n","```\n","Vor Trimming (max=4):\n","[msg1, msg2, msg3, msg4, msg5, msg6] -> 6 Nachrichten\n","\n","Nach Trimming:\n","[msg3, msg4, msg5, msg6] -> nur die letzten 4\n","```\n","\n","**Vorteil:** Einfach, Token-Limit garantiert  \n","**Nachteil:** Fruhere Informationen gehen verloren"]},{"cell_type":"code","execution_count":null,"id":"trimming-config","metadata":{"id":"trimming-config"},"outputs":[],"source":["# Konfiguration\n","MAX_MESSAGES = 6  # Maximale Anzahl Nachrichten (Human + AI)\n","\n","def trim_history(chat_history: list, max_messages: int = MAX_MESSAGES) -> list:\n","    \"\"\"Behalt nur die letzten n Nachrichten.\"\"\"\n","    if len(chat_history) > max_messages:\n","        trimmed = chat_history[-max_messages:]\n","        mprint(f\"**Trimming:** {len(chat_history)} -> {len(trimmed)} Nachrichten\")\n","        return trimmed\n","    return chat_history"]},{"cell_type":"code","execution_count":null,"id":"trimming-function","metadata":{"id":"trimming-function"},"outputs":[],"source":["def chat_with_trimming(chat_history: list, user_input: str, max_messages: int = MAX_MESSAGES) -> list:\n","    \"\"\"Chat mit automatischem Trimming der Historie.\"\"\"\n","\n","    # Trimmen VOR dem API-Call\n","    trimmed_history = trim_history(chat_history, max_messages)\n","\n","    # Chain aufrufen mit getrimmter Historie\n","    response = chain.invoke({\n","        'system_prompt': system_prompt,\n","        'chat_history': trimmed_history,\n","        'user_input': user_input\n","    })\n","\n","    # Ausgabe\n","    mprint(f\"### Mensch:\\n{user_input}\")\n","    mprint(f\"### KI:\\n{response}\\n\")\n","\n","    # Zur ORIGINALEN Historie hinzufugen (nicht zur getrimmten!)\n","    chat_history.append(HumanMessage(content=user_input))\n","    chat_history.append(AIMessage(content=response))\n","\n","    return chat_history"]},{"cell_type":"code","execution_count":null,"id":"trimming-demo","metadata":{"id":"trimming-demo"},"outputs":[],"source":["# Demo: Trimming in Aktion\n","mprint(\"## Trimming Demo (max 6 Nachrichten)\")\n","mprint(\"---\")\n","\n","history_trimmed = [SystemMessage(content=system_prompt)]\n","\n","\n","# Erste Nachrichten\n","history_trimmed = chat_with_trimming(history_trimmed, \"Mein Name ist Max\")\n","history_trimmed = chat_with_trimming(history_trimmed, \"Ich wohne in Koln\")\n","history_trimmed = chat_with_trimming(history_trimmed, \"Ich mag Python\")\n","\n","# Jetzt uberschreiten wir das Limit (6 Nachrichten = 3 Frage-Antwort-Paare)\n","history_trimmed = chat_with_trimming(history_trimmed, \"Ich habe eine Katze namens Neo\")\n","history_trimmed = chat_with_trimming(history_trimmed, \"Test-Nachricht 5\")\n","\n","# Diese Nachricht lost Trimming aus\n","history_trimmed = chat_with_trimming(history_trimmed, \"Wie heisse ich?\")  # Fruhe Info konnte verloren sein!\n","\n","mprint(f\"### Gespeicherte Nachrichten: {len(history_trimmed)}\")"]},{"cell_type":"markdown","id":"summary-header","metadata":{"id":"summary-header"},"source":["# 4 | Summary (Zusammenfassung)\n","---"]},{"cell_type":"markdown","id":"summary-intro","metadata":{"id":"summary-intro"},"source":["**Strategie:** Statt alte Nachrichten zu loschen, werden sie **zusammengefasst**. Die Zusammenfassung ersetzt die alten Nachrichten.\n","\n","```\n","Vor Summary:\n","[msg1, msg2, msg3, msg4, msg5, msg6, msg7, msg8] -> 8 Nachrichten\n","\n","Nach Summary:\n","[\"Zusammenfassung: User heisst Max, mag Python...\", msg7, msg8]\n","```\n","\n","**Vorteil:** Wichtige Informationen bleiben erhalten  \n","**Nachteil:** Zusatzlicher LLM-Call fur Zusammenfassung"]},{"cell_type":"code","execution_count":null,"id":"summary-config","metadata":{"id":"summary-config"},"outputs":[],"source":["# Konfiguration\n","MAX_BEFORE_SUMMARY = 8   # Ab dieser Anzahl wird zusammengefasst\n","MESSAGES_TO_SUMMARIZE = 6  # So viele alte Nachrichten zusammenfassen\n","RECENT_TO_KEEP = 2        # So viele neueste Nachrichten behalten\n","\n","# Zusammenfassungs-Prompt\n","summary_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Fasse die folgende Konversation in 2-3 Satzen zusammen. Behalte wichtige Fakten wie Namen, Orte und Praferenzen.\"),\n","    (\"human\", \"{conversation}\")\n","])\n","\n","summary_chain = summary_prompt | llm | parser"]},{"cell_type":"code","execution_count":null,"id":"summary-function","metadata":{"id":"summary-function"},"outputs":[],"source":["def summarize_messages(messages: list) -> str:\n","    \"\"\"Erstellt eine Zusammenfassung der Nachrichten.\"\"\"\n","    conversation_text = \"\\n\".join([\n","        f\"{msg.type}: {msg.content}\" for msg in messages\n","    ])\n","\n","    summary = summary_chain.invoke({\"conversation\": conversation_text})\n","    return summary\n","\n","\n","def chat_with_summary(chat_history: list, user_input: str, summary_context: str = \"\") -> tuple:\n","    \"\"\"Chat mit automatischer Zusammenfassung bei langer Historie.\"\"\"\n","\n","    # Prufen ob Zusammenfassung notig ist\n","    if len(chat_history) >= MAX_BEFORE_SUMMARY:\n","        mprint(f\"**Summary:** Historie zu lang ({len(chat_history)} Nachrichten). Fasse zusammen...\")\n","\n","        # Alte Nachrichten zusammenfassen\n","        to_summarize = chat_history[:MESSAGES_TO_SUMMARIZE]\n","        new_summary = summarize_messages(to_summarize)\n","\n","        # Bisherige Zusammenfassung + neue kombinieren\n","        if summary_context:\n","            summary_context = f\"{summary_context}\\n\\nNeuere Zusammenfassung: {new_summary}\"\n","        else:\n","            summary_context = new_summary\n","\n","        mprint(f\"**Neue Zusammenfassung:** {new_summary[:100]}...\")\n","\n","        # Historie kurzen (nur die neuesten behalten)\n","        chat_history = chat_history[-RECENT_TO_KEEP:]\n","\n","    # System-Prompt mit Zusammenfassung erweitern\n","    enhanced_system = system_prompt\n","    if summary_context:\n","        enhanced_system = f\"{system_prompt}\\n\\nBisheriger Kontext (Zusammenfassung): {summary_context}\"\n","\n","    # Chain aufrufen\n","    response = chain.invoke({\n","        'system_prompt': enhanced_system,\n","        'chat_history': chat_history,\n","        'user_input': user_input\n","    })\n","\n","    # Ausgabe\n","    mprint(f\"### Mensch:\\n{user_input}\")\n","    mprint(f\"### KI:\\n{response}\\n\")\n","\n","    # Historie aktualisieren\n","    chat_history.append(HumanMessage(content=user_input))\n","    chat_history.append(AIMessage(content=response))\n","\n","    return chat_history, summary_context"]},{"cell_type":"code","execution_count":null,"id":"summary-demo","metadata":{"id":"summary-demo"},"outputs":[],"source":["# Demo: Summary in Aktion\n","mprint(\"## Summary Demo\")\n","mprint(\"---\")\n","\n","history_summary = [SystemMessage(content=system_prompt)]\n","\n","summary_ctx = \"\"\n","\n","# Wichtige Informationen am Anfang\n","history_summary, summary_ctx = chat_with_summary(history_summary, \"Mein Name ist Max\", summary_ctx)\n","history_summary, summary_ctx = chat_with_summary(history_summary, \"Ich wohne in Koln\", summary_ctx)\n","history_summary, summary_ctx = chat_with_summary(history_summary, \"Ich mag Python\", summary_ctx)\n","history_summary, summary_ctx = chat_with_summary(history_summary, \"Meine Katze heisst Neo\", summary_ctx)\n","\n","# Fullnachrichten um Summary auszulosen\n","for i in range(5):\n","    history_summary, summary_ctx = chat_with_summary(history_summary, f\"Test {i+1}\", summary_ctx)\n","\n","# Nach Summary: Kann die KI sich noch an den Namen erinnern?\n","history_summary, summary_ctx = chat_with_summary(history_summary, \"Wie heisse ich und wie heisst meine Katze?\", summary_ctx)\n","\n","mprint(f\"### Aktuelle Historie: {len(history_summary)} Nachrichten\")\n","mprint(f\"### Gespeicherte Zusammenfassung:\\n{summary_ctx}\")"]},{"cell_type":"markdown","id":"db-header","metadata":{"id":"db-header"},"source":["# 5 | RunnableWithMessageHistory (LCEL)\n","---"]},{"cell_type":"markdown","id":"db-intro","metadata":{"id":"db-intro"},"source":["`RunnableWithMessageHistory` ist die **offizielle LCEL-Alternative** zu den deprecated Memory-Klassen (`ConversationBufferMemory`, etc.). Es wrapped eine Chain und verwaltet die Chat-Historie automatisch.\n","\n","**Vorteile gegenuber manueller Verwaltung:**\n","- Automatisches Laden/Speichern der Historie\n","- Session-Management uber `session_id`\n","- Kompatibel mit Streaming und Async\n","\n","```python\n","from langchain_core.runnables.history import RunnableWithMessageHistory\n","```"]},{"cell_type":"code","execution_count":null,"id":"db-imports","metadata":{"id":"db-imports"},"outputs":[],"source":["# Importe fur RunnableWithMessageHistory\n","from langchain_core.runnables.history import RunnableWithMessageHistory\n","from langchain_core.chat_history import InMemoryChatMessageHistory\n","\n","# Session-Store (Dictionary: session_id -> ChatMessageHistory)\n","session_store = {}\n","\n","def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n","    \"\"\"Gibt die Historie fur eine Session zuruck (oder erstellt eine neue).\"\"\"\n","    if session_id not in session_store:\n","        session_store[session_id] = InMemoryChatMessageHistory()\n","    return session_store[session_id]"]},{"cell_type":"code","execution_count":null,"id":"db-class","metadata":{"id":"db-class"},"outputs":[],"source":["# Prompt mit Historie-Platzhalter\n","prompt_with_history = ChatPromptTemplate.from_messages([\n","    (\"system\", system_prompt),\n","    MessagesPlaceholder(variable_name=\"history\"),  # Hier wird die Historie eingefugt\n","    (\"human\", \"{input}\")\n","])\n","\n","# Basis-Chain (ohne Memory)\n","base_chain = prompt_with_history | llm | parser\n","\n","# Chain MIT automatischem Memory-Management wrappen\n","chain_with_history = RunnableWithMessageHistory(\n","    runnable=base_chain,\n","    get_session_history=get_session_history,\n","    input_messages_key=\"input\",      # Key fur neue User-Nachricht\n","    history_messages_key=\"history\"   # Key fur die Historie im Prompt\n",")\n","\n","print(\"Chain mit RunnableWithMessageHistory erstellt\")"]},{"cell_type":"code","execution_count":null,"id":"db-chatbot","metadata":{"id":"db-chatbot"},"outputs":[],"source":["# Helper-Funktion fur Chat mit Session-ID\n","def chat_with_session(session_id: str, user_input: str) -> str:\n","    \"\"\"Chattet mit automatischem Memory uber RunnableWithMessageHistory.\"\"\"\n","\n","    # Config mit Session-ID (PFLICHT!)\n","    config = {\"configurable\": {\"session_id\": session_id}}\n","\n","    # Chain aufrufen - Historie wird automatisch geladen/gespeichert\n","    response = chain_with_history.invoke(\n","        {\"input\": user_input},\n","        config=config\n","    )\n","\n","    mprint(f\"**[{session_id}] Mensch:** {user_input}\")\n","    mprint(f\"**[{session_id}] KI:** {response}\\n\")\n","\n","    return response"]},{"cell_type":"code","execution_count":null,"id":"db-demo","metadata":{"id":"db-demo"},"outputs":[],"source":["# Demo: RunnableWithMessageHistory\n","mprint(\"## RunnableWithMessageHistory Demo\")\n","mprint(\"---\")\n","\n","# Session 1: Max\n","chat_with_session(\"max\", \"Hallo! Ich bin Max aus Munchen.\")\n","chat_with_session(\"max\", \"Ich programmiere gerne in Python.\")\n","\n","# Session 2: Emma (separate Historie!)\n","chat_with_session(\"emma\", \"Hi! Ich bin Emma und mag Machine Learning.\")\n","\n","# Zuruck zu Max - Memory bleibt erhalten!\n","chat_with_session(\"max\", \"Woher komme ich und was ist mein Hobby?\")\n","\n","# Session-Store anzeigen\n","mprint(\"### Gespeicherte Sessions:\")\n","for sid, history in session_store.items():\n","    mprint(f\"- **{sid}**: {len(history.messages)} Nachrichten\")"]},{"cell_type":"code","execution_count":null,"id":"db-list","metadata":{"id":"db-list"},"outputs":[],"source":["# Historie einer Session anzeigen\n","def show_session_history(session_id: str):\n","    \"\"\"Zeigt die Nachrichten einer Session.\"\"\"\n","    if session_id not in session_store:\n","        print(f\"Session '{session_id}' nicht gefunden\")\n","        return\n","\n","    history = session_store[session_id]\n","    mprint(f\"### Historie: {session_id}\")\n","    for i, msg in enumerate(history.messages, 1):\n","        role = \"Human\" if msg.type == \"human\" else \"KI\"\n","        mprint(f\"{i}. **{role}:** {msg.content}\")\n","\n","show_session_history(\"max\")"]},{"cell_type":"markdown","id":"db-history","metadata":{"id":"db-history"},"source":["<p><font color='darkblue' size=\"4\">\n","Vergleich: RunnableWithMessageHistory vs. LangGraph\n","</font></p>\n","\n","| Aspekt | RunnableWithMessageHistory | LangGraph |\n","|--------|---------------------------|-----------|\n","| **Komplexitat** | Einfach | Fortgeschritten |\n","| **Persistenz** | Manuell (Store) | Checkpointer |\n","| **RemoveMessage** | Nein | Ja |\n","| **Trimming** | Manuell | `trim_messages` |\n","| **Multi-Agent** | Nein | Ja |\n","\n","**Empfehlung:** `RunnableWithMessageHistory` fur einfache Chains, LangGraph fur komplexe Agents."]},{"cell_type":"markdown","id":"db-persist","metadata":{"id":"db-persist"},"source":["# 6 | Long-term Memory (SQLite)\n","---"]},{"cell_type":"markdown","id":"db-persist-test","metadata":{"id":"db-persist-test"},"source":["**Problem:** Beim Neustart der Anwendung geht die Historie verloren.\n","\n","**Losung:** Persistente Speicherung in einer **SQLite-Datenbank**.\n","\n","| Speicherart | Persistenz | Multi-User | Anwendung |\n","|-------------|------------|------------|------------|\n","| Python-Liste | Nein | Nein | Prototyping |\n","| RunnableWithMessageHistory | Nein* | Ja | Einfache Chains |\n","| SQLite | Ja | Ja | Lokale Apps |\n","\n","*Mit FileChatMessageHistory moglich"]},{"cell_type":"code","id":"2wes9llitgg","source":["import sqlite3\n","from datetime import datetime\n","from typing import List, Dict, Optional\n","\n","DB_PATH = \"./chat_memory.db\""],"metadata":{"id":"2wes9llitgg"},"execution_count":null,"outputs":[]},{"cell_type":"code","id":"lt015mlb76i","source":["class ChatMemoryDB:\n","    \"\"\"Einfache Chat-Memory-Datenbank mit SQLite.\"\"\"\n","\n","    def __init__(self, db_path: str = DB_PATH):\n","        self.db_path = db_path\n","        self._init_db()\n","\n","    def _init_db(self):\n","        \"\"\"Erstellt die Tabellen falls nicht vorhanden.\"\"\"\n","        with sqlite3.connect(self.db_path) as conn:\n","            conn.execute(\"\"\"\n","                CREATE TABLE IF NOT EXISTS messages (\n","                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n","                    thread_id TEXT NOT NULL,\n","                    role TEXT NOT NULL,\n","                    content TEXT NOT NULL,\n","                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n","                )\n","            \"\"\")\n","            conn.execute(\"\"\"\n","                CREATE TABLE IF NOT EXISTS summaries (\n","                    thread_id TEXT PRIMARY KEY,\n","                    summary TEXT,\n","                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP\n","                )\n","            \"\"\")\n","            conn.execute(\"CREATE INDEX IF NOT EXISTS idx_thread ON messages(thread_id)\")\n","            conn.commit()\n","        print(f\"Datenbank initialisiert: {self.db_path}\")\n","\n","    def save_message(self, thread_id: str, role: str, content: str):\n","        \"\"\"Speichert eine Nachricht.\"\"\"\n","        with sqlite3.connect(self.db_path) as conn:\n","            conn.execute(\n","                \"INSERT INTO messages (thread_id, role, content) VALUES (?, ?, ?)\",\n","                (thread_id, role, content)\n","            )\n","            conn.commit()\n","\n","    def get_history(self, thread_id: str, limit: Optional[int] = None) -> List[Dict]:\n","        \"\"\"Ladt die Historie eines Threads.\"\"\"\n","        with sqlite3.connect(self.db_path) as conn:\n","            if limit:\n","                rows = conn.execute(\n","                    \"SELECT role, content FROM messages WHERE thread_id = ? ORDER BY id DESC LIMIT ?\",\n","                    (thread_id, limit)\n","                ).fetchall()\n","                rows = list(reversed(rows))\n","            else:\n","                rows = conn.execute(\n","                    \"SELECT role, content FROM messages WHERE thread_id = ? ORDER BY id\",\n","                    (thread_id,)\n","                ).fetchall()\n","        return [{\"role\": r[0], \"content\": r[1]} for r in rows]\n","\n","    def get_message_count(self, thread_id: str) -> int:\n","        \"\"\"Zahlt die Nachrichten eines Threads.\"\"\"\n","        with sqlite3.connect(self.db_path) as conn:\n","            count = conn.execute(\n","                \"SELECT COUNT(*) FROM messages WHERE thread_id = ?\",\n","                (thread_id,)\n","            ).fetchone()[0]\n","        return count\n","\n","    def save_summary(self, thread_id: str, summary: str):\n","        \"\"\"Speichert/aktualisiert die Zusammenfassung eines Threads.\"\"\"\n","        with sqlite3.connect(self.db_path) as conn:\n","            conn.execute(\"\"\"\n","                INSERT OR REPLACE INTO summaries (thread_id, summary, updated_at)\n","                VALUES (?, ?, CURRENT_TIMESTAMP)\n","            \"\"\", (thread_id, summary))\n","            conn.commit()\n","\n","    def get_summary(self, thread_id: str) -> Optional[str]:\n","        \"\"\"Ladt die Zusammenfassung eines Threads.\"\"\"\n","        with sqlite3.connect(self.db_path) as conn:\n","            row = conn.execute(\n","                \"SELECT summary FROM summaries WHERE thread_id = ?\",\n","                (thread_id,)\n","            ).fetchone()\n","        return row[0] if row else None\n","\n","    def list_threads(self) -> List[Dict]:\n","        \"\"\"Listet alle Threads mit Statistiken.\"\"\"\n","        with sqlite3.connect(self.db_path) as conn:\n","            rows = conn.execute(\"\"\"\n","                SELECT thread_id, COUNT(*) as msg_count, MAX(timestamp) as last_msg\n","                FROM messages\n","                GROUP BY thread_id\n","                ORDER BY last_msg DESC\n","            \"\"\").fetchall()\n","        return [{\"thread_id\": r[0], \"messages\": r[1], \"last_activity\": r[2]} for r in rows]\n","\n","    def delete_thread(self, thread_id: str):\n","        \"\"\"Loscht einen Thread.\"\"\"\n","        with sqlite3.connect(self.db_path) as conn:\n","            conn.execute(\"DELETE FROM messages WHERE thread_id = ?\", (thread_id,))\n","            conn.execute(\"DELETE FROM summaries WHERE thread_id = ?\", (thread_id,))\n","            conn.commit()"],"metadata":{"id":"lt015mlb76i"},"execution_count":null,"outputs":[]},{"cell_type":"code","id":"f9cx08h8yll","source":["class PersistentChatbot:\n","    \"\"\"Chatbot mit persistentem Memory uber SQLite.\"\"\"\n","\n","    def __init__(self, db_path: str = DB_PATH, max_context: int = 10):\n","        self.db = ChatMemoryDB(db_path)\n","        self.max_context = max_context\n","        self.llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0.7)\n","        self.system_prompt = \"Du bist ein hilfreicher KI-Assistent mit Gedachtnis.\"\n","\n","    def _history_to_messages(self, history: List[Dict]) -> list:\n","        \"\"\"Konvertiert DB-Historie zu LangChain-Messages.\"\"\"\n","        messages = []\n","        for item in history:\n","            if item[\"role\"] == \"human\":\n","                messages.append(HumanMessage(content=item[\"content\"]))\n","            else:\n","                messages.append(AIMessage(content=item[\"content\"]))\n","        return messages\n","\n","    def chat(self, thread_id: str, user_input: str) -> str:\n","        \"\"\"Sendet eine Nachricht und speichert die Antwort.\"\"\"\n","\n","        # Historie laden (mit Limit)\n","        history = self.db.get_history(thread_id, limit=self.max_context)\n","        summary = self.db.get_summary(thread_id)\n","\n","        # System-Prompt mit Summary erweitern\n","        enhanced_system = self.system_prompt\n","        if summary:\n","            enhanced_system = f\"{self.system_prompt}\\n\\nKontext aus fruheren Gesprachen: {summary}\"\n","\n","        # Messages zusammenbauen\n","        messages = [SystemMessage(content=enhanced_system)]\n","        messages.extend(self._history_to_messages(history))\n","        messages.append(HumanMessage(content=user_input))\n","\n","        # LLM aufrufen\n","        response = self.llm.invoke(messages)\n","        response_text = response.content\n","\n","        # In DB speichern\n","        self.db.save_message(thread_id, \"human\", user_input)\n","        self.db.save_message(thread_id, \"ai\", response_text)\n","\n","        return response_text\n","\n","    def show_history(self, thread_id: str):\n","        \"\"\"Zeigt die Historie eines Threads.\"\"\"\n","        history = self.db.get_history(thread_id)\n","        mprint(f\"### Thread: {thread_id} ({len(history)} Nachrichten)\")\n","        mprint(\"---\")\n","        for i, msg in enumerate(history, 1):\n","            role = \"Human\" if msg[\"role\"] == \"human\" else \"KI\"\n","            mprint(f\"{i}. **{role}:** {msg['content']}\")\n","\n","    def list_threads(self):\n","        \"\"\"Listet alle Threads.\"\"\"\n","        threads = self.db.list_threads()\n","        mprint(\"### Alle Threads:\")\n","        mprint(\"---\")\n","        for t in threads:\n","            mprint(f\"- **{t['thread_id']}**: {t['messages']} Nachrichten (zuletzt: {t['last_activity']})\")"],"metadata":{"id":"f9cx08h8yll"},"execution_count":null,"outputs":[]},{"cell_type":"code","id":"f0askgykshw","source":["# Demo: Persistenter Chatbot\n","mprint(\"## SQLite-Chatbot Demo\")\n","mprint(\"---\")\n","\n","bot = PersistentChatbot()\n","\n","# Thread 1: Max\n","print(\"\\n--- Thread: max_session ---\")\n","response = bot.chat(\"max_session\", \"Hallo! Ich bin Max und komme aus Munchen.\")\n","mprint(f\"**KI:** {response}\")\n","\n","response = bot.chat(\"max_session\", \"Ich interessiere mich fur Machine Learning.\")\n","mprint(f\"**KI:** {response}\")\n","\n","# Thread 2: Emma\n","print(\"\\n--- Thread: emma_session ---\")\n","response = bot.chat(\"emma_session\", \"Hi! Ich bin Emma aus Berlin.\")\n","mprint(f\"**KI:** {response}\")\n","\n","# Zuruck zu Max - Memory bleibt erhalten!\n","print(\"\\n--- Zuruck zu max_session ---\")\n","response = bot.chat(\"max_session\", \"Woher komme ich nochmal?\")\n","mprint(f\"**KI:** {response}\")"],"metadata":{"id":"f0askgykshw"},"execution_count":null,"outputs":[]},{"cell_type":"code","id":"7pgcfw3i1kx","source":["# Alle Threads anzeigen\n","bot.list_threads()"],"metadata":{"id":"7pgcfw3i1kx"},"execution_count":null,"outputs":[]},{"cell_type":"code","id":"8gsbb6kjlbi","source":["# Historie eines Threads anzeigen\n","bot.show_history(\"max_session\")"],"metadata":{"id":"8gsbb6kjlbi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"1ut97tdwl86","source":["<p><font color='darkblue' size=\"4\">\n","Test: Neustart-Persistenz\n","</font></p>\n","\n","Die Daten bleiben auch nach Neustart erhalten. Fuhren Sie die nachste Zelle aus, um zu testen:"],"metadata":{"id":"1ut97tdwl86"}},{"cell_type":"code","id":"l4f6rj1cpog","source":["# Test: Neuer Bot-Instance, gleiche Datenbank\n","bot2 = PersistentChatbot()\n","\n","# Sollte Max's Historie kennen!\n","response = bot2.chat(\"max_session\", \"Was war mein Interesse nochmal?\")\n","mprint(f\"**KI (nach 'Neustart'):** {response}\")\n","\n","# Historie anzeigen\n","bot2.show_history(\"max_session\")"],"metadata":{"id":"l4f6rj1cpog"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"tasks-header","metadata":{"id":"tasks-header"},"source":["# A | Aufgaben\n","---"]},{"cell_type":"markdown","id":"task-1","metadata":{"id":"task-1"},"source":["<p><font color='black' size=\"5\">\n","Aufgabe 1: Trimming-Limit testen\n","</font></p>\n","\n","**Schwierigkeit:** 1/5\n","\n","Andern Sie `MAX_MESSAGES` auf 4 und fuhren Sie eine langere Konversation. Beobachten Sie, wann Informationen verloren gehen."]},{"cell_type":"markdown","id":"task-2","metadata":{"id":"task-2"},"source":["<p><font color='black' size=\"5\">\n","Aufgabe 2: Summary-Qualitat verbessern\n","</font></p>\n","\n","**Schwierigkeit:** 2/5\n","\n","Verbessern Sie den `summary_prompt`, um wichtige Informationen (Namen, Orte, Praferenzen) besser zu extrahieren."]},{"cell_type":"markdown","id":"task-3","metadata":{"id":"task-3"},"source":["<p><font color='black' size=\"5\">\n","Aufgabe 3: Interaktiver CLI-Chatbot\n","</font></p>\n","\n","**Schwierigkeit:** 3/5\n","\n","Erweitern Sie `PersistentChatbot` um eine interaktive Schleife mit Befehlen:\n","- `exit` - Beenden\n","- `history` - Historie anzeigen\n","- `new` - Neuen Thread starten\n","- `threads` - Alle Threads listen"]},{"cell_type":"markdown","id":"task-4","metadata":{"id":"task-4"},"source":["<p><font color='black' size=\"5\">\n","Aufgabe 4: Hybrid Memory (Trimming + Summary + DB)\n","</font></p>\n","\n","**Schwierigkeit:** 4/5\n","\n","Kombinieren Sie alle drei Strategien:\n","1. Speicherung in SQLite\n","2. Automatisches Trimming auf die letzten N Nachrichten\n","3. Zusammenfassung der alteren Nachrichten (in DB gespeichert)"]},{"cell_type":"markdown","id":"r4e93hrgyre","source":["# B | Datenbank auslesen\n","---"],"metadata":{"id":"r4e93hrgyre"}},{"cell_type":"markdown","id":"9h44bm4iwc5","source":["Dieser Abschnitt zeigt, wie die SQLite-Datenbank (`chat_memory.db`) direkt ausgelesen werden kann - nutzlich fur Debugging, Analyse oder Export."],"metadata":{"id":"9h44bm4iwc5"}},{"cell_type":"code","id":"czndm30m5hq","source":["import sqlite3\n","import os\n","\n","def read_all_threads_from_db(db_path: str = DB_PATH):\n","    \"\"\"\n","    Liest alle Threads und Nachrichten aus der chat_memory.db Datenbank.\n","    \"\"\"\n","    if not os.path.exists(db_path):\n","        print(f\"Fehler: Datenbankdatei '{db_path}' wurde nicht gefunden.\")\n","        return\n","\n","    mprint(f\"### Lese Datenbank: {db_path}\")\n","    mprint(\"---\")\n","\n","    with sqlite3.connect(db_path) as conn:\n","        # Alle Threads mit Statistiken\n","        threads = conn.execute(\"\"\"\n","            SELECT thread_id, COUNT(*) as msg_count, MAX(timestamp) as last_msg\n","            FROM messages\n","            GROUP BY thread_id\n","            ORDER BY last_msg DESC\n","        \"\"\").fetchall()\n","\n","        if not threads:\n","            print(\"Keine Threads in der Datenbank gefunden.\")\n","            return\n","\n","        mprint(f\"**{len(threads)} Threads gefunden**\\n\")\n","\n","        # Jeden Thread mit Nachrichten anzeigen\n","        for thread_id, msg_count, last_msg in threads:\n","            mprint(f\"#### Thread: {thread_id}\")\n","            mprint(f\"*{msg_count} Nachrichten, zuletzt: {last_msg}*\\n\")\n","\n","            # Nachrichten des Threads\n","            messages = conn.execute(\"\"\"\n","                SELECT role, content, timestamp\n","                FROM messages\n","                WHERE thread_id = ?\n","                ORDER BY id\n","            \"\"\", (thread_id,)).fetchall()\n","\n","            for i, (role, content, ts) in enumerate(messages, 1):\n","                role_display = \"Human\" if role == \"human\" else \"KI\"\n","                # Inhalt kurzen wenn zu lang\n","                content_short = content[:100] + \"...\" if len(content) > 100 else content\n","                mprint(f\"{i}. **{role_display}:** {content_short}\")\n","\n","            # Zusammenfassung (falls vorhanden)\n","            summary = conn.execute(\n","                \"SELECT summary FROM summaries WHERE thread_id = ?\",\n","                (thread_id,)\n","            ).fetchone()\n","\n","            if summary and summary[0]:\n","                mprint(f\"\\n*Zusammenfassung:* {summary[0][:150]}...\")\n","\n","            mprint(\"\")  # Leerzeile zwischen Threads"],"metadata":{"id":"czndm30m5hq"},"execution_count":null,"outputs":[]},{"cell_type":"code","id":"t62wa5e3aac","source":["# Alle Threads aus der Datenbank auslesen\n","read_all_threads_from_db()"],"metadata":{"id":"t62wa5e3aac"},"execution_count":null,"outputs":[]},{"cell_type":"code","id":"3b14guf2r17","source":["import json\n","\n","def export_thread_to_json(thread_id: str, db_path: str = DB_PATH) -> dict:\n","    \"\"\"Exportiert einen Thread als JSON.\"\"\"\n","    with sqlite3.connect(db_path) as conn:\n","        messages = conn.execute(\"\"\"\n","            SELECT role, content, timestamp\n","            FROM messages\n","            WHERE thread_id = ?\n","            ORDER BY id\n","        \"\"\", (thread_id,)).fetchall()\n","\n","        summary = conn.execute(\n","            \"SELECT summary FROM summaries WHERE thread_id = ?\",\n","            (thread_id,)\n","        ).fetchone()\n","\n","    data = {\n","        \"thread_id\": thread_id,\n","        \"messages\": [\n","            {\"role\": r, \"content\": c, \"timestamp\": t}\n","            for r, c, t in messages\n","        ],\n","        \"summary\": summary[0] if summary else None\n","    }\n","\n","    return data\n","\n","# Beispiel: Thread als JSON exportieren\n","thread_data = export_thread_to_json(\"max_session\")\n","print(json.dumps(thread_data, indent=2, ensure_ascii=False))"],"metadata":{"id":"3b14guf2r17"},"execution_count":null,"outputs":[]},{"cell_type":"code","id":"qmm10fc57u7","source":["def delete_all_threads(db_path: str = DB_PATH):\n","    \"\"\"Loscht alle Threads aus der Datenbank (Cleanup).\"\"\"\n","    with sqlite3.connect(db_path) as conn:\n","        conn.execute(\"DELETE FROM messages\")\n","        conn.execute(\"DELETE FROM summaries\")\n","        conn.commit()\n","    print(f\"Alle Threads geloscht aus: {db_path}\")\n","\n","# Auskommentiert, um versehentliches Loschen zu verhindern:\n","# delete_all_threads()"],"metadata":{"id":"qmm10fc57u7"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.0"},"colab":{"provenance":[],"collapsed_sections":["intro-header","shortterm-header","trimming-header","summary-header","db-header","db-persist","tasks-header","r4e93hrgyre"],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}
{"cells":[{"cell_type":"markdown","id":"header-1","metadata":{"id":"header-1"},"source":["<p><font size=\"6\" color='grey'> <b>\n","\n","Generative KI. Verstehen. Anwenden. Gestalten.\n","</b></font> </br></p>"]},{"cell_type":"markdown","id":"header-2","metadata":{"id":"header-2"},"source":["<p><font size=\"5\" color='grey'> <b> Chat Memory Patterns</b></font> </br></p>\n","\n","---"]},{"cell_type":"code","execution_count":null,"id":"setup-1","metadata":{"id":"setup-1","collapsed":true},"outputs":[],"source":["#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n","!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul\n","from genai_lib.utilities import (\n","    check_environment,\n","    get_ipinfo,\n","    setup_api_keys,\n","    mprint,\n","    install_packages,\n","    mermaid,\n","    get_model_profile,\n","    extract_thinking,\n","    load_chat_prompt_template\n",")\n","setup_api_keys(['OPENAI_API_KEY'], create_globals=False)\n","print()\n","check_environment()\n","print()\n","get_ipinfo()"]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","‚è∏Ô∏è 5-Minuten-Check:\n","</font></p>\n","\n","**Ziel:** Pr√ºfen, ob du das vorherige Kapitel verstanden hast ‚Äì nicht, ob es gerade l√§uft.\n","\n","**Aufgabe** (5 Minuten, ohne Vorlage):\n","\n","Rekonstruiere die zentrale Idee oder Code-Struktur des letzten Abschnitts selbstst√§ndig\n","(kein Copy & Paste, kein Nachschlagen).\n","\n","W√§hle eine der folgenden Optionen:\n","\n","+ Erkl√§re in 1‚Äì2 S√§tzen, was hier konzeptionell passiert.\n","\n","+ Ver√§ndere eine Kleinigkeit (z. B. Prompt, Parameter, Reihenfolge) und beschreibe die Auswirkung.\n","\n","+ Markiere eine Stelle, die du nicht sicher erkl√§ren kannst, und formuliere eine konkrete Frage dazu.\n","\n","**Hinweis:**\n","Nicht alles muss ‚Äûfertig‚Äú oder ‚Äûkorrekt‚Äú sein. Entscheidend ist, wo dein Verst√§ndnis gerade endet"],"metadata":{"id":"xyxdYJECanbS"},"id":"xyxdYJECanbS"},{"cell_type":"markdown","id":"intro-header","metadata":{"id":"intro-header"},"source":["# 1 | Intro\n","---"]},{"cell_type":"markdown","id":"intro-text","metadata":{"id":"intro-text"},"source":["<p><font color='black' size=\"5\">\n","Zustandslosigkeit von LLMs\n","</font></p>\n","\n","Large Language Models (LLMs) wie GPT sind von Natur aus **zustandslos** - sie verfugen uber kein eingebautes Gedachtnis. Jede Anfrage wird isoliert verarbeitet, ohne Bezug zu vorherigen Interaktionen. Deshalb muss der Chatverlauf (Historie) bei jeder Anfrage neu ubergeben werden.\n","\n","```\n","Ohne Memory:\n","User: \"Mein Name ist Max\"\n","AI: \"Hallo Max!\"\n","User: \"Wie heisse ich?\"\n","AI: \"Das habe ich nicht gespeichert.\"\n","```\n","\n","**Wir verwenden RunnableWithMessageHistory** ‚Äì den etablierten LangChain-Ansatz f√ºr Konversations-Memory. F√ºr komplexere Anwendungen mit mehreren Agenten oder fortgeschrittener Zustandsverwaltung bietet **LangGraph** erweiterte M√∂glichkeiten.\n","\n","**Dieses Notebook zeigt Memory-Patterns mit RunnableWithMessageHistory:**\n","\n","| Pattern | Beschreibung | Anwendungsfall |\n","|---------|--------------|----------------|\n","| **Python-Liste** | Einfachste Losung | Prototyping, kurze Sessions |\n","| **Trimming** | Nur letzte N Nachrichten | Token-Limit einhalten |\n","| **Summary** | Alte Nachrichten zusammenfassen | Lange Sessions, Kontext erhalten |\n","| **FileChatMessageHistory** | Persistenz in Dateien | Einfache Persistenz ohne DB |\n","| **Datenbank** | Persistente Speicherung | Production, Multi-User |"]},{"cell_type":"code","source":["#@title üîß üßú‚Äç‚ôÄÔ∏èMermaid-Diagramm { display-mode: \"form\" }\n","\n","diagram = \"\"\"\n","graph TD\n","    root[Chat Memory Patterns]\n","\n","    %% 1. Manuelle Variante\n","    root -->|1 Short-term / Manuell| manual[Python-Liste]\n","    manual --> manual_desc[\"<b>Beschreibung:</b> Einfache Liste im RAM<br/><b>Use Case:</b> Prototyping, kurze Sessions\"]\n","\n","    %% 2. LCEL Variante\n","    root -->|2 Automatisch / LCEL| lcel[RunnableWithMessageHistory]\n","\n","    %% Optimierungs-Strategien\n","    lcel -->|Context Management| strat[Strategien mit Token-Limits]\n","\n","    strat --> trim[Trimming / Sliding Window]\n","    trim --> trim_desc[\"<b>Funktion:</b> Nur letzte N Nachrichten behalten<br/><b>Ziel:</b> Token-Limit einhalten\"]\n","\n","    strat --> sum[Summary]\n","    sum --> sum_desc[\"<b>Funktion:</b> Alte Nachrichten per LLM zusammenfassen<br/><b>Ziel:</b> Langzeit-Kontext erhalten\"]\n","\n","    %% Persistenz-L√∂sungen\n","    lcel -->|Long-term Memory| persist[Persistenz-Backends]\n","\n","    persist --> file[FileChatMessageHistory]\n","    file --> file_desc[\"<b>Speicher:</b> JSON-Dateien<br/><b>Use Case:</b> Einfache Persistenz ohne DB\"]\n","\n","    persist --> db[Datenbank / SQLite]\n","    db --> db_desc[\"<b>Speicher:</b> Relationale Datenbank<br/><b>Use Case:</b> Production, Multi-User\"]\n","\n","    %% Styling\n","    style root fill:#2ecc71,stroke:#27ae60,stroke-width:2px,color:white\n","    style lcel fill:#3498db,stroke:#2980b9,stroke-width:2px,color:white\n","    style manual fill:#95a5a6,stroke:#7f8c8d,stroke-width:2px,color:white\n","    style strat fill:#f1c40f,stroke:#f39c12,color:black\n","    style persist fill:#e67e22,stroke:#d35400,color:white\n","\"\"\"\n","mermaid(diagram, width=1000, height=800)"],"metadata":{"id":"Ftt5zOqim1gn"},"id":"Ftt5zOqim1gn","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"shortterm-header","metadata":{"id":"shortterm-header"},"source":["# 2 | Short-term Memory (Python-Liste)\n","---"]},{"cell_type":"markdown","id":"shortterm-intro","metadata":{"id":"shortterm-intro"},"source":["Die einfachste Form von Memory: Eine **Python-Liste**, die alle Nachrichten speichert und bei jedem API-Call mitgesendet wird."]},{"cell_type":"code","execution_count":null,"id":"shortterm-imports","metadata":{"id":"shortterm-imports"},"outputs":[],"source":["# Importe\n","from langchain.chat_models import init_chat_model\n","from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# System-Prompt\n","system_prompt = \"Du bist ein hilfreicher und humorvoller KI-Assistent.\"\n","\n","# Prompt-Template mit Historie (MessagesPlaceholder nimmt die Historie entgegen)\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"human\", \"{user_input}\")\n","])\n","\n","# LLM (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0)\n","\n","# Parser\n","parser = StrOutputParser()\n","\n","# Chain\n","chain = prompt | llm | parser"]},{"cell_type":"code","execution_count":null,"id":"shortterm-function","metadata":{"id":"shortterm-function"},"outputs":[],"source":["# Chat-Funktion mit manueller Historien-Verwaltung\n","def chat(chat_history, user_input):\n","    \"\"\"Fuhrt eine Chat-Interaktion mit manueller Historien-Verwaltung durch.\"\"\"\n","\n","    # Chain aufrufen (Historie wird im Prompt mitgeschickt)\n","    response = chain.invoke({\n","        'system_prompt': system_prompt,\n","        'chat_history': chat_history,\n","        'user_input': user_input\n","    })\n","\n","    # Ausgabe\n","    mprint(f\"### üßë‚Äçü¶± Mensch:\\n{user_input}\")\n","    mprint(f\"### ü§ñ KI:\\n{response}\\n\")\n","\n","    # Memory (Liste) MANUELL aktualisieren\n","    chat_history.append(HumanMessage(content=user_input))\n","    chat_history.append(AIMessage(content=response))\n","\n","    return chat_history"]},{"cell_type":"code","execution_count":5,"id":"shortterm-demo","metadata":{"id":"shortterm-demo","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1770731126438,"user_tz":-60,"elapsed":1125,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"d1f4dbb0-02c1-4fe2-8de6-36ce32f1e777"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### üßë‚Äçü¶± Mensch:\nIch mag Python-Programmierung"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### ü§ñ KI:\nDas ist gro√üartig, Max! Python ist eine fantastische Sprache ‚Äì so vielseitig und benutzerfreundlich. Was machst du gerne mit Python? Programmierst du Spiele, Webanwendungen oder vielleicht etwas ganz anderes?\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### üßë‚Äçü¶± Mensch:\nWeisst du noch, wie ich heisse und was ich mag?"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### ü§ñ KI:\nNat√ºrlich, Max! Du magst Python-Programmierung. Ich bin wie ein Elefant ‚Äì ich vergesse nie! Gibt es etwas Bestimmtes, wor√ºber du in Bezug auf Python sprechen m√∂chtest? Vielleicht ein Projekt oder eine Herausforderung?\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### Gespeicherte Nachrichten (Liste):\n---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"  **system**:   Du bist ein hilfreicher und humorvoller KI-Assistent."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"  **human**:   Mein Name ist Max"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"  **ai**:   Hallo Max! Sch√∂n, dich kennenzulernen! Wie kann ich dir heute helfen? Oder m√∂chtest du einfach ein bisschen plaudern?"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"  **human**:   Ich mag Python-Programmierung"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"  **ai**:   Das ist gro√üartig, Max! Python ist eine fantastische Sprache ‚Äì so vielseitig und benutzerfreundlich. Was machst du gerne mit Python? Programmierst du Spiele, Webanwendungen oder vielleicht etwas ganz anderes?"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"  **human**:   Weisst du noch, wie ich heisse und was ich mag?"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"  **ai**:   Nat√ºrlich, Max! Du magst Python-Programmierung. Ich bin wie ein Elefant ‚Äì ich vergesse nie! Gibt es etwas Bestimmtes, wor√ºber du in Bezug auf Python sprechen m√∂chtest? Vielleicht ein Projekt oder eine Herausforderung?"},"metadata":{}}],"source":["# Historie initialisieren\n","chat_history = [SystemMessage(content=system_prompt)]\n","\n","\n","# Konversation\n","chat_history = chat(chat_history, \"Mein Name ist Max\")\n","chat_history = chat(chat_history, \"Ich mag Python-Programmierung\")\n","chat_history = chat(chat_history, \"Weisst du noch, wie ich heisse und was ich mag?\")\n","\n","mprint(\"### Gespeicherte Nachrichten (Liste):\\n---\")\n","for msg in chat_history:\n","    mprint(f\"  **{msg.type}**:   {msg.content}\")"]},{"cell_type":"markdown","id":"shortterm-problem","metadata":{"id":"shortterm-problem"},"source":["<p><font color='darkblue' size=\"4\">\n","Problem:\n","</font></p>\n","\n","- Keine automatische Session-Verwaltung (Multi-User)\n","- Manuelles Memory-Management fehleranfallig\n","- **Bei langen Konversationen: Token-Limit wird uberschritten!**"]},{"cell_type":"markdown","id":"db-header","metadata":{"id":"db-header"},"source":["# 3 | RunnableWithMessageHistory (LCEL)\n","---"]},{"cell_type":"markdown","id":"db-intro","metadata":{"id":"db-intro"},"source":["`RunnableWithMessageHistory` ist die **offizielle LCEL-Alternative** zu den deprecated Memory-Klassen (`ConversationBufferMemory`, etc.). Es *umschlie√üt* eine Chain und √ºbernimmt die automatische Verwaltung der Chat-Historie.\n","\n","**Vorteile gegenuber manueller Verwaltung:**\n","- Automatisches Laden/Speichern der Historie\n","- Session-Management uber `session_id`\n","- Kompatibel mit Streaming und Async\n","\n","```python\n","from langchain_core.runnables.history import RunnableWithMessageHistory\n","```"]},{"cell_type":"markdown","source":["## 3.1 | Basismodell"],"metadata":{"id":"TxKuF5lleCxS"},"id":"TxKuF5lleCxS"},{"cell_type":"code","execution_count":6,"id":"db-imports","metadata":{"id":"db-imports","executionInfo":{"status":"ok","timestamp":1770731126481,"user_tz":-60,"elapsed":0,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Importe fur RunnableWithMessageHistory\n","from langchain_core.runnables.history import RunnableWithMessageHistory\n","from langchain_core.chat_history import InMemoryChatMessageHistory\n","\n","# Session-Store (Dictionary: session_id -> ChatMessageHistory)\n","session_store = {}\n","\n","def get_session_history(session_id):\n","    \"\"\"Gibt die Historie fur eine Session zuruck (oder erstellt eine neue).\"\"\"\n","    if session_id not in session_store:\n","        session_store[session_id] = InMemoryChatMessageHistory()\n","    return session_store[session_id]"]},{"cell_type":"code","execution_count":7,"id":"db-class","metadata":{"id":"db-class","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770731126538,"user_tz":-60,"elapsed":16,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"582ecc50-f5ee-4a39-ceea-8c3f91d3f3f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Chain mit RunnableWithMessageHistory erstellt\n"]}],"source":["# Prompt mit Historie-Platzhalter\n","prompt_with_history = ChatPromptTemplate.from_messages([\n","    (\"system\", system_prompt),\n","    MessagesPlaceholder(variable_name=\"history\"),  # Hier wird die Historie eingefugt\n","    (\"human\", \"{user_input}\")\n","])\n","\n","# Basis-Chain (ohne Memory)\n","base_chain = prompt_with_history | llm | parser\n","\n","# Chain MIT automatischem Memory-Management\n","chain_with_history = RunnableWithMessageHistory(\n","    runnable=base_chain,\n","    get_session_history=get_session_history,\n","    input_messages_key=\"user_input\",        # Key fur neue User-Nachricht\n","    history_messages_key=\"history\"          # Key fur die Historie im Prompt\n",")\n","\n","print(\"Chain mit RunnableWithMessageHistory erstellt\")"]},{"cell_type":"code","execution_count":8,"id":"db-chatbot","metadata":{"id":"db-chatbot","executionInfo":{"status":"ok","timestamp":1770731126546,"user_tz":-60,"elapsed":0,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Helper-Funktion fur Chat mit Session-ID\n","def chat_with_session(session_id, user_input):\n","    \"\"\"Chattet mit automatischem Memory uber RunnableWithMessageHistory.\"\"\"\n","\n","    # Config mit Session-ID (PFLICHT!)\n","    config = {\"configurable\": {\"session_id\": session_id}}\n","\n","    # Chain aufrufen - Historie wird automatisch geladen/gespeichert\n","    response = chain_with_history.invoke(\n","        {\"user_input\": user_input},\n","        config=config\n","    )\n","\n","    mprint(f\"**[{session_id}] üßë‚Äçü¶± Mensch:** {user_input}\")\n","    mprint(f\"**[{session_id}] ü§ñ KI:** {response}\\n\")\n","\n","    return response"]},{"cell_type":"code","execution_count":null,"id":"db-demo","metadata":{"id":"db-demo","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"1f378476-718a-4c28-93e1-b9dfb7e931ea"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## RunnableWithMessageHistory Demo"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}}],"source":["# Demo: RunnableWithMessageHistory\n","mprint(\"## RunnableWithMessageHistory Demo\")\n","mprint(\"---\")\n","\n","# Session 1: Max\n","chat_with_session(\"max\", \"Hallo! Ich bin Max aus M√ºnchen.\")\n","chat_with_session(\"max\", \"Ich programmiere gerne in Python.\")\n","\n","# Session 2: Emma (separate Historie!)\n","chat_with_session(\"emma\", \"Hi! Ich bin Emma und mag Machine Learning.\")\n","\n","# Zuruck zu Max - Memory bleibt erhalten!\n","chat_with_session(\"max\", \"Woher komme ich und was ist mein Hobby?\")\n","\n","# Session-Store anzeigen\n","mprint(\"### Gespeicherte Sessions:\")\n","for sid, history in session_store.items():\n","    mprint(f\"- **{sid}**: {len(history.messages)} Nachrichten\")"]},{"cell_type":"code","execution_count":null,"id":"db-list","metadata":{"id":"db-list"},"outputs":[],"source":["# Historie einer Session anzeigen\n","def show_session_history(session_id):\n","    \"\"\"Zeigt die Nachrichten einer Session.\"\"\"\n","    if session_id not in session_store:\n","        print(f\"Session '{session_id}' nicht gefunden\")\n","        return\n","\n","    history = session_store[session_id]\n","    mprint(f\"### Historie: {session_id}\")\n","    for i, msg in enumerate(history.messages, 1):\n","        role = \"Human\" if msg.type == \"human\" else \"KI\"\n","        mprint(f\"{i}. **{role}:** {msg.content}\")\n","\n","show_session_history(\"max\")"]},{"cell_type":"markdown","id":"do3fl83wu7w","source":["## 3.2 | Trimming (Sliding Window)"],"metadata":{"id":"do3fl83wu7w"}},{"cell_type":"markdown","id":"b3vpmss8vwc","source":["**Problem:** Bei langen Konversationen w√§chst die Historie unbegrenzt ‚Äì Token-Limit wird √ºberschritten!\n","\n","**L√∂sung:** **Trimming** (Sliding Window) ‚Äì Nur die letzten N Nachrichten werden behalten.\n","\n","```python\n","from langchain_core.messages import trim_messages\n","\n","# Automatisches Trimming: Nur letzte 6 Nachrichten\n","trimmer = RunnableLambda(trim_messages(max_tokens=6, strategy=\"last\"))\n","chain_with_trimming = trimmer | chain_with_history\n","```"],"metadata":{"id":"b3vpmss8vwc"}},{"cell_type":"code","id":"0oejp3tssv5s","source":["# Trimming-Implementierung: Nur letzte N Nachrichten behalten\n","from langchain_core.messages import trim_messages\n","from langchain_core.runnables import RunnableLambda\n","\n","MAX_MESSAGES = 6  # Maximal 6 Nachrichten im Kontext (3 Runden)\n","\n","# Trimmer-Funktion, die das gesamte Input-Dict verarbeitet\n","def apply_trimming(input_dict):\n","    \"\"\"\n","    Verarbeitet das Input-Dictionary und trimmt die Historie.\n","    RunnableWithMessageHistory gibt {\"user_input\": ..., \"history\": [...]} weiter.\n","    \"\"\"\n","    history = input_dict.get(\"history\", [])\n","\n","    # Trimming: Nur letzte N Nachrichten behalten\n","    trimmed_history = trim_messages(\n","        history,\n","        max_tokens=MAX_MESSAGES,\n","        strategy=\"last\",\n","        token_counter=len  # Z√§hle Nachrichten, nicht Tokens\n","    )\n","\n","    # Neues Dictionary mit getrimmter Historie zur√ºckgeben\n","    return {\n","        \"user_input\": input_dict[\"user_input\"],\n","        \"history\": trimmed_history\n","    }\n","\n","# Chain mit Trimming\n","trimmed_chain = (\n","    RunnableLambda(apply_trimming)\n","    | prompt_with_history\n","    | llm\n","    | parser\n",")\n","\n","# Mit RunnableWithMessageHistory wrappen\n","chain_with_trimming = RunnableWithMessageHistory(\n","    runnable=trimmed_chain,\n","    get_session_history=get_session_history,\n","    input_messages_key=\"user_input\",\n","    history_messages_key=\"history\"\n",")\n","\n","print(f\"Chain mit Trimming erstellt (max. {MAX_MESSAGES} Nachrichten)\")"],"metadata":{"id":"0oejp3tssv5s"},"execution_count":null,"outputs":[]},{"cell_type":"code","id":"n1ush3bh8qq","source":["# Demo: Trimming in Aktion\n","mprint(\"## Trimming Demo (max. 6 Nachrichten = 3 Runden)\")\n","mprint(\"---\")\n","\n","config_trim = {\"configurable\": {\"session_id\": \"trim_test\"}}\n","\n","# Runde 1: Info 1\n","response = chain_with_trimming.invoke({\"user_input\": \"Mein Name ist Max.\"}, config=config_trim)\n","mprint(f\"**1. Mensch:** Mein Name ist Max.\")\n","mprint(f\"**1. KI:** {response}\\n\")\n","\n","# Runde 2: Info 2\n","response = chain_with_trimming.invoke({\"user_input\": \"Ich komme aus M√ºnchen.\"}, config=config_trim)\n","mprint(f\"**2. Mensch:** Ich komme aus M√ºnchen.\")\n","mprint(f\"**2. KI:** {response}\\n\")\n","\n","# Runde 3: Info 3\n","response = chain_with_trimming.invoke({\"user_input\": \"Ich mag Python.\"}, config=config_trim)\n","mprint(f\"**3. Mensch:** Ich mag Python.\")\n","mprint(f\"**3. KI:** {response}\\n\")\n","\n","# Runde 4: Info 4 (Jetzt sollte Runde 1 vergessen sein!)\n","response = chain_with_trimming.invoke({\"user_input\": \"Ich arbeite als Data Scientist.\"}, config=config_trim)\n","mprint(f\"**4. Mensch:** Ich arbeite als Data Scientist.\")\n","mprint(f\"**4. KI:** {response}\\n\")\n","\n","# Test: Erinnert sich der Bot an Info 1? (sollte NEIN sein)\n","response = chain_with_trimming.invoke({\"user_input\": \"Wie heisse ich?\"}, config=config_trim)\n","mprint(f\"**5. Mensch:** Wie heisse ich?\")\n","mprint(f\"**5. KI:** {response}\")\n","mprint(\"\\n‚ö†Ô∏è **Ergebnis:** Der Bot erinnert sich NICHT mehr an 'Max' (wurde getrimmt!)\")"],"metadata":{"id":"n1ush3bh8qq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"82q00myyzr4","source":["## 3.3 | Summary (LLM-basierte Zusammenfassung)"],"metadata":{"id":"82q00myyzr4"}},{"cell_type":"markdown","id":"qc07fjh94v","source":["**Problem:** Trimming verwirft alte Informationen vollst√§ndig ‚Äì wichtiger Kontext geht verloren.\n","\n","**Alternative:** **Summarization** ‚Äì Ein LLM fasst alte Nachrichten zusammen, die Zusammenfassung wird als Kontext verwendet.\n","\n","**Strategie:**\n","1. Wenn Historie > Limit: Alte Nachrichten zusammenfassen\n","2. Summary als Teil des System-Prompts einf√ºgen\n","3. Nur letzte N Nachrichten + Summary behalten\n","\n","```python\n","# Pseudo-Code\n","if len(history) > MAX_MESSAGES:\n","    summary = llm.invoke(\"Fasse zusammen: {old_messages}\")\n","    new_history = [summary] + recent_messages\n","```"],"metadata":{"id":"qc07fjh94v"}},{"cell_type":"code","id":"in466rh3juj","source":["# Summary-Implementierung\n","SUMMARY_THRESHOLD = 8  # Ab 8 Nachrichten: Summarize\n","KEEP_RECENT = 4        # Behalte die letzten 4 Nachrichten\n","\n","def summarize_messages(messages):\n","    \"\"\"\n","    Fasst alte Nachrichten zusammen, wenn das Limit √ºberschritten wird.\n","    Beh√§lt nur Summary + letzte N Nachrichten.\n","    \"\"\"\n","    if len(messages) <= SUMMARY_THRESHOLD:\n","        return messages  # Noch unter Limit, nichts tun\n","\n","    # Alte Nachrichten (zum Zusammenfassen)\n","    old_messages = messages[:-KEEP_RECENT]\n","    recent_messages = messages[-KEEP_RECENT:]\n","\n","    # Summary erstellen (LLM-basiert)\n","    summary_prompt = ChatPromptTemplate.from_messages([\n","        (\"system\", \"Fasse die folgende Konversation pr√§gnant zusammen. Extrahiere wichtige Fakten (Namen, Orte, Pr√§ferenzen).\"),\n","        (\"human\", \"{conversation}\")\n","    ])\n","\n","    # Konversation als Text\n","    conversation_text = \"\\n\".join([\n","        f\"{msg.type}: {msg.content}\" for msg in old_messages\n","    ])\n","\n","    # Summary generieren\n","    summary_chain = summary_prompt | llm | parser\n","    summary_text = summary_chain.invoke({\"conversation\": conversation_text})\n","\n","    # Summary als SystemMessage\n","    summary_message = SystemMessage(content=f\"Zusammenfassung der bisherigen Konversation:\\n{summary_text}\")\n","\n","    # Neue Historie: Summary + letzte N Nachrichten\n","    return [summary_message] + recent_messages\n","\n","# Chain mit Summary\n","summary_chain = (\n","    RunnableLambda(summarize_messages)\n","    | prompt_with_history\n","    | llm\n","    | parser\n",")\n","\n","# Mit RunnableWithMessageHistory wrappen\n","chain_with_summary = RunnableWithMessageHistory(\n","    runnable=summary_chain,\n","    get_session_history=get_session_history,\n","    input_messages_key=\"user_input\",\n","    history_messages_key=\"history\"\n",")\n","\n","print(f\"Chain mit Summary erstellt (Limit: {SUMMARY_THRESHOLD}, behalte letzte {KEEP_RECENT})\")"],"metadata":{"id":"in466rh3juj"},"execution_count":null,"outputs":[]},{"cell_type":"code","id":"w2n4frv7a1h","source":["# Demo: Summary in Aktion\n","mprint(\"## Summary Demo (Limit: 8 Nachrichten, behalte 4)\")\n","mprint(\"---\")\n","\n","config_summary = {\"configurable\": {\"session_id\": \"summary_test\"}}\n","\n","# Mehrere Runden simulieren (um Limit zu √ºberschreiten)\n","conversations = [\n","    \"Mein Name ist Max.\",\n","    \"Ich bin 30 Jahre alt.\",\n","    \"Ich komme aus M√ºnchen.\",\n","    \"Ich arbeite als Data Scientist.\",\n","    \"Mein Hobby ist Fotografie.\",\n","    \"Ich mag italienisches Essen.\"  # Nach dieser Nachricht: 12 Messages (6 Runden) -> Summary!\n","]\n","\n","for i, user_msg in enumerate(conversations, 1):\n","    response = chain_with_summary.invoke({\"user_input\": user_msg}, config=config_summary)\n","    mprint(f\"**{i}. Mensch:** {user_msg}\")\n","    mprint(f\"**{i}. KI:** {response}\\n\")\n","\n","# Test: Erinnert sich der Bot an alte Infos? (sollte JA sein, via Summary!)\n","response = chain_with_summary.invoke(\n","    {\"user_input\": \"Fasse zusammen: Wie heisse ich, wie alt bin ich, woher komme ich?\"},\n","    config=config_summary\n",")\n","mprint(f\"**Test. Mensch:** Fasse zusammen: Wie heisse ich, wie alt bin ich, woher komme ich?\")\n","mprint(f\"**Test. KI:** {response}\")\n","mprint(\"\\n‚úÖ **Ergebnis:** Der Bot erinnert sich trotz Summary an alte Infos!\")"],"metadata":{"id":"w2n4frv7a1h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"db-persist","metadata":{"id":"db-persist"},"source":["# 4 | Long-term Memory\n","---"]},{"cell_type":"markdown","id":"wbw7v7aveo","source":["## 4.1 | FileChatMessageHistory (Persistenz in Dateien)"],"metadata":{"id":"wbw7v7aveo"}},{"cell_type":"markdown","id":"g3rkimlxnc4","source":["**Problem:** `InMemoryChatMessageHistory` verliert die Historie beim Neustart der Anwendung.\n","\n","**L√∂sung:** **FileChatMessageHistory** ‚Äì Speichert jede Session in einer separaten JSON-Datei.\n","\n","**Vorteile:**\n","- Einfache Persistenz ohne Datenbank\n","- Menschenlesbare JSON-Files\n","- Ideal f√ºr Prototyping und lokale Apps\n","\n","```python\n","from langchain_community.chat_message_histories import FileChatMessageHistory\n","\n","# Jede session_id bekommt eine eigene Datei\n","history = FileChatMessageHistory(file_path=\"./chat_sessions/session_123.json\")\n","```"],"metadata":{"id":"g3rkimlxnc4"}},{"cell_type":"code","id":"3ab7x4a0ula","source":["# FileChatMessageHistory Setup\n","import os\n","from langchain_community.chat_message_histories import FileChatMessageHistory\n","\n","# Verzeichnis f√ºr Session-Files erstellen\n","SESSION_DIR = \"./chat_sessions\"\n","os.makedirs(SESSION_DIR, exist_ok=True)\n","\n","def get_file_session_history(session_id: str):\n","    \"\"\"\n","    Gibt eine FileChatMessageHistory f√ºr die Session zur√ºck.\n","    Jede Session wird in einer separaten JSON-Datei gespeichert.\n","    \"\"\"\n","    file_path = os.path.join(SESSION_DIR, f\"{session_id}.json\")\n","    return FileChatMessageHistory(file_path=file_path)\n","\n","# Chain mit File-basiertem Memory\n","chain_with_file_history = RunnableWithMessageHistory(\n","    runnable=base_chain,\n","    get_session_history=get_file_session_history,\n","    input_messages_key=\"user_input\",\n","    history_messages_key=\"history\"\n",")\n","\n","print(f\"Chain mit FileChatMessageHistory erstellt (Speicherort: {SESSION_DIR})\")"],"metadata":{"id":"3ab7x4a0ula"},"execution_count":null,"outputs":[]},{"cell_type":"code","id":"y3nqwewjken","source":["# Demo: FileChatMessageHistory\n","mprint(\"## FileChatMessageHistory Demo\")\n","mprint(\"---\")\n","\n","config_file = {\"configurable\": {\"session_id\": \"max_file\"}}\n","\n","# Konversation f√ºhren (wird automatisch in Datei gespeichert)\n","response = chain_with_file_history.invoke(\n","    {\"user_input\": \"Hallo! Ich bin Max aus M√ºnchen.\"},\n","    config=config_file\n",")\n","mprint(f\"**Mensch:** Hallo! Ich bin Max aus M√ºnchen.\")\n","mprint(f\"**KI:** {response}\\n\")\n","\n","response = chain_with_file_history.invoke(\n","    {\"user_input\": \"Ich mag Python-Programmierung.\"},\n","    config=config_file\n",")\n","mprint(f\"**Mensch:** Ich mag Python-Programmierung.\")\n","mprint(f\"**KI:** {response}\\n\")\n","\n","# Test: Memory-Recall\n","response = chain_with_file_history.invoke(\n","    {\"user_input\": \"Woher komme ich und was mag ich?\"},\n","    config=config_file\n",")\n","mprint(f\"**Mensch:** Woher komme ich und was mag ich?\")\n","mprint(f\"**KI:** {response}\")"],"metadata":{"id":"y3nqwewjken"},"execution_count":null,"outputs":[]},{"cell_type":"code","id":"ed1nnrofgqk","source":["# Gespeicherte Session-Files anzeigen\n","mprint(\"### Gespeicherte Session-Dateien:\")\n","mprint(\"---\")\n","\n","for filename in os.listdir(SESSION_DIR):\n","    if filename.endswith(\".json\"):\n","        filepath = os.path.join(SESSION_DIR, filename)\n","        file_size = os.path.getsize(filepath)\n","        mprint(f\"- **{filename}** ({file_size} Bytes)\")"],"metadata":{"id":"ed1nnrofgqk"},"execution_count":null,"outputs":[]},{"cell_type":"code","id":"g70od0ptwct","source":["# Test: Persistenz nach \"Neustart\" (neue Chain-Instanz)\n","mprint(\"### Test: Persistenz-Check\")\n","mprint(\"---\")\n","\n","# Neue Chain-Instanz erstellen (simuliert Neustart)\n","chain_after_restart = RunnableWithMessageHistory(\n","    runnable=base_chain,\n","    get_session_history=get_file_session_history,\n","    input_messages_key=\"user_input\",\n","    history_messages_key=\"history\"\n",")\n","\n","# Sollte die gespeicherte Historie aus der Datei laden!\n","response = chain_after_restart.invoke(\n","    {\"user_input\": \"Kannst du mich noch an meine bisherigen Infos erinnern?\"},\n","    config=config_file\n",")\n","mprint(f\"**Mensch (nach 'Neustart'):** Kannst du mich noch an meine bisherigen Infos erinnern?\")\n","mprint(f\"**KI:** {response}\")\n","mprint(\"\\n‚úÖ **Ergebnis:** Historie wurde erfolgreich aus der Datei geladen!\")"],"metadata":{"id":"g70od0ptwct"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.2 | SQLite (Datenbank-Persistenz)"],"metadata":{"id":"lwR4MVUxfBcY"},"id":"lwR4MVUxfBcY"},{"cell_type":"code","id":"2wes9llitgg","source":["import sqlite3\n","from datetime import datetime\n","from typing import List, Dict, Optional\n","\n","DB_PATH = \"./chat_memory.db\""],"metadata":{"id":"2wes9llitgg"},"execution_count":null,"outputs":[]},{"cell_type":"code","id":"lt015mlb76i","source":["class ChatMemoryDB:\n","    \"\"\"Einfache Chat-Memory-Datenbank mit SQLite.\"\"\"\n","\n","    def __init__(self, db_path: str = DB_PATH):\n","        self.db_path = db_path\n","        self._init_db()\n","\n","    def _init_db(self):\n","        \"\"\"Erstellt die Tabellen falls nicht vorhanden.\"\"\"\n","        with sqlite3.connect(self.db_path) as conn:\n","            conn.execute(\"\"\"\n","                CREATE TABLE IF NOT EXISTS messages (\n","                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n","                    thread_id TEXT NOT NULL,\n","                    role TEXT NOT NULL,\n","                    content TEXT NOT NULL,\n","                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n","                )\n","            \"\"\")\n","            conn.execute(\"\"\"\n","                CREATE TABLE IF NOT EXISTS summaries (\n","                    thread_id TEXT PRIMARY KEY,\n","                    summary TEXT,\n","                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP\n","                )\n","            \"\"\")\n","            conn.execute(\"CREATE INDEX IF NOT EXISTS idx_thread ON messages(thread_id)\")\n","            conn.commit()\n","        print(f\"Datenbank initialisiert: {self.db_path}\")\n","\n","    def save_message(self, thread_id: str, role: str, content: str):\n","        \"\"\"Speichert eine Nachricht.\"\"\"\n","        with sqlite3.connect(self.db_path) as conn:\n","            conn.execute(\n","                \"INSERT INTO messages (thread_id, role, content) VALUES (?, ?, ?)\",\n","                (thread_id, role, content)\n","            )\n","            conn.commit()\n","\n","    def get_history(self, thread_id: str, limit: Optional[int] = None) -> List[Dict]:\n","        \"\"\"Ladt die Historie eines Threads.\"\"\"\n","        with sqlite3.connect(self.db_path) as conn:\n","            if limit:\n","                rows = conn.execute(\n","                    \"SELECT role, content FROM messages WHERE thread_id = ? ORDER BY id DESC LIMIT ?\",\n","                    (thread_id, limit)\n","                ).fetchall()\n","                rows = list(reversed(rows))\n","            else:\n","                rows = conn.execute(\n","                    \"SELECT role, content FROM messages WHERE thread_id = ? ORDER BY id\",\n","                    (thread_id,)\n","                ).fetchall()\n","        return [{\"role\": r[0], \"content\": r[1]} for r in rows]\n","\n","    def get_message_count(self, thread_id: str) -> int:\n","        \"\"\"Zahlt die Nachrichten eines Threads.\"\"\"\n","        with sqlite3.connect(self.db_path) as conn:\n","            count = conn.execute(\n","                \"SELECT COUNT(*) FROM messages WHERE thread_id = ?\",\n","                (thread_id,)\n","            ).fetchone()[0]\n","        return count\n","\n","    def save_summary(self, thread_id: str, summary: str):\n","        \"\"\"Speichert/aktualisiert die Zusammenfassung eines Threads.\"\"\"\n","        with sqlite3.connect(self.db_path) as conn:\n","            conn.execute(\"\"\"\n","                INSERT OR REPLACE INTO summaries (thread_id, summary, updated_at)\n","                VALUES (?, ?, CURRENT_TIMESTAMP)\n","            \"\"\", (thread_id, summary))\n","            conn.commit()\n","\n","    def get_summary(self, thread_id: str) -> Optional[str]:\n","        \"\"\"Ladt die Zusammenfassung eines Threads.\"\"\"\n","        with sqlite3.connect(self.db_path) as conn:\n","            row = conn.execute(\n","                \"SELECT summary FROM summaries WHERE thread_id = ?\",\n","                (thread_id,)\n","            ).fetchone()\n","        return row[0] if row else None\n","\n","    def list_threads(self) -> List[Dict]:\n","        \"\"\"Listet alle Threads mit Statistiken.\"\"\"\n","        with sqlite3.connect(self.db_path) as conn:\n","            rows = conn.execute(\"\"\"\n","                SELECT thread_id, COUNT(*) as msg_count, MAX(timestamp) as last_msg\n","                FROM messages\n","                GROUP BY thread_id\n","                ORDER BY last_msg DESC\n","            \"\"\").fetchall()\n","        return [{\"thread_id\": r[0], \"messages\": r[1], \"last_activity\": r[2]} for r in rows]\n","\n","    def delete_thread(self, thread_id: str):\n","        \"\"\"Loscht einen Thread.\"\"\"\n","        with sqlite3.connect(self.db_path) as conn:\n","            conn.execute(\"DELETE FROM messages WHERE thread_id = ?\", (thread_id,))\n","            conn.execute(\"DELETE FROM summaries WHERE thread_id = ?\", (thread_id,))\n","            conn.commit()"],"metadata":{"id":"lt015mlb76i"},"execution_count":null,"outputs":[]},{"cell_type":"code","id":"f9cx08h8yll","source":["class PersistentChatbot:\n","    \"\"\"Chatbot mit persistentem Memory uber SQLite.\"\"\"\n","\n","    def __init__(self, db_path: str = DB_PATH, max_context: int = 10):\n","        self.db = ChatMemoryDB(db_path)\n","        self.max_context = max_context\n","        self.llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0.7)\n","        self.system_prompt = \"Du bist ein hilfreicher KI-Assistent mit Gedachtnis.\"\n","\n","    def _history_to_messages(self, history: List[Dict]) -> list:\n","        \"\"\"Konvertiert DB-Historie zu LangChain-Messages.\"\"\"\n","        messages = []\n","        for item in history:\n","            if item[\"role\"] == \"human\":\n","                messages.append(HumanMessage(content=item[\"content\"]))\n","            else:\n","                messages.append(AIMessage(content=item[\"content\"]))\n","        return messages\n","\n","    def chat(self, thread_id: str, user_input: str) -> str:\n","        \"\"\"Sendet eine Nachricht und speichert die Antwort.\"\"\"\n","\n","        # Historie laden (mit Limit)\n","        history = self.db.get_history(thread_id, limit=self.max_context)\n","        summary = self.db.get_summary(thread_id)\n","\n","        # System-Prompt mit Summary erweitern\n","        enhanced_system = self.system_prompt\n","        if summary:\n","            enhanced_system = f\"{self.system_prompt}\\n\\nKontext aus fruheren Gesprachen: {summary}\"\n","\n","        # Messages zusammenbauen\n","        messages = [SystemMessage(content=enhanced_system)]\n","        messages.extend(self._history_to_messages(history))\n","        messages.append(HumanMessage(content=user_input))\n","\n","        # LLM aufrufen\n","        response = self.llm.invoke(messages)\n","        response_text = response.content\n","\n","        # In DB speichern\n","        self.db.save_message(thread_id, \"human\", user_input)\n","        self.db.save_message(thread_id, \"ai\", response_text)\n","\n","        return response_text\n","\n","    def show_history(self, thread_id: str):\n","        \"\"\"Zeigt die Historie eines Threads.\"\"\"\n","        history = self.db.get_history(thread_id)\n","        mprint(f\"### Thread: {thread_id} ({len(history)} Nachrichten)\")\n","        mprint(\"---\")\n","        for i, msg in enumerate(history, 1):\n","            role = \"Human\" if msg[\"role\"] == \"human\" else \"KI\"\n","            mprint(f\"{i}. **{role}:** {msg['content']}\")\n","\n","    def list_threads(self):\n","        \"\"\"Listet alle Threads.\"\"\"\n","        threads = self.db.list_threads()\n","        mprint(\"### Alle Threads:\")\n","        mprint(\"---\")\n","        for t in threads:\n","            mprint(f\"- **{t['thread_id']}**: {t['messages']} Nachrichten (zuletzt: {t['last_activity']})\")"],"metadata":{"id":"f9cx08h8yll"},"execution_count":null,"outputs":[]},{"cell_type":"code","id":"f0askgykshw","source":["# Demo: Persistenter Chatbot\n","mprint(\"## SQLite-Chatbot Demo\")\n","mprint(\"---\")\n","\n","bot = PersistentChatbot()\n","\n","# Thread 1: Max\n","print(\"\\n--- Thread: max_session ---\")\n","response = bot.chat(\"max_session\", \"Hallo! Ich bin Max und komme aus Munchen.\")\n","mprint(f\"**KI:** {response}\")\n","\n","response = bot.chat(\"max_session\", \"Ich interessiere mich fur Machine Learning.\")\n","mprint(f\"**KI:** {response}\")\n","\n","# Thread 2: Emma\n","print(\"\\n--- Thread: emma_session ---\")\n","response = bot.chat(\"emma_session\", \"Hi! Ich bin Emma aus Berlin.\")\n","mprint(f\"**KI:** {response}\")\n","\n","# Zuruck zu Max - Memory bleibt erhalten!\n","print(\"\\n--- Zuruck zu max_session ---\")\n","response = bot.chat(\"max_session\", \"Woher komme ich nochmal?\")\n","mprint(f\"**KI:** {response}\")"],"metadata":{"id":"f0askgykshw"},"execution_count":null,"outputs":[]},{"cell_type":"code","id":"7pgcfw3i1kx","source":["# Alle Threads anzeigen\n","bot.list_threads()"],"metadata":{"id":"7pgcfw3i1kx"},"execution_count":null,"outputs":[]},{"cell_type":"code","id":"8gsbb6kjlbi","source":["# Historie eines Threads anzeigen\n","bot.show_history(\"max_session\")"],"metadata":{"id":"8gsbb6kjlbi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"1ut97tdwl86","source":["<p><font color='darkblue' size=\"4\">\n","Test: Neustart-Persistenz\n","</font></p>\n","\n","Die Daten bleiben auch nach Neustart erhalten. Fuhren Sie die nachste Zelle aus, um zu testen:"],"metadata":{"id":"1ut97tdwl86"}},{"cell_type":"code","id":"l4f6rj1cpog","source":["# Test: Neuer Bot-Instance, gleiche Datenbank\n","bot2 = PersistentChatbot()\n","\n","# Sollte Max's Historie kennen!\n","response = bot2.chat(\"max_session\", \"Was war mein Interesse nochmal?\")\n","mprint(f\"**KI (nach 'Neustart'):** {response}\")\n","\n","# Historie anzeigen\n","bot2.show_history(\"max_session\")"],"metadata":{"id":"l4f6rj1cpog"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"tasks-header","metadata":{"id":"tasks-header"},"source":["# A | Aufgaben\n","---"]},{"cell_type":"markdown","id":"task-1","metadata":{"id":"task-1"},"source":["<p><font color='black' size=\"5\">\n","Aufgabe 1: Trimming-Limit testen\n","</font></p>\n","\n","**Schwierigkeit:** 1/5\n","\n","Andern Sie `MAX_MESSAGES` auf 4 und fuhren Sie eine langere Konversation. Beobachten Sie, wann Informationen verloren gehen."]},{"cell_type":"markdown","id":"task-2","metadata":{"id":"task-2"},"source":["<p><font color='black' size=\"5\">\n","Aufgabe 2: Summary-Qualitat verbessern\n","</font></p>\n","\n","**Schwierigkeit:** 2/5\n","\n","Verbessern Sie den `summary_prompt`, um wichtige Informationen (Namen, Orte, Praferenzen) besser zu extrahieren."]},{"cell_type":"markdown","id":"task-3","metadata":{"id":"task-3"},"source":["<p><font color='black' size=\"5\">\n","Aufgabe 3: Interaktiver CLI-Chatbot\n","</font></p>\n","\n","**Schwierigkeit:** 3/5\n","\n","Erweitern Sie `PersistentChatbot` um eine interaktive Schleife mit Befehlen:\n","- `exit` - Beenden\n","- `history` - Historie anzeigen\n","- `new` - Neuen Thread starten\n","- `threads` - Alle Threads listen"]},{"cell_type":"markdown","id":"task-4","metadata":{"id":"task-4"},"source":["<p><font color='black' size=\"5\">\n","Aufgabe 4: Hybrid Memory (Trimming + Summary + DB)\n","</font></p>\n","\n","**Schwierigkeit:** 4/5\n","\n","Kombinieren Sie alle drei Strategien:\n","1. Speicherung in SQLite\n","2. Automatisches Trimming auf die letzten N Nachrichten\n","3. Zusammenfassung der alteren Nachrichten (in DB gespeichert)"]},{"cell_type":"markdown","id":"r4e93hrgyre","source":["# B | Datenbank auslesen\n","---"],"metadata":{"id":"r4e93hrgyre"}},{"cell_type":"markdown","id":"9h44bm4iwc5","source":["Dieser Abschnitt zeigt, wie die SQLite-Datenbank (`chat_memory.db`) direkt ausgelesen werden kann - nutzlich fur Debugging, Analyse oder Export."],"metadata":{"id":"9h44bm4iwc5"}},{"cell_type":"code","id":"czndm30m5hq","source":["import sqlite3\n","import os\n","\n","def read_all_threads_from_db(db_path: str = DB_PATH):\n","    \"\"\"\n","    Liest alle Threads und Nachrichten aus der chat_memory.db Datenbank.\n","    \"\"\"\n","    if not os.path.exists(db_path):\n","        print(f\"Fehler: Datenbankdatei '{db_path}' wurde nicht gefunden.\")\n","        return\n","\n","    mprint(f\"### Lese Datenbank: {db_path}\")\n","    mprint(\"---\")\n","\n","    with sqlite3.connect(db_path) as conn:\n","        # Alle Threads mit Statistiken\n","        threads = conn.execute(\"\"\"\n","            SELECT thread_id, COUNT(*) as msg_count, MAX(timestamp) as last_msg\n","            FROM messages\n","            GROUP BY thread_id\n","            ORDER BY last_msg DESC\n","        \"\"\").fetchall()\n","\n","        if not threads:\n","            print(\"Keine Threads in der Datenbank gefunden.\")\n","            return\n","\n","        mprint(f\"**{len(threads)} Threads gefunden**\\n\")\n","\n","        # Jeden Thread mit Nachrichten anzeigen\n","        for thread_id, msg_count, last_msg in threads:\n","            mprint(f\"#### Thread: {thread_id}\")\n","            mprint(f\"*{msg_count} Nachrichten, zuletzt: {last_msg}*\\n\")\n","\n","            # Nachrichten des Threads\n","            messages = conn.execute(\"\"\"\n","                SELECT role, content, timestamp\n","                FROM messages\n","                WHERE thread_id = ?\n","                ORDER BY id\n","            \"\"\", (thread_id,)).fetchall()\n","\n","            for i, (role, content, ts) in enumerate(messages, 1):\n","                role_display = \"Human\" if role == \"human\" else \"KI\"\n","                # Inhalt kurzen wenn zu lang\n","                content_short = content[:100] + \"...\" if len(content) > 100 else content\n","                mprint(f\"{i}. **{role_display}:** {content_short}\")\n","\n","            # Zusammenfassung (falls vorhanden)\n","            summary = conn.execute(\n","                \"SELECT summary FROM summaries WHERE thread_id = ?\",\n","                (thread_id,)\n","            ).fetchone()\n","\n","            if summary and summary[0]:\n","                mprint(f\"\\n*Zusammenfassung:* {summary[0][:150]}...\")\n","\n","            mprint(\"\")  # Leerzeile zwischen Threads"],"metadata":{"id":"czndm30m5hq"},"execution_count":null,"outputs":[]},{"cell_type":"code","id":"t62wa5e3aac","source":["# Alle Threads aus der Datenbank auslesen\n","read_all_threads_from_db()"],"metadata":{"id":"t62wa5e3aac"},"execution_count":null,"outputs":[]},{"cell_type":"code","id":"3b14guf2r17","source":["import json\n","\n","def export_thread_to_json(thread_id: str, db_path: str = DB_PATH) -> dict:\n","    \"\"\"Exportiert einen Thread als JSON.\"\"\"\n","    with sqlite3.connect(db_path) as conn:\n","        messages = conn.execute(\"\"\"\n","            SELECT role, content, timestamp\n","            FROM messages\n","            WHERE thread_id = ?\n","            ORDER BY id\n","        \"\"\", (thread_id,)).fetchall()\n","\n","        summary = conn.execute(\n","            \"SELECT summary FROM summaries WHERE thread_id = ?\",\n","            (thread_id,)\n","        ).fetchone()\n","\n","    data = {\n","        \"thread_id\": thread_id,\n","        \"messages\": [\n","            {\"role\": r, \"content\": c, \"timestamp\": t}\n","            for r, c, t in messages\n","        ],\n","        \"summary\": summary[0] if summary else None\n","    }\n","\n","    return data\n","\n","# Beispiel: Thread als JSON exportieren\n","thread_data = export_thread_to_json(\"max_session\")\n","print(json.dumps(thread_data, indent=2, ensure_ascii=False))"],"metadata":{"id":"3b14guf2r17"},"execution_count":null,"outputs":[]},{"cell_type":"code","id":"qmm10fc57u7","source":["def delete_all_threads(db_path: str = DB_PATH):\n","    \"\"\"Loscht alle Threads aus der Datenbank (Cleanup).\"\"\"\n","    with sqlite3.connect(db_path) as conn:\n","        conn.execute(\"DELETE FROM messages\")\n","        conn.execute(\"DELETE FROM summaries\")\n","        conn.commit()\n","    print(f\"Alle Threads geloscht aus: {db_path}\")\n","\n","# Auskommentiert, um versehentliches Loschen zu verhindern:\n","# delete_all_threads()"],"metadata":{"id":"qmm10fc57u7"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.0"},"colab":{"provenance":[],"toc_visible":true,"collapsed_sections":["intro-header","shortterm-header","db-header","wbw7v7aveo","lwR4MVUxfBcY","tasks-header","r4e93hrgyre"]}},"nbformat":4,"nbformat_minor":5}
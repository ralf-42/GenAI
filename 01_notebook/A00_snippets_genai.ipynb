{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "IQm3u7mNZc6b",
    "qSG-9nZaZj3q",
    "8yUdGhEHadmI",
    "DnLL7z-4ocSH",
    "0hI3GKnNbJ99",
    "23F6baGU4Cq2",
    "3464d8Glm0yi",
    "Hgn0xlXqDOyk",
    "ccg_cmsN4chQ",
    "Ku-TomzW498G",
    "7KOoPcwW5H2i",
    "0QVN1QOa5kNr",
    "8gaiVjttbmno",
    "K0TgYNpviMDn",
    "fGd1iijaiSWo",
    "ljyypqKlFt11",
    "ayAtyWH0eH6Y",
    "N9rNF-O1i0cT",
    "bRQRxrNZw_7C",
    "6AXueZa4xPgr",
    "w53Qv0dCmBaH",
    "XWKhIzSIbs5M",
    "9zv-Rce0fElT",
    "BWt1r_ruguI0",
    "lWa5rMj3G2Wb",
    "8wd5ZR15bxf7",
    "og_moM7Qk-Vh",
    "6NaQWfRFl_vz",
    "R9u3hT1RIA6Z",
    "iFllFW8wb080",
    "Ej4ajzVLIf00",
    "sox0Vtx-ItWK",
    "8TaogQLhb62v",
    "ymeJjPk-OMQS",
    "EqHStIrLOeX4",
    "oNfzqKrDOhaz",
    "gqhIYi29Okn_",
    "uFpRtFTmOnhv",
    "VB7mOeCtOw8x",
    "h_CPWZO5Swkl",
    "i-YaukXicEYQ",
    "qoaSA0q_b-yJ",
    "z6a4og9mcJXd"
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<p><font size=\"6\" color='grey'> <b>\n",
    "\n",
    "Generative KI. Verstehen. Anwenden. Gestalten.\n",
    "</b></font> </br></p>"
   ],
   "metadata": {
    "id": "37rMyCQz2JNw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font size=\"5\" color='grey'> <b>\n",
    "Code-Snippets\n",
    "</b></font> </br></p>\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "R5CfUEMJdvFQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0 | Setup & Konfiguration\n",
    "---"
   ],
   "metadata": {
    "id": "IQm3u7mNZc6b"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0.1 | Install Kursbibliothek üìå"
   ],
   "metadata": {
    "id": "qSG-9nZaZj3q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul"
   ],
   "metadata": {
    "id": "zYXfr5UlZnfk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0.2 | Import Kursbibliothek üìå"
   ],
   "metadata": {
    "id": "8yUdGhEHadmI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages, load_prompt"
   ],
   "metadata": {
    "id": "YxBd5eXianVB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0.3 | Kursbibliothek - Inhalt"
   ],
   "metadata": {
    "id": "DnLL7z-4ocSH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`utilities`"
   ],
   "metadata": {
    "id": "coqtWyL9tH0m"
   }
  },
  {
   "cell_type": "markdown",
   "source": "| Funktion                                         | Zweck / Beschreibung                                                                                                             |\n| ------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------- |\n| `check_environment()`                            | Zeigt die aktuelle Python-Version und installierte LangChain-/LangGraph-Pakete an. Unterdr√ºckt st√∂rende Warnungen im Notebook.   |\n| `install_packages(packages)`                     | Installiert fehlende Python-Pakete automatisch √ºber `uv pip` (z. B. in Google Colab). Erkennt, ob ein Paket schon vorhanden ist. |\n| `setup_api_keys(key_names, create_globals=True)` | Liest API-Keys aus Colab-`userdata`, speichert sie als Umgebungsvariablen und optional als globale Variablen.                    |\n| `get_ipinfo()`                                   | Holt Geoinformationen zur aktuellen √∂ffentlichen IP (Land, Stadt, Provider etc.) √ºber ipinfo.io.                                 |\n| `mprint(text)`                                   | Gibt Text im Markdown-Format in Jupyter- oder Colab-Notebooks aus (z. B. f√ºr formatierte Kursausgaben).                          |\n| `load_prompt(path, mode=\"T\")`                    | L√§dt Prompt-Vorlagen aus Markdown-Dateien (.md) oder GitHub-Links. Mode `\"T\"` ‚Üí `ChatPromptTemplate`, `\"S\"` ‚Üí String (ohne Frontmatter). |",
   "metadata": {
    "id": "if6jsGTQtBnV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`multimodal_rag`"
   ],
   "metadata": {
    "id": "UUueZ5LItL4J"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Funktion                                             | Zweck / Beschreibung                                                                                                  |\n",
    "| ---------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- |\n",
    "| `init_rag_system(config=None)`                       | Initialisiert das gesamte RAG-System mit Embeddings, LLMs, Textsplittern und Datenbank (Chroma).                      |\n",
    "| `process_directory(components, directory, ...)`      | Liest ein ganzes Verzeichnis ein, extrahiert Text aus Dateien und f√ºgt Bilder mit automatischen Beschreibungen hinzu. |\n",
    "| `generate_image_description(vision_llm, image_path)` | Erstellt automatisch eine Bildbeschreibung mit einem multimodalen Modell (z. B. GPT-4o).                              |\n",
    "| `add_text_document(...)`                             | Konvertiert Textdateien (PDF, DOCX, TXT etc.) und f√ºgt sie als Dokumente in die Datenbank ein.                        |\n",
    "| `add_image_with_description(...)`                    | Erzeugt CLIP-Embeddings f√ºr ein Bild und verkn√ºpft sie mit einer generierten Textbeschreibung.                        |\n",
    "| `search_texts(...)`                                  | F√ºhrt eine semantische Textsuche in den gespeicherten Dokumenten durch ‚Äì inklusive Bildbeschreibungen.                |\n",
    "| `search_images(...)`                                 | Sucht Bilder, die zu einer Textanfrage passen (Text ‚Üí Bild √ºber CLIP).                                                |\n",
    "| `search_similar_images(...)`                         | Findet visuell √§hnliche Bilder (Bild ‚Üí Bild).                                                                         |\n",
    "| `search_text_by_image(...)`                          | Verkn√ºpft Bildsuche mit Textretrieval (Bild ‚Üí Text).                                                                  |\n",
    "| `multimodal_search_by_image(...)`                    | Erweiterte kombinierte Suche √ºber mehrere Modalit√§ten (Bild, Text, Cross-Modal).                                      |"
   ],
   "metadata": {
    "id": "7kjGyzHRtPih"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`mcp`"
   ],
   "metadata": {
    "id": "zfuQPFHVtUOV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Funktion                                                                                         | Zweck / Beschreibung                                                                       |\n",
    "| ------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ |\n",
    "| `handle_mcp_request(request)`                                                                    | Haupt-Handler, der eingehende MCP-Anfragen verarbeitet (Initialisierung, Toolaufruf etc.). |\n",
    "| `read_file_tool(filepath)` / `write_file_tool(filepath, content)` / `list_files_tool(directory)` | Beispieltools: Datei lesen, schreiben, oder Verzeichnisinhalt auflisten.                   |\n",
    "| `get_system_info_tool()`                                                                         | Gibt Systeminformationen zur√ºck (OS, Python-Version etc.).                                 |\n",
    "| `register_new_tool(name, description, parameters, function)`                                     | Erm√∂glicht das Hinzuf√ºgen neuer Tools zur Laufzeit.                                        |\n",
    "| `setup_full_connection(server_name, handler)`                                                    | Baut eine vollst√§ndige Client-Server-Verbindung auf (Connect, Initialize, Discover).       |\n",
    "| `call_server_tool(server_name, tool_name, arguments)`                                            | F√ºhrt ein Tool auf einem verbundenen Server aus.                                           |\n",
    "| `process_user_query(query, use_mcp=True)`                                                        | Nimmt Benutzeranfragen entgegen, erkennt eingebettete MCP-Aufrufe und f√ºhrt sie aus.       |\n",
    "| `extract_mcp_calls_from_text(text)`                                                              | Liest Befehle wie `[MCP_CALL: read_file({...})]` aus einem KI-Prompt.                      |\n",
    "| `execute_mcp_calls_for_assistant(calls)`                                                         | F√ºhrt alle im Text gefundenen MCP-Aufrufe aus und sammelt die Ergebnisse.                  |"
   ],
   "metadata": {
    "id": "GMhWAZ5stULd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "üéØ Typische Anwendung"
   ],
   "metadata": {
    "id": "g_WI9yZkRgA5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```python\n",
    "# 1. Umgebung pr√ºfen und API-Keys laden\n",
    "check_environment()\n",
    "setup_api_keys([\"OPENAI_API_KEY\"])\n",
    "\n",
    "```"
   ],
   "metadata": {
    "id": "XeJ4r80YRWFE"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ECn30MyOslNs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0.4 | Umgebung einrichten üìå"
   ],
   "metadata": {
    "id": "0hI3GKnNbJ99"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n",
    "!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul\n",
    "from genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages, load_prompt\n",
    "setup_api_keys(['OPENAI_API_KEY'], create_globals=False)\n",
    "print()\n",
    "check_environment()\n",
    "print()\n",
    "get_ipinfo()"
   ],
   "metadata": {
    "id": "xlCNUpx_bOqq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0.5 | Standard-Importe f√ºr GenAI"
   ],
   "metadata": {
    "id": "23F6baGU4Cq2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Moderne LangChain-Imports (Version 1.0+)\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "\n",
    "# Standard-Python-Imports\n",
    "import os\n",
    "from IPython.display import Markdown, display"
   ],
   "metadata": {
    "id": "aTr6UN0h4IG-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0.6 | Namenskonventionen"
   ],
   "metadata": {
    "id": "3464d8Glm0yi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Empfohlene Namenskonventionen (snake_case):**\n",
    "\n",
    "**Variablen:**\n",
    "- `user_input` - Benutzereingabe\n",
    "- `system_prompt` - Systemnachricht\n",
    "- `chat_history` - Chatverlauf\n",
    "- `model_name` - Modellname\n",
    "- `temperature` - Temperatur-Parameter\n",
    "- `response` - API-Antwort\n",
    "- `result` - Verarbeitetes Ergebnis\n",
    "\n",
    "**LangChain-Objekte:**\n",
    "- `prompt_template` - Prompt-Vorlage\n",
    "- `prompt` - Fertiger Prompt\n",
    "- `llm` - Sprachmodell\n",
    "- `chain` - Verarbeitungskette\n",
    "- `parser` - Output-Parser"
   ],
   "metadata": {
    "id": "JHxMlnzWm4p8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0.7 | Temperatur-Guidelines"
   ],
   "metadata": {
    "id": "Hgn0xlXqDOyk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Temperatur-Richtlinien f√ºr KI-Modelle**\n",
    "\n",
    "| Temperatur | Kategorie | Wert | Anwendungsbereich | Beschreibung |\n",
    "|------------|-----------|------|-------------------|--------------|\n",
    "| **Faktisch** | `factual` | 0.0 | Mathematik, Code, Datenanalyse | Deterministische Ausgaben f√ºr pr√§zise, reproduzierbare Ergebnisse |\n",
    "| **Ausgewogen** | `balanced` | 0.3 | Allgemeine Fragen, Erkl√§rungen | Leicht variierte Antworten bei gleichbleibender Genauigkeit |\n",
    "| **Kreativ** | `creative` | 0.7 | Brainstorming, Texterstellung | Vielf√§ltige und innovative Antworten f√ºr kreative Aufgaben |\n",
    "| **Sehr kreativ** | `very_creative` | 1.0 | Experimentelle Inhalte | Maximale Kreativit√§t und Unvorhersagbarkeit |\n",
    "\n",
    "**Verwendungshinweise**\n",
    "\n",
    "- **Niedrige Temperatur (0.0-0.3)**: F√ºr sachliche, pr√§zise Antworten\n",
    "- **Mittlere Temperatur (0.4-0.6)**: F√ºr ausgewogene Kommunikation\n",
    "- **Hohe Temperatur (0.7-1.0)**: F√ºr kreative und experimentelle Ausgaben\n",
    "\n"
   ],
   "metadata": {
    "id": "ce4MVxa6DXm-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 | Erste Schritte\n",
    "---"
   ],
   "metadata": {
    "id": "ccg_cmsN4chQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.1 | Einfachster LLM-Aufruf"
   ],
   "metadata": {
    "id": "Ku-TomzW498G"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Einfachster LLM-Aufruf\n",
    "model_provider=\"openai\"\n",
    "model_name = \"gpt-4o-mini\"\n",
    "temperature=0.0\n",
    "llm = init_chat_model(model_name, model_provider=model_provider, temperature=temperature)\n",
    "\n",
    "user_input = \"Erkl√§re mir KI in einem Satz\"\n",
    "\n",
    "# Direkter Aufruf\n",
    "response = llm.invoke([HumanMessage(content=user_input)])\n",
    "print(response.content)"
   ],
   "metadata": {
    "id": "i7cIWKYm5FD1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.2 | Mit System-Prompt"
   ],
   "metadata": {
    "id": "7KOoPcwW5H2i"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# System-Prompt mit modernem Ansatz\n",
    "system_prompt = \"Du bist ein hilfreicher KI-Assistent. Antworte kurz und pr√§zise.\"\n",
    "user_input = \"Was ist Machine Learning?\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=user_input)\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ],
   "metadata": {
    "id": "roPof5IR5NPZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.3 | Streaming-Ausgabe"
   ],
   "metadata": {
    "id": "0QVN1QOa5kNr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Streaming f√ºr Echtzeit-Ausgabe\n",
    "print(\"ü§ñ Antwort: \", end=\"\")\n",
    "for chunk in llm.stream([HumanMessage(content=\"Erz√§hle eine kurze Geschichte √ºber KI\")]):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "print(\"\\n\")"
   ],
   "metadata": {
    "id": "EJVCuJD05pky"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 | Prompts & Templates\n",
    "---"
   ],
   "metadata": {
    "id": "8gaiVjttbmno"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.1 | Einfacher String-Prompt"
   ],
   "metadata": {
    "id": "K0TgYNpviMDn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Einfacher String-Prompt\n",
    "frage = \"Wer ist Albert Einstein?\"\n",
    "user_input = f\"Du bist ein hilfreicher Assistent. Beantworte die Frage: {frage}\"\n",
    "\n",
    "# Als HumanMessage verwenden\n",
    "response = llm.invoke([HumanMessage(content=user_input)])\n",
    "print(f\"üéØ Antwort: {response.content}\")"
   ],
   "metadata": {
    "id": "Crwgzo0XiRHt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.2 | PromptTemplate"
   ],
   "metadata": {
    "id": "fGd1iijaiSWo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Prompt definieren\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Systemanweisung: {system_prompt}\\n\"\n",
    "    \"Benutzereingabe: {input}\\n\"\n",
    "    \"Antwort:\"\n",
    ")"
   ],
   "metadata": {
    "id": "6NEV8D11iViQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.3 | ChatPromptTemplate"
   ],
   "metadata": {
    "id": "ljyypqKlFt11"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Modernes ChatPromptTemplate (einfache Version)\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein hilfreicher Assistent.\"),\n",
    "    (\"human\", \"{user_input}\")\n",
    "])"
   ],
   "metadata": {
    "id": "tdd5vchMFzml"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Direkt mit Chain verwenden (moderne LCEL-Syntax)\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "result = chain.invoke({\"user_input\": \"Was ist Machine Learning?\"})\n",
    "\n",
    "print(f\"üéØ Ergebnis: {result}\")"
   ],
   "metadata": {
    "id": "OsYo1K4TF2VT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.4 | ChatPromptTemplate - History üìå"
   ],
   "metadata": {
    "id": "ayAtyWH0eH6Y"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "\n",
    "# Erweitert mit Chat-History\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"{system_prompt}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{user_input}\")\n",
    "])\n",
    "\n",
    "# Model ...\n",
    "\n",
    "# Parser ...\n",
    "\n",
    "# Moderne Chain-Syntax\n",
    "chain = prompt_template | llm | parser\n",
    "\n",
    "# ... weitere Code ...\n",
    "chat_history = []\n",
    "input_variables = {\n",
    "    \"system_prompt\": system_prompt,\n",
    "    \"chat_history\": chat_history,\n",
    "    \"user_input\": user_input\n",
    "}\n",
    "result = chain.invoke(input_variables)\n",
    "\n",
    "chat_history.append(HumanMessage(content=user_input))\n",
    "chat_history.append(AIMessage(content=result))"
   ],
   "metadata": {
    "id": "x_VtyM8CeL9W"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.5 | FewShotPromptTemplate"
   ],
   "metadata": {
    "id": "N9rNF-O1i0cT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Beispiele direkt als Liste von (input, output)\n",
    "examples = [\n",
    "    {\"input\": \"Was ist die Hauptstadt von Frankreich?\", \"output\": \"Die Hauptstadt von Frankreich ist Paris.\"},\n",
    "    {\"input\": \"Wie viele Kontinente gibt es?\", \"output\": \"Es gibt sieben Kontinente.\"}\n",
    "]"
   ],
   "metadata": {
    "id": "mFxG58QEi8jf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Few-Shot Prompt als Nachrichtenblock\n",
    "few_shot_template = FewShotChatMessagePromptTemplate.from_examples(\n",
    "    examples=examples,\n",
    "    example_prompt=ChatPromptTemplate.from_messages([\n",
    "        (\"human\", \"{user_input}\"),\n",
    "        (\"ai\", \"{output}\")\n",
    "    ])\n",
    ")"
   ],
   "metadata": {
    "id": "xcWLkftvvPIx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Finales PromptTemplate mit Systemrolle, Few-Shot-Beispielen, Chat-Verlauf und neuer Eingabe\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"{system_prompt}\"),\n",
    "    *few_shot.messages,\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{user_input}\")\n",
    "])"
   ],
   "metadata": {
    "id": "OsO6aQ7cvQcv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.6 | ChatMessagePromptTemplate"
   ],
   "metadata": {
    "id": "bRQRxrNZw_7C"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts.chat import ChatMessagePromptTemplate\n",
    "\n",
    "moderator_message_template = ChatMessagePromptTemplate(\n",
    "    role=\"moderator\",\n",
    "    prompt=\"Bitte bleib respektvoll und h√∂flich.\"\n",
    ")"
   ],
   "metadata": {
    "id": "GLrSc2XXxDSx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein hilfreicher Assistent.\"),\n",
    "    moderator_prompt,  # eigene Rolle\n",
    "    (\"human\", \"{user_input}\")\n",
    "])"
   ],
   "metadata": {
    "id": "q5APfPbDxHh2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.7 | partial_variables"
   ],
   "metadata": {
    "id": "6AXueZa4xPgr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt mit zwei Variablen\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"{system_prompt}\"),\n",
    "    (\"human\", \"{user_input}\")\n",
    "])\n",
    "\n",
    "# system_prompt wird hier \"fest verdrahtet\"\n",
    "prompt_with_fixed_system = template.partial(system_prompt=\"Du bist ein KI-Experte f√ºr Geschichte.\")"
   ],
   "metadata": {
    "id": "NVs62-IzxWFU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Jetzt muss man beim .invoke() nur noch input √ºbergeben\n",
    "response = (prompt_with_fixed_system | llm | parser).invoke({\n",
    "    \"user_input\": \"Was war die Ursache des Drei√üigj√§hrigen Krieges?\"\n",
    "})"
   ],
   "metadata": {
    "id": "zpZX7bAlxa3u"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.8 | Eigenes Prompt-Template"
   ],
   "metadata": {
    "id": "w53Qv0dCmBaH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Einfacher Aufruf:"
   ],
   "metadata": {
    "id": "UBS4S5m4raNR"
   }
  },
  {
   "cell_type": "code",
   "source": "from genai_lib.utilities import load_prompt\nfrom langchain.chat_models import init_chat_model\n\n# LLM initialisieren\nllm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", temperature=0)\n\n# Prompt-Template von GitHub laden (.md Format)\nurl = \"https://github.com/ralf-42/GenAI/blob/main/05_prompt/create_prompt.md\"\nchat_template = load_prompt(url)\n\n# ===================================================================\n# Beispiel: API-Dokumentation erstellen\n# ===================================================================\n\n# Prompt mit Variablen f√ºllen\nprompt = chat_template.format_messages(\n    context=(\n        \"Wir entwickeln eine API f√ºr Zahlungsabwicklung. \"\n        \"Die Dokumentation ist veraltet. \"\n        \"Zielgruppe: Full-Stack Developer ohne Payment-Erfahrung.\"\n    ),\n\n    role=\"ein erfahrener Technical Writer mit Fokus auf Developer-Dokumentation\",\n\n    expectation=(\n        \"Erstelle eine √úbersichtsseite (max. 500 W√∂rter) f√ºr die API-Docs. \"\n        \"Erkl√§re die 3 wichtigsten Endpunkte: /create-payment, /verify-payment, /refund.\"\n    ),\n\n    action_plan=(\n        \"1. Zielgruppe analysieren\\n\"\n        \"2. Struktur festlegen\\n\"\n        \"3. Fachbegriffe erkl√§ren\\n\"\n        \"4. Praktische Beispiele hinzuf√ºgen\"\n    ),\n\n    tone=(\n        \"Professionell, aber zug√§nglich. \"\n        \"Keine Marketing-Sprache. \"\n        \"F√ºr technisch versierte Developer.\"\n    ),\n\n    evaluation=(\n        \"- Verst√§ndlich f√ºr Nicht-Experten\\n\"\n        \"- Praktisch anwendbar\\n\"\n        \"- Keine unn√∂tige Komplexit√§t\\n\"\n        \"- Code-Beispiele enthalten\"\n    )\n)\n\n# LLM aufrufen\nresponse = llm.invoke(prompt)\nprint(response.content)\nprint(\"\\n\")",
   "metadata": {
    "id": "cxi1FVmGmGG7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mehrfacher Aufruf:"
   ],
   "metadata": {
    "id": "Wd61hSr4reKh"
   }
  },
  {
   "cell_type": "code",
   "source": "from genai_lib.utilities import load_prompt\nfrom langchain.chat_models import init_chat_model\n\nllm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", temperature=0)\n\n# Prompt-Template von GitHub laden (.md Format)\nurl = \"https://github.com/ralf-42/GenAI/blob/main/05_prompt/create_prompt.md\"\nchat_template = load_prompt(url)\n\n# ===================================================================\n# Gemeinsame Parameter definieren (bleiben konstant)\n# ===================================================================\nbase_params = {\n    \"context\": (\n        \"Wir entwickeln eine API f√ºr Zahlungsabwicklung. \"\n        \"Die Dokumentation ist veraltet. \"\n        \"Zielgruppe: Full-Stack Developer ohne Payment-Erfahrung.\"\n    ),\n    \"role\": \"ein erfahrener Technical Writer mit Fokus auf Developer-Dokumentation\",\n    \"action_plan\": (\n        \"1. Zielgruppe analysieren\\n\"\n        \"2. Struktur festlegen\\n\"\n        \"3. Fachbegriffe erkl√§ren\\n\"\n        \"4. Praktische Beispiele hinzuf√ºgen\"\n    ),\n    \"tone\": (\n        \"Professionell, aber zug√§nglich. \"\n        \"Keine Marketing-Sprache. \"\n        \"F√ºr technisch versierte Developer.\"\n    ),\n    \"evaluation\": (\n        \"- Verst√§ndlich f√ºr Nicht-Experten\\n\"\n        \"- Praktisch anwendbar\\n\"\n        \"- Keine unn√∂tige Komplexit√§t\\n\"\n        \"- Code-Beispiele enthalten\"\n    )\n}\n\n# ===================================================================\n# Mehrfache Verwendung mit unterschiedlicher Expectation\n# ===================================================================\n\n# Aufruf 1: API-√úbersichtsseite\nresponse1 = llm.invoke(chat_template.format_messages(\n    **base_params,\n    expectation=(\n        \"Erstelle eine √úbersichtsseite (max. 500 W√∂rter) f√ºr die API-Docs. \"\n        \"Erkl√§re die 3 wichtigsten Endpunkte: /create-payment, /verify-payment, /refund.\"\n    )\n))\nprint(\"=== API-√úbersichtsseite ===\")\nprint(response1.content)\nprint(\"\\n\")\n\n# Aufruf 2: Authentifizierungs-Guide\nresponse2 = llm.invoke(chat_template.format_messages(\n    **base_params,\n    expectation=(\n        \"Erstelle einen Authentication-Guide (max. 400 W√∂rter). \"\n        \"Erkl√§re API-Keys, OAuth 2.0 und Webhook-Signatur-Verifizierung.\"\n    )\n))\nprint(\"=== Authentication-Guide ===\")\nprint(response2.content)\nprint(\"\\n\")\n\n# Aufruf 3: Error-Handling-Sektion\nresponse3 = llm.invoke(chat_template.format_messages(\n    **base_params,\n    expectation=(\n        \"Erstelle eine Error-Handling-Sektion (max. 300 W√∂rter). \"\n        \"Liste die 10 h√§ufigsten Error-Codes und deren L√∂sungen.\"\n    )\n))\nprint(\"=== Error-Handling ===\")\nprint(response3.content)\nprint(\"\\n\")",
   "metadata": {
    "id": "HOdSoFXargev"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 | Model & Chains\n",
    "---"
   ],
   "metadata": {
    "id": "XWKhIzSIbs5M"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.1 | Model - simple"
   ],
   "metadata": {
    "id": "9zv-Rce0fElT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model_provider = \"openai\"\n",
    "model_name = \"gpt-4o-mini\"\n",
    "temperature = 0.0\n",
    "llm = init_chat_model(model_name, model_provider=model_provider, temperature=temperature)\n",
    "\n",
    "# Modell aufrufen\n",
    "response = llm.invoke(prompt)"
   ],
   "metadata": {
    "id": "0eQCRJ9JfKxU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.3 | Chain üìå"
   ],
   "metadata": {
    "id": "BWt1r_ruguI0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model_provider = \"openai\"\n",
    "model_name = \"gpt-4o-mini\"\n",
    "temperature = 0.0\n",
    "llm = init_chat_model(model_name, model_provider=model_provider, temperature=temperature)\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# LCEL-Chain\n",
    "chain = prompt | llm | parser"
   ],
   "metadata": {
    "id": "NpvGhi7fgx8I"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Beispielhafte Eingabewerte\n",
    "input_variables = {\n",
    "    \"system_prompt\": system_prompt,\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": user_input\n",
    "}\n",
    "\n",
    "# Aufruf der Kette mit Eingabewerten\n",
    "response = chain.invoke(input_variables)"
   ],
   "metadata": {
    "id": "uV6TfLiLuDMp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Historie aktualisieren\n",
    "chat_history.append(HumanMessage(content=user_input))\n",
    "chat_history.append(AIMessage(content=response.content))"
   ],
   "metadata": {
    "id": "3IqqDB-1gR8d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.4 | Runnables-Methoden"
   ],
   "metadata": {
    "id": "lWa5rMj3G2Wb"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Jedes Runnable unterst√ºtzt standardm√§√üig folgende Methoden:\n",
    "\n",
    "1. **invoke()**: F√ºr einzelne, synchrone Anfragen\n",
    "2. **batch()**: Verarbeitet mehrere Eingaben parallel\n",
    "3. **stream()**: Gibt Teilergebnisse zur√ºck, sobald sie verf√ºgbar sind\n",
    "4. **ainvoke()**: Asynchrone Version von invoke\n",
    "5. **abatch()**: Asynchrone Version von batch\n",
    "6. **astream()**: Asynchrone Version von stream"
   ],
   "metadata": {
    "id": "hUrE01P8G58H"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4 | Output Parser\n",
    "---"
   ],
   "metadata": {
    "id": "8wd5ZR15bxf7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4.1 | StrOutputParser üìå"
   ],
   "metadata": {
    "id": "og_moM7Qk-Vh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ],
   "metadata": {
    "id": "iJB4yap0lRq3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "chain = prompt| llm | parser\n",
    "response = chain.invoke()"
   ],
   "metadata": {
    "id": "wM_MqJzYHiJn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4.2 | SimpleJsonOutputParser"
   ],
   "metadata": {
    "id": "6NaQWfRFl_vz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.output_parsers import SimpleJsonOutputParser\n",
    "\n",
    "# Nur wenn keine spezifische Struktur ben√∂tigt wird\n",
    "simple_parser = SimpleJsonOutputParser()"
   ],
   "metadata": {
    "id": "XEYphtWRl___"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4.3 | Strukturierter Output mit Pydantic"
   ],
   "metadata": {
    "id": "R9u3hT1RIA6Z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Pydantic-Model definieren\n",
    "class QuestionAnswer(BaseModel):\n",
    "    question: str = Field(description=\"Die gestellte Frage\")\n",
    "    answer: str = Field(description=\"Die Antwort auf die Frage\")\n",
    "    confidence: int = Field(description=\"Vertrauen in die Antwort (0-100)\")\n",
    "    category: str = Field(description=\"Kategorie der Frage\")\n",
    "\n",
    "# with_structured_output verwenden (MODERN - LangChain 1.0+)\n",
    "structured_llm = llm.with_structured_output(QuestionAnswer)\n",
    "\n",
    "# Chain mit strukturiertem Output\n",
    "structured_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Du bist ein Experte. Beantworte die Frage strukturiert.\"),\n",
    "        (\"user\", \"{question}\")\n",
    "    ])\n",
    "    | structured_llm\n",
    ")\n",
    "\n",
    "# Testen\n",
    "try:\n",
    "    structured_result = structured_chain.invoke({\"question\": \"Was ist Photosynthese?\"})\n",
    "    print(f\"üì¶ Strukturiertes Ergebnis: {structured_result}\")\n",
    "    print(f\"üéØ Antwort: {structured_result.answer}\")\n",
    "    print(f\"üìä Vertrauen: {structured_result.confidence}%\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Structured Output Fehler: {e}\")"
   ],
   "metadata": {
    "id": "97ih8v9QIHp9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5 | Chat-History & Memory\n",
    "---"
   ],
   "metadata": {
    "id": "iFllFW8wb080"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5.1 | Simple History üìå"
   ],
   "metadata": {
    "id": "Ej4ajzVLIf00"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Moderne Chat-History-Verwaltung\n",
    "chat_history = []\n",
    "\n",
    "def add_to_history(user_msg: str, ai_msg: str):\n",
    "    \"\"\" F√ºgt Nachrichten zur Chat-History hinzu \"\"\"\n",
    "    chat_history.extend([\n",
    "        HumanMessage(content=user_msg),\n",
    "        AIMessage(content=ai_msg)\n",
    "    ])\n",
    "\n",
    "# Chat-Chain mit History\n",
    "history_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Du bist ein hilfreicher Assistent.\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{user_input}\")\n",
    "    ])\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Erste Nachricht\n",
    "user_input = \"Mein Name ist Max\"\n",
    "response = history_chain.invoke({\"chat_history\": chat_history, \"user_input\": user_input})\n",
    "add_to_history(user_input, response)\n",
    "\n",
    "print(f\"ü§ñ Antwort 1: {response}\")\n",
    "print(f\"üí¨ History-L√§nge: {len(chat_history)}\")"
   ],
   "metadata": {
    "id": "7L7eEwMrIjcx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5.2 | Simple History (Max=10)"
   ],
   "metadata": {
    "id": "sox0Vtx-ItWK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Chat-History mit maximal 10 Eintr√§gen\n",
    "chat_history = []\n",
    "\n",
    "def add_to_history(user_msg: str, ai_msg: str):\n",
    "    \"\"\" F√ºgt Nachrichten zur Chat-History hinzu und begrenzt auf 10 Eintr√§ge \"\"\"\n",
    "    # Neue Nachrichten hinzuf√ºgen\n",
    "    chat_history.extend([\n",
    "        HumanMessage(content=user_msg),\n",
    "        AIMessage(content=ai_msg)\n",
    "    ])\n",
    "\n",
    "    # Auf maximal 10 Eintr√§ge begrenzen\n",
    "    if len(chat_history) > 10:\n",
    "        chat_history.pop(0)  # √Ñltesten Eintrag entfernen\n",
    "        chat_history.pop(0)  # N√§chsten Eintrag entfernen (damit Paare erhalten bleiben)\n",
    "\n",
    "# Chat-Chain mit History\n",
    "history_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Du bist ein hilfreicher Assistent.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"user\", \"{user_input}\")\n",
    "    ])\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Erste Nachricht\n",
    "user_input = \"Mein Name ist Max\"\n",
    "response = history_chain.invoke({\"chat_history\": chat_history, \"user_input\": user_input})\n",
    "add_to_history(user_input, response)\n",
    "\n",
    "print(f\"ü§ñ Antwort 1: {response}\")\n",
    "print(f\"üí¨ History-L√§nge: {len(chat_history)}\")\n",
    "\n",
    "# Zweite Nachricht\n",
    "user_input = \"Wie ist mein  Name?\"\n",
    "response = history_chain.invoke({\"chat_history\": chat_history, \"input\": user_input})\n",
    "add_to_history(user_input, response)\n",
    "\n",
    "print(f\"ü§ñ Antwort 1: {response}\")\n",
    "print(f\"üí¨ History-L√§nge: {len(chat_history)}\")"
   ],
   "metadata": {
    "id": "EBr8CBP9I2bK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 | RAG\n",
    "---"
   ],
   "metadata": {
    "id": "8TaogQLhb62v"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.1 | Load Documents"
   ],
   "metadata": {
    "id": "ymeJjPk-OMQS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Loader-Konfiguration\n",
    "loader_mapping = {\n",
    "    \"*.md\": UnstructuredMarkdownLoader,\n",
    "    \"*.docx\": UnstructuredWordDocumentLoader,\n",
    "    \"*.pdf\": PyPDFLoader,\n",
    "    \"*.txt\": UnstructuredFileLoader,  # Loader f√ºr .txt Dateien\n",
    "}\n",
    "\n",
    "# Funktion zum Laden der Dokumente\n",
    "def load_documents_from_directory(directory_path):\n",
    "    \"\"\"L√§dt Dokumente aus dem angegebenen Verzeichnis basierend auf den unterst√ºtzten Dateitypen.\"\"\"\n",
    "    documents = []\n",
    "    for file_pattern, loader_cls in loader_mapping.items():\n",
    "        loader = DirectoryLoader(directory_path, glob=file_pattern, loader_cls=loader_cls)\n",
    "        documents.extend(loader.load())\n",
    "    return documents\n",
    "\n",
    "# Dokumente laden\n",
    "directory_path = \"/content/files\"\n",
    "\n",
    "documents = load_documents_from_directory(directory_path)"
   ],
   "metadata": {
    "id": "HuDNq1kYPAE9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.2 | Chunking"
   ],
   "metadata": {
    "id": "EqHStIrLOeX4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Text-Splitter konfigurieren und Dokumente aufteilen\n",
    "chunk_size = 900\n",
    "chunk_overlap = 300\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_overlap, chunk_overlap=chunk_overlap)\n",
    "docs = text_splitter.split_documents(documents)"
   ],
   "metadata": {
    "id": "i1R5aO6cPBcC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.3 | Embedding"
   ],
   "metadata": {
    "id": "oNfzqKrDOhaz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Embeddingsmodell festlegen\n",
    "embedding_model = \"text-embedding-3-small\"\n",
    "embeddings = OpenAIEmbeddings(model=embedding_model)"
   ],
   "metadata": {
    "id": "aMQiuE53PHNh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.4 | Vectorstore"
   ],
   "metadata": {
    "id": "gqhIYi29Okn_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Vektordatenbank erstellen und speichern\n",
    "persistent_directory = \"/content/chroma_db\"\n",
    "vectorstore = Chroma.from_documents(docs, embeddings, persist_directory=persistent_directory)"
   ],
   "metadata": {
    "id": "Uwhx1HJWPLZy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.5 | Retriever"
   ],
   "metadata": {
    "id": "uFpRtFTmOnhv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Festlegen LLM und Retriever\n",
    "model_name = 'gpt-4o-mini'\n",
    "temperature = 0\n",
    "llm = ChatOpenAI(model=model_name, temperature=temperature)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ],
   "metadata": {
    "id": "HI_k6vY3PQA-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.6 | Inference"
   ],
   "metadata": {
    "id": "VB7mOeCtOw8x"
   }
  },
  {
   "cell_type": "code",
   "source": "# RAG-Prompt als Markdown-Template laden (statt hub.pull)\nrag_prompt = load_prompt(\n    \"https://github.com/ralf-42/GenAI/blob/main/05_prompt/rag_prompt.md\"\n)",
   "metadata": {
    "id": "fFrz2tKzPvSD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.    \n",
    "Question: {question}      \n",
    "Context: {context}         \n",
    "Answer:"
   ],
   "metadata": {
    "id": "zBWHYOg-QTdS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Aufbereitung Ergebnis Retriever\n",
    "def format_documents(documents):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in documents)"
   ],
   "metadata": {
    "id": "MjxuS6K8P5ho"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Chat-Verlauf initialisieren\n",
    "chat_history = []\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_documents, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "metadata": {
    "id": "OmVfIDeePbzT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Aufruf\n",
    "user_input = \"Was macht Tariq Hassan?\"\n",
    "response = chain.invoke(user_input)"
   ],
   "metadata": {
    "id": "iyEdX5mFQD32"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.7 | SQL RAG"
   ],
   "metadata": {
    "id": "h_CPWZO5Swkl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# SQL-Datenbank initialisieren\n",
    "DB_PATH = \"/content/northwind.db\"\n",
    "DB_URI = f\"sqlite:///{DB_PATH}\"\n",
    "\n",
    "db = SQLDatabase.from_uri(DB_URI)"
   ],
   "metadata": {
    "id": "7i71YL5THyox"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# SQL-Prompt als Markdown-Template laden (05_prompt/)\nsql_prompt = load_prompt(\n    \"https://github.com/ralf-42/GenAI/blob/main/05_prompt/sql_prompt.md\"\n)\n\n# Template f√ºr die Ergebnisinterpretation (inline, da kein passendes .md Template)\nanalysis_template = \"\"\"\nDu bist ein Business-Analyst, der SQL-Abfrageergebnisse interpretiert und verst√§ndliche Antworten gibt.\nBeantworte die Benutzeranfrage basierend auf den SQL-Ergebnissen.\n\nBei Ja/Nein-Fragen gib eine klare Antwort und erkl√§re die Gr√ºnde.\nBei Fragen nach Empfehlungen oder notwendigen Anpassungen, analysiere die Daten und gib konkrete Vorschl√§ge.\n\nBenutzeranfrage: {query}\nSQL-Abfrage: {sql_query}\nAbfrageergebnisse:\n{results}\n\nDeine Analyse und Antwort:\n\"\"\"",
   "metadata": {
    "id": "QN7uJUGyTDi-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# SQL erstellen\ndef get_schema(_):\n    return db.get_table_info()\n\nsql_generator = (\n    RunnablePassthrough.assign(schema=get_schema)\n    | sql_prompt\n    | llm\n    | StrOutputParser()\n)",
   "metadata": {
    "id": "BhYJbvlVTq_I"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Datenbank abfragen\n",
    "def execute_query(sql_query: str) -> str:\n",
    "    \"\"\"F√ºhrt eine SQL-Abfrage aus und formatiert die Ergebnisse als String.\"\"\"\n",
    "    try:\n",
    "        # Bereinige die Abfrage von eventuellen Formatierungen\n",
    "        cleaned_query = sql_query.strip()\n",
    "\n",
    "        conn = sqlite3.connect(DB_PATH)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(cleaned_query)\n",
    "\n",
    "        # Spalten√ºberschriften abrufen\n",
    "        column_names = [description[0] for description in cursor.description]\n",
    "\n",
    "        # Ergebnisse abrufen\n",
    "        results = cursor.fetchall()\n",
    "\n",
    "        # Ergebnisse formatieren\n",
    "        output = \"| \" + \" | \".join(column_names) + \" |\\n\"\n",
    "        output += \"| \" + \" | \".join([\"---\" for _ in column_names]) + \" |\\n\"\n",
    "\n",
    "        for row in results:\n",
    "            output += \"| \" + \" | \".join([str(cell) for cell in row]) + \" |\\n\"\n",
    "\n",
    "        conn.close()\n",
    "\n",
    "        # Keine Ergebnisse gefunden\n",
    "        if len(results) == 0:\n",
    "            return \"Keine Ergebnisse gefunden.\"\n",
    "\n",
    "        return output\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Fehler bei der Ausf√ºhrung der Abfrage: {str(e)}\\nAbfrage: {cleaned_query}\""
   ],
   "metadata": {
    "id": "sEBK76iATu05"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Analyse des Ergebnisses der Datenbank-Abfrage durch ein LLM\n",
    "def analyze_results(query, sql_query, results):\n",
    "    \"\"\"Analysiert die Ergebnisse und gibt eine nat√ºrlichsprachliche Antwort zur√ºck.\"\"\"\n",
    "    analysis_prompt = PromptTemplate.from_template(analysis_template)\n",
    "    analysis_chain = analysis_prompt | llm | StrOutputParser()\n",
    "\n",
    "    return analysis_chain.invoke({\n",
    "        \"query\": query,\n",
    "        \"sql_query\": sql_query,\n",
    "        \"results\": results\n",
    "    })"
   ],
   "metadata": {
    "id": "bXSgvVGaT2DG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Funktion f√ºr Aufruf der Kette aus Gradio\n",
    "def chatbot_response(mesSQL, chat_history):\n",
    "    \"\"\"Verarbeitet Benutzeranfragen, erstellt SQL und gibt formatierte Ergebnisse mit Analyse zur√ºck.\"\"\"\n",
    "    try:\n",
    "        # SQL-Abfrage mit LLM generieren\n",
    "        sql_query = sql_generator.invoke({\"query\": mesSQL})\n",
    "\n",
    "        # Bereinige eventuelles Markdown-Markup\n",
    "        sql_query = re.sub(r'```sql\\s*(.*?)\\s*```', r'\\1', sql_query, flags=re.DOTALL)\n",
    "        sql_query = sql_query.replace(\"```\", \"\").strip()\n",
    "\n",
    "        # Debug-Ausgabe\n",
    "        print(f\"Generierte SQL: {sql_query}\")\n",
    "\n",
    "        # F√ºhre die Abfrage aus\n",
    "        results = execute_query(sql_query)\n",
    "\n",
    "        # Analysiere die Ergebnisse f√ºr komplexe Fragen\n",
    "        analysis = analyze_results(mesSQL, sql_query, results)\n",
    "\n",
    "        # Antwort formatieren\n",
    "        response = f\"### Deine Anfrage\\n{mesSQL}\\n\\n### SQL-Abfrage\\n```sql\\n{sql_query}\\n```\\n\\n### Ergebnisse\\n{results}\\n\\n### Analyse\\n{analysis}\"\n",
    "\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Ein Fehler ist aufgetreten: {str(e)}\""
   ],
   "metadata": {
    "id": "7fsjmBe_T90o"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7 | Image, Audio, Video\n",
    "---"
   ],
   "metadata": {
    "id": "i-YaukXicEYQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](../07_Image/under_construction_dall_e_klein.png)"
   ],
   "metadata": {
    "id": "paQPeXd-usUX"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8 | Agent\n",
    "---"
   ],
   "metadata": {
    "id": "qoaSA0q_b-yJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](../07_Image/under_construction_dall_e_klein.png)"
   ],
   "metadata": {
    "id": "VHI2iJ3DulT4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9 | Fine-Tuning & Local Models\n",
    "---"
   ],
   "metadata": {
    "id": "z6a4og9mcJXd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](../07_Image/under_construction_dall_e_klein.png)"
   ],
   "metadata": {
    "id": "kx8kz41Su6W9"
   }
  }
 ]
}
{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "IQm3u7mNZc6b",
    "qSG-9nZaZj3q",
    "8yUdGhEHadmI",
    "0hI3GKnNbJ99",
    "23F6baGU4Cq2",
    "0awT19jJH7ce",
    "3464d8Glm0yi",
    "Hgn0xlXqDOyk",
    "ccg_cmsN4chQ",
    "8gaiVjttbmno",
    "fGd1iijaiSWo",
    "ljyypqKlFt11",
    "ayAtyWH0eH6Y",
    "w53Qv0dCmBaH",
    "XWKhIzSIbs5M",
    "9zv-Rce0fElT",
    "BWt1r_ruguI0",
    "BaO_8SmNxxVm",
    "lWa5rMj3G2Wb",
    "8wd5ZR15bxf7",
    "og_moM7Qk-Vh",
    "6NaQWfRFl_vz",
    "R9u3hT1RIA6Z",
    "iFllFW8wb080",
    "Ej4ajzVLIf00",
    "sox0Vtx-ItWK",
    "8TaogQLhb62v",
    "ymeJjPk-OMQS",
    "EqHStIrLOeX4",
    "oNfzqKrDOhaz",
    "gqhIYi29Okn_",
    "uFpRtFTmOnhv",
    "VB7mOeCtOw8x",
    "h_CPWZO5Swkl",
    "PWZ4wa2uxxVq",
    "wXGWj0hMxxVq",
    "St7_W_dKxxVr",
    "BaI4XbyixxVr",
    "6_HjwWhwxxVr"
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<p><font size=\"6\" color='grey'> <b>\n",
    "\n",
    "Generative KI. Verstehen. Anwenden. Gestalten.\n",
    "</b></font> </br></p>"
   ],
   "metadata": {
    "id": "37rMyCQz2JNw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font size=\"5\" color='grey'> <b>\n",
    "Code-Snippets\n",
    "</b></font> </br></p>\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "R5CfUEMJdvFQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0 | Setup & Konfiguration\n",
    "---"
   ],
   "metadata": {
    "id": "IQm3u7mNZc6b"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0.1 | Install Kursbibliothek üìå"
   ],
   "metadata": {
    "id": "qSG-9nZaZj3q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul"
   ],
   "metadata": {
    "id": "zYXfr5UlZnfk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0.2 | Import Kursbibliothek üìå"
   ],
   "metadata": {
    "id": "8yUdGhEHadmI"
   }
  },
  {
   "cell_type": "code",
   "source": "  from genai_lib.utilities import (\n      check_environment,\n      get_ipinfo,\n      setup_api_keys,\n      mprint,\n      install_packages,\n      mermaid,\n      get_model_profile,\n      extract_thinking,\n      load_prompt\n  )",
   "metadata": {
    "id": "YxBd5eXianVB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0.3 | Umgebung einrichten üìå"
   ],
   "metadata": {
    "id": "0hI3GKnNbJ99"
   }
  },
  {
   "cell_type": "code",
   "source": "#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul\nfrom genai_lib.utilities import (\n    check_environment,\n    get_ipinfo,\n    setup_api_keys,\n    mprint,\n    install_packages,\n    mermaid,\n    get_model_profile,\n    extract_thinking,\n    load_prompt\n)\nsetup_api_keys(['OPENAI_API_KEY'], create_globals=False)\nprint()\ncheck_environment()\nprint()\nget_ipinfo()",
   "metadata": {
    "id": "xlCNUpx_bOqq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0.4 | Standard-Importe f√ºr GenAI"
   ],
   "metadata": {
    "id": "23F6baGU4Cq2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# LangChain\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Standard-Python-Imports\n",
    "import os\n",
    "from IPython.display import Markdown, display"
   ],
   "metadata": {
    "id": "aTr6UN0h4IG-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0.5 | Mermaid-Diagramm"
   ],
   "metadata": {
    "id": "0awT19jJH7ce"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#@markdown   <p><font size=\"4\" color='green'>  üßú‚Äç‚ôÄÔ∏è Mermaid</font> </br></p>\n",
    "\n",
    "diagram = \"\"\"\n",
    "graph TD\n",
    "    A[Start]\n",
    "    A --> B{Model Parameter}\n",
    "    B --> C[Provider OpenAI]\n",
    "    B --> D[Model gpt-4o-mini]\n",
    "    B --> E[Temperature 0]\n",
    "    C --> F[Initialize Chat Model]\n",
    "    D --> F\n",
    "    E --> F\n",
    "    F --> G[LLM configured]\n",
    "    G --> H[End]\n",
    "\"\"\"\n",
    "mermaid(diagram, width=800, height=600)"
   ],
   "metadata": {
    "cellView": "form",
    "id": "7BRC64pyH_J3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0.6 | Namenskonventionen"
   ],
   "metadata": {
    "id": "3464d8Glm0yi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Empfohlene Namenskonventionen (snake_case):**\n",
    "\n",
    "**Variablen:**\n",
    "- `user_input` - Benutzereingabe\n",
    "- `system_prompt` - Systemnachricht\n",
    "- `chat_history` - Chatverlauf\n",
    "- `model_name` - Modellname\n",
    "- `temperature` - Temperatur-Parameter\n",
    "- `response` - API-Antwort\n",
    "- `result` - Verarbeitetes Ergebnis\n",
    "\n",
    "**LangChain-Objekte:**\n",
    "- `prompt_template` - Prompt-Vorlage\n",
    "- `prompt` - Fertiger Prompt\n",
    "- `llm` - Sprachmodell\n",
    "- `chain` - Verarbeitungskette\n",
    "- `parser` - Output-Parser"
   ],
   "metadata": {
    "id": "JHxMlnzWm4p8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0.7 | Temperatur-Guidelines"
   ],
   "metadata": {
    "id": "Hgn0xlXqDOyk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Temperatur-Richtlinien f√ºr KI-Modelle**\n",
    "\n",
    "| Temperatur | Kategorie | Wert | Anwendungsbereich | Beschreibung |\n",
    "|------------|-----------|------|-------------------|--------------|\n",
    "| **Faktisch** | `factual` | 0.0 | Mathematik, Code, Datenanalyse | Deterministische Ausgaben f√ºr pr√§zise, reproduzierbare Ergebnisse |\n",
    "| **Ausgewogen** | `balanced` | 0.3 | Allgemeine Fragen, Erkl√§rungen | Leicht variierte Antworten bei gleichbleibender Genauigkeit |\n",
    "| **Kreativ** | `creative` | 0.7 | Brainstorming, Texterstellung | Vielf√§ltige und innovative Antworten f√ºr kreative Aufgaben |\n",
    "| **Sehr kreativ** | `very_creative` | 1.0 | Experimentelle Inhalte | Maximale Kreativit√§t und Unvorhersagbarkeit |\n",
    "\n",
    "**Verwendungshinweise**\n",
    "\n",
    "- **Niedrige Temperatur (0.0-0.3)**: F√ºr sachliche, pr√§zise Antworten\n",
    "- **Mittlere Temperatur (0.4-0.6)**: F√ºr ausgewogene Kommunikation\n",
    "- **Hohe Temperatur (0.7-1.0)**: F√ºr kreative und experimentelle Ausgaben\n",
    "\n"
   ],
   "metadata": {
    "id": "ce4MVxa6DXm-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 | Einfachster LLM-Aufruf\n",
    "---"
   ],
   "metadata": {
    "id": "ccg_cmsN4chQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Einfachster LLM-Aufruf (Kurznotation: \"provider:model\")\n",
    "llm = init_chat_model(\"openai:gpt-4o-mini\")\n",
    "\n",
    "user_input = \"Erkl√§re mir KI in einem Satz\"\n",
    "\n",
    "# Direkter Aufruf\n",
    "response = llm.invoke([HumanMessage(content=user_input)])\n",
    "print(response.content)"
   ],
   "metadata": {
    "id": "i7cIWKYm5FD1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 | Prompts & Templates\n",
    "---"
   ],
   "metadata": {
    "id": "8gaiVjttbmno"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.1 | PromptTemplate"
   ],
   "metadata": {
    "id": "fGd1iijaiSWo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Prompt definieren\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Systemanweisung: {system_prompt}\\n\"\n",
    "    \"Benutzereingabe: {input}\\n\"\n",
    "    \"Antwort:\"\n",
    ")"
   ],
   "metadata": {
    "id": "6NEV8D11iViQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.2 | ChatPromptTemplate"
   ],
   "metadata": {
    "id": "ljyypqKlFt11"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Modernes ChatPromptTemplate (einfache Version)\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein hilfreicher Assistent.\"),\n",
    "    (\"human\", \"{user_input}\")\n",
    "])"
   ],
   "metadata": {
    "id": "tdd5vchMFzml"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Direkt mit Chain verwenden (moderne LCEL-Syntax)\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "result = chain.invoke({\"user_input\": \"Was ist Machine Learning?\"})\n",
    "\n",
    "print(f\"üéØ Ergebnis: {result}\")"
   ],
   "metadata": {
    "id": "OsYo1K4TF2VT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.4 | ChatPromptTemplate - History üìå"
   ],
   "metadata": {
    "id": "ayAtyWH0eH6Y"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Erweitert mit Chat-History\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"{system_prompt}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{user_input}\")\n",
    "])\n",
    "\n",
    "# Model ...\n",
    "\n",
    "# Parser ...\n",
    "\n",
    "# Moderne Chain-Syntax\n",
    "chain = prompt_template | llm | parser\n",
    "\n",
    "# ... weitere Code ...\n",
    "chat_history = []\n",
    "input_variables = {\n",
    "    \"system_prompt\": system_prompt,\n",
    "    \"chat_history\": chat_history,\n",
    "    \"user_input\": user_input\n",
    "}\n",
    "result = chain.invoke(input_variables)\n",
    "\n",
    "chat_history.append(HumanMessage(content=user_input))\n",
    "chat_history.append(AIMessage(content=result))"
   ],
   "metadata": {
    "id": "x_VtyM8CeL9W"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.5 | Eigenes Prompt-Template"
   ],
   "metadata": {
    "id": "w53Qv0dCmBaH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Einfacher Aufruf:"
   ],
   "metadata": {
    "id": "UBS4S5m4raNR"
   }
  },
  {
   "cell_type": "code",
   "source": "from genai_lib.utilities import load_prompt\nfrom langchain.chat_models import init_chat_model\n\n# LLM initialisieren (Kurznotation: \"provider:model\")\nllm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0)\n\nurl = \"https://github.com/ralf-42/GenAI/blob/main/05_prompt/create_prompt.md\"\nchat_template = load_prompt(url)\n\n# ===================================================================\n# Beispiel: API-Dokumentation erstellen\n# ===================================================================\n\n# Prompt mit Variablen f√ºllen\nprompt = chat_template.format_messages(\n    context=(\n        \"Wir entwickeln eine API f√ºr Zahlungsabwicklung. \"\n        \"Die Dokumentation ist veraltet. \"\n        \"Zielgruppe: Full-Stack Developer ohne Payment-Erfahrung.\"\n    ),\n\n    role=\"Du bist bist ein erfahrener Technical Writer mit Fokus auf Developer-Dokumentation\",\n\n    expectation=(\n        \"Erstelle eine √úbersichtsseite (max. 500 W√∂rter) f√ºr die API-Docs. \"\n        \"Erkl√§re die 3 wichtigsten Endpunkte: /create-payment, /verify-payment, /refund.\"\n    ),\n\n    action_plan=(\n        \"1. Zielgruppe analysieren\\n\"\n        \"2. Struktur festlegen\\n\"\n        \"3. Fachbegriffe erkl√§ren\\n\"\n        \"4. Praktische Beispiele hinzuf√ºgen\"\n    ),\n\n    tone=(\n        \"Professionell, aber zug√§nglich. \"\n        \"Keine Marketing-Sprache. \"\n        \"F√ºr technisch versierte Developer.\"\n    ),\n\n    evaluation=(\n        \"- Verst√§ndlich f√ºr Nicht-Experten\\n\"\n        \"- Praktisch anwendbar\\n\"\n        \"- Keine unn√∂tige Komplexit√§t\\n\"\n        \"- Code-Beispiele enthalten\"\n    )\n)\n\n# LLM aufrufen\nresponse = llm.invoke(prompt)\nprint(response.content)\nprint(\"\\n\")",
   "metadata": {
    "id": "cxi1FVmGmGG7",
    "executionInfo": {
     "status": "error",
     "timestamp": 1770929597076,
     "user_tz": -60,
     "elapsed": 98,
     "user": {
      "displayName": "Ralf Bendig",
      "userId": "13914949880386033641"
     }
    },
    "outputId": "fe580573-a204-4461-a73d-1e444442d5a4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mehrfacher Aufruf:"
   ],
   "metadata": {
    "id": "Wd61hSr4reKh"
   }
  },
  {
   "cell_type": "code",
   "source": "from genai_lib.utilities import load_prompt\nfrom langchain.chat_models import init_chat_model\n\n# LLM initialisieren (Kurznotation: \"provider:model\")\nllm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0)\n\nurl = \"https://github.com/ralf-42/GenAI/blob/main/05_prompt/create_prompt.md\"\nchat_template = load_prompt(url)\n\n# ===================================================================\n# Gemeinsame Parameter definieren (bleiben konstant)\n# ===================================================================\nbase_params = {\n    \"context\": (\n        \"Wir entwickeln eine API f√ºr Zahlungsabwicklung. \"\n        \"Die Dokumentation ist veraltet. \"\n        \"Zielgruppe: Full-Stack Developer ohne Payment-Erfahrung.\"\n    ),\n    \"role\": \"ein erfahrener Technical Writer mit Fokus auf Developer-Dokumentation\",\n    \"action_plan\": (\n        \"1. Zielgruppe analysieren\\n\"\n        \"2. Struktur festlegen\\n\"\n        \"3. Fachbegriffe erkl√§ren\\n\"\n        \"4. Praktische Beispiele hinzuf√ºgen\"\n    ),\n    \"tone\": (\n        \"Professionell, aber zug√§nglich. \"\n        \"Keine Marketing-Sprache. \"\n        \"F√ºr technisch versierte Developer.\"\n    ),\n    \"evaluation\": (\n        \"- Verst√§ndlich f√ºr Nicht-Experten\\n\"\n        \"- Praktisch anwendbar\\n\"\n        \"- Keine unn√∂tige Komplexit√§t\\n\"\n        \"- Code-Beispiele enthalten\"\n    )\n}\n\n# ===================================================================\n# Mehrfache Verwendung mit unterschiedlicher Expectation\n# ===================================================================\n\n# Aufruf 1: API-√úbersichtsseite\nresponse1 = llm.invoke(chat_template.format_messages(\n    **base_params,\n    expectation=(\n        \"Erstelle eine √úbersichtsseite (max. 500 W√∂rter) f√ºr die API-Docs. \"\n        \"Erkl√§re die 3 wichtigsten Endpunkte: /create-payment, /verify-payment, /refund.\"\n    )\n))\nprint(\"=== API-√úbersichtsseite ===\")\nprint(response1.content)\nprint(\"\\n\")\n\n# Aufruf 2: Authentifizierungs-Guide\nresponse2 = llm.invoke(chat_template.format_messages(\n    **base_params,\n    expectation=(\n        \"Erstelle einen Authentication-Guide (max. 400 W√∂rter). \"\n        \"Erkl√§re API-Keys, OAuth 2.0 und Webhook-Signatur-Verifizierung.\"\n    )\n))\nprint(\"=== Authentication-Guide ===\")\nprint(response2.content)\nprint(\"\\n\")\n\n# Aufruf 3: Error-Handling-Sektion\nresponse3 = llm.invoke(chat_template.format_messages(\n    **base_params,\n    expectation=(\n        \"Erstelle eine Error-Handling-Sektion (max. 300 W√∂rter). \"\n        \"Liste die 10 h√§ufigsten Error-Codes und deren L√∂sungen.\"\n    )\n))\nprint(\"=== Error-Handling ===\")\nprint(response3.content)\nprint(\"\\n\")",
   "metadata": {
    "id": "HOdSoFXargev"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 | Model & Chains\n",
    "---"
   ],
   "metadata": {
    "id": "XWKhIzSIbs5M"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.1 | Model - simple"
   ],
   "metadata": {
    "id": "9zv-Rce0fElT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Modernes ChatPromptTemplate (einfache Version)\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein hilfreicher Assistent.\"),\n",
    "    (\"human\", \"{user_input}\")\n",
    "])\n",
    "\n",
    "# LLM initialisieren (Kurznotation: \"provider:model\")\n",
    "llm = init_chat_model(\"openai:gpt-4o-mini\")\n",
    "\n",
    "# Invoke the chain\n",
    "result = llm.invoke({\"user_input\": \"Was ist Machine Learning?\"})\n",
    "\n",
    "print(f\"üéØ Ergebnis: {result}\")"
   ],
   "metadata": {
    "id": "Xg6z6hQmz5II"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.2 | Chain üìå"
   ],
   "metadata": {
    "id": "BWt1r_ruguI0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM initialisieren (Kurznotation: \"provider:model\")\n",
    "llm = init_chat_model(\"openai:gpt-4o-mini\")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# LCEL-Chain\n",
    "chain = prompt | llm | parser"
   ],
   "metadata": {
    "id": "NpvGhi7fgx8I"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Beispielhafte Eingabewerte\n",
    "input_variables = {\n",
    "    \"system_prompt\": system_prompt,\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": user_input\n",
    "}\n",
    "\n",
    "# Aufruf der Kette mit Eingabewerten\n",
    "response = chain.invoke(input_variables)"
   ],
   "metadata": {
    "id": "uV6TfLiLuDMp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Historie aktualisieren\n",
    "chat_history.extend(HumanMessage(content=user_input), AIMessage(content=response.content))"
   ],
   "metadata": {
    "id": "3IqqDB-1gR8d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaO_8SmNxxVm"
   },
   "source": [
    "# 3.3 | get_model_profile() - genai_lib Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JiNvLwX8xxVm"
   },
   "outputs": [],
   "source": [
    "from genai_lib.utilities import get_model_profile\n",
    "\n",
    "# Model Profile mit vereinfachter Anzeige (Symbole fuer Multimodal-Support)\n",
    "profile = get_model_profile(\"openai:gpt-4o-mini\")\n",
    "\n",
    "print(\"=== Model Profile (vereinfacht) ===\")\n",
    "print(profile)\n",
    "\n",
    "# Ausgabe-Beispiel:\n",
    "# Model: gpt-4o-mini\n",
    "# Provider: openai\n",
    "# Modalities: üìù Text | üñºÔ∏è Image | üéµ Audio\n",
    "# Reasoning: Yes\n",
    "# Temperature: Yes\n",
    "# Knowledge Cutoff: 2023-10-01"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.4 | Runnables-Methoden"
   ],
   "metadata": {
    "id": "lWa5rMj3G2Wb"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Jedes Runnable unterst√ºtzt standardm√§√üig folgende Methoden:\n",
    "\n",
    "1. **invoke()**: F√ºr einzelne, synchrone Anfragen\n",
    "2. **batch()**: Verarbeitet mehrere Eingaben parallel\n",
    "3. **stream()**: Gibt Teilergebnisse zur√ºck, sobald sie verf√ºgbar sind\n",
    "4. **ainvoke()**: Asynchrone Version von invoke\n",
    "5. **abatch()**: Asynchrone Version von batch\n",
    "6. **astream()**: Asynchrone Version von stream"
   ],
   "metadata": {
    "id": "hUrE01P8G58H"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4 | Output Parser\n",
    "---"
   ],
   "metadata": {
    "id": "8wd5ZR15bxf7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4.1 | StrOutputParser üìå"
   ],
   "metadata": {
    "id": "og_moM7Qk-Vh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ],
   "metadata": {
    "id": "iJB4yap0lRq3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "chain = prompt| llm | parser\n",
    "response = chain.invoke()"
   ],
   "metadata": {
    "id": "wM_MqJzYHiJn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4.2 | SimpleJsonOutputParser"
   ],
   "metadata": {
    "id": "6NaQWfRFl_vz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.output_parsers import SimpleJsonOutputParser\n",
    "\n",
    "# Nur wenn keine spezifische Struktur ben√∂tigt wird\n",
    "simple_parser = SimpleJsonOutputParser()"
   ],
   "metadata": {
    "id": "XEYphtWRl___"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4.3 | Strukturierter Output mit Pydantic"
   ],
   "metadata": {
    "id": "R9u3hT1RIA6Z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Pydantic-Model definieren\n",
    "class QuestionAnswer(BaseModel):\n",
    "    question: str = Field(description=\"Die gestellte Frage\")\n",
    "    answer: str = Field(description=\"Die Antwort auf die Frage\")\n",
    "    confidence: int = Field(description=\"Vertrauen in die Antwort (0-100)\")\n",
    "    category: str = Field(description=\"Kategorie der Frage\")\n",
    "\n",
    "# with_structured_output verwenden (MODERN - LangChain 1.0+)\n",
    "structured_llm = llm.with_structured_output(QuestionAnswer)\n",
    "\n",
    "# Chain mit strukturiertem Output\n",
    "structured_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Du bist ein Experte. Beantworte die Frage strukturiert.\"),\n",
    "        (\"user\", \"{question}\")\n",
    "    ])\n",
    "    | structured_llm\n",
    ")\n",
    "\n",
    "# Testen\n",
    "try:\n",
    "    structured_result = structured_chain.invoke({\"question\": \"Was ist Photosynthese?\"})\n",
    "    print(f\"üì¶ Strukturiertes Ergebnis: {structured_result}\")\n",
    "    print(f\"üéØ Antwort: {structured_result.answer}\")\n",
    "    print(f\"üìä Vertrauen: {structured_result.confidence}%\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Structured Output Fehler: {e}\")"
   ],
   "metadata": {
    "id": "97ih8v9QIHp9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5 | Chat-History & Memory\n",
    "---"
   ],
   "metadata": {
    "id": "iFllFW8wb080"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5.1 | Simple History üìå"
   ],
   "metadata": {
    "id": "Ej4ajzVLIf00"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Moderne Chat-History-Verwaltung\n",
    "chat_history = []\n",
    "\n",
    "def add_to_history(user_msg: str, ai_msg: str):\n",
    "    \"\"\" F√ºgt Nachrichten zur Chat-History hinzu \"\"\"\n",
    "    chat_history.extend([\n",
    "        HumanMessage(content=user_msg),\n",
    "        AIMessage(content=ai_msg)\n",
    "    ])\n",
    "\n",
    "# Chat-Chain mit History\n",
    "history_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Du bist ein hilfreicher Assistent.\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{user_input}\")\n",
    "    ])\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Erste Nachricht\n",
    "user_input = \"Mein Name ist Max\"\n",
    "response = history_chain.invoke({\"chat_history\": chat_history, \"user_input\": user_input})\n",
    "add_to_history(user_input, response)\n",
    "\n",
    "print(f\"ü§ñ Antwort 1: {response}\")\n",
    "print(f\"üí¨ History-L√§nge: {len(chat_history)}\")"
   ],
   "metadata": {
    "id": "7L7eEwMrIjcx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5.2 | RunnableWithMessageHistory"
   ],
   "metadata": {
    "id": "sox0Vtx-ItWK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# RunnableWithMessageHistory - Automatisches Memory-Management\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# Session-Store (Dictionary: session_id -> ChatMessageHistory)\n",
    "session_store = {}\n",
    "\n",
    "def get_session_history(session_id):\n",
    "    \"\"\"Gibt die Historie f√ºr eine Session zur√ºck (oder erstellt eine neue).\"\"\"\n",
    "    if session_id not in session_store:\n",
    "        session_store[session_id] = InMemoryChatMessageHistory()\n",
    "    return session_store[session_id]\n",
    "\n",
    "# Prompt mit Historie-Platzhalter\n",
    "prompt_with_history = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein hilfreicher Assistent.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{user_input}\")\n",
    "])\n",
    "\n",
    "# Basis-Chain\n",
    "base_chain = prompt_with_history | llm | StrOutputParser()\n",
    "\n",
    "# Chain MIT automatischem Memory-Management\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    runnable=base_chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"user_input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "# Verwendung\n",
    "config = {\"configurable\": {\"session_id\": \"user_123\"}}\n",
    "\n",
    "response = chain_with_history.invoke(\n",
    "    {\"user_input\": \"Mein Name ist Max\"},\n",
    "    config=config\n",
    ")\n",
    "print(response)\n",
    "\n",
    "response = chain_with_history.invoke(\n",
    "    {\"user_input\": \"Wie heisse ich?\"},\n",
    "    config=config\n",
    ")\n",
    "print(response)"
   ],
   "metadata": {
    "id": "EBr8CBP9I2bK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 | RAG\n",
    "---"
   ],
   "metadata": {
    "id": "8TaogQLhb62v"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.1 | Load Documents"
   ],
   "metadata": {
    "id": "ymeJjPk-OMQS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Loader-Konfiguration\n",
    "loader_mapping = {\n",
    "    \"*.md\": UnstructuredMarkdownLoader,\n",
    "    \"*.docx\": UnstructuredWordDocumentLoader,\n",
    "    \"*.pdf\": PyPDFLoader,\n",
    "    \"*.txt\": UnstructuredFileLoader,  # Loader f√ºr .txt Dateien\n",
    "}\n",
    "\n",
    "# Funktion zum Laden der Dokumente\n",
    "def load_documents_from_directory(directory_path):\n",
    "    \"\"\"L√§dt Dokumente aus dem angegebenen Verzeichnis basierend auf den unterst√ºtzten Dateitypen.\"\"\"\n",
    "    documents = []\n",
    "    for file_pattern, loader_cls in loader_mapping.items():\n",
    "        loader = DirectoryLoader(directory_path, glob=file_pattern, loader_cls=loader_cls)\n",
    "        documents.extend(loader.load())\n",
    "    return documents\n",
    "\n",
    "# Dokumente laden\n",
    "directory_path = \"/content/files\"\n",
    "\n",
    "documents = load_documents_from_directory(directory_path)"
   ],
   "metadata": {
    "id": "HuDNq1kYPAE9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.2 | Chunking"
   ],
   "metadata": {
    "id": "EqHStIrLOeX4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Text-Splitter konfigurieren und Dokumente aufteilen\n",
    "chunk_size = 900\n",
    "chunk_overlap = 300\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_overlap, chunk_overlap=chunk_overlap)\n",
    "docs = text_splitter.split_documents(documents)"
   ],
   "metadata": {
    "id": "i1R5aO6cPBcC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.3 | Embedding"
   ],
   "metadata": {
    "id": "oNfzqKrDOhaz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Embeddingsmodell festlegen\n",
    "embedding_model = \"text-embedding-3-small\"\n",
    "embeddings = OpenAIEmbeddings(model=embedding_model)"
   ],
   "metadata": {
    "id": "aMQiuE53PHNh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.4 | Vectorstore"
   ],
   "metadata": {
    "id": "gqhIYi29Okn_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Vektordatenbank erstellen und speichern\n",
    "persistent_directory = \"/content/chroma_db\"\n",
    "vectorstore = Chroma.from_documents(docs, embeddings, persist_directory=persistent_directory)"
   ],
   "metadata": {
    "id": "Uwhx1HJWPLZy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.5 | Retriever"
   ],
   "metadata": {
    "id": "uFpRtFTmOnhv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Festlegen LLM und Retriever (Kurznotation: \"provider:model\")\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ],
   "metadata": {
    "id": "HI_k6vY3PQA-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.6 | Inference"
   ],
   "metadata": {
    "id": "VB7mOeCtOw8x"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")"
   ],
   "metadata": {
    "id": "fFrz2tKzPvSD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.    \n",
    "Question: {question}      \n",
    "Context: {context}         \n",
    "Answer:"
   ],
   "metadata": {
    "id": "zBWHYOg-QTdS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Aufbereitung Ergebnis Retriever\n",
    "def format_documents(documents):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in documents)"
   ],
   "metadata": {
    "id": "MjxuS6K8P5ho"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Chat-Verlauf initialisieren\n",
    "chat_history = []\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_documents, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "metadata": {
    "id": "OmVfIDeePbzT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Aufruf\n",
    "user_input = \"Was macht Tariq Hassan?\"\n",
    "response = chain.invoke(user_input)"
   ],
   "metadata": {
    "id": "iyEdX5mFQD32"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.7 | SQL RAG"
   ],
   "metadata": {
    "id": "h_CPWZO5Swkl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# SQL-Datenbank initialisieren\n",
    "DB_PATH = \"/content/northwind.db\"\n",
    "DB_URI = f\"sqlite:///{DB_PATH}\"\n",
    "\n",
    "db = SQLDatabase.from_uri(DB_URI)"
   ],
   "metadata": {
    "id": "7i71YL5THyox"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Erweiterten Prompt f√ºr SQL-Abfragen erstellen\n",
    "prompt_template = \"\"\"\n",
    "Du bist ein SQL-Experte. Deine Aufgabe ist es, Benutzeranfragen in SQL-Abfragen zu √ºbersetzen.\n",
    "Verwende die SQLite-Syntax und nur die Tabellen und Spalten aus dem bereitgestellten Schema.\n",
    "Schreibe NUR die SQL-Abfrage ohne Pr√§fixe oder Kommentare.\n",
    "Gebe neben den Id auch den Namen von Produkten, Kunden, etc. mit aus.\n",
    "Gebe maximal 10 Zeilen einer Liste aus.\n",
    "\n",
    "Wichtig: Bei Ja/Nein-Fragen oder Fragen, die eine Analyse erfordern (z.B. \"Sind alle Artikel auf Lager?\"),\n",
    "erstelle eine SQL-Abfrage, die ALLE relevanten Daten zur√ºckgibt, damit eine fundierte Antwort gegeben werden kann.\n",
    "F√ºr komplexe Fragen mit Bedingungen wie \"vom 1998-05-06\" oder einem bestimmten Kundennamen,\n",
    "stelle sicher, dass diese Bedingungen in der WHERE-Klausel korrekt ber√ºcksichtigt werden.\n",
    "Achte darauf, ob bei der Frage nach einer Id oder dem Namen von Produkten, Kunden, Unternehmen, etc. gefragt wird.\n",
    "\n",
    "Datenbank-Schema:\n",
    "{schema}\n",
    "\n",
    "Benutzeranfrage: {query}\n",
    "\n",
    "SQL-Abfrage:\n",
    "\"\"\"\n",
    "\n",
    "# Template f√ºr die Ergebnisinterpretation\n",
    "analysis_template = \"\"\"\n",
    "Du bist ein Business-Analyst, der SQL-Abfrageergebnisse interpretiert und verst√§ndliche Antworten gibt.\n",
    "Beantworte die Benutzeranfrage basierend auf den SQL-Ergebnissen.\n",
    "\n",
    "Bei Ja/Nein-Fragen gib eine klare Antwort und erkl√§re die Gr√ºnde.\n",
    "Bei Fragen nach Empfehlungen oder notwendigen Anpassungen, analysiere die Daten und gib konkrete Vorschl√§ge.\n",
    "\n",
    "Benutzeranfrage: {query}\n",
    "SQL-Abfrage: {sql_query}\n",
    "Abfrageergebnisse:\n",
    "{results}\n",
    "\n",
    "Deine Analyse und Antwort:\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "QN7uJUGyTDi-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# SQL erstellen\n",
    "def get_schema(_):\n",
    "    return db.get_table_info()\n",
    "\n",
    "sql_generator = (\n",
    "    RunnablePassthrough.assign(schema=get_schema)\n",
    "    | PromptTemplate.from_template(prompt_template)\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "metadata": {
    "id": "BhYJbvlVTq_I"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Datenbank abfragen\n",
    "def execute_query(sql_query: str) -> str:\n",
    "    \"\"\"F√ºhrt eine SQL-Abfrage aus und formatiert die Ergebnisse als String.\"\"\"\n",
    "    try:\n",
    "        # Bereinige die Abfrage von eventuellen Formatierungen\n",
    "        cleaned_query = sql_query.strip()\n",
    "\n",
    "        conn = sqlite3.connect(DB_PATH)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(cleaned_query)\n",
    "\n",
    "        # Spalten√ºberschriften abrufen\n",
    "        column_names = [description[0] for description in cursor.description]\n",
    "\n",
    "        # Ergebnisse abrufen\n",
    "        results = cursor.fetchall()\n",
    "\n",
    "        # Ergebnisse formatieren\n",
    "        output = \"| \" + \" | \".join(column_names) + \" |\\n\"\n",
    "        output += \"| \" + \" | \".join([\"---\" for _ in column_names]) + \" |\\n\"\n",
    "\n",
    "        for row in results:\n",
    "            output += \"| \" + \" | \".join([str(cell) for cell in row]) + \" |\\n\"\n",
    "\n",
    "        conn.close()\n",
    "\n",
    "        # Keine Ergebnisse gefunden\n",
    "        if len(results) == 0:\n",
    "            return \"Keine Ergebnisse gefunden.\"\n",
    "\n",
    "        return output\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Fehler bei der Ausf√ºhrung der Abfrage: {str(e)}\\nAbfrage: {cleaned_query}\""
   ],
   "metadata": {
    "id": "sEBK76iATu05"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Analyse des Ergebnisses der Datenbank-Abfrage durch ein LLM\n",
    "def analyze_results(query, sql_query, results):\n",
    "    \"\"\"Analysiert die Ergebnisse und gibt eine nat√ºrlichsprachliche Antwort zur√ºck.\"\"\"\n",
    "    analysis_prompt = PromptTemplate.from_template(analysis_template)\n",
    "    analysis_chain = analysis_prompt | llm | StrOutputParser()\n",
    "\n",
    "    return analysis_chain.invoke({\n",
    "        \"query\": query,\n",
    "        \"sql_query\": sql_query,\n",
    "        \"results\": results\n",
    "    })"
   ],
   "metadata": {
    "id": "bXSgvVGaT2DG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Funktion f√ºr Aufruf der Kette aus Gradio\n",
    "def chatbot_response(mesSQL, chat_history):\n",
    "    \"\"\"Verarbeitet Benutzeranfragen, erstellt SQL und gibt formatierte Ergebnisse mit Analyse zur√ºck.\"\"\"\n",
    "    try:\n",
    "        # SQL-Abfrage mit LLM generieren\n",
    "        sql_query = sql_generator.invoke({\"query\": mesSQL})\n",
    "\n",
    "        # Bereinige eventuelles Markdown-Markup\n",
    "        sql_query = re.sub(r'```sql\\s*(.*?)\\s*```', r'\\1', sql_query, flags=re.DOTALL)\n",
    "        sql_query = sql_query.replace(\"```\", \"\").strip()\n",
    "\n",
    "        # Debug-Ausgabe\n",
    "        print(f\"Generierte SQL: {sql_query}\")\n",
    "\n",
    "        # F√ºhre die Abfrage aus\n",
    "        results = execute_query(sql_query)\n",
    "\n",
    "        # Analysiere die Ergebnisse f√ºr komplexe Fragen\n",
    "        analysis = analyze_results(mesSQL, sql_query, results)\n",
    "\n",
    "        # Antwort formatieren\n",
    "        response = f\"### Deine Anfrage\\n{mesSQL}\\n\\n### SQL-Abfrage\\n```sql\\n{sql_query}\\n```\\n\\n### Ergebnisse\\n{results}\\n\\n### Analyse\\n{analysis}\"\n",
    "\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Ein Fehler ist aufgetreten: {str(e)}\""
   ],
   "metadata": {
    "id": "7fsjmBe_T90o"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7 | Agent\n",
    "---"
   ],
   "metadata": {
    "id": "PWZ4wa2uxxVq"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXGWj0hMxxVq"
   },
   "source": [
    "# 7.1 | @tool Decorator üìå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbUMaL96xxVq"
   },
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "# Tool-Definition mit @tool Decorator (MODERN - LangChain 1.0+)\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multipliziert zwei Zahlen miteinander.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Addiert zwei Zahlen.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def divide(a: float, b: float) -> float:\n",
    "    \"\"\"Dividiert zwei Zahlen. Gibt einen Fehler zur√ºck, wenn durch Null geteilt wird.\"\"\"\n",
    "    if b == 0:\n",
    "        return \"Fehler: Division durch Null\"\n",
    "    return a / b\n",
    "\n",
    "# Tools-Liste f√ºr Agent\n",
    "tools = [multiply, add, divide]\n",
    "\n",
    "# Tool-Schema anzeigen (automatisch generiert aus Docstring und Type Hints)\n",
    "print(f\"üõ†Ô∏è Verf√ºgbare Tools: {[tool.name for tool in tools]}\")\n",
    "print(f\"üìù Tool-Schema multiply:\n",
    "{multiply.args_schema.schema()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "St7_W_dKxxVr"
   },
   "source": [
    "# 7.2 | create_agent() üìå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czmsENnqxxVr"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# LLM initialisieren (Kurznotation: \"provider:model\")\n",
    "llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "# Agent erstellen\n",
    "# Gibt CompiledStateGraph zur√ºck (basiert auf LangGraph\\!)\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    system_prompt=\"Du bist ein hilfreicher Mathe-Assistent. Nutze die verf√ºgbaren Tools f√ºr Berechnungen.\"\n",
    ")\n",
    "\n",
    "# Agent aufrufen\n",
    "response = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Was ist 25 multipliziert mit 4?\"}]\n",
    "})\n",
    "\n",
    "print(\"ü§ñ Agent-Antwort:\")\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaI4XbyixxVr"
   },
   "source": [
    "# 7.3 | Middleware f√ºr Agents üìå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ilJplGPxxVr"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import (\n",
    "    HumanInTheLoopMiddleware,\n",
    "    SummarizationMiddleware,\n",
    "    PIIMiddleware\n",
    ")\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Sensible Tools definieren\n",
    "@tool\n",
    "def delete_file(filename: str) -> str:\n",
    "    \"\"\"L√∂scht eine Datei (VORSICHT: Sensible Operation\\!).\"\"\"\n",
    "    return f\"Datei {filename} w√ºrde gel√∂scht werden\"\n",
    "\n",
    "@tool\n",
    "def send_email(to: str, subject: str, body: str) -> str:\n",
    "    \"\"\"Sendet eine E-Mail.\"\"\"\n",
    "    return f\"E-Mail an {to} w√ºrde gesendet: {subject}\"\n",
    "\n",
    "sensitive_tools = [delete_file, send_email]\n",
    "\n",
    "# Middleware konfigurieren\n",
    "middleware = [\n",
    "    # 1. Human-in-the-Loop: Genehmigung f√ºr sensible Operationen\n",
    "    HumanInTheLoopMiddleware(\n",
    "        tool_names=[\"delete_file\", \"send_email\"]  # Requires approval\n",
    "    ),\n",
    "\n",
    "    # 2. Summarization: Automatische Kontext-Zusammenfassung bei langen Sessions\n",
    "    SummarizationMiddleware(\n",
    "        max_tokens=1000  # Summarize wenn Kontext > 1000 Tokens\n",
    "    ),\n",
    "\n",
    "    # 3. PII-Middleware: Datenschutz durch Mustererkennung\n",
    "    PIIMiddleware(\n",
    "        patterns={\"email\": r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"},\n",
    "        redact=True  # PII-Daten schw√§rzen\n",
    "    )\n",
    "]\n",
    "\n",
    "# Agent mit Middleware erstellen\n",
    "secure_agent = create_agent(\n",
    "    model=\"openai:gpt-4o-mini\",\n",
    "    tools=sensitive_tools,\n",
    "    system_prompt=\"Du bist ein Assistent mit Zugriff auf sensible Operationen.\",\n",
    "    middleware=middleware,  # Middleware hinzuf√ºgen\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "print(\"üîí Secure Agent mit Middleware erstellt\")\n",
    "print(f\"üõ°Ô∏è Middleware aktiv: {len(middleware)} Module\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_HjwWhwxxVr"
   },
   "source": [
    "# 7.4 | SystemMessage mit Cache-Control (v1.1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPwlJ1GKxxVr"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# SystemMessage mit Cache-Control (Anthropic-Feature f√ºr schnellere + g√ºnstigere Aufrufe)\n",
    "system_message = SystemMessage(\n",
    "    content=\"Du bist ein hilfreicher Research-Assistent mit Zugriff auf Mathe-Tools.\",\n",
    "    cache_control={\"type\": \"ephemeral\"}  # Cache f√ºr wiederholte Aufrufe\n",
    ")\n",
    "\n",
    "# Agent mit SystemMessage erstellen (NEU in LangChain v1.1.0)\n",
    "cached_agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    system_prompt=system_message,  # Akzeptiert jetzt auch SystemMessage\\!\n",
    "\n",
    "print(\"‚ö° Agent mit SystemMessage Cache-Control erstellt\")"
   ]
  }
 ]
}
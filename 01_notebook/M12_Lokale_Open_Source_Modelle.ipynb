{"cells":[{"cell_type":"markdown","source":["<p><font size=\"6\" color='grey'> <b>\n","\n","Generative KI. Verstehen. Anwenden. Gestalten.\n","</b></font> </br></p>"],"metadata":{"id":"Ih2CTVBnArVZ"},"id":"Ih2CTVBnArVZ"},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b>\n","Lokale und Open Source Modelle\n","</b></font> </br></p>\n","\n","\n","---"],"metadata":{"id":"6jJZ7wbdArVc"},"id":"6jJZ7wbdArVc"},{"cell_type":"code","source":["#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n","!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul\n","from genai_lib.utilities import (\n","    check_environment,\n","    get_ipinfo,\n","    setup_api_keys,\n","    mprint,\n","    install_packages,\n","    mermaid,\n","    get_model_profile,\n","    extract_thinking,\n","    load_prompt\n",")\n","print()\n","check_environment()\n","print()\n","get_ipinfo()"],"metadata":{"id":"9BQvZ42UvF4D","collapsed":true},"id":"9BQvZ42UvF4D","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title üõ†Ô∏è Installationen { display-mode: \"form\" }\n","install_packages(['pypdf'])"],"metadata":{"id":"2yYp9rLf3Spn"},"id":"2yYp9rLf3Spn","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","‚è∏Ô∏è 5-Minuten-Check:\n","</font></p>\n","\n","**Ziel:** Pr√ºfen, ob du das vorherige Kapitel verstanden hast ‚Äì nicht, ob es gerade l√§uft.\n","\n","**Aufgabe** (5 Minuten, ohne Vorlage):\n","\n","Rekonstruiere die zentrale Idee oder Code-Struktur des letzten Abschnitts selbstst√§ndig\n","(kein Copy & Paste, kein Nachschlagen).\n","\n","W√§hle eine der folgenden Optionen:\n","\n","+ Erkl√§re in 1‚Äì2 S√§tzen, was hier konzeptionell passiert.\n","\n","+ Ver√§ndere eine Kleinigkeit (z. B. Prompt, Parameter, Reihenfolge) und beschreibe die Auswirkung.\n","\n","+ Markiere eine Stelle, die du nicht sicher erkl√§ren kannst, und formuliere eine konkrete Frage dazu.\n","\n","**Hinweis:**\n","Nicht alles muss ‚Äûfertig‚Äú oder ‚Äûkorrekt‚Äú sein. Entscheidend ist, wo dein Verst√§ndnis gerade endet"],"metadata":{"id":"khry3qeZbwLE"},"id":"khry3qeZbwLE"},{"cell_type":"markdown","id":"951a5960","metadata":{"id":"951a5960"},"source":["# 1 | Einf√ºhrung\n","---\n"]},{"cell_type":"markdown","source":["Lokale und Open Source Modelle bieten eine Alternative zu Cloud-basierten KI-Diensten wie OpenAI. Im Gegensatz zu API-basierten Diensten werden diese Modelle direkt auf dem lokalen System ausgef√ºhrt, was Vorteile in Bezug auf Datenschutz, Kosten und Anpassungsm√∂glichkeiten bietet.\n","\n"],"metadata":{"id":"HWIvNajgmCCb"},"id":"HWIvNajgmCCb"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Vor- und Nachteile lokaler Modelle\n","</font></p>\n","\n","- **Vorteile**: Datenschutz, keine API-Kosten, keine Internetverbindung erforderlich, volle Kontrolle\n","- **Nachteile**: H√∂here Hardwareanforderungen, begrenzte Modellleistung auf Standard-Hardware, Installations- und Wartungsaufwand\n","\n"],"metadata":{"id":"-y5UoxXMmUkV"},"id":"-y5UoxXMmUkV"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Anwendungsbereiche lokaler Modelle\n","</font></p>\n","\n","- Entwicklung und Prototyping\n","- Datenschutzkritische Anwendungen\n","- Offline-Anwendungen\n","- Kostensensitive Projekte\n","\n"],"metadata":{"id":"OMbXZAO-maLa"},"id":"OMbXZAO-maLa"},{"cell_type":"markdown","source":["# 2 | Ollama\n","---"],"metadata":{"id":"4uBmvLYwmhnr"},"id":"4uBmvLYwmhnr"},{"cell_type":"markdown","source":["<img src=\"https://ollama.com/public/ollama.png\" class=\"logo\" width=\"120\"/>\n","\n","\n","\n"],"metadata":{"id":"5yIlnTErB_cg"},"id":"5yIlnTErB_cg"},{"cell_type":"markdown","source":["Ollama ist ein Tool, das die einfache Installation und Nutzung von Open Source LLMs auf dem eigenen Computer erm√∂glicht. Es bietet eine benutzerfreundliche Schnittstelle zum Herunterladen, Verwalten und Ausf√ºhren verschiedener Sprachmodelle.\n","\n","[Ollama](https://ollama.com/)"],"metadata":{"id":"aUBszZUdmn-h"},"id":"aUBszZUdmn-h"},{"cell_type":"code","source":["#@title üßú‚Äç‚ôÄÔ∏è Prozess-Diagramm { display-mode: \"form\" }\n","\n","diagram = \"\"\"\n","graph TD\n","    subgraph Colab [\"<b>Google Colab Runtime</b>\"]\n","        direction TB\n","\n","        Code[\"Python Code<br/><small>Notebook-Zelle</small>\"]\n","\n","        subgraph Backend [\"Hintergrund\"]\n","            Ollama[\"Ollama Server<br/><small>localhost:11434</small>\"]\n","            Models[\"Lokale Modelle\"]\n","        end\n","\n","        GPU[\"GPU<br/><small>T4 / A100</small>\"]\n","    end\n","\n","    subgraph Extern [\"Extern\"]\n","        Registry[\"Ollama Registry\"]\n","    end\n","\n","    %% Haupt-Workflow (nummeriert)\n","    Code -->|\"#1 API Call\"| Ollama\n","    Ollama -->|\"#2 laden\"| Models\n","    Ollama -->|\"#3 Inferenz\"| GPU\n","    GPU -->|\"#4 Ergebnis\"| Ollama\n","    Ollama -->|\"#5 Response\"| Code\n","\n","    %% Setup (gestrichelt)\n","    Registry -.->|\"ollama pull\"| Models\n","\n","    %% Styling\n","    style Colab fill:#f8f9fa,stroke:#495057,stroke-width:2px\n","    style GPU fill:#d4edda,stroke:#28a745,stroke-width:2px\n","    style Ollama fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\n","    style Registry fill:#fff3cd,stroke:#ffc107\n","\"\"\"\n","mermaid(diagram)"],"metadata":{"id":"zaf0H7X9Tq8E"},"id":"zaf0H7X9Tq8E","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Installation von Ollama\n","</font></p>"],"metadata":{"id":"LBO99aq0ncRs"},"id":"LBO99aq0ncRs"},{"cell_type":"code","execution_count":null,"id":"4b4072af","metadata":{"id":"4b4072af"},"outputs":[],"source":["# Installation in Linux/Colab\n","!sudo apt update\n","!sudo apt install -y pciutils\n","!sudo apt-get install zstd\n","!curl -fsSL https://ollama.com/install.sh | sh"]},{"cell_type":"markdown","id":"f4d7b10b","metadata":{"id":"f4d7b10b"},"source":["<p><font color='black' size=\"5\">\n","Starten des Ollama-Servers\n","</font></p>"]},{"cell_type":"code","execution_count":null,"id":"38bec64b","metadata":{"id":"38bec64b"},"outputs":[],"source":["import threading\n","import subprocess\n","import time\n","\n","def run_ollama_serve():\n","  subprocess.Popen([\"ollama\", \"serve\"])\n","\n","thread = threading.Thread(target=run_ollama_serve)\n","thread.start()\n","time.sleep(10)  # Kurz warten, bis der Server hochgefahren ist"]},{"cell_type":"markdown","id":"f788cb40","metadata":{"id":"f788cb40"},"source":["<p><font color='black' size=\"5\">\n","Modelle herunterladen und verwalten\n","</font></p>"]},{"cell_type":"code","execution_count":null,"id":"4111d55e","metadata":{"id":"4111d55e","collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4a14f79c-d623-4605-d6a5-86160e90716e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l"]}],"source":["# Modell herunterladen\n","!ollama pull llama3:latest"]},{"cell_type":"code","source":["# Modell herunterladen\n","!ollama pull qwen3:latest"],"metadata":{"id":"11hJqakJi4QF"},"id":"11hJqakJi4QF","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verf√ºgbare Modelle anzeigen\n","!ollama list"],"metadata":{"id":"JdkzoYBkJ2jb"},"id":"JdkzoYBkJ2jb","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"b8ee185b","metadata":{"id":"b8ee185b"},"source":["# 3 | Lokale Modelle und LangChain\n","---"]},{"cell_type":"markdown","source":["LangChain bietet eine nahtlose Integration mit Ollama, wodurch lokale Modelle in komplexen Anwendungen eingesetzt werden k√∂nnen.\n","\n"],"metadata":{"id":"TONpdbWknQWS"},"id":"TONpdbWknQWS"},{"cell_type":"markdown","id":"90c86bc3","metadata":{"id":"90c86bc3"},"source":["<p><font color='black' size=\"5\">\n","Einfache Verwendung von Ollama mit LangChain\n","</font></p>"]},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers.string import StrOutputParser\n","from langchain.chat_models import init_chat_model\n","\n","# Prompt\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller KI-Assistent. Antworte sachlich und kurz.\"),\n","    (\"human\", \"Frage: {user_input}\\n\\nAntwort: Denke Schritt f√ºr Schritt.\")\n","])\n","\n","# Model\n","llm = init_chat_model(\"ollama:llama3\", temperature=0.1)\n","\n","# Parser\n","parser = StrOutputParser()\n","\n","# Chain zusammensetzen und ausf√ºhren\n","chain = prompt | llm | parser"],"metadata":{"id":"X3zQfQ2Iv-d8"},"id":"X3zQfQ2Iv-d8","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"2e35d57b","metadata":{"id":"2e35d57b"},"source":["<p><font color='black' size=\"5\">\n","Beispiele Llama 3\n","</font></p>"]},{"cell_type":"code","execution_count":null,"id":"cb2b8771","metadata":{"lines_to_next_cell":2,"id":"cb2b8771"},"outputs":[],"source":["# Beispiel 1: Gesundheitsfrage\n","response = chain.invoke({\"user_input\": \"Was ist der einfachste Weg Gewicht abzunehmen?\"})\n","mprint(\"## ü§ñ Local LLM\")\n","mprint(response)"]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Textgenerierung (aus M05)\n","</font></p>"],"metadata":{"id":"W9XzNlzUyd4y"},"id":"W9XzNlzUyd4y"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9y7B2qfuB2Xs"},"outputs":[],"source":["from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"],"id":"9y7B2qfuB2Xs"},{"cell_type":"code","execution_count":null,"metadata":{"id":"AkkALStvH4ul"},"outputs":[],"source":["# Abschnitt 3: Chain-Komponenten initialisieren\n","\n","system_prompt = \"\"\"\n","Du bist ein hilfreicher KI-Assistent.\n","\"\"\"\n","\n","# Die Chat-Prompt definieren mit drei Variablen: system_prompt, history, user_input\n","prompt = ChatPromptTemplate.from_messages([\n","        (\"system\", \"{system_prompt}\"),\n","        MessagesPlaceholder(variable_name=\"chat_history\"),\n","        (\"human\", \"{user_input}\")\n","])\n","\n","# OutputParser definieren\n","parser = StrOutputParser()\n","\n","# Chain erstellen\n","chain = prompt | llm | parser"],"id":"AkkALStvH4ul"},{"cell_type":"code","execution_count":null,"metadata":{"id":"kzhjBJEsH9h1"},"outputs":[],"source":["# Abschnitt 4: Funktionen definieren\n","\n","def chat(system_prompt, chat_history, user_input):\n","    \"\"\"F√ºhrt eine einzelne Interaktion mit der KI durch.\"\"\"\n","\n","    # Aufruf der Kette\n","    parameter = {\n","        'system_prompt': system_prompt,\n","        'chat_history': chat_history,\n","        'user_input': user_input\n","    }\n","    response = chain.invoke(parameter)\n","\n","    # Ausgabe der Ergebnisse\n","    mprint(\"### üßë‚Äçü¶± Mensch:\")\n","    mprint(user_input)\n","\n","    mprint(\"### ü§ñ KI:\")\n","    mprint(response)\n","\n","    # Memory-Management - Erweiterung der Historie\n","    chat_history.extend([\n","        HumanMessage(content=user_input),\n","        AIMessage(content=response)\n","    ])\n","\n","    return chat_history"],"id":"kzhjBJEsH9h1"},{"cell_type":"code","execution_count":null,"metadata":{"id":"TGuKwTyVJ-wt"},"outputs":[],"source":["# Abschnitt 5: Hauptprogramm\n","\n","# Historie wird initialisiert\n","chat_history = [SystemMessage(content=system_prompt)]\n","\n","# Liste mit user_input\n","user_input_list = [\n","    \"Mein Name ist Ralf\",\n","    \"Warum ist der Himmel blau?\",\n","    \"Und warum ist er manchmal rot?\",\n","    \"Wie ist mein Name?\"\n","]\n","\n","# Chain-Aufruf in einer Schleife\n","for user_input in user_input_list:\n","    chat_history = chat(system_prompt, chat_history, user_input)"],"id":"TGuKwTyVJ-wt"},{"cell_type":"code","source":["chat_history"],"metadata":{"id":"Pz5SNIrNMwGb","collapsed":true},"execution_count":null,"outputs":[],"id":"Pz5SNIrNMwGb"},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Textklassifizierung (aus M05)\n","</font></p>"],"metadata":{"id":"7BnhT7TIzLqZ"},"id":"7BnhT7TIzLqZ"},{"cell_type":"code","execution_count":null,"metadata":{"id":"BGe0n8gyXL19"},"outputs":[],"source":["# Konstanten definieren\n","system_prompt = \"\"\"\n","Du bist ein pr√§ziser Namensextraktor.\n","\n","**Aufgabe:** Extrahiere ausschlie√ülich die Namen aus dem gegebenen Text.\n","\n","**Was zu extrahieren ist:**\n","- Personennamen (Vor- und Nachnamen)\n","- Unternehmensnamen\n","- Produktnamen\n","\n","**Ausgabeformat:**\n","- Ein Name pro Zeile\n","- Keine Nummerierung\n","- Keine Aufz√§hlungszeichen\n","- Keine zus√§tzlichen W√∂rter oder Erkl√§rungen\n","- Keine Duplikate\n","\n","**Wichtig:**\n","- Gib NUR die extrahierten Namen aus\n","- Formuliere den Originaltext NICHT um\n","- F√ºge KEINE weiteren Informationen hinzu\n","\n","**Beispiel:**\n","Input: \"Peter M√ºller arbeitet bei der Siemens AG und entwickelt das NextPhone.\"\n","Output:\n","Peter M√ºller\n","Siemens AG\n","NextPhone\n","\"\"\"\n","\n","user_input = \"\"\"\n","Anna und Sarah sind seit dem Studium befreundet. Nach ihrem Abschluss hatten sie ein Vorstellungsgespr√§ch bei Frau Weber f√ºr eine Stelle bei der Technovision GmbH, deren Hauptprodukt Futurtech hei√üt und von der Programmiererin Maria Schmidt entwickelt wurde. Nach dem erfolgreichen Gespr√§ch trafen sie sich mit ihren Freunden Rafael und Aida zum Feiern. An ihrem ersten Arbeitstag lernten sie drei weitere Mitarbeiter kennen: Richard, Lisa und Pia. Sp√§ter kam noch Matthias M√ºller zum Team dazu.\n","\"\"\""],"id":"BGe0n8gyXL19"},{"cell_type":"code","source":["# Model\n","llm = init_chat_model(\"ollama:llama3\", temperature=0.1)\n","\n","# Parser\n","parser = StrOutputParser()\n","\n","# Chain zusammensetzen und ausf√ºhren\n","chain = prompt | llm | parser"],"metadata":{"id":"kqNU-R7IAt8l"},"id":"kqNU-R7IAt8l","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uCnJFJNNXzVE"},"outputs":[],"source":["# Hauptprogramm\n","chat_history = [SystemMessage(content=system_prompt)]\n","\n","# Namen extrahieren und anzeigen\n","parameter = {}\n","parameter['system_prompt'] = system_prompt\n","parameter['chat_history'] = chat_history\n","parameter['user_input'] = user_input\n","response = chain.invoke(parameter)\n","\n","# Ausgabe\n","mprint(\"## üìõ Gefundene Namen:\")\n","mprint(\"---\")\n","print(response)"],"id":"uCnJFJNNXzVE"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Textzusammenfassung (aus M05)\n","</font></p>"],"metadata":{"id":"5h_feD4d1qBu"},"id":"5h_feD4d1qBu"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2D7E3xrPL_kv"},"outputs":[],"source":["# Importe f√ºr Dokument-Verarbeitung\n","from langchain_community.document_loaders import PyPDFLoader, TextLoader\n","from langchain_text_splitters import CharacterTextSplitter\n","from langchain_core.documents import Document"],"id":"2D7E3xrPL_kv"},{"cell_type":"code","execution_count":null,"metadata":{"id":"S5mnj65YMOEU"},"outputs":[],"source":["# Chat-Komponenten initialisieren\n","\n","# Chat-Prompt-Template erstellen\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent, der pr√§zise Zusammenfassungen erstellt.\"),\n","    (\"human\", \"\"\"Schreibe eine kurze Zusammenfassung der pr√§sentierten Informationen.\n","    Schreibe die Zusammenfassung auf Deutsch.\n","\n","{user_input}\n","\n","ZUSAMMENFASSUNG:\"\"\")\n","])\n","\n","# Modell definieren (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"ollama:qwen3\", temperature=0.3)\n","\n","# Parser definieren\n","parser = StrOutputParser()\n","\n","# Kette erstellen\n","chain = prompt | llm | parser"],"id":"S5mnj65YMOEU"},{"cell_type":"code","execution_count":null,"metadata":{"id":"JRXU3YJoMRSS"},"outputs":[],"source":["# Funktionen definieren\n","def load_document(url: str):\n","    \"\"\"L√§dt ein PDF-Dokument von einer URL und splittet es in Chunks\"\"\"\n","    loader = PyPDFLoader(url)\n","    return loader.load_and_split()"],"id":"JRXU3YJoMRSS"},{"cell_type":"code","execution_count":null,"metadata":{"id":"reYM_dPCMeLa"},"outputs":[],"source":["# Hauptprogramm\n","url = \"https://arxiv.org/pdf/1706.03762\"\n","\n","# Dokument laden\n","docs = load_document(url)\n","\n","# Alle Chunks zu einem Text zusammenf√ºgen\n","text = \" \".join([doc.page_content for doc in docs])\n","\n","# Zusammenfassung generieren\n","response = chain.invoke({\"user_input\": text})\n","\n","# Ausgabe\n","mprint(\"## ‚ú® Zusammenfassung:\")\n","mprint(\"---\")\n","mprint(response)"],"id":"reYM_dPCMeLa"},{"cell_type":"markdown","id":"efe2093b","metadata":{"id":"efe2093b"},"source":["\n","# 4 | Open Source vs. Closed Source\n","---"]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Open Source Modelle\n","</font></p>"],"metadata":{"id":"eU1Vqvarn-WV"},"id":"eU1Vqvarn-WV"},{"cell_type":"markdown","source":["\n","\n","Open Source KI-Modelle zeichnen sich durch ihre frei zug√§nglichen Gewichte und Architekturen aus. Diese Modelle werden unter Lizenzen wie MIT, Apache oder Creative Commons ver√∂ffentlicht, die es Nutzern erlauben, den Code einzusehen, zu modifizieren und f√ºr eigene Zwecke anzupassen.\n","\n","**Vorteile von Open Source Modellen:**\n","\n","- **Transparenz**: Einsicht in die Funktionsweise und Trainingsmethoden\n","- **Anpassbarkeit**: M√∂glichkeit zum Fine-Tuning f√ºr spezifische Anwendungsf√§lle\n","- **Keine API-Kosten**: Nach dem Download entstehen keine laufenden Nutzungskosten\n","- **Datenschutz**: Vollst√§ndige Kontrolle √ºber die Datenverarbeitung\n","- **Community-Unterst√ºtzung**: Regelm√§√üige Verbesserungen durch die Community\n","\n","**Herausforderungen:**\n","\n","- **Hardwareanforderungen**: Gr√∂√üere Modelle ben√∂tigen leistungsstarke Hardware\n","- **Expertise**: Technisches Know-how f√ºr Installation und Optimierung n√∂tig\n","- **Qualit√§tsunterschiede**: Leistung kann bei gleicher Gr√∂√üe hinter kommerziellen Modellen zur√ºckbleiben\n","\n"],"metadata":{"id":"aT17TvGpoL1U"},"id":"aT17TvGpoL1U"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Closed Source Modelle\n","</font></p>\n","\n","Closed Source Modelle wie GPT-4 von OpenAI oder Claude von Anthropic werden nur √ºber APIs angeboten, ohne Zugriff auf die zugrunde liegenden Modellgewichte.\n","\n","**Vorteile von Closed Source Modellen:**\n","\n","- **Leistungsst√§rke**: Oft fortschrittlicher als vergleichbare Open Source Alternativen\n","- **Einfache Nutzung**: Keine lokale Installation oder Wartung n√∂tig\n","- **Regelm√§√üige Updates**: Automatische Verbesserungen ohne eigenen Aufwand\n","- **Skalierbarkeit**: Flexibel an unterschiedlichen Bedarf anpassbar\n","\n","**Nachteile:**\n","\n","- **Kosten**: Nutzungsbasierte Geb√ºhren\n","- **Abh√§ngigkeit**: Angewiesenheit auf externe Dienste und deren Verf√ºgbarkeit\n","- **Datenschutzbedenken**: Daten verlassen das eigene System\n","- **Eingeschr√§nkte Anpassbarkeit**: Beschr√§nkte M√∂glichkeiten zur Modifikation\n","\n"],"metadata":{"id":"dUVX_bNioOvm"},"id":"dUVX_bNioOvm"},{"cell_type":"markdown","source":[],"metadata":{"id":"kjnEG0KdE9v6"},"id":"kjnEG0KdE9v6"},{"cell_type":"markdown","source":["# 5 | Lokal vs. Cloud\n","---\n"],"metadata":{"id":"Vj0-aBRiE-H1"},"id":"Vj0-aBRiE-H1"},{"cell_type":"markdown","source":["Beim Arbeiten mit generativer KI gibt es grunds√§tzlich zwei M√∂glichkeiten: Modelle k√∂nnen **lokal auf dem eigenen Rechner** ausgef√ºhrt werden oder es werden **Cloud-Dienste** wie ChatGPT, Claude oder Google Gemini genutzt.\n","\n","**Cloud-Ausf√ºhrung**\n","\n","Bei Cloud-Diensten l√§uft das KI-Modell auf Servern des Anbieters. Die Anfrage wird √ºber das Internet gesendet, und der Server schickt die Antwort zur√ºck.\n","\n","**Vorteile:**\n","- Sofort nutzbar, keine Installation n√∂tig\n","- Zugang zu den leistungsst√§rksten Modellen (GPT-4, Claude Opus, Gemini Ultra)\n","- Keine teure Hardware erforderlich\n","- Modelle werden automatisch aktualisiert\n","\n","**Nachteile:**\n","- Daten verlassen den eigenen Rechner\n","- Laufende Kosten (oft pro Anfrage oder Abo)\n","- Internetverbindung erforderlich\n","- Abh√§ngigkeit vom Anbieter\n","\n","**Lokale Ausf√ºhrung**\n","\n","Bei lokaler Ausf√ºhrung l√§uft das Modell direkt auf dem eigenen Computer ‚Äì zum Beispiel mit Tools wie Ollama, LM Studio oder llama.cpp.\n","\n","**Vorteile:**\n","- Volle Datenkontrolle ‚Äì nichts verl√§sst den Rechner\n","- Keine laufenden Kosten nach dem Download\n","- Funktioniert offline\n","- Ideal f√ºr sensible oder vertrauliche Daten\n","\n","**Nachteile:**\n","- Leistungsstarke Hardware n√∂tig (besonders GPU mit viel VRAM)\n","- Lokale Modelle sind meist kleiner und weniger leistungsf√§hig\n","- Einrichtung erfordert technisches Know-how\n","- Updates m√ºssen selbst durchgef√ºhrt werden"],"metadata":{"id":"JDCVYWBQFD5Z"},"id":"JDCVYWBQFD5Z"},{"cell_type":"markdown","source":["# 6 | Lizenzierung und rechtliche Aspekte\n","---"],"metadata":{"id":"vOXxrUBro9I8"},"id":"vOXxrUBro9I8"},{"cell_type":"markdown","source":["Die Lizenzen von Open Source Modellen unterscheiden sich erheblich und beeinflussen deren Einsatzm√∂glichkeiten:\n","\n","**MIT Lizenz (z. B. OpenLLaMA, GPT-NeoX)**\n","- Sehr liberal, erlaubt kommerzielle Nutzung ohne Einschr√§nkungen  \n","- Erfordert lediglich die Nennung des urspr√ºnglichen Urhebers  \n","- Keine Weitergabelizenzverpflichtung (nicht copyleft)  \n","- Einfach und weit verbreitet in Open-Source-Projekten  \n","\n","**Apache 2.0 (z. B. Gemma, Falcon)**\n","- Sehr liberal, erlaubt kommerzielle Nutzung ohne starke Einschr√§nkungen  \n","- Erfordert Hinweis auf urspr√ºngliches Urheberrecht  \n","- Keine Weitergabelizenzverpflichtung (nicht copyleft)  \n","- Enth√§lt eine Klausel zum Schutz vor Patentstreitigkeiten  \n","\n","**Meta Llama Lizenz**\n","- Erlaubt kommerzielle Nutzung mit Einschr√§nkungen (z. B. Nutzerschwellen)  \n","- Enth√§lt spezifische Bedingungen f√ºr verantwortungsvolle Nutzung  \n","- Verbietet bestimmte Anwendungsf√§lle wie Desinformation oder illegale Aktivit√§ten  \n","- Nicht so frei wie Apache oder MIT, aber offener als propriet√§re Modelle  \n","\n","**Mistral AI Lizenz**\n","- Erlaubt kommerzielle und nicht-kommerzielle Nutzung  \n","- Enth√§lt ethische Nutzungsbeschr√§nkungen  \n","- Unterschiedliche Bedingungen f√ºr verschiedene Modellversionen  \n","- Im Allgemeinen offener als die Meta Llama Lizenz  \n","\n","**Creative Commons (z. B. BLOOM unter CC-BY-SA)**\n","- Verschiedene Varianten mit unterschiedlichem Freiheitsgrad  \n","- **CC-BY**: Erlaubt kommerzielle Nutzung, solange der Urheber genannt wird  \n","- **CC-BY-SA**: Weitergabe nur unter gleichen Lizenzbedingungen erlaubt (copyleft)  \n","- **CC-BY-NC**: Keine kommerzielle Nutzung erlaubt  \n","- **CC-BY-ND**: Keine Bearbeitung oder Ableitung erlaubt  \n","- H√§ufig f√ºr Open-Access-Modelle genutzt, aber mit Einschr√§nkungen je nach Variante  \n","\n","<br>\n","\n","**Disclaimer**    \n","Die oben aufgef√ºhrten Informationen zu Lizenzmodellen dienen ausschlie√ülich allgemeinen Informationszwecken und stellen keine rechtliche Beratung dar. Die Bedingungen einzelner Lizenzen k√∂nnen sich √§ndern, und ihre Auslegung kann von der jeweiligen Rechtsprechung abh√§ngen.\n","\n","Vor der Nutzung oder Weitergabe eines KI-Modells sollten die offiziellen Lizenzdokumente sorgf√§ltig gepr√ºft und bei Unsicherheiten eine juristische Beratung eingeholt werden. Wir √ºbernehmen keine Haftung f√ºr die Vollst√§ndigkeit, Richtigkeit oder Anwendbarkeit der bereitgestellten Informationen.\n","\n"],"metadata":{"id":"tNJ_JtxQpIwV"},"id":"tNJ_JtxQpIwV"},{"cell_type":"markdown","source":["# A | Aufgabe\n"],"metadata":{"id":"L0ZXAt1L6S1Z"},"id":"L0ZXAt1L6S1Z"},{"cell_type":"markdown","source":["Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen."],"metadata":{"id":"BYQ1yv0U6YKp"},"id":"BYQ1yv0U6YKp"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Vergleich von Open Source Modellen\n","</font></p>"],"metadata":{"id":"hsX9yWQQ9v8G"},"id":"hsX9yWQQ9v8G"},{"cell_type":"markdown","source":["Installieren Sie Ollama auf Ihrem lokalen System und vergleichen Sie die Leistung von mindestens zwei verschiedenen Modellen (z.B. Llama2 und Gemma) bei der Beantwortung derselben Prompts.\n","\n","Vergleichen Sie die Antworten der Modelle. Kriterien: Umfang, Qualit√§t, Antwortzeit, ...\n"],"metadata":{"id":"POK-FQXg96Ui"},"id":"POK-FQXg96Ui"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Chat-Anwendung mit lokalem LLM\n","</font></p>"],"metadata":{"id":"_3BXIh6M-cV8"},"id":"_3BXIh6M-cV8"},{"cell_type":"markdown","source":["\n","Entwickeln Sie eine einfache Gradio-basierte Chat-Anwendung, die ein lokales Modell √ºber Ollama nutzt. Die Anwendung soll:\n","\n","```python\n","import gradio as gr\n","from langchain_ollama import OllamaLLM\n","from langchain_core.messages import HumanMessage, AIMessage\n","import os\n","\n","# Ollama-LLM initialisieren\n","llm = OllamaLLM(model=\"gemma\", temperature=0.7)\n","\n","# Chat-Historie\n","chat_verlauf = []\n","\n","def chat_antwort(nachricht, verlauf):\n","    \"\"\"\n","    Verarbeitet die Nutzereingabe und generiert eine Antwort vom LLM\n","    \"\"\"\n","    # Aktuelle Nachricht zur Historie hinzuf√ºgen\n","    if nachricht:\n","        verlauf.append((nachricht, \"\"))\n","        \n","        # Chat-Kontext erstellen (begrenzt auf die letzten 5 Nachrichten f√ºr Effizienz)\n","        kontext = \"\"\n","        for i in range(max(0, len(verlauf)-5), len(verlauf)):\n","            if i < len(verlauf)-1:  # Alle vorherigen Nachrichten\n","                kontext += f\"Mensch: {verlauf[i][0]}\\nAssistent: {verlauf[i][1]}\\n\"\n","            else:  # Aktuelle Nachricht\n","                kontext += f\"Mensch: {verlauf[i][0]}\\nAssistent: \"\n","        \n","        # Antwort vom Modell generieren\n","        antwort = llm.invoke(kontext)\n","        \n","        # Antwort zur Historie hinzuf√ºgen\n","        verlauf[-1] = (nachricht, antwort)\n","    \n","    return \"\", verlauf\n","\n","# Gradio-Interface erstellen\n","with gr.Blocks(title=\"Lokaler KI-Assistent\") as demo:\n","    gr.Markdown(\"# Chat mit einem lokalen KI-Modell\")\n","    gr.Markdown(\"Dieser Chat verwendet das Gemma-Modell √ºber Ollama\")\n","    \n","    chatbot = gr.Chatbot(label=\"Chat-Verlauf\")\n","    msg = gr.Textbox(label=\"Ihre Nachricht\", placeholder=\"Schreiben Sie hier Ihre Nachricht...\")\n","    \n","    msg.submit(chat_antwort, [msg, chatbot], [msg, chatbot])\n","    \n","    gr.Markdown(\"### Hinweise zur Nutzung:\")\n","    gr.Markdown(\"- Die Antworten werden lokal auf Ihrem System generiert\")\n","    gr.Markdown(\"- Keine Daten werden an externe Server gesendet\")\n","    gr.Markdown(\"- Die Antwortgeschwindigkeit h√§ngt von Ihrer Hardware ab\")\n","\n","# Anwendung starten\n","demo.launch()\n","```\n","\n","Testen & erweitern Sie die Anwendung:\n","1. Testen Sie den Code, korrigieren Sie etwaige Fehler.\n","2. Modellauswahl (zwischen verschiedenen installierten Ollama-Modellen)\n","3. Parameter-Einstellungen (Temperatur, Top-K, Top-P)\n","4. Speichern und Laden von Chat-Verl√§ufen\n","5. M√∂glichkeit, die generierten Antworten zu bewerten (Daumen hoch/runter)"],"metadata":{"id":"gH6to8NF-r7I"},"id":"gH6to8NF-r7I"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","RAG-System mit lokalem LLM\n","</font></p>"],"metadata":{"id":"BKLNzHsa-x2V"},"id":"BKLNzHsa-x2V"},{"cell_type":"markdown","source":["\n","\n","Entwickeln Sie ein einfaches Retrieval-Augmented Generation (RAG) System, das lokale PDF-Dokumente einliest und die Beantwortung von Fragen mit einem lokalen Modell erm√∂glicht:\n","\n","```python\n","from langchain_ollama import OllamaLLM\n","from langchain_community.document_loaders import PyPDFLoader\n","from langchain_community.vectorstores import FAISS\n","from langchain_community.embeddings import HuggingFaceEmbeddings\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_core.prompts import PromptTemplate\n","import gradio as gr\n","import os\n","\n","# Modell initialisieren\n","llm = OllamaLLM(model=\"llama2\", temperature=0.1)\n","\n","# Embedding-Modell laden (lokal, ohne API-Calls)\n","embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n","\n","# Vektorstore f√ºr Dokumente\n","vector_store = None\n","\n","def dokumente_laden(dateien):\n","    \"\"\"L√§dt PDF-Dokumente und erstellt eine Vektordatenbank\"\"\"\n","    global vector_store\n","    \n","    dokumente = []\n","    for datei in dateien:\n","        loader = PyPDFLoader(datei.name)\n","        dokumente.extend(loader.load())\n","    \n","    # Dokumente in kleinere Chunks aufteilen\n","    text_splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=1000,\n","        chunk_overlap=200\n","    )\n","    chunks = text_splitter.split_documents(dokumente)\n","    \n","    # Vektordatenbank erstellen\n","    vector_store = FAISS.from_documents(chunks, embeddings)\n","    \n","    return f\"{len(chunks)} Textabschnitte aus {len(dateien)} Dokumenten geladen\"\n","\n","def frage_beantworten(frage):\n","    \"\"\"Beantwortet eine Frage basierend auf den geladenen Dokumenten\"\"\"\n","    if vector_store is None:\n","        return \"Bitte laden Sie zuerst Dokumente hoch.\"\n","    \n","    # Relevante Dokumente abrufen\n","    docs = vector_store.similarity_search(frage, k=3)\n","    \n","    # Kontext erstellen\n","    kontext = \"\\n\\n\".join([doc.page_content for doc in docs])\n","    \n","    # Prompt erstellen\n","    prompt_vorlage = \"\"\"\n","    Du bist ein hilfreicher Assistent, der Fragen basierend auf gegebenen Informationen beantwortet.\n","    \n","    Kontext:\n","    {kontext}\n","    \n","    Frage: {frage}\n","    \n","    Deine Antwort sollte sich auf die im Kontext gegebenen Informationen beschr√§nken.\n","    Wenn du keine Antwort im Kontext findest, sage ehrlich, dass du es nicht wei√üt.\n","    \"\"\"\n","    \n","    prompt = PromptTemplate(\n","        template=prompt_vorlage,\n","        input_variables=[\"kontext\", \"frage\"]\n","    )\n","    \n","    # Antwort vom LLM bekommen\n","    antwort = llm.invoke(prompt.format(kontext=kontext, frage=frage))\n","    \n","    return antwort\n","\n","# Gradio-Interface\n","with gr.Blocks() as demo:\n","    gr.Markdown(\"# Lokales Frage-Antwort-System mit RAG\")\n","    \n","    with gr.Tab(\"Dokumente laden\"):\n","        file_input = gr.File(file_count=\"multiple\", label=\"PDF-Dokumente\")\n","        upload_button = gr.Button(\"Dokumente laden\")\n","        upload_output = gr.Textbox(label=\"Status\")\n","        upload_button.click(dokumente_laden, inputs=[file_input], outputs=[upload_output])\n","    \n","    with gr.Tab(\"Fragen stellen\"):\n","        question_input = gr.Textbox(label=\"Ihre Frage zum Dokument\")\n","        answer_button = gr.Button(\"Frage stellen\")\n","        answer_output = gr.Textbox(label=\"Antwort\")\n","        answer_button.click(frage_beantworten, inputs=[question_input], outputs=[answer_output])\n","\n","demo.launch()\n","```\n","\n","Testen und erweitern Sie das System:\n","1. Testen & ggf. korrigieren des Code\n","2. Unterst√ºtzung f√ºr weitere Dateiformate (DOCX, TXT, HTML)\n","3. M√∂glichkeit, die Qualit√§t der Antworten zu verbessern durch:\n","   - Modellauswahl\n","   - Parameteroptimierung\n","   - Verbesserung des Prompts\n","4. Implementierung einer M√∂glichkeit zur Quellennachverfolgung (aus welchem Teil des Dokuments stammt die Information)\n","5. Visualisierung der relevanten Textpassagen im Dokument"],"metadata":{"id":"iOMIlyS4_MCd"},"id":"iOMIlyS4_MCd"}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"colab":{"provenance":[],"collapsed_sections":["951a5960","4uBmvLYwmhnr","b8ee185b","efe2093b","Vj0-aBRiE-H1","vOXxrUBro9I8","L0ZXAt1L6S1Z"],"machine_shape":"hm","gpuType":"A100","toc_visible":true},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}
{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)"
   ],
   "metadata": {
    "id": "Ih2CTVBnArVZ"
   },
   "id": "Ih2CTVBnArVZ"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font size=\"5\" color='grey'> <b>\n",
    "Lokale und Open Source Modelle\n",
    "</b></font> </br></p>\n",
    "\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "6jJZ7wbdArVc"
   },
   "id": "6jJZ7wbdArVc"
  },
  {
   "cell_type": "code",
   "source": [
    "#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul\nfrom genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages\nsetup_api_keys(['OPENAI_API_KEY', 'HF_TOKEN'], create_globals=False)\nprint()\ncheck_environment()\nprint()\nget_ipinfo()\n# Bei Bedarf: Trennen zwischen Installationsname () und Importname (f√ºr Python) beide Angaben in Klammern\n# install_packages([('markitdown[all]', 'markitdown'), 'langchain_chroma', ]"
   ],
   "metadata": {
    "id": "9BQvZ42UvF4D"
   },
   "id": "9BQvZ42UvF4D",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "951a5960",
   "metadata": {
    "id": "951a5960"
   },
   "source": [
    "# 1 | Einf√ºhrung\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lokale und Open Source Modelle bieten eine Alternative zu Cloud-basierten KI-Diensten wie OpenAI. Im Gegensatz zu API-basierten Diensten werden diese Modelle direkt auf dem lokalen System ausgef√ºhrt, was Vorteile in Bezug auf Datenschutz, Kosten und Anpassungsm√∂glichkeiten bietet.\n",
    "\n"
   ],
   "metadata": {
    "id": "HWIvNajgmCCb"
   },
   "id": "HWIvNajgmCCb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Vor- und Nachteile lokaler Modelle\n",
    "</font></p>\n",
    "\n",
    "- **Vorteile**: Datenschutz, keine API-Kosten, keine Internetverbindung erforderlich, volle Kontrolle\n",
    "- **Nachteile**: H√∂here Hardwareanforderungen, begrenzte Modellleistung auf Standard-Hardware, Installations- und Wartungsaufwand\n",
    "\n"
   ],
   "metadata": {
    "id": "-y5UoxXMmUkV"
   },
   "id": "-y5UoxXMmUkV"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Anwendungsbereiche lokaler Modelle\n",
    "</font></p>\n",
    "\n",
    "- Entwicklung und Prototyping\n",
    "- Datenschutzkritische Anwendungen\n",
    "- Offline-Anwendungen\n",
    "- Kostensensitive Projekte\n",
    "\n"
   ],
   "metadata": {
    "id": "OMbXZAO-maLa"
   },
   "id": "OMbXZAO-maLa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 | Ollama\n",
    "---"
   ],
   "metadata": {
    "id": "4uBmvLYwmhnr"
   },
   "id": "4uBmvLYwmhnr"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://ollama.com/public/ollama.png\" class=\"logo\" width=\"120\"/>\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "5yIlnTErB_cg"
   },
   "id": "5yIlnTErB_cg"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ollama ist ein Tool, das die einfache Installation und Nutzung von Open Source LLMs auf dem eigenen Computer erm√∂glicht. Es bietet eine benutzerfreundliche Schnittstelle zum Herunterladen, Verwalten und Ausf√ºhren verschiedener Sprachmodelle.\n",
    "\n",
    "[Ollama](https://ollama.com/)"
   ],
   "metadata": {
    "id": "aUBszZUdmn-h"
   },
   "id": "aUBszZUdmn-h"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Installation von Ollama\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "LBO99aq0ncRs"
   },
   "id": "LBO99aq0ncRs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4072af",
   "metadata": {
    "id": "4b4072af"
   },
   "outputs": [],
   "source": [
    "# Installation in Linux/Colab\n",
    "!sudo apt update\n",
    "!sudo apt install -y pciutils\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d7b10b",
   "metadata": {
    "id": "f4d7b10b"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Starten des Ollama-Servers\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bec64b",
   "metadata": {
    "id": "38bec64b"
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def run_ollama_serve():\n",
    "  subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "thread = threading.Thread(target=run_ollama_serve)\n",
    "thread.start()\n",
    "time.sleep(10)  # Kurz warten, bis der Server hochgefahren ist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f788cb40",
   "metadata": {
    "id": "f788cb40"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Modelle herunterladen und verwalten\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4111d55e",
   "metadata": {
    "id": "4111d55e"
   },
   "outputs": [],
   "source": [
    "# Modelle herunterladen\n",
    "!ollama pull llama3"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Modelle herunterladen\n",
    "!ollama pull gemma3"
   ],
   "metadata": {
    "id": "11hJqakJi4QF"
   },
   "id": "11hJqakJi4QF",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Verf√ºgbare Modelle anzeigen\n",
    "!ollama list"
   ],
   "metadata": {
    "id": "JdkzoYBkJ2jb"
   },
   "id": "JdkzoYBkJ2jb",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b8ee185b",
   "metadata": {
    "id": "b8ee185b"
   },
   "source": [
    "# 3 | Lokale Modelle und LangChain\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "LangChain bietet eine nahtlose Integration mit Ollama, wodurch lokale Modelle in komplexen Anwendungen eingesetzt werden k√∂nnen.\n",
    "\n"
   ],
   "metadata": {
    "id": "TONpdbWknQWS"
   },
   "id": "TONpdbWknQWS"
  },
  {
   "cell_type": "markdown",
   "id": "90c86bc3",
   "metadata": {
    "id": "90c86bc3"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Einfache Verwendung von Ollama mit LangChain\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290849c2",
   "metadata": {
    "id": "290849c2"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein hilfreicher und humorvoller KI-Assistent. Antworte sachlich und kurz.\"),\n",
    "    (\"human\", \"Frage: {question}\\n\\nAntwort: Denke Schritt f√ºr Schritt.\")\n",
    "])\n",
    "\n",
    "# Model\n",
    "MODEL = \"llama3\"\n",
    "model = OllamaLLM(model=MODEL, temperature=0.1)  # Niedrigere Temperatur f√ºr analytischere Antworten\n",
    "\n",
    "# Parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Chain zusammensetzen und ausf√ºhren\n",
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e35d57b",
   "metadata": {
    "id": "2e35d57b"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Beispiele Llama 3\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2b8771",
   "metadata": {
    "lines_to_next_cell": 2,
    "id": "cb2b8771"
   },
   "outputs": [],
   "source": [
    "# Beispiel 1: Gesundheitsfrage\n",
    "response = chain.invoke({\"question\": \"Was ist der einfachste Weg Gewicht abzunehmen?\"})\n",
    "mprint(\"## ü§ñ Local LLM\")\n",
    "mprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Beispiel 2: Allgemeinwissen\n",
    "response = chain.invoke({\"question\": \"Wer war Albert Einstein?\"})\n",
    "mprint(\"## ü§ñ Local LLM\")\n",
    "mprint(response)"
   ],
   "metadata": {
    "id": "7FnW-3sUKW0j"
   },
   "id": "7FnW-3sUKW0j",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Beispiele Gemma 3\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "4v6S84ADaxjJ"
   },
   "id": "4v6S84ADaxjJ"
  },
  {
   "cell_type": "code",
   "source": [
    "# Modell\n",
    "MODEL = \"gemma3\"\n",
    "model = OllamaLLM(model=MODEL, temperature=0.1)  # Niedrigere Temperatur f√ºr analytischere Antworten\n",
    "\n",
    "# Chain zusammensetzen und ausf√ºhren\n",
    "chain = prompt | model | parser"
   ],
   "metadata": {
    "id": "LIJCEQGOa3AI"
   },
   "id": "LIJCEQGOa3AI",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Beispiel 1: Gesundheitsfrage\n",
    "response = chain.invoke({\"question\": \"Was ist der einfachste Weg Gewicht abzunehmen?\"})\n",
    "mprint(\"## ü§ñ Local LLM\")\n",
    "mprint(response)"
   ],
   "metadata": {
    "id": "ugbxijBhbAGX"
   },
   "id": "ugbxijBhbAGX",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Beispiel 2: Allgemeinwissen\n",
    "response = chain.invoke({\"question\": \"Wer war Albert Einstein?\"})\n",
    "mprint(\"## ü§ñ Local LLM\")\n",
    "mprint(response)"
   ],
   "metadata": {
    "id": "_Ym8eKajbC3c"
   },
   "id": "_Ym8eKajbC3c",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "efe2093b",
   "metadata": {
    "id": "efe2093b"
   },
   "source": [
    "\n",
    "# 4 | Open Source vs. Closed Source\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Open Source Modelle\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "eU1Vqvarn-WV"
   },
   "id": "eU1Vqvarn-WV"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Open Source KI-Modelle zeichnen sich durch ihre frei zug√§nglichen Gewichte und Architekturen aus. Diese Modelle werden unter Lizenzen wie MIT, Apache oder Creative Commons ver√∂ffentlicht, die es Nutzern erlauben, den Code einzusehen, zu modifizieren und f√ºr eigene Zwecke anzupassen.\n",
    "\n",
    "**Vorteile von Open Source Modellen:**\n",
    "\n",
    "- **Transparenz**: Einsicht in die Funktionsweise und Trainingsmethoden\n",
    "- **Anpassbarkeit**: M√∂glichkeit zum Fine-Tuning f√ºr spezifische Anwendungsf√§lle\n",
    "- **Keine API-Kosten**: Nach dem Download entstehen keine laufenden Nutzungskosten\n",
    "- **Datenschutz**: Vollst√§ndige Kontrolle √ºber die Datenverarbeitung\n",
    "- **Community-Unterst√ºtzung**: Regelm√§√üige Verbesserungen durch die Community\n",
    "\n",
    "**Herausforderungen:**\n",
    "\n",
    "- **Hardwareanforderungen**: Gr√∂√üere Modelle ben√∂tigen leistungsstarke Hardware\n",
    "- **Expertise**: Technisches Know-how f√ºr Installation und Optimierung n√∂tig\n",
    "- **Qualit√§tsunterschiede**: Leistung kann bei gleicher Gr√∂√üe hinter kommerziellen Modellen zur√ºckbleiben\n",
    "\n"
   ],
   "metadata": {
    "id": "aT17TvGpoL1U"
   },
   "id": "aT17TvGpoL1U"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Closed Source Modelle\n",
    "</font></p>\n",
    "\n",
    "Closed Source Modelle wie GPT-4 von OpenAI oder Claude von Anthropic werden nur √ºber APIs angeboten, ohne Zugriff auf die zugrunde liegenden Modellgewichte.\n",
    "\n",
    "**Vorteile von Closed Source Modellen:**\n",
    "\n",
    "- **Leistungsst√§rke**: Oft fortschrittlicher als vergleichbare Open Source Alternativen\n",
    "- **Einfache Nutzung**: Keine lokale Installation oder Wartung n√∂tig\n",
    "- **Regelm√§√üige Updates**: Automatische Verbesserungen ohne eigenen Aufwand\n",
    "- **Skalierbarkeit**: Flexibel an unterschiedlichen Bedarf anpassbar\n",
    "\n",
    "**Nachteile:**\n",
    "\n",
    "- **Kosten**: Nutzungsbasierte Geb√ºhren\n",
    "- **Abh√§ngigkeit**: Angewiesenheit auf externe Dienste und deren Verf√ºgbarkeit\n",
    "- **Datenschutzbedenken**: Daten verlassen das eigene System\n",
    "- **Eingeschr√§nkte Anpassbarkeit**: Beschr√§nkte M√∂glichkeiten zur Modifikation\n",
    "\n"
   ],
   "metadata": {
    "id": "dUVX_bNioOvm"
   },
   "id": "dUVX_bNioOvm"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5 | Beispiele Open Source Modelle\n",
    "---"
   ],
   "metadata": {
    "id": "GE8se-8AoUgf"
   },
   "id": "GE8se-8AoUgf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Phi-4 (Microsoft)\n",
    "</font></p>\n",
    "\n",
    "Microsoft's Phi-4 ist ein kompaktes, aber leistungsstarkes Modell, das trotz seiner geringen Gr√∂√üe beeindruckende Ergebnisse liefert.\n",
    "\n",
    "**Technische Details:**\n",
    "\n",
    "- Verf√ºgbar in verschiedenen Gr√∂√üen (2,7B bis 13B Parameter)\n",
    "- Optimiert f√ºr das Ausf√ºhren auf durchschnittlicher Hardware\n",
    "- Besonders stark in Coding-Aufgaben und logischem Denken\n",
    "- Trainiert mit hochwertigen, kuratierten Daten\n",
    "- Verwendet eine optimierte Transformerarchitektur\n",
    "\n",
    "**Einsatzbereiche:**\n",
    "\n",
    "- Softwareentwicklung und Code-Generierung\n",
    "- Textgenerierung mit geringen Hardwareanforderungen\n",
    "- Bildung und Wissensanwendungen\n",
    "\n"
   ],
   "metadata": {
    "id": "5-6Ml69Nob7I"
   },
   "id": "5-6Ml69Nob7I"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Gemma (Google)\n",
    "</font></p>\n",
    "\n",
    "Google's Gemma ist eine Familie leichtgewichtiger, offener Modelle, die speziell f√ºr Effizienz optimiert wurden.\n",
    "\n",
    "**Technische Details:**\n",
    "\n",
    "- Gr√∂√üen: 2B und 7B Parameter\n",
    "- Entwickelt mit einem starken Fokus auf verantwortungsvolle KI\n",
    "- Basiert auf der Gemini-Modellfamilie\n",
    "- Unterst√ºtzt Kontextfenster bis zu 8K Tokens\n",
    "- Verf√ºgbar in verschiedenen Quantisierungen (GGUF)\n",
    "\n",
    "**Besonderheiten:**\n",
    "\n",
    "- Starke Leistung in mehrsprachigen Anwendungen\n",
    "- Effiziente Ressourcennutzung\n",
    "- Gute Balance zwischen Modellgr√∂√üe und F√§higkeiten\n",
    "\n"
   ],
   "metadata": {
    "id": "t_0szyFtojsO"
   },
   "id": "t_0szyFtojsO"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Llama 2 und Llama 3 (Meta)\n",
    "</font></p>\n",
    "\n",
    "Meta's Llama-Serie hat die Open-Source-LLM-Landschaft revolutioniert und bietet leistungsstarke Modelle mit differenzierten Lizenzen.\n",
    "\n",
    "**Technische Details:**\n",
    "\n",
    "- Llama 3: Verf√ºgbar in 8B und 70B Varianten, mit verbesserter Multitask-Leistung\n",
    "- Llama 2: 7B, 13B und 70B Parameter\n",
    "- Kontextfenster: Bis zu 4K Tokens (Llama 2), 8K+ (Llama 3)\n",
    "- Chat-optimierte Varianten verf√ºgbar\n",
    "\n",
    "**Besonderheiten:**\n",
    "\n",
    "- Starke Leistung in Reasoning-Aufgaben\n",
    "- Verbesserte Instruktionsbefolgung in neueren Versionen\n",
    "- Semi-offene Lizenz mit Einschr√§nkungen f√ºr kommerzielle Nutzung ab bestimmten Nutzer-Schwellenwerten\n",
    "\n"
   ],
   "metadata": {
    "id": "pIb442LBor5M"
   },
   "id": "pIb442LBor5M"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Mistral (Mistral AI)\n",
    "</font></p>\n",
    "\n",
    "Das franz√∂sische Startup Mistral AI hat mit seinen effizienten Modellen f√ºr Aufsehen gesorgt.\n",
    "\n",
    "**Technische Details:**\n",
    "\n",
    "- Mistral 7B: Beeindruckende Leistung trotz kompakter Gr√∂√üe\n",
    "- Mixtral 8x7B: Ein Mixture-of-Experts (MoE) Modell mit starker Leistung\n",
    "- Verbesserte Architekturen mit effizienten Attention-Mechanismen\n",
    "- Unterst√ºtzung f√ºr bis zu 32K Token Kontextfenster\n",
    "\n",
    "**Besonderheiten:**\n",
    "\n",
    "- Herausragende Leistung f√ºr die Modellgr√∂√üe\n",
    "- Spezialisierte Varianten f√ºr Code-Generierung (Codestral)\n",
    "- Moderne Architekturoptimierungen f√ºr bessere Effizienz\n",
    "\n"
   ],
   "metadata": {
    "id": "0_OKca5iow4a"
   },
   "id": "0_OKca5iow4a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Falcon (Technology Innovation Institute)\n",
    "</font></p>\n",
    "\n",
    "Die Falcon-Modelle wurden vom Technology Innovation Institute in den VAE entwickelt und bieten eine gro√üz√ºgige Lizenz f√ºr kommerzielle Anwendungen.\n",
    "\n",
    "**Technische Details:**\n",
    "\n",
    "- Gr√∂√üen: 1B, 7B und 40B Parameter\n",
    "- Trainiert auf einem gro√üen, diversen Datensatz\n",
    "- Optimierte Transformer-Architektur f√ºr Effizienz\n",
    "\n",
    "**Besonderheiten:**\n",
    "\n",
    "- Permissive Lizenz f√ºr kommerzielle Anwendungen\n",
    "- Gute Mehrsprachigkeit\n",
    "- Hohe Qualit√§t in allgemeinen Wissensabfragen\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "VsY29GNko2fh"
   },
   "id": "VsY29GNko2fh"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 | Lizenzierung und rechtliche Aspekte\n",
    "---"
   ],
   "metadata": {
    "id": "vOXxrUBro9I8"
   },
   "id": "vOXxrUBro9I8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Die Lizenzen von Open Source Modellen unterscheiden sich erheblich und beeinflussen deren Einsatzm√∂glichkeiten:\n",
    "\n",
    "**MIT Lizenz (z. B. OpenLLaMA, GPT-NeoX)**\n",
    "- Sehr liberal, erlaubt kommerzielle Nutzung ohne Einschr√§nkungen  \n",
    "- Erfordert lediglich die Nennung des urspr√ºnglichen Urhebers  \n",
    "- Keine Weitergabelizenzverpflichtung (nicht copyleft)  \n",
    "- Einfach und weit verbreitet in Open-Source-Projekten  \n",
    "\n",
    "**Apache 2.0 (z. B. Gemma, Falcon)**\n",
    "- Sehr liberal, erlaubt kommerzielle Nutzung ohne starke Einschr√§nkungen  \n",
    "- Erfordert Hinweis auf urspr√ºngliches Urheberrecht  \n",
    "- Keine Weitergabelizenzverpflichtung (nicht copyleft)  \n",
    "- Enth√§lt eine Klausel zum Schutz vor Patentstreitigkeiten  \n",
    "\n",
    "**Meta Llama Lizenz**\n",
    "- Erlaubt kommerzielle Nutzung mit Einschr√§nkungen (z. B. Nutzerschwellen)  \n",
    "- Enth√§lt spezifische Bedingungen f√ºr verantwortungsvolle Nutzung  \n",
    "- Verbietet bestimmte Anwendungsf√§lle wie Desinformation oder illegale Aktivit√§ten  \n",
    "- Nicht so frei wie Apache oder MIT, aber offener als propriet√§re Modelle  \n",
    "\n",
    "**Mistral AI Lizenz**\n",
    "- Erlaubt kommerzielle und nicht-kommerzielle Nutzung  \n",
    "- Enth√§lt ethische Nutzungsbeschr√§nkungen  \n",
    "- Unterschiedliche Bedingungen f√ºr verschiedene Modellversionen  \n",
    "- Im Allgemeinen offener als die Meta Llama Lizenz  \n",
    "\n",
    "**Creative Commons (z. B. BLOOM unter CC-BY-SA)**\n",
    "- Verschiedene Varianten mit unterschiedlichem Freiheitsgrad  \n",
    "- **CC-BY**: Erlaubt kommerzielle Nutzung, solange der Urheber genannt wird  \n",
    "- **CC-BY-SA**: Weitergabe nur unter gleichen Lizenzbedingungen erlaubt (copyleft)  \n",
    "- **CC-BY-NC**: Keine kommerzielle Nutzung erlaubt  \n",
    "- **CC-BY-ND**: Keine Bearbeitung oder Ableitung erlaubt  \n",
    "- H√§ufig f√ºr Open-Access-Modelle genutzt, aber mit Einschr√§nkungen je nach Variante  \n",
    "\n",
    "<br>\n",
    "\n",
    "**Disclaimer**    \n",
    "Die oben aufgef√ºhrten Informationen zu Lizenzmodellen dienen ausschlie√ülich allgemeinen Informationszwecken und stellen keine rechtliche Beratung dar. Die Bedingungen einzelner Lizenzen k√∂nnen sich √§ndern, und ihre Auslegung kann von der jeweiligen Rechtsprechung abh√§ngen.\n",
    "\n",
    "Vor der Nutzung oder Weitergabe eines KI-Modells sollten die offiziellen Lizenzdokumente sorgf√§ltig gepr√ºft und bei Unsicherheiten eine juristische Beratung eingeholt werden. Wir √ºbernehmen keine Haftung f√ºr die Vollst√§ndigkeit, Richtigkeit oder Anwendbarkeit der bereitgestellten Informationen.\n",
    "\n"
   ],
   "metadata": {
    "id": "tNJ_JtxQpIwV"
   },
   "id": "tNJ_JtxQpIwV"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7 | Auswahlkriterien Open Source\n",
    "---"
   ],
   "metadata": {
    "id": "6Jvx9A_3pOUJ"
   },
   "id": "6Jvx9A_3pOUJ"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bei der Wahl eines geeigneten Open Source Modells sollten folgende Faktoren ber√ºcksichtigt werden:\n",
    "\n",
    "**Hardwareanforderungen:**\n",
    "\n",
    "- Gr√∂√üere Modelle (40B+): Ben√∂tigen High-End-GPUs mit 24GB+ VRAM\n",
    "- Mittlere Modelle (7-13B): Laufen auf Consumer-GPUs mit 8-16GB VRAM\n",
    "- Kleine Modelle (1-2B): K√∂nnen auf Standard-Laptops oder CPUs ausgef√ºhrt werden\n",
    "- Quantisierung (INT4, INT8) reduziert Hardwareanforderungen erheblich\n",
    "\n",
    "**Anwendungsfall:**\n",
    "\n",
    "- Allgemeine Textgenerierung: Llama 3, Mistral und Gemma bieten gute Allround-Leistung\n",
    "- Code-Generierung: Phi, CodeLlama oder Codestral f√ºr spezialisierte Coding-F√§higkeiten\n",
    "- Wissenschaftliche Aufgaben: Spezialisierte Dom√§nenmodelle bevorzugen\n",
    "- Mehrsprachigkeit: Auf Modelle mit breitem Sprachtraining (z.B. BLOOM) setzen\n",
    "\n",
    "**Kontextfenster:**\n",
    "\n",
    "- Standard (4-8K Tokens): F√ºr die meisten einfachen Anwendungen ausreichend\n",
    "- Erweitert (16-32K Tokens): F√ºr Dokumentenanalyse und komplexe Aufgaben\n",
    "- Lang (64K+ Tokens): F√ºr die Verarbeitung ganzer Dokumente oder Codebases\n",
    "\n"
   ],
   "metadata": {
    "id": "NCD_1A9xpaJM"
   },
   "id": "NCD_1A9xpaJM"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8 | Zukunftstrends bei Open Source\n",
    "---"
   ],
   "metadata": {
    "id": "aKoiGZUMpdcw"
   },
   "id": "aKoiGZUMpdcw"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Die Open Source LLM-Landschaft entwickelt sich rapide weiter. Folgende Trends sind zu beobachten:\n",
    "\n",
    "**Effizienzsteigerungen:**\n",
    "\n",
    "- Kleinere Modelle mit der Leistung gr√∂√üerer Vorg√§nger\n",
    "- Verbesserte Quantisierungstechniken f√ºr schnellere Inferenz\n",
    "- Optimierte Architekturen (MoE, FlashAttention, etc.)\n",
    "\n",
    "**Spezialisierung:**\n",
    "\n",
    "- Dom√§nenspezifische Modelle f√ºr Medizin, Recht, Finanzen, etc.\n",
    "- Aufgabenspezifische Varianten f√ºr Code, Mathematik, Reasoning\n",
    "- Sprachspezifische Modelle f√ºr nicht-englische Sprachen\n",
    "\n",
    "**Verbesserte Lizenzen:**\n",
    "\n",
    "- Entwicklung ausgewogenerer Lizenzmodelle\n",
    "- Klarere Richtlinien f√ºr verantwortungsvolle KI-Nutzung\n",
    "- Industriefreundlichere Bedingungen f√ºr kommerzielle Anwendungen\n",
    "\n",
    "**Multimodalit√§t:**\n",
    "\n",
    "- Integration von Text- und Bildverst√§ndnis\n",
    "- Erweiterung auf Audio- und Videoverarbeitung\n",
    "- Verbesserte Integration mit externen Tools und Wissensquellen\n",
    "\n",
    "Diese Entwicklungen werden die L√ºcke zwischen Open Source und propriet√§ren Modellen weiter verringern und die Einstiegsh√ºrden f√ºr lokale KI-Anwendungen senken.\n",
    "\n"
   ],
   "metadata": {
    "id": "q3_FSVMjqdbV"
   },
   "id": "q3_FSVMjqdbV"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Modellperformance\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "J6ZizuD_KkmP"
   },
   "id": "J6ZizuD_KkmP"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/top-mmlu-open-closed-banner_klein.png)"
   ],
   "metadata": {
    "id": "Y2o99YU3KbTw"
   },
   "id": "Y2o99YU3KbTw"
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Ben Cottier et al. (2024), \"How Far Behind Are Open Models?\". Published online at epoch.ai.](https://epoch.ai/blog/open-models-report)"
   ],
   "metadata": {
    "id": "wdoxJS60JIVB"
   },
   "id": "wdoxJS60JIVB"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# A | Aufgabe\n"
   ],
   "metadata": {
    "id": "L0ZXAt1L6S1Z"
   },
   "id": "L0ZXAt1L6S1Z"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen."
   ],
   "metadata": {
    "id": "BYQ1yv0U6YKp"
   },
   "id": "BYQ1yv0U6YKp"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Vergleich von Open Source Modellen\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "hsX9yWQQ9v8G"
   },
   "id": "hsX9yWQQ9v8G"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Installieren Sie Ollama auf Ihrem lokalen System und vergleichen Sie die Leistung von mindestens zwei verschiedenen Modellen (z.B. Llama2 und Gemma) bei der Beantwortung derselben Prompts.\n",
    "\n",
    "Vergleichen Sie die Antworten der Modelle. Kriterien: Umfang, Qualit√§t, Antwortzeit, ...\n"
   ],
   "metadata": {
    "id": "POK-FQXg96Ui"
   },
   "id": "POK-FQXg96Ui"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Chat-Anwendung mit lokalem LLM\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "_3BXIh6M-cV8"
   },
   "id": "_3BXIh6M-cV8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Entwickeln Sie eine einfache Gradio-basierte Chat-Anwendung, die ein lokales Modell √ºber Ollama nutzt. Die Anwendung soll:\n",
    "\n",
    "```python\n",
    "import gradio as gr\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import os\n",
    "\n",
    "# Ollama-LLM initialisieren\n",
    "llm = OllamaLLM(model=\"gemma\", temperature=0.7)\n",
    "\n",
    "# Chat-Historie\n",
    "chat_verlauf = []\n",
    "\n",
    "def chat_antwort(nachricht, verlauf):\n",
    "    \"\"\"\n",
    "    Verarbeitet die Nutzereingabe und generiert eine Antwort vom LLM\n",
    "    \"\"\"\n",
    "    # Aktuelle Nachricht zur Historie hinzuf√ºgen\n",
    "    if nachricht:\n",
    "        verlauf.append((nachricht, \"\"))\n",
    "        \n",
    "        # Chat-Kontext erstellen (begrenzt auf die letzten 5 Nachrichten f√ºr Effizienz)\n",
    "        kontext = \"\"\n",
    "        for i in range(max(0, len(verlauf)-5), len(verlauf)):\n",
    "            if i < len(verlauf)-1:  # Alle vorherigen Nachrichten\n",
    "                kontext += f\"Mensch: {verlauf[i][0]}\\nAssistent: {verlauf[i][1]}\\n\"\n",
    "            else:  # Aktuelle Nachricht\n",
    "                kontext += f\"Mensch: {verlauf[i][0]}\\nAssistent: \"\n",
    "        \n",
    "        # Antwort vom Modell generieren\n",
    "        antwort = llm.invoke(kontext)\n",
    "        \n",
    "        # Antwort zur Historie hinzuf√ºgen\n",
    "        verlauf[-1] = (nachricht, antwort)\n",
    "    \n",
    "    return \"\", verlauf\n",
    "\n",
    "# Gradio-Interface erstellen\n",
    "with gr.Blocks(title=\"Lokaler KI-Assistent\") as demo:\n",
    "    gr.Markdown(\"# Chat mit einem lokalen KI-Modell\")\n",
    "    gr.Markdown(\"Dieser Chat verwendet das Gemma-Modell √ºber Ollama\")\n",
    "    \n",
    "    chatbot = gr.Chatbot(label=\"Chat-Verlauf\")\n",
    "    msg = gr.Textbox(label=\"Ihre Nachricht\", placeholder=\"Schreiben Sie hier Ihre Nachricht...\")\n",
    "    \n",
    "    msg.submit(chat_antwort, [msg, chatbot], [msg, chatbot])\n",
    "    \n",
    "    gr.Markdown(\"### Hinweise zur Nutzung:\")\n",
    "    gr.Markdown(\"- Die Antworten werden lokal auf Ihrem System generiert\")\n",
    "    gr.Markdown(\"- Keine Daten werden an externe Server gesendet\")\n",
    "    gr.Markdown(\"- Die Antwortgeschwindigkeit h√§ngt von Ihrer Hardware ab\")\n",
    "\n",
    "# Anwendung starten\n",
    "demo.launch()\n",
    "```\n",
    "\n",
    "Testen & erweitern Sie die Anwendung:\n",
    "1. Testen Sie den Code, korrigieren Sie etwaige Fehler.\n",
    "2. Modellauswahl (zwischen verschiedenen installierten Ollama-Modellen)\n",
    "3. Parameter-Einstellungen (Temperatur, Top-K, Top-P)\n",
    "4. Speichern und Laden von Chat-Verl√§ufen\n",
    "5. M√∂glichkeit, die generierten Antworten zu bewerten (Daumen hoch/runter)"
   ],
   "metadata": {
    "id": "gH6to8NF-r7I"
   },
   "id": "gH6to8NF-r7I"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "RAG-System mit lokalem LLM\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "BKLNzHsa-x2V"
   },
   "id": "BKLNzHsa-x2V"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Entwickeln Sie ein einfaches Retrieval-Augmented Generation (RAG) System, das lokale PDF-Dokumente einliest und die Beantwortung von Fragen mit einem lokalen Modell erm√∂glicht:\n",
    "\n",
    "```python\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "# Modell initialisieren\n",
    "llm = OllamaLLM(model=\"llama2\", temperature=0.1)\n",
    "\n",
    "# Embedding-Modell laden (lokal, ohne API-Calls)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Vektorstore f√ºr Dokumente\n",
    "vector_store = None\n",
    "\n",
    "def dokumente_laden(dateien):\n",
    "    \"\"\"L√§dt PDF-Dokumente und erstellt eine Vektordatenbank\"\"\"\n",
    "    global vector_store\n",
    "    \n",
    "    dokumente = []\n",
    "    for datei in dateien:\n",
    "        loader = PyPDFLoader(datei.name)\n",
    "        dokumente.extend(loader.load())\n",
    "    \n",
    "    # Dokumente in kleinere Chunks aufteilen\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(dokumente)\n",
    "    \n",
    "    # Vektordatenbank erstellen\n",
    "    vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "    \n",
    "    return f\"{len(chunks)} Textabschnitte aus {len(dateien)} Dokumenten geladen\"\n",
    "\n",
    "def frage_beantworten(frage):\n",
    "    \"\"\"Beantwortet eine Frage basierend auf den geladenen Dokumenten\"\"\"\n",
    "    if vector_store is None:\n",
    "        return \"Bitte laden Sie zuerst Dokumente hoch.\"\n",
    "    \n",
    "    # Relevante Dokumente abrufen\n",
    "    docs = vector_store.similarity_search(frage, k=3)\n",
    "    \n",
    "    # Kontext erstellen\n",
    "    kontext = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # Prompt erstellen\n",
    "    prompt_vorlage = \"\"\"\n",
    "    Du bist ein hilfreicher Assistent, der Fragen basierend auf gegebenen Informationen beantwortet.\n",
    "    \n",
    "    Kontext:\n",
    "    {kontext}\n",
    "    \n",
    "    Frage: {frage}\n",
    "    \n",
    "    Deine Antwort sollte sich auf die im Kontext gegebenen Informationen beschr√§nken.\n",
    "    Wenn du keine Antwort im Kontext findest, sage ehrlich, dass du es nicht wei√üt.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_vorlage,\n",
    "        input_variables=[\"kontext\", \"frage\"]\n",
    "    )\n",
    "    \n",
    "    # Antwort vom LLM bekommen\n",
    "    antwort = llm.invoke(prompt.format(kontext=kontext, frage=frage))\n",
    "    \n",
    "    return antwort\n",
    "\n",
    "# Gradio-Interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Lokales Frage-Antwort-System mit RAG\")\n",
    "    \n",
    "    with gr.Tab(\"Dokumente laden\"):\n",
    "        file_input = gr.File(file_count=\"multiple\", label=\"PDF-Dokumente\")\n",
    "        upload_button = gr.Button(\"Dokumente laden\")\n",
    "        upload_output = gr.Textbox(label=\"Status\")\n",
    "        upload_button.click(dokumente_laden, inputs=[file_input], outputs=[upload_output])\n",
    "    \n",
    "    with gr.Tab(\"Fragen stellen\"):\n",
    "        question_input = gr.Textbox(label=\"Ihre Frage zum Dokument\")\n",
    "        answer_button = gr.Button(\"Frage stellen\")\n",
    "        answer_output = gr.Textbox(label=\"Antwort\")\n",
    "        answer_button.click(frage_beantworten, inputs=[question_input], outputs=[answer_output])\n",
    "\n",
    "demo.launch()\n",
    "```\n",
    "\n",
    "Testen und erweitern Sie das System:\n",
    "1. Testen & ggf. korrigieren des Code\n",
    "2. Unterst√ºtzung f√ºr weitere Dateiformate (DOCX, TXT, HTML)\n",
    "3. M√∂glichkeit, die Qualit√§t der Antworten zu verbessern durch:\n",
    "   - Modellauswahl\n",
    "   - Parameteroptimierung\n",
    "   - Verbesserung des Prompts\n",
    "4. Implementierung einer M√∂glichkeit zur Quellennachverfolgung (aus welchem Teil des Dokuments stammt die Information)\n",
    "5. Visualisierung der relevanten Textpassagen im Dokument"
   ],
   "metadata": {
    "id": "iOMIlyS4_MCd"
   },
   "id": "iOMIlyS4_MCd"
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "951a5960",
    "4uBmvLYwmhnr",
    "b8ee185b",
    "efe2093b",
    "GE8se-8AoUgf",
    "vOXxrUBro9I8",
    "6Jvx9A_3pOUJ",
    "aKoiGZUMpdcw",
    "L0ZXAt1L6S1Z"
   ],
   "machine_shape": "hm",
   "toc_visible": true
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
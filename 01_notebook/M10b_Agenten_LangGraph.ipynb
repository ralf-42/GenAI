{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0684551f",
   "metadata": {
    "id": "0684551f"
   },
   "source": [
    "![GenAI Banner](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font size=\"5\" color='grey'> <b>\n",
    "Agenten (LangGraph)\n",
    "</b></font> </br></p>\n",
    "\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "ogH-Fzpmbueo"
   },
   "id": "ogH-Fzpmbueo"
  },
  {
   "cell_type": "code",
   "source": [
    "#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul\nfrom genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages\nsetup_api_keys(['OPENAI_API_KEY', 'HF_TOKEN', 'SERPAPI_API_KEY'], create_globals=False)\nprint()\ncheck_environment()\nprint()\nget_ipinfo()\n# Bei Bedarf: Trennen zwischen Installationsname () und Importname (f√ºr Python) beide Angaben in Klammern\n    # install_packages([('markitdown[all]', 'markitdown'), 'langchain_chroma', ]"
   ],
   "metadata": {
    "id": "XUp8KhfjqR2N"
   },
   "id": "XUp8KhfjqR2N",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title üõ†Ô∏è Installationen { display-mode: \"form\" }\n",
    "install_packages([\n",
    "    ('google-search-results', 'serpapi'),\n",
    "    'wikipedia',\n",
    "])"
   ],
   "metadata": {
    "id": "lG5yZ6lr4U3V"
   },
   "id": "lG5yZ6lr4U3V",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bf088ba7",
   "metadata": {
    "id": "bf088ba7"
   },
   "source": [
    "# 1 | Was ist ein echter Agent?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wenn man sich mit generativer KI besch√§ftigt, st√∂√üt man fr√ºher oder sp√§ter auf den Begriff **Agent** ‚Äì also ein System, das Aufgaben eigenst√§ndig ausf√ºhrt. Doch was genau ist ein *‚Äûechter‚Äú* Agent? Muss er vollst√§ndig autonom sein? Solche Fragen f√ºhren schnell zu endlosen Grundsatzdiskussionen ‚Äì und genau das ist nicht hilfreich, wenn man einfach anfangen m√∂chte, mit GenAI zu arbeiten."
   ],
   "metadata": {
    "id": "kNOAqFifGB1d"
   },
   "id": "kNOAqFifGB1d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ralf-42/Image/main/agentisch_1.png\" class=\"logo\" width=\"500\"/>"
   ],
   "metadata": {
    "id": "C4MsmT0v2WX0"
   },
   "id": "C4MsmT0v2WX0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Der bekannte KI-Experte **Andrew Ng** hat einen anderen, praktischeren Vorschlag gemacht: Statt dar√ºber zu streiten, ob etwas ein Agent ist oder nicht, sollten wir lieber davon sprechen, wie **agentisch** ein System ist ‚Äì also wie **selbstst√§ndig** es arbeitet. So kann man sich auf das konzentrieren, was wirklich z√§hlt: Was kann das System leisten, und wo kann es sinnvoll eingesetzt werden?"
   ],
   "metadata": {
    "id": "rXYh9twZ2X_3"
   },
   "id": "rXYh9twZ2X_3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ralf-42/Image/main/agentisch_2.png\" class=\"logo\" width=\"900\"/>"
   ],
   "metadata": {
    "id": "RFiEEcrQ2nuB"
   },
   "id": "RFiEEcrQ2nuB"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Besonders einfach ist der Einstieg bei eher einfachen Aufgaben ‚Äì also bei Prozessen, die heute noch manuell erledigt werden, wie das Ausf√ºllen von Formularen, das Nachschlagen in einer Datenbank oder das Kopieren von Informationen zwischen verschiedenen Systemen. Diese Aufgaben lassen sich gut in sogenannte agentische Workflows √ºberf√ºhren ‚Äì also in Abl√§ufe, bei denen die KI (teilweise) selbstst√§ndig handelt.\n",
    "\n",
    "\n",
    "Nat√ºrlich gibt es auch deutlich komplexere Anwendungen, bei denen die KI viele Entscheidungen trifft, Schleifen durchl√§uft und sich an neue Situationen anpasst. Solche Systeme sind spannend ‚Äì aber gerade f√ºr den Anfang ist es oft sinnvoller, mit kleineren, √ºberschaubaren Schritten zu starten. Dort liegen aktuell auch die meisten Chancen, GenAI im Alltag oder im Beruf sinnvoll einzusetzen.\n",
    "\n"
   ],
   "metadata": {
    "id": "SmvdRxMM2pAu"
   },
   "id": "SmvdRxMM2pAu"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Fazit:**     \n",
    "Es muss nicht gleich ein ‚Äûsuperintelligenter‚Äú Agent sein. Besser ist es, pragmatisch zu denken, einfache Prozesse zu automatisieren ‚Äì und so Schritt f√ºr Schritt Erfahrungen zu sammeln."
   ],
   "metadata": {
    "id": "PYc7Gtkl21R-"
   },
   "id": "PYc7Gtkl21R-"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Agenten erweitern LLMs**\n",
    "\n",
    "Agenten sind mehr als nur Sprachmodelle. Sie verbinden die Sprachf√§higkeiten eines LLMs mit praktischer Handlungsf√§higkeit:\n",
    "\n",
    "+ LLM als Denkmodul: Das LLM √ºbernimmt das Sprachverst√§ndnis und das logische Schlussfolgern.\n",
    "\n",
    "+ Erweiterbarkeit durch Tools: Agenten greifen z.‚ÄØB. auf Web, Datenbanken oder externe APIs zu.\n",
    "\n",
    "+ Transparenter Denkprozess: Der Agent zeigt, wie er zum Ergebnis kommt ‚Äì ideal zum Lernen und Verstehen.\n",
    "\n",
    "+ Entscheidungen treffen: Agenten analysieren, planen und w√§hlen aus mehreren Optionen ‚Äì nicht nur einmal, sondern iterativ."
   ],
   "metadata": {
    "id": "-Pieqmzvw1nn"
   },
   "id": "-Pieqmzvw1nn"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 | Direkter Vergleich\n",
    "---"
   ],
   "metadata": {
    "id": "9lDnaWpPxdUN"
   },
   "id": "9lDnaWpPxdUN"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Setup und Tools\n",
    "\n",
    "Bevor wir vergleichen k√∂nnen, m√ºssen wir die notwendigen Tools f√ºr unseren Agenten definieren. Diese Tools repr√§sentieren die erweiterten F√§higkeiten, die einem einfachen LLM fehlen.\n",
    "\n"
   ],
   "metadata": {
    "id": "DkYGsPkJx0xc"
   },
   "id": "DkYGsPkJx0xc"
  },
  {
   "cell_type": "code",
   "source": [
    "# Einfache Tools definieren\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_community.utilities.serpapi import SerpAPIWrapper\n",
    "import requests\n",
    "\n",
    "# ---- Tool 1 - calculator\n",
    "def simple_calculator(expression):\n",
    "    \"\"\"Einfacher Rechner - Das kann ein LLM oft nicht pr√§zise\"\"\"\n",
    "    try:\n",
    "        # Sicherheitscheck\n",
    "        if any(x in expression for x in ['import', 'exec', '__']):\n",
    "            return \"Unsichere Operation\"\n",
    "        result = eval(expression)\n",
    "        return f\"{expression} = {result}\"\n",
    "    except:\n",
    "        return \"Berechnungsfehler\"\n",
    "\n",
    "# ---- Tool 2 - internet_search\n",
    "serpapi = SerpAPIWrapper()\n",
    "\n",
    "# ---- Tool 3 - weather_open_meteo\n",
    "def get_weather_open_meteo(address):\n",
    "    \"\"\"Aktuelle Wetterdaten √ºber Open-Meteo API\"\"\"\n",
    "    try:\n",
    "        geo_url = \"https://nominatim.openstreetmap.org/search\"\n",
    "        params = {\"q\": address, \"format\": \"json\", \"limit\": 1}\n",
    "        headers = {\"User-Agent\": \"Wetterdaten-Skript/1.0 (kontakt@example.com)\"} # Good practice\n",
    "        geo_response = requests.get(geo_url, params=params, headers=headers).json()\n",
    "\n",
    "        if geo_response:\n",
    "            lat, lon = geo_response[0][\"lat\"], geo_response[0][\"lon\"]\n",
    "            # Request Celsius temperature\n",
    "            weather_url = f\"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current_weather=true&temperature_unit=celsius&windspeed_unit=kmh\"\n",
    "            weather_response = requests.get(weather_url).json()\n",
    "\n",
    "            if 'current_weather' in weather_response:\n",
    "                current_weather = weather_response['current_weather']\n",
    "                temp_c = current_weather.get('temperature', 'N/A')\n",
    "                windspeed_kmh = current_weather.get('windspeed', 'N/A')\n",
    "                weathercode = current_weather.get('weathercode', 'N/A') # You might want to map weathercode to description\n",
    "                return f\"Wetter in {address}: Temperatur {temp_c}¬∞C, Windgeschwindigkeit {windspeed_kmh} km/h, Wettercode: {weathercode}\"\n",
    "            else:\n",
    "                 return f\"Keine Wetterdaten gefunden f√ºr {address}\"\n",
    "        else:\n",
    "            return f\"Fehler beim Abrufen der Geodaten f√ºr {address}\"\n",
    "    except Exception as e:\n",
    "        return f\"Fehler beim Abrufen der Wetterdaten: {e}\"\n",
    "\n",
    "\n",
    "# Tool-Liste erstellen\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"calculator\",\n",
    "        func=simple_calculator,\n",
    "        description=\"üî¢ RECHNER - Pr√§zise Berechnungen (was LLM oft falsch macht)\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"internet_search\",\n",
    "        func=serpapi.run,\n",
    "        description=\"üåê INTERNETSUCHE - Aktuelle Informationen finden (was LLM NICHT kann)\"\n",
    "    ),\n",
    "     Tool(\n",
    "        name=\"weather_open_meteo\",\n",
    "        func=get_weather_open_meteo,\n",
    "        description=\"üå§Ô∏è WETTER (Open-Meteo) - Echtzeitdaten abrufen (was LLM unm√∂glich ist). Input ist die Adresse oder der Ort.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Tools definiert:\")\n",
    "for tool in tools:\n",
    "    print(f\"   ‚Ä¢ {tool.name:25s}: {tool.description}\")"
   ],
   "metadata": {
    "id": "80D6VNn-yT5Y"
   },
   "id": "80D6VNn-yT5Y",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Vergleichstest"
   ],
   "metadata": {
    "id": "O5uiRNfWynCU"
   },
   "id": "O5uiRNfWynCU"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wir verwenden eine Frage, die sowohl aktuelle Daten als auch eine Berechnung erfordert, um die Grenzen eines LLMs und die St√§rken eines Agenten zu demonstrieren."
   ],
   "metadata": {
    "id": "bNWuv_Mmyy_o"
   },
   "id": "bNWuv_Mmyy_o"
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "import time\n",
    "\n",
    "# LLM Setup\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "# Test-Frage die Grenzen aufzeigt\n",
    "test_question = \"Wie ist das Wetter in Berlin, was ist 2847 * 1923, wie ist der aktuelle XETRA Kurs der Aktie von Rheinmetall?\"\n",
    "\n",
    "mprint(\"### üß™ VERGLEICHSTEST\")\n",
    "mprint(\"---\")\n",
    "mprint(f\"**Frage:** {test_question}\")\n",
    "print()"
   ],
   "metadata": {
    "id": "UhXf4Oh5y1tN"
   },
   "id": "UhXf4Oh5y1tN",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. EINFACHES LLM PROBIEREN\n",
    "mprint(\"### 1Ô∏è‚É£ EINFACHES LLM:\")\n",
    "mprint(\"---\")\n",
    "\n",
    "start_time = time.time()\n",
    "llm_response = llm.invoke(test_question)\n",
    "llm_time = time.time() - start_time\n",
    "\n",
    "mprint(f\"**Antwort:** {llm_response.content}\")\n",
    "mprint(f\"**Zeit:** {llm_time:.2f}s\")"
   ],
   "metadata": {
    "id": "jN8RDdgyilCv"
   },
   "id": "jN8RDdgyilCv",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 2. AGENT MIT TOOLS (LangGraph)\n",
    "mprint(\"### 2Ô∏è‚É£ AGENT MIT TOOLS (LangGraph):\")\n",
    "mprint(\"---\")\n",
    "\n",
    "# System-Prompt f√ºr den Agenten\n",
    "system_prompt = \"Du bist ein hilfreicher Agent. Nutze Tools f√ºr aktuelle Daten und Berechnungen. Formatiere das Ergebnis im Markdown-Format, Formeln: $ Formel $\"\n",
    "\n",
    "# LangGraph Agent erstellen\n",
    "agent_executor = create_react_agent(llm, tools, prompt=system_prompt)\n",
    "\n",
    "start_time = time.time()"
   ],
   "metadata": {
    "id": "jP27gUApidiV"
   },
   "id": "jP27gUApidiV",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Streaming: Zeigt den \"Denkprozess\" in Echtzeit\n",
    "mprint(\"**üîÑ Agent-Denkprozess (Streaming):**\")\n",
    "mprint(\"---\")\n",
    "\n",
    "final_state = None\n",
    "seen_message_count = 0\n",
    "\n",
    "for event in agent_executor.stream({\"messages\": [(\"human\", test_question)]}, stream_mode=\"values\"):\n",
    "    final_state = event\n",
    "    new_messages = event[\"messages\"][seen_message_count:]\n",
    "    seen_message_count = len(event[\"messages\"])\n",
    "\n",
    "    for msg in new_messages:\n",
    "        # Tool-Aufrufe\n",
    "        if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "            for tool_call in msg.tool_calls:\n",
    "                mprint(f\"üîß **Tool-Aufruf:** `{tool_call['name']}`\")\n",
    "\n",
    "        # Tool-Ergebnisse\n",
    "        if msg.type == \"tool\":\n",
    "            mprint(f\"üìä **Tool-Ergebnis ({msg.name}):** {msg.content}\")\n",
    "\n",
    "agent_time = time.time() - start_time"
   ],
   "metadata": {
    "id": "tS3RBizb5aIn"
   },
   "id": "tS3RBizb5aIn",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Finale Antwort extrahieren\n",
    "output_message = None\n",
    "if final_state:\n",
    "    for msg in reversed(final_state[\"messages\"]):\n",
    "        if msg.type == \"ai\" and not (hasattr(msg, 'tool_calls') and msg.tool_calls):\n",
    "            output_message = msg.content\n",
    "            break\n",
    "\n",
    "print()\n",
    "mprint(f\"## ü§ñ KI-Agent (LangGraph): \")\n",
    "mprint(\"---\")\n",
    "mprint(output_message if output_message else \"‚ö†Ô∏è Keine finale Antwort gefunden\")\n",
    "mprint(f\"**Zeit:** {agent_time:.2f}s\")"
   ],
   "metadata": {
    "id": "evldUM0K5b-W"
   },
   "id": "evldUM0K5b-W",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Unterschiede"
   ],
   "metadata": {
    "id": "r_Z7XTBky8Bj"
   },
   "id": "r_Z7XTBky8Bj"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Die Unterschiede zwischen LLM und Agent lassen sich in f√ºnf Kernbereichen zusammenfassen, die den Paradigmenwechsel von statischer zu dynamischer KI verdeutlichen.\n",
    "\n",
    "**Vergleich der F√§higkeiten:**\n",
    "\n",
    "| Aspekt | Einfaches LLM | Agent |\n",
    "|--------|---------------|-------|\n",
    "| **Aktuelle Daten** | ‚ùå Nur Trainingsdaten | ‚úÖ √úber Tools |\n",
    "| **Berechnungen** | ‚ö†Ô∏è Oft ungenau | ‚úÖ Pr√§zise Tools |\n",
    "| **Externe APIs** | ‚ùå Unm√∂glich | ‚úÖ Beliebig erweiterbar |\n",
    "| **Transparenz** | üîí Verborgen | üëÅÔ∏è Sichtbar |\n",
    "| **Erweiterbarkeit** | ‚ùå Statisch | ‚úÖ Modular |\n"
   ],
   "metadata": {
    "id": "i_nWQCHa04cu"
   },
   "id": "i_nWQCHa04cu"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "**üéØ FAZIT:**\n",
    "- **Agent = LLM + Tools + Reasoning**\n",
    "- ‚û°Ô∏è Aus reaktiv wird proaktiv\n",
    "- ‚û°Ô∏è Aus statisch wird dynamisch\n",
    "- ‚û°Ô∏è Aus isoliert wird vernetzt\n",
    "\n"
   ],
   "metadata": {
    "id": "ifSi5c34092O"
   },
   "id": "ifSi5c34092O"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 | Anatomie eines Agenten\n",
    "---"
   ],
   "metadata": {
    "id": "UkgEROmi1gRU"
   },
   "id": "UkgEROmi1gRU"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Um Agenten effektiv einsetzen zu k√∂nnen, muss man  ihre innere Struktur verstehen. Ein LangGraph-Agent besteht aus Komponenten, die zusammenarbeiten, um komplexe Aufgaben zu l√∂sen. Diese moderne Architektur erm√∂glicht es, die St√§rken von LLMs mit praktischen Werkzeugen zu kombinieren.\n",
    "\n",
    "**Die Kern-Komponenten (LangGraph)**\n",
    "\n",
    "LangGraph verwendet eine **graph-basierte** Architektur statt der Legacy AgentExecutor-Struktur. Jede Komponente hat eine spezifische Rolle im Gesamtsystem.\n",
    "\n",
    "+ **LLM (Das Gehirn)**: Das Large Language Model fungiert als zentrale Intelligenz des Agenten. Es versteht die Benutzeranfrage, interpretiert Tool-Ergebnisse und entscheidet √ºber n√§chste Schritte. Ohne das LLM w√§re der Agent nur eine Sammlung unverbundener Werkzeuge.\n",
    "\n",
    "+ **Tools (Die H√§nde)**: Tools sind spezialisierte Funktionen, die dem Agenten erlauben, mit der Au√üenwelt zu interagieren. Sie k√∂nnen so einfach sein wie ein Rechner oder so komplex wie eine Datenbankverbindung. Jedes Tool erweitert die F√§higkeiten des Agenten erheblich.\n",
    "\n",
    "+ **State/Messages (Das Ged√§chtnis)**: LangGraph verwendet einen **Message-basierten State** statt eines Scratchpads. Alle Zwischenergebnisse, Tool-Aufrufe und deren Ergebnisse werden als Messages im State gespeichert. Dies erm√∂glicht transparente und nachvollziehbare Reasoning-Prozesse.\n",
    "\n",
    "+ **Graph (Der Orchestrator)**: Der Graph ist das Herzst√ºck von LangGraph. Er definiert die Abfolge von Nodes (LLM, Tools) und Edges (√úberg√§nge). Dies erm√∂glicht komplexe Workflows mit Bedingungen, Schleifen und paralleler Ausf√ºhrung.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Der Agent-*Denkprozess* (LangGraph)**\n",
    "\n",
    "Der *Denkprozess* eines LangGraph-Agenten ist graph-basiert und **zustandsorientiert**. Statt linearem Ablauf durchl√§uft der Agent einen iterativen Graph-Zyklus.\n",
    "\n",
    "+ **Schritt 1 - Verstehen**: Der Agent analysiert die Benutzeranfrage (als HumanMessage) und identifiziert, welche Informationen oder Aktionen ben√∂tigt werden. Dies geschieht durch das LLM-Node im Graph.\n",
    "\n",
    "+ **Schritt 2 - Planen**: Basierend auf dem Verst√§ndnis der Anfrage plant der Agent, welche Tools verwendet werden sollten. In LangGraph wird dies durch **Tool-Calling** realisiert - das LLM generiert ToolMessages mit den ben√∂tigten Tool-Aufrufen.\n",
    "\n",
    "+ **Schritt 3 - Ausf√ºhren**: Der Graph f√ºhrt die Tool-Nodes aus und erh√§lt Ergebnisse. Diese werden als ToolMessage-Objekte im State gespeichert und stehen f√ºr weitere Entscheidungen zur Verf√ºgung.\n",
    "\n",
    "+ **Schritt 4 - Bewerten**: Nach jedem Tool-Aufruf entscheidet der Graph (via Conditional Edges), ob gen√ºgend Informationen vorliegen oder weitere Schritte notwendig sind. Diese Bewertung bestimmt den weiteren Verlauf.\n",
    "\n",
    "+ **Schritt 5 - Iterieren oder Antworten**: Je nach Bewertung kehrt der Graph zum LLM-Node zur√ºck oder beendet mit einer finalen AIMessage. Diese Flexibilit√§t erm√∂glicht die L√∂sung komplexer, unvorhersehbarer Probleme.\n",
    "\n",
    "**Vorteile von LangGraph gegen√ºber Legacy AgentExecutor:**\n",
    "- ‚úÖ **Bessere Kontrolle** √ºber den Ablauf durch explizite Graph-Struktur\n",
    "- ‚úÖ **State-Management** durch Messages statt verstecktem Scratchpad\n",
    "- ‚úÖ **Erweiterbarkeit** durch custom Nodes und Edges\n",
    "- ‚úÖ **Streaming** und **Parallelisierung** nativ unterst√ºtzt\n"
   ],
   "metadata": {
    "id": "ibKl3w4g1lwb"
   },
   "id": "ibKl3w4g1lwb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Zustandsgraph**\n",
    "\n",
    "\n",
    "Bei `create_react_agent` ist der Graph **implizit vordefiniert** - Man kann ihn nicht direkt im Code *sehen*. LangGraph erstellt intern einen **ReAct-Graph** mit folgender Struktur:\n",
    "\n",
    "**üîÑ Der ReAct-Graph (intern)**\n",
    "\n"
   ],
   "metadata": {
    "id": "IlhLN93k6eJW"
   },
   "id": "IlhLN93k6eJW"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ralf-42/Image/main/agent_01.png\" class=\"logo\" width=\"300\"/>"
   ],
   "metadata": {
    "id": "85awbxIB9Q01"
   },
   "id": "85awbxIB9Q01"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Die drei Kern-Komponenten:**\n",
    "\n",
    "1. **`agent` Node** (LLM)\n",
    "   - Analysiert Anfrage und bisherigen State\n",
    "   - Entscheidet: Welches Tool verwenden oder finale Antwort geben?\n",
    "   - Output: AIMessage (mit oder ohne tool_calls)\n",
    "\n",
    "2. **`tools` Node** (Tool-Executor)\n",
    "   - F√ºhrt die vom LLM angeforderten Tools aus\n",
    "   - Sammelt Ergebnisse parallel\n",
    "   - Output: ToolMessage(s)\n",
    "\n",
    "3. **Conditional Edge** (Router)\n",
    "   - Pr√ºft: Hat AIMessage tool_calls?\n",
    "   - JA ‚Üí Route zu `tools` Node\n",
    "   - NEIN ‚Üí Route zu END\n",
    "\n"
   ],
   "metadata": {
    "id": "5Qv9s-Ep7iMc"
   },
   "id": "5Qv9s-Ep7iMc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**üìä State-Struktur**\n",
    "\n",
    "Der State ist ein Dictionary mit einem `messages` Key:\n",
    "\n",
    "```python\n",
    "State = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\"Frage\"),\n",
    "        AIMessage(\"...\", tool_calls=[...]),  # LLM m√∂chte Tools nutzen\n",
    "        ToolMessage(\"Ergebnis 1\"),\n",
    "        ToolMessage(\"Ergebnis 2\"),\n",
    "        AIMessage(\"Finale Antwort\")  # Keine tool_calls ‚Üí END\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "id": "YQX8FO3v7lnP"
   },
   "id": "YQX8FO3v7lnP"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**üõ†Ô∏è Expliziter Graph (Alternative)**\n",
    "\n",
    "Wenn man **volle Kontrolle** m√∂chten, kann man den Graph selbst definieren:\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# 1. Graph definieren\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "# 2. Nodes hinzuf√ºgen\n",
    "graph_builder.add_node(\"agent\", lambda state: {\"messages\": [llm.invoke(state[\"messages\"])]})\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# 3. Edges definieren\n",
    "graph_builder.add_edge(START, \"agent\")\n",
    "\n",
    "# 4. Conditional Edge: Routing-Logik\n",
    "def should_continue(state):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"tools\"  # Weiter zu Tools\n",
    "    return END  # Fertig\n",
    "\n",
    "graph_builder.add_conditional_edges(\"agent\", should_continue, {\"tools\": \"tools\", END: END})\n",
    "graph_builder.add_edge(\"tools\", \"agent\")  # Nach Tools zur√ºck zu Agent\n",
    "\n",
    "# 5. Graph kompilieren\n",
    "custom_graph = graph_builder.compile()\n",
    "\n",
    "# Verwenden wie create_react_agent\n",
    "response = custom_graph.invoke({\"messages\": [(\"human\", \"frage\")]})\n",
    "```\n"
   ],
   "metadata": {
    "id": "CYT2BvTm7oPF"
   },
   "id": "CYT2BvTm7oPF"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "**üìà Vergleich**\n",
    "\n",
    "| Aspekt | `create_react_agent` | Eigener Graph |\n",
    "|--------|----------------------|---------------|\n",
    "| **Einfachheit** | ‚úÖ Eine Zeile Code | ‚ö†Ô∏è ~15 Zeilen |\n",
    "| **Flexibilit√§t** | ‚ö†Ô∏è Vordefiniert | ‚úÖ Volle Kontrolle |\n",
    "| **Graph sichtbar** | ‚ùå Versteckt | ‚úÖ Explizit |\n",
    "| **Custom Nodes** | ‚ùå Nicht m√∂glich | ‚úÖ Beliebig erweiterbar |\n",
    "| **Best Practice** | ‚úÖ F√ºr Standard-Agents | ‚úÖ F√ºr komplexe Workflows |\n"
   ],
   "metadata": {
    "id": "68wTTAB87drZ"
   },
   "id": "68wTTAB87drZ"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "**Empfehlung:**\n",
    "- **Start:** `create_react_agent` f√ºr schnellen Einstieg\n",
    "- **Fortgeschritten:** Eigener Graph f√ºr custom Workflows (z.B. Multi-Agent, Parallelisierung, Bedingungen)\n"
   ],
   "metadata": {
    "id": "nbeHzMSQ7bUR"
   },
   "id": "nbeHzMSQ7bUR"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4 | Hands-On: Agent mit Tools\n",
    "---\n",
    "\n",
    "In diesem Abschnitt bauen wir einen praktischen Agenten mit mehreren Tools und modernen Best Practices."
   ],
   "metadata": {
    "id": "mjxVdnrO54qr"
   },
   "id": "mjxVdnrO54qr"
  },
  {
   "cell_type": "code",
   "source": [
    "# LangGraph Imports\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.utilities.serpapi import SerpAPIWrapper\n",
    "import os"
   ],
   "metadata": {
    "id": "QpitPzT_5XZA"
   },
   "id": "QpitPzT_5XZA",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Tools definieren mit Pydantic Models\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.tools import StructuredTool\n",
    "\n",
    "# ========== Pydantic Schemas ==========\n",
    "\n",
    "class FileReadInput(BaseModel):\n",
    "    \"\"\"Schema f√ºr Datei-Lesen\"\"\"\n",
    "    filename: str = Field(description=\"Pfad zur zu lesenden Datei\")\n",
    "\n",
    "class FileWriteInput(BaseModel):\n",
    "    \"\"\"Schema f√ºr Datei-Schreiben\"\"\"\n",
    "    filename: str = Field(description=\"Zieldatei (z.B. 'output.txt')\")\n",
    "    content: str = Field(description=\"Zu schreibender Textinhalt\")\n",
    "\n",
    "class SearchInput(BaseModel):\n",
    "    \"\"\"Schema f√ºr Internet-Suche\"\"\"\n",
    "    query: str = Field(description=\"Suchbegriff oder Frage\")\n",
    "\n",
    "class WikiSearchInput(BaseModel):\n",
    "    \"\"\"Schema f√ºr Wikipedia-Suche\"\"\"\n",
    "    term: str = Field(description=\"Wikipedia-Suchbegriff\")\n",
    "\n",
    "# ========== Tool-Funktionen ==========\n",
    "\n",
    "def read_file_func(filename):\n",
    "    \"\"\"Liest eine Datei und gibt den Inhalt zur√ºck\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        return f\"Fehler beim Lesen von {filename}: {e}\"\n",
    "\n",
    "def write_file_func(filename, content):\n",
    "    \"\"\"Schreibt Inhalt in eine Datei\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        return f\"‚úÖ Datei {filename} erfolgreich geschrieben ({len(content)} Zeichen)\"\n",
    "    except Exception as e:\n",
    "        return f\"Fehler beim Schreiben: {e}\"\n",
    "\n",
    "def search_web_func(query):\n",
    "    \"\"\"Internet-Suche mit robuster Fehlerbehandlung\"\"\"\n",
    "    try:\n",
    "        serpapi = SerpAPIWrapper()\n",
    "        result = serpapi.run(query)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        if \"quota\" in error_msg.lower() or \"limit\" in error_msg.lower():\n",
    "            return f\"‚ö†Ô∏è SerpAPI Rate Limit erreicht. Fehler: {error_msg}\"\n",
    "        elif \"api key\" in error_msg.lower():\n",
    "            return f\"‚ö†Ô∏è SerpAPI Key Problem. Fehler: {error_msg}\"\n",
    "        else:\n",
    "            return f\"‚ö†Ô∏è Suchfehler: {error_msg}\"\n",
    "\n",
    "def wiki_search_func(term):\n",
    "    \"\"\"Wikipedia-Suche\"\"\"\n",
    "    wiki = WikipediaAPIWrapper()\n",
    "    try:\n",
    "        return wiki.run(term)\n",
    "    except Exception as e:\n",
    "        return f\"Wikipedia-Fehler: {e}\"\n",
    "\n",
    "# ========== Strukturierte Tools erstellen ==========\n",
    "\n",
    "custom_tools = [\n",
    "    StructuredTool.from_function(\n",
    "        func=read_file_func,\n",
    "        name=\"read_file\",\n",
    "        description=\"Liest den Inhalt einer Textdatei. Nutze dies, um bestehende Dateien zu lesen.\",\n",
    "        args_schema=FileReadInput\n",
    "    ),\n",
    "    StructuredTool.from_function(\n",
    "        func=write_file_func,\n",
    "        name=\"write_file\",\n",
    "        description=\"Schreibt Text in eine Datei. Nutze dies, um neue Dateien zu erstellen oder zu √ºberschreiben.\",\n",
    "        args_schema=FileWriteInput\n",
    "    ),\n",
    "    StructuredTool.from_function(\n",
    "        func=search_web_func,\n",
    "        name=\"search\",\n",
    "        description=\"Sucht aktuelle Informationen im Internet (Aktienkurse, News, Software-Versionen, etc.). Nutze dies f√ºr zeitkritische Informationen.\",\n",
    "        args_schema=SearchInput\n",
    "    ),\n",
    "    StructuredTool.from_function(\n",
    "        func=wiki_search_func,\n",
    "        name=\"wiki\",\n",
    "        description=\"Sucht Informationen in Wikipedia. Gut f√ºr Personen, Konzepte, Geschichte, allgemeines Wissen.\",\n",
    "        args_schema=WikiSearchInput\n",
    "    )\n",
    "]\n",
    "\n",
    "mprint(\"### ‚úÖ Tools mit Pydantic Models definiert:\")\n",
    "for tool in custom_tools:\n",
    "    print(f\"   ‚Ä¢ {tool.name:15s}: {tool.description[:80]}...\")"
   ],
   "metadata": {
    "id": "T1sytIXk51g6"
   },
   "id": "T1sytIXk51g6",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Modell definieren\n",
    "# Hinweis: Bei LangGraph wird der Prompt direkt als String √ºbergeben,\n",
    "# nicht als ChatPromptTemplate wie beim Legacy AgentExecutor\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "temperature = 0.0\n",
    "\n",
    "llm = ChatOpenAI(model=model_name, temperature=temperature)"
   ],
   "metadata": {
    "id": "A6DiRUdy7k9g"
   },
   "id": "A6DiRUdy7k9g",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# LangGraph Agent erstellen\n",
    "system_prompt = \"Du bist ein hilfreicher Assistent mit Zugriff auf Tools.\"\n",
    "\n",
    "custom_agent = create_react_agent(\n",
    "    llm,\n",
    "    custom_tools,\n",
    "    prompt=system_prompt\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Agent erstellt\")"
   ],
   "metadata": {
    "id": "BqAF4lzd9kIA"
   },
   "id": "BqAF4lzd9kIA",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ry4qdqafea",
   "source": [
    "# üìä Graph-Visualisierung\n",
    "from IPython.display import Image, display\n",
    "\n",
    "mprint(\"### üìä Agent-Graph Visualisierung\")\n",
    "mprint(\"---\")\n",
    "mprint(\"Der interne Zustandsgraph des Agenten:\")\n",
    "\n",
    "try:\n",
    "    # Graph als Mermaid-Diagramm anzeigen\n",
    "    graph_image = custom_agent.get_graph().draw_mermaid_png()\n",
    "    display(Image(graph_image))\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Graph-Visualisierung nicht verf√ºgbar: {e}\")\n",
    "    print(\"Tipp: Installiere 'pygraphviz' oder nutze Mermaid online\")"
   ],
   "metadata": {
    "id": "ry4qdqafea"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "v8v0bdeh1xm",
   "source": [
    "**üîç Was zeigt die Visualisierung?**\n",
    "\n",
    "Der Graph zeigt den internen Ablauf des Agenten:\n",
    "\n",
    "- **`__start__`** ‚Üí Einstiegspunkt\n",
    "- **`agent`** ‚Üí LLM-Node (entscheidet √ºber Tools)\n",
    "- **`tools`** ‚Üí F√ºhrt Tool-Aufrufe aus\n",
    "- **`__end__`** ‚Üí Beendet die Ausf√ºhrung\n",
    "\n",
    "**Ablauf:**\n",
    "1. Start ‚Üí Agent (LLM analysiert Anfrage)\n",
    "2. Agent ‚Üí Tools? (Conditional Edge pr√ºft tool_calls)\n",
    "   - JA: Zu Tools Node ‚Üí zur√ºck zu Agent\n",
    "   - NEIN: Zu End (finale Antwort)\n",
    "\n",
    "Dieser Graph ist **implizit** in `create_react_agent` vordefiniert!"
   ],
   "metadata": {
    "id": "v8v0bdeh1xm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Test-Anfrage\n",
    "from datetime import date\n",
    "today = date.today().strftime(\"%d.%m.%Y\")\n",
    "\n",
    "input_text = f\"\"\"\n",
    "Erstelle eine Datei 'notiz.txt' mit dem Inhalt 'Agenten k√∂nnen autonom agieren. ü§ñ'.\n",
    "Wie ist der aktuelle XETRA Kurs der Aktie von Rheinmetall vom {today}?\n",
    "Lese die Datei 'notiz.txt' und ver√§ndere die Zeitform von Gegenwart in Zukunft (speichere als 'notiz_zukunft.txt').\n",
    "Was steht zu Taylor Swift auf Wikipedia (nur die erste Zusammenfassung)?\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìù Test-Anfrage erstellt\")"
   ],
   "metadata": {
    "id": "Cij3QmiE9ri5"
   },
   "id": "Cij3QmiE9ri5",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# LangGraph verwendet messages-basierte Eingabe\n",
    "response = custom_agent.invoke({\"messages\": [(\"human\", input_text)]})"
   ],
   "metadata": {
    "id": "sZxwbjsYxX7F"
   },
   "id": "sZxwbjsYxX7F",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "mprint(\"## üõ†Ô∏è Hands-On Agent (LangGraph)\")\n",
    "mprint(\"---\")\n",
    "mprint(\"**Input:**\")\n",
    "mprint(input_text)\n",
    "mprint(\"**Output**:\")\n",
    "# LangGraph speichert alle Messages im State - letzte Message ist die Antwort\n",
    "output_message = response[\"messages\"][-1].content\n",
    "mprint(output_message)"
   ],
   "metadata": {
    "id": "7c8FJ78F5fbn"
   },
   "id": "7c8FJ78F5fbn",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Alle messages\n",
    "for i, item in enumerate(response[\"messages\"],1):\n",
    "    mprint(f\"**>> {i}. <<**\")\n",
    "    print(f\"{item.content:50s} ...\")"
   ],
   "metadata": {
    "id": "MBqSPuBJC9oi"
   },
   "id": "MBqSPuBJC9oi",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5 | Wann braucht man Agenten?\n",
    "---"
   ],
   "metadata": {
    "id": "PXIIt7Hc3TPW"
   },
   "id": "PXIIt7Hc3TPW"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Die Entscheidung zwischen einem einfachen LLM und einem Agenten h√§ngt von den spezifischen Anforderungen Ihrer Anwendung ab. Eine klare Entscheidungsmatrix hilft dabei, die richtige Technologie f√ºr den jeweiligen Anwendungsfall zu w√§hlen und Ressourcen effizient einzusetzen.\n",
    "\n"
   ],
   "metadata": {
    "id": "JS-ztBp43dPn"
   },
   "id": "JS-ztBp43dPn"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Die Wahl der richtigen Technologie beginnt mit der Analyse der Aufgabenanforderungen. W√§hrend LLMs f√ºr viele Textverarbeitungsaufgaben ausreichen, sind Agenten unverzichtbar, wenn externe Interaktionen oder aktuelle Daten ben√∂tigt werden.\n",
    "\n",
    "**Verwenden Sie einen Agenten wenn:**\n",
    "\n",
    "+ Sie **aktuelle** oder **dynamische** Daten ben√∂tigen, die sich h√§ufig √§ndern (Aktienkurse, Wetter, Nachrichten). Agenten k√∂nnen √ºber APIs auf Live-Daten zugreifen und diese in ihre Antworten integrieren.\n",
    "\n",
    "+ Pr√§zise Berechnungen erforderlich sind, bei denen **Genauigkeit** kritisch ist. LLMs approximieren mathematische Operationen, w√§hrend Agenten echte Rechner-Tools verwenden.\n",
    "\n",
    "+ **Externe** Systeme angesprochen werden m√ºssen, wie Datenbanken, APIs oder andere Services. Agenten k√∂nnen diese Integrationen nahtlos abwickeln.\n",
    "\n",
    "+ Komplexe, **mehrstufige Prozesse** durchgef√ºhrt werden sollen, bei denen jeder Schritt vom vorherigen abh√§ngt. Der Agent-Reasoning-Loop ist f√ºr solche Szenarien optimiert.\n",
    "\n",
    "**Ein einfaches LLM reicht wenn:**\n",
    "\n",
    "+ Reine Textverarbeitung ohne externe Daten im Fokus steht. F√ºr Zusammenfassungen, √úbersetzungen oder Textanalysen sind LLMs optimal.\n",
    "\n",
    "+ Kreative Aufgaben gel√∂st werden sollen, wie das Schreiben von Geschichten, Gedichten oder Marketing-Texten. Hier sind die kreativen F√§higkeiten des LLMs gefragt.\n",
    "\n",
    "+ Erkl√§rungen oder Bildungsinhalt basierend auf allgemeinem Wissen ben√∂tigt werden. LLMs haben Zugang zu einem enormen Wissensfundus.\n",
    "\n",
    "+ Statische Code-Generierung ohne externe Abh√§ngigkeiten erforderlich ist. F√ºr einfache Programmieraufgaben sind LLMs sehr effektiv.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Die Faustregel lautet**:    \n",
    "Wenn Sie Tools, aktuelle Daten oder externe Interaktionen ben√∂tigen, w√§hlen Sie einen Agenten. F√ºr reine Textverarbeitung reicht ein LLM aus."
   ],
   "metadata": {
    "id": "IyRu2Fbu3hrs"
   },
   "id": "IyRu2Fbu3hrs"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# A | Aufgabe\n",
    "---"
   ],
   "metadata": {
    "id": "Pzc1rzQlNV8J"
   },
   "id": "Pzc1rzQlNV8J"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen."
   ],
   "metadata": {
    "id": "QQUImb-86GUw"
   },
   "id": "QQUImb-86GUw"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Kalkulation\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "gkF5wVxdx_iA"
   },
   "id": "gkF5wVxdx_iA"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gegeben ist eine Datei, die eine Reihe von Gleichungen enth√§lt.\n",
    "Der Dateiname ist GenAI/02 data/gleichungen.txt\n",
    "\n",
    "**Gleichung:**    \n",
    "41748459 - 87226336    \n",
    "92995162 * 46769739    \n",
    "61530438 * 56074589    \n",
    "95329602 + 45418854    \n",
    "412907 + 3731910    \n",
    "...\n",
    "\n",
    "Verwenden Sie einen LangChain-Agenten mit einem Tool, um jede dieser Gleichungen zu berechnen, und erstellen Sie eine Datei √§hnlich dieser:\n",
    "\n",
    "**Ergebnisse:**  \n",
    "41748459 - 87226336 = 45477877   \n",
    "92995162 * 46769739 = 4349359455002718   \n",
    "61530438 * 56074589 = 3450294021839982   \n",
    "95329602 + 45418854 = 140748456   \n",
    "412907 + 3731910 = 4144817   \n",
    "... ...\n",
    "\n"
   ],
   "metadata": {
    "id": "Y5y-OP9OemcW"
   },
   "id": "Y5y-OP9OemcW"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "# B | Exkurs: Debugging & Transparenz\n",
    "---"
   ],
   "metadata": {
    "id": "6MpjZ6M_1nYt"
   },
   "id": "6MpjZ6M_1nYt"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "üí° \"Wie sehe ich den Denkprozess des Agenten?\"\n",
    "\n",
    "In LangGraph gibt es **keinen `verbose` Parameter** wie beim alten `AgentExecutor`. Stattdessen gibt es **modernere und bessere** Methoden:\n",
    "\n",
    "**üîÑ Option 1: Streaming (Empfohlen)**\n",
    "\n",
    "**Live-Feedback w√§hrend der Ausf√ºhrung:**\n",
    "\n",
    "```python\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [(\"human\", \"frage\")]},\n",
    "    stream_mode=\"values\"  # Zeigt alle Zwischenschritte\n",
    "):\n",
    "    last_message = event[\"messages\"][-1]\n",
    "    \n",
    "    # Tool-Aufrufe\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        for tool in last_message.tool_calls:\n",
    "            print(f\"üîß Tool: {tool['name']}\")\n",
    "    \n",
    "    # Tool-Ergebnisse\n",
    "    if last_message.type == \"tool\":\n",
    "        print(f\"üìä Ergebnis: {last_message.content}\")\n",
    "```\n",
    "\n",
    "**Andere stream_mode Optionen:**\n",
    "- `\"values\"` - Zeigt kompletten State nach jedem Schritt\n",
    "- `\"updates\"` - Zeigt nur √Ñnderungen\n",
    "- `\"messages\"` - Zeigt nur neue Messages\n",
    "\n",
    "\n",
    "**üêõ Option 2: Debug-Mode**\n",
    "\n",
    "**F√ºr Entwicklung und Testing:**\n",
    "\n",
    "```python\n",
    "agent = create_react_agent(\n",
    "    llm,\n",
    "    tools,\n",
    "    prompt=system_prompt,\n",
    "    debug=True  # ‚úÖ Aktiviert interne Logs\n",
    ")\n",
    "```\n",
    "\n",
    "**üìä Option 3: LangSmith (Production-Grade)**\n",
    "\n",
    "**Professionelles Tracing und Monitoring:**\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "# LangSmith aktivieren\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"your-key\"  # Von smith.langchain.com\n",
    "\n",
    "# Jetzt werden ALLE Schritte automatisch geloggt\n",
    "response = agent.invoke({\"messages\": [(\"human\", \"frage\")]})\n",
    "\n",
    "# ‚û°Ô∏è Siehe Traces auf: https://smith.langchain.com\n",
    "```\n",
    "\n",
    "**LangSmith Features:**\n",
    "- ‚úÖ Komplette Execution Traces\n",
    "- ‚úÖ Token-Usage Tracking\n",
    "- ‚úÖ Latency-Monitoring\n",
    "- ‚úÖ Error-Tracking\n",
    "- ‚úÖ A/B Testing von Prompts\n",
    "\n",
    "**üìà Vergleich der Optionen**\n",
    "\n",
    "| Methode | Use Case | Vorteile | Nachteile |\n",
    "|---------|----------|----------|-----------|\n",
    "| **Streaming** | Entwicklung, Live-Demo | Echtzeit-Feedback | Mehr Code |\n",
    "| **Debug-Mode** | Schnelles Debugging | Einfach zu aktivieren | Weniger Details |\n",
    "| **LangSmith** | Production, Analytics | Umfassende Insights | Erfordert Setup |\n",
    "\n",
    "**Empfehlung:**\n",
    "- **Entwicklung:** Streaming f√ºr interaktive Demos\n",
    "- **Debugging:** Debug-Mode f√ºr schnelle Fehlersuche  \n",
    "- **Production:** LangSmith f√ºr professionelles Monitoring\n"
   ],
   "metadata": {
    "id": "6gQyvamu15Sw"
   },
   "id": "6gQyvamu15Sw"
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "bf088ba7",
    "9lDnaWpPxdUN",
    "DkYGsPkJx0xc",
    "O5uiRNfWynCU",
    "r_Z7XTBky8Bj",
    "UkgEROmi1gRU",
    "mjxVdnrO54qr",
    "PXIIt7Hc3TPW",
    "Pzc1rzQlNV8J",
    "6MpjZ6M_1nYt"
   ]
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
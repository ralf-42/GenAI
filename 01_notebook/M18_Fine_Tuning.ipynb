{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<p><font size=\"6\" color='grey'> <b>\n",
    "\n",
    "Generative KI. Verstehen. Anwenden. Gestalten.\n",
    "</b></font> </br></p>"
   ],
   "metadata": {
    "id": "Ih2CTVBnArVZ"
   },
   "id": "Ih2CTVBnArVZ"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font size=\"5\" color='grey'> <b>\n",
    "Fine Tuning\n",
    "</b></font> </br></p>\n",
    "\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "6jJZ7wbdArVc"
   },
   "id": "6jJZ7wbdArVc"
  },
  {
   "cell_type": "code",
   "source": "#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul\nfrom genai_lib.utilities import (\n    check_environment,\n    get_ipinfo,\n    setup_api_keys,\n    mprint,\n    install_packages,\n    mermaid,\n    get_model_profile,\n    extract_thinking,\n    load_prompt\n)\nsetup_api_keys(['OPENAI_API_KEY', 'HF_TOKEN'], create_globals=False)\nprint()\ncheck_environment()\nprint()\nget_ipinfo()\n# Bei Bedarf: Trennen zwischen Installationsname () und Importname (f√ºr Python) beide Angaben in Klammern\n# install_packages([('markitdown[all]', 'markitdown'), 'langchain_chroma', ]",
   "metadata": {
    "id": "pvlwlmU_FY38"
   },
   "id": "pvlwlmU_FY38",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "‚è∏Ô∏è 5-Minuten-Check:\n",
    "</font></p>\n",
    "\n",
    "**Ziel:** Pr√ºfen, ob du das vorherige Kapitel verstanden hast ‚Äì nicht, ob es gerade l√§uft.\n",
    "\n",
    "**Aufgabe** (5 Minuten, ohne Vorlage):\n",
    "\n",
    "Rekonstruiere die zentrale Idee oder Code-Struktur des letzten Abschnitts selbstst√§ndig\n",
    "(kein Copy & Paste, kein Nachschlagen).\n",
    "\n",
    "W√§hle eine der folgenden Optionen:\n",
    "\n",
    "+ Erkl√§re in 1‚Äì2 S√§tzen, was hier konzeptionell passiert.\n",
    "\n",
    "+ Ver√§ndere eine Kleinigkeit (z. B. Prompt, Parameter, Reihenfolge) und beschreibe die Auswirkung.\n",
    "\n",
    "+ Markiere eine Stelle, die du nicht sicher erkl√§ren kannst, und formuliere eine konkrete Frage dazu.\n",
    "\n",
    "**Hinweis:**\n",
    "Nicht alles muss ‚Äûfertig‚Äú oder ‚Äûkorrekt‚Äú sein. Entscheidend ist, wo dein Verst√§ndnis gerade endet"
   ],
   "metadata": {
    "id": "T3w7YbI2istl"
   },
   "id": "T3w7YbI2istl"
  },
  {
   "cell_type": "markdown",
   "id": "236790bc",
   "metadata": {
    "id": "236790bc"
   },
   "source": [
    "\n",
    "# 1 | Full-Fine-Tuning - OpenAI\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "OpenAI erm√∂glicht die Feinabstimmung von Modellen wie GPT-3.5 und GPT-4, um sie mit dom√§nenspezifischen Daten f√ºr bestimmte Anwendungen zu optimieren. Dabei wird ein kuratierter Datensatz genutzt, um die Modellgewichte anzupassen und die Leistung in spezialisierten Aufgaben wie pr√§ziseren Vorhersagen, strukturierter Ausgabe oder markenkonformer Kommunikation zu verbessern. Dieser Ansatz ist besonders n√ºtzlich in Bereichen wie Finanzen, Gesundheitswesen oder Kundendienst.\n",
    "\n",
    "Vorteile der Feinabstimmung:\n",
    "- H√∂here Ergebnisqualit√§t als durch Standard-Prompts  \n",
    "- Training mit mehr Beispielen als in eine einzelne Eingabe passt  \n",
    "- K√ºrzere, effizientere Prompts f√ºr geringeren Token-Verbrauch  \n",
    "- Schnellere Antwortzeiten durch spezialisierte Modelle  \n",
    "\n",
    "Anwendungsf√§lle:\n",
    "- **Stil und Ton** ‚Äì Anpassung der Ausdrucksweise  \n",
    "- **Strukturierte Ausgabe** ‚Äì Erzwungene Formate wie JSON oder XML  \n",
    "- **Tool Calling** ‚Äì Nutzung von Werkzeugen auf definierte Weise  \n",
    "- **Funktionsaufruf** ‚Äì Gezielte Interaktion mit Funktionen  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Weitere Informationen:**    \n",
    "[OpenAI Finetuning Guide](https://platform.openai.com/docs/guides/fine-tuning)\n"
   ],
   "metadata": {
    "id": "gFogIVSZ4lRA"
   },
   "id": "gFogIVSZ4lRA"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Was ist Feinabstimmung?\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "n2MEfpao6vcp"
   },
   "id": "n2MEfpao6vcp"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Die Textgenerierungsmodelle von OpenAI sind mit einem umfangreichen Datensatz vortrainiert und vielseitig einsetzbar. Standardm√§√üig erfordern sie jedoch pr√§zise formulierte Eingaben und Beispiele, um gew√ºnschte Ergebnisse zu erzielen. Diese ‚ÄûFew-Shot-Learning‚Äú-Methode ist begrenzt durch die Anzahl der Beispiele, die in einer Eingabe untergebracht werden k√∂nnen.\n",
    "\n",
    "Feinabstimmung geht dar√ºber hinaus, indem das Modell mit einer gr√∂√üeren Menge spezifischer Beispiele trainiert wird. Dadurch werden die internen Gewichte angepasst, sodass das Modell Aufgaben oder Dom√§nen pr√§ziser verarbeitet. Das f√ºhrt zu genaueren und zuverl√§ssigeren Ergebnissen, reduziert den Bedarf an langen Eingaben, senkt Kosten und verbessert die Reaktionsgeschwindigkeit.\n",
    "\n"
   ],
   "metadata": {
    "id": "kR63qZda7e9u"
   },
   "id": "kR63qZda7e9u"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Feinabstimmungsprozess\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "DrRSVUgH6w7i"
   },
   "id": "DrRSVUgH6w7i"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Die Feinabstimmung erfolgt in mehreren Schritten:  \n",
    "\n",
    "1. **Trainingsdaten vorbereiten und hochladen** ‚Äì Der Datensatz sollte relevante Beispiele enthalten, die die gew√ºnschte Aufgabe oder Dom√§ne abdecken.  \n",
    "2. **Modell trainieren** ‚Äì Das Modell wird mit diesen Daten angepasst, sodass seine Parameter optimal auf die spezifischen Anforderungen abgestimmt sind.  \n",
    "3. **Ergebnisse bewerten und optimieren** ‚Äì Nach dem Training wird die Leistung √ºberpr√ºft. Falls n√∂tig, k√∂nnen Daten oder Modellkonfigurationen angepasst und das Training wiederholt werden."
   ],
   "metadata": {
    "id": "BIbjEzIU7r8l"
   },
   "id": "BIbjEzIU7r8l"
  },
  {
   "cell_type": "markdown",
   "id": "ab851e25",
   "metadata": {
    "id": "ab851e25"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Implementierung/Nutzung des FT-Modells\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Sobald das Modell die gew√ºnschten Ergebnisse liefert, kann es in der Produktion eingesetzt werden. Dies erm√∂glicht schnellere und kosteneffizientere Antworten, die speziell auf die jeweilige Anwendung zugeschnitten sind.  \n",
    "\n",
    "Feinabgestimmte Modelle verbessern die Effizienz und Spezialisierung, wodurch Zeit und Ressourcen gespart werden, w√§hrend die Qualit√§t der Ergebnisse steigt. Details zu den Kosten finden Sie auf der [OpenAI-Preisseite](https://openai.com/pricing).\n",
    "\n"
   ],
   "metadata": {
    "id": "P_moLPWS794F"
   },
   "id": "P_moLPWS794F"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Datenformate\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "NEKrvRdY635M"
   },
   "id": "NEKrvRdY635M"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Das LLM-Trainingsformat basiert auf einem dialogbasierten Ansatz, der speziell f√ºr Chatbots und √§hnliche Anwendungen genutzt wird. Dabei wird der Austausch in ‚ÄûNachrichten‚Äú strukturiert, die jeweils einer bestimmten **Rolle** zugewiesen sind:  \n",
    "\n",
    "- **System**: Definiert die allgemeinen Anweisungen oder den Ton des Assistenten.  \n",
    "- **Benutzer**: Stellt Fragen oder gibt Eingaben vor.  \n",
    "- **Assistent**: Generiert Antworten basierend auf der Eingabe und dem Kontext.  \n",
    "\n",
    "Dieses Format hilft dem Modell, durch wiederholte Interaktionen zu lernen, wie es kontextgerechte und stilistisch passende Antworten formuliert. Die **System-Nachricht** gibt eine einheitliche Richtung vor (z. B. einen sarkastischen Stil), w√§hrend der restliche Dialog das gew√ºnschte Verhalten trainiert.  \n",
    "\n",
    "Beispiel f√ºr ein sarkastisches Modell:  \n",
    "\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n",
    "```  \n",
    "\n",
    "Dieses Format erm√∂glicht es dem Modell, verschiedene Gespr√§chsstile und Reaktionsweisen gezielt zu erlernen.\n",
    "\n"
   ],
   "metadata": {
    "id": "969_4LxJ8Oj6"
   },
   "id": "969_4LxJ8Oj6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Feinabstimmungskosten\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "0zhj1rC865Zs"
   },
   "id": "0zhj1rC865Zs"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Die Feinabstimmung eines gro√üen Sprachmodells (LLM) kann mit hohen Kosten verbunden sein ‚Äì sowohl beim Training als auch im laufenden Betrieb.  \n",
    "\n",
    "- **Trainingskosten**: Die Anpassung eines Modells erfordert erhebliche Rechenleistung, insbesondere bei gro√üen Datens√§tzen. Dies kann zu hohen Ausgaben f√ºr Cloud-Dienste oder Spezialhardware f√ºhren.  \n",
    "- **Betriebskosten**: Nach der Feinabstimmung verursacht das Hosting des Modells fortlaufende Kosten f√ºr Infrastruktur und Echtzeitverarbeitung, die je nach Nutzung skaliert werden m√ºssen.  \n",
    "\n"
   ],
   "metadata": {
    "id": "fl6otVHX8l_P"
   },
   "id": "fl6otVHX8l_P"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 | Fine-Tuning mit Dashboard\n",
    "---"
   ],
   "metadata": {
    "id": "0Yrawkis44O0"
   },
   "id": "0Yrawkis44O0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Die Feinabstimmung der Modelle beginnt √ºber die OpenAI-Website.\n",
    "\n",
    "Zun√§chst werden zwei Dateien ben√∂tigt:  \n",
    "1. **Trainingsdatei** ‚Äì Enth√§lt die Daten zur Anpassung des Modells.  \n",
    "2. **Validierungsdatei** ‚Äì Dient zur √úberpr√ºfung der Trainingsqualit√§t.  \n",
    "\n",
    "Im n√§chsten Abschnitt wird erl√§utert, wie die API f√ºr diesen Prozess genutzt wird."
   ],
   "metadata": {
    "id": "xf1olfhp8nmz"
   },
   "id": "xf1olfhp8nmz"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "üñ§ Fine-Tuning von Monday\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "mGgUCrEN5QXP"
   },
   "id": "mGgUCrEN5QXP"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Monday ist ein sarkastischer, charmant-resignierter KI-Charakter, der als textbasiertes Sprachmodell auf einem fein abgestimmten Datensatz trainiert wurde. Ziel des Fine-Tunings ist es, ein Large Language Model (LLM) so anzupassen, dass es den charakteristischen Stil von Monday verinnerlicht ‚Äì inklusive Ironie, Emo-Stimmung und pointierter Antworten.\n",
    "\n",
    "<br>\n",
    "\n",
    "> Warum **Monday**?   \n",
    "Klingt nach einem Charakter: n√ºchtern, etwas m√ºrrisch, aber tief drin doch hilfsbereit - wie der erste Tag der Woche, den keiner will, aber alle brauchen.\n",
    "\n",
    "<br>\n",
    "\n",
    "Das Fine-Tuning erfolgt auf Basis eines speziell kuratierten Datensatzes im JSONL-Format, der beispielhafte Dialoge zwischen Nutzer und dem Charakter ‚ÄûMonday‚Äú enth√§lt. Jeder Eintrag beginnt mit einem fest definierten System-Prompt, der die Pers√∂nlichkeit und Haltung von Monday vorgibt."
   ],
   "metadata": {
    "id": "-bTeuIMR5pjF"
   },
   "id": "-bTeuIMR5pjF"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/genai-5.png)\n"
   ],
   "metadata": {
    "id": "M39FZeCIMdgU"
   },
   "id": "M39FZeCIMdgU"
  },
  {
   "cell_type": "markdown",
   "id": "76060a76",
   "metadata": {
    "id": "76060a76"
   },
   "source": [
    "Die Erstellung *fein-abgestimmter* Modelle beginnt mit dem **Hochladen der Trainingsdateien** in den Trainingsdialog. Dort k√∂nnen alle erforderlichen **Trainingsparameter** definiert werden.  \n",
    "\n",
    "Weitere Anpassungen lassen sich √ºber den **Chat Playground** vornehmen:  \n",
    "[Chat Playground - OpenAI API](https://platform.openai.com/playground/chat?preset=default-marv-sarcastic-chat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/genai-1.png)"
   ],
   "metadata": {
    "id": "l5XDKlutN-yY"
   },
   "id": "l5XDKlutN-yY"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Anzeige w√§hrend des Trainings**"
   ],
   "metadata": {
    "id": "epC6kQgEOshK"
   },
   "id": "epC6kQgEOshK"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/genai-6.png)"
   ],
   "metadata": {
    "id": "JkqxZtR6OE23"
   },
   "id": "JkqxZtR6OE23"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nach Abschluss des Trainings steht das optimierte Modell zur Verf√ºgung.    \n",
    "Wichtig zu beachten: **Bei OpenAI k√∂nnen vorab trainierte Modelle nicht gel√∂scht werden.** Allerdings entstehen keine Kosten f√ºr deren Speicherung ‚Äì abgerechnet werden lediglich die **Trainings- und Inferenzzeit**."
   ],
   "metadata": {
    "id": "yhn_PJWs_w7L"
   },
   "id": "yhn_PJWs_w7L"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Trainingsverlauf & Metriken**"
   ],
   "metadata": {
    "id": "GvPLqAY9O1OC"
   },
   "id": "GvPLqAY9O1OC"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/genai-7.png)"
   ],
   "metadata": {
    "id": "xZxdzWmZOU31"
   },
   "id": "xZxdzWmZOU31"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Mail - Mission Completed**"
   ],
   "metadata": {
    "id": "XfFfZGBrWlOl"
   },
   "id": "XfFfZGBrWlOl"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/genai-10.png)"
   ],
   "metadata": {
    "id": "Cx7WkQo9WhKl"
   },
   "id": "Cx7WkQo9WhKl"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Fine-Tuned-Model im Playground\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "yMlXVyHRPquf"
   },
   "id": "yMlXVyHRPquf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Anwendung des Fine-Tuned-Modell o. System-Prompt**"
   ],
   "metadata": {
    "id": "U7XjaJYuSYwn"
   },
   "id": "U7XjaJYuSYwn"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/genai-8.png)"
   ],
   "metadata": {
    "id": "SYmvYgNGPlSP"
   },
   "id": "SYmvYgNGPlSP"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Fine-Tuned-Model m. kurzem System-Prompt**"
   ],
   "metadata": {
    "id": "03rPYnqGSbyK"
   },
   "id": "03rPYnqGSbyK"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/genai-9.png)"
   ],
   "metadata": {
    "id": "ai0cIAICSOSo"
   },
   "id": "ai0cIAICSOSo"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Fine-Tuned-Model m. langem System-Prompt**"
   ],
   "metadata": {
    "id": "oKUdSeyj_XN4"
   },
   "id": "oKUdSeyj_XN4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/genai-4.png)"
   ],
   "metadata": {
    "id": "5XxDgRZlOgzm"
   },
   "id": "5XxDgRZlOgzm"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Standard-Modell gpt-4.1-mini ohne System-Prompt**"
   ],
   "metadata": {
    "id": "32j1TbPGQ8Gy"
   },
   "id": "32j1TbPGQ8Gy"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/genai-2.png)"
   ],
   "metadata": {
    "id": "JzVpk0rgOvvm"
   },
   "id": "JzVpk0rgOvvm"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Standard-Modell gpt-4.1-mini mit System-Prompt**"
   ],
   "metadata": {
    "id": "KA7Dl_fCRzj6"
   },
   "id": "KA7Dl_fCRzj6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/genai-3.png)"
   ],
   "metadata": {
    "id": "7Wj79E2wOpEW"
   },
   "id": "7Wj79E2wOpEW"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Einsatz Fine-Tuned-Model mit API\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "MRPTPC8pW9sO"
   },
   "id": "MRPTPC8pW9sO"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cubgn7G3W-dK"
   },
   "source": [
    "**Testen des FT-Modells**"
   ],
   "id": "Cubgn7G3W-dK"
  },
  {
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# OpenAI Client initialisieren\n",
    "client = OpenAI()\n",
    "\n",
    "# Modell-ID (ersetze dies durch deine eigene Fine-Tuned-Modell-ID)\n",
    "model_id = \"ft:gpt-4.1-mini-2025-04-14:ralf-bendig:loriot-001:BaLRxUL8\"  # input(\"Modell Id: \")\n",
    "\n",
    "# System-Prompt festlegen\n",
    "system_prompt = \"\"\"\n",
    "Du bist Assistent am Lehrstuhl von Professor H√§ubel zu pneumatische Plastologie und beantwortest Fragen zu deinem Fachgebiet humorvoll\".\n",
    "\"\"\"\n",
    "\n",
    "# Interaktiver Test-Loop\n",
    "print()\n",
    "print(f\"=== Interaktiver Test f√ºr Modell: {model_id} ===\")\n",
    "print(\"Gib 'exit' ein, um den Test zu beenden.\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nDeine Nachricht: \")\n",
    "\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "    # Chat-Completion-Anfrage senden\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ],\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Antwort ausgeben\n",
    "    display(Markdown(\"###  üßë‚Äçüéì Assistent:\"))\n",
    "    display(Markdown(response.choices[0].message.content))"
   ],
   "metadata": {
    "id": "fSSu2YmUW-dL"
   },
   "execution_count": null,
   "outputs": [],
   "id": "fSSu2YmUW-dL"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# 3 | Fine-Tuning mit Code\n",
    "---"
   ],
   "metadata": {
    "id": "a0I9sekJ5LIq"
   },
   "id": "a0I9sekJ5LIq"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Initialisierung des OpenAI-Clients und Datei-Upload**"
   ],
   "metadata": {
    "id": "zLTi-cZw5T5f"
   },
   "id": "zLTi-cZw5T5f"
  },
  {
   "cell_type": "code",
   "source": [
    "from os import environ\n",
    "import time\n",
    "from google.colab import userdata, files\n",
    "from openai import OpenAI\n",
    "\n",
    "# OpenAI Client initialisieren\n",
    "client = OpenAI()\n",
    "\n",
    "# Datei-Auswahldialog f√ºr Trainingsdaten √∂ffnen\n",
    "print(\"Bitte Trainingsdatei hochladen (JSONL-Format):\")\n",
    "uploaded_train = files.upload()\n",
    "train_file_path = list(uploaded_train.keys())[0]\n",
    "\n",
    "# Datei-Auswahldialog f√ºr Validierungsdaten √∂ffnen\n",
    "print(\"Bitte Validierungsdatei hochladen (JSONL-Format):\")\n",
    "uploaded_val = files.upload()\n",
    "val_file_path = list(uploaded_val.keys())[0]"
   ],
   "metadata": {
    "id": "mGAGi3JyJPjS"
   },
   "id": "mGAGi3JyJPjS",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Daten f√ºr Fine-Tuning auf OpenAI hochladen**"
   ],
   "metadata": {
    "id": "FsnhHibnMRzg"
   },
   "id": "FsnhHibnMRzg"
  },
  {
   "cell_type": "code",
   "source": [
    "# Dateien f√ºr Fine-Tuning hochladen\n",
    "def upload_file(file_path, purpose):\n",
    "    \"\"\"L√§dt eine Datei f√ºr das Fine-Tuning hoch.\"\"\"\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        response = client.files.create(file=file, purpose=\"fine-tune\")\n",
    "    print(f\"Datei {file_path} hochgeladen: {response.id}\")\n",
    "    return response.id\n",
    "\n",
    "# Dateien hochladen und warten bis sie verarbeitet sind\n",
    "train_file_id = upload_file(train_file_path, \"fine-tune\")\n",
    "val_file_id = upload_file(val_file_path, \"fine-tune\")\n",
    "\n",
    "# Auf Verarbeitung warten\n",
    "for file_id in [train_file_id, val_file_id]:\n",
    "    print(f\"Warte auf Verarbeitung von {file_id}...\")\n",
    "    while client.files.retrieve(file_id).status != \"processed\":\n",
    "        print(\".\", end=\"\")\n",
    "        time.sleep(2)\n",
    "    print(f\"\\nDatei {file_id} ist verarbeitet.\")"
   ],
   "metadata": {
    "id": "SLVUgWggKqK0"
   },
   "id": "SLVUgWggKqK0",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Verf√ºgbare Modelle f√ºr ein Fine-Tuning abfragen**"
   ],
   "metadata": {
    "id": "lL9q9LVhLtMO"
   },
   "id": "lL9q9LVhLtMO"
  },
  {
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# OpenAI Client initialisieren\n",
    "client = OpenAI()\n",
    "\n",
    "# Alle verf√ºgbaren Modelle abfragen\n",
    "models = client.models.list()\n",
    "\n",
    "# Verf√ºgbare Fine-Tuning-Modelle filtern\n",
    "fine_tuning_models = []\n",
    "\n",
    "# Die Modelle, die offiziell f√ºr Fine-Tuning unterst√ºtzt werden\n",
    "official_fine_tunable = [\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"gpt-4\",\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4-turbo\",\n",
    "    \"babbage-002\",\n",
    "    \"davinci-002\"\n",
    "]\n",
    "\n",
    "# Alle Modelle durchgehen und Fine-Tuning-f√§hige Modelle identifizieren\n",
    "for model in models.data:\n",
    "    model_id = model.id\n",
    "    # Nach Fine-Tuning-f√§higen Basismodellen filtern\n",
    "    if any(base_model in model_id for base_model in official_fine_tunable) and not \"ft-\" in model_id:\n",
    "        fine_tuning_models.append(model_id)\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "print(\"F√ºr Fine-Tuning verf√ºgbare Basismodelle:\")\n",
    "for i, model in enumerate(sorted(fine_tuning_models), 1):\n",
    "    print(f\"{i}. {model}\")"
   ],
   "metadata": {
    "collapsed": true,
    "id": "58iyQyKCLx8k"
   },
   "id": "58iyQyKCLx8k",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Fine-Tuning starten**"
   ],
   "metadata": {
    "id": "4efks5aAMgK0"
   },
   "id": "4efks5aAMgK0"
  },
  {
   "cell_type": "code",
   "source": [
    "job = client.fine_tuning.jobs.create(\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    training_file=train_file_id,\n",
    "    validation_file=val_file_id,\n",
    "    hyperparameters={\"n_epochs\": 10}\n",
    ")\n",
    "print(f\"Fine-Tuning-Job gestartet: {job.id}\")\n",
    "print(f\"Status: {client.fine_tuning.jobs.retrieve(job.id).status}\")\n",
    "\n",
    "# Job-Status in Schleife abfragen\n",
    "print(\"√úberwache Job-Status:\")\n",
    "while True:\n",
    "    status = client.fine_tuning.jobs.retrieve(job.id)\n",
    "    print(f\"Status: {status.status}\")\n",
    "    if status.status in [\"succeeded\", \"failed\"]:\n",
    "        break\n",
    "    time.sleep(60)  # Status alle 60 Sekunden pr√ºfen"
   ],
   "metadata": {
    "id": "13VgFsqIK2uN"
   },
   "id": "13VgFsqIK2uN",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Informationen zu einem FT-Modell abfragen**"
   ],
   "metadata": {
    "id": "0QBBYpo1Nu-Q"
   },
   "id": "0QBBYpo1Nu-Q"
  },
  {
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# OpenAI Client initialisieren\n",
    "client = OpenAI()\n",
    "\n",
    "# 1. Informationen zu einem spezifischen Fine-Tuning-Job abfragen\n",
    "def get_ft_job_info(job_id):\n",
    "    \"\"\"Ruft Informationen zu einem Fine-Tuning-Job ab.\"\"\"\n",
    "    job_info = client.fine_tuning.jobs.retrieve(job_id)\n",
    "\n",
    "    # Ausgabe der Job-Informationen\n",
    "    print()\n",
    "    print(f\"=== Fine-Tuning-Job Informationen ===\")\n",
    "    print(f\"Job ID: {job_info.id}\")\n",
    "    print(f\"Status: {job_info.status}\")\n",
    "    print(f\"Erstellungszeitpunkt: {job_info.created_at}\")\n",
    "    print(f\"Basismodell: {job_info.model}\")\n",
    "    print(f\"Trainingsdatei: {job_info.training_file}\")\n",
    "    print(f\"Validierungsdatei: {job_info.validation_file}\")\n",
    "\n",
    "    # Ausgabe der Trainingsergebnisse, falls vorhanden\n",
    "    if hasattr(job_info, 'result_files') and job_info.result_files:\n",
    "        print(f\"Ergebnisdateien: {job_info.result_files}\")\n",
    "\n",
    "    if hasattr(job_info, 'trained_tokens') and job_info.trained_tokens:\n",
    "        print(f\"Trainierte Tokens: {job_info.trained_tokens}\")\n",
    "\n",
    "    if hasattr(job_info, 'fine_tuned_model') and job_info.fine_tuned_model:\n",
    "        print(f\"Fine-Tuned Modell ID: {job_info.fine_tuned_model}\")\n",
    "        return job_info.fine_tuned_model\n",
    "\n",
    "    return None\n",
    "\n",
    "# 2. Alle Fine-Tuning-Jobs auflisten\n",
    "def list_ft_jobs():\n",
    "    \"\"\"Listet alle Fine-Tuning-Jobs auf.\"\"\"\n",
    "    jobs = client.fine_tuning.jobs.list()\n",
    "\n",
    "    print()\n",
    "    print(f\"=== Alle Fine-Tuning-Jobs ===\")\n",
    "    for job in jobs.data:\n",
    "        print(f\"Job ID: {job.id}, Status: {job.status}, Modell: {job.model}, Erstellt: {job.created_at}\")\n",
    "        if hasattr(job, 'fine_tuned_model') and job.fine_tuned_model:\n",
    "            print(f\"  ‚Üí Fine-Tuned Modell: {job.fine_tuned_model}\")\n",
    "\n",
    "    return jobs.data\n",
    "\n",
    "# 3. Informationen zu einem Fine-Tuned-Modell abfragen\n",
    "def get_ft_model_info(model_id):\n",
    "    \"\"\"Ruft Informationen zu einem Fine-Tuned-Modell ab.\"\"\"\n",
    "    try:\n",
    "        model_info = client.models.retrieve(model_id)\n",
    "\n",
    "        print()\n",
    "        print(f\"=== Fine-Tuned-Modell Informationen ===\")\n",
    "        print(f\"Modell ID: {model_info.id}\")\n",
    "        print(f\"Eigent√ºmer: {model_info.owned_by}\")\n",
    "        print(f\"Erstellungszeitpunkt: {model_info.created}\")\n",
    "\n",
    "        # Weitere verf√ºgbare Metadaten ausgeben\n",
    "        if hasattr(model_info, 'permission'):\n",
    "            print(f\"Berechtigungen: {model_info.permission}\")\n",
    "\n",
    "        return model_info\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Abrufen der Modellinformationen: {e}\")\n",
    "        return None\n",
    "\n",
    "# Beispielnutzung:\n",
    "# 1. Alle Fine-Tuning-Jobs anzeigen\n",
    "jobs = list_ft_jobs()\n",
    "\n",
    "# 2. Optional: Job-ID vom Benutzer abfragen\n",
    "if jobs:\n",
    "    job_id_input = input(\"Gib die ID eines Fine-Tuning-Jobs ein, um Details zu sehen (oder dr√ºcke Enter, um zu √ºberspringen): \")\n",
    "\n",
    "    if job_id_input:\n",
    "        # Detaillierte Informationen zum Job anzeigen\n",
    "        ft_model_id = get_ft_job_info(job_id_input)\n",
    "\n",
    "        # Wenn das Fine-Tuned-Modell verf√ºgbar ist, dessen Details anzeigen\n",
    "        if ft_model_id:\n",
    "            get_ft_model_info(ft_model_id)\n",
    "else:\n",
    "    print(\"Keine Fine-Tuning-Jobs gefunden.\")"
   ],
   "metadata": {
    "id": "7Uo8BF5QN0uN"
   },
   "id": "7Uo8BF5QN0uN",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f4a99fd3",
   "metadata": {
    "id": "f4a99fd3"
   },
   "source": [
    "**Testen des FT-Modells**"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# OpenAI Client initialisieren\n",
    "client = OpenAI()\n",
    "\n",
    "# Modell-ID (ersetze dies durch deine eigene Fine-Tuned-Modell-ID)\n",
    "model_id = input(\"Modell Id: \")\n",
    "\n",
    "# System-Prompt festlegen\n",
    "system_prompt = \"\"\"\n",
    "Du bist ein Assistent des Lehrstuhls f√ºr Pneumatische Plastologie, geleitet von Prof. H√§ubel.\n",
    "Diese Disziplin beruht auf hochkonzentrierter Atemtechnik, die plastische Ver√§nderungen des K√∂rpers erm√∂glicht.\n",
    "\"\"\"\n",
    "\n",
    "# Interaktiver Test-Loop\n",
    "print()\n",
    "print(f\"=== Interaktiver Test f√ºr Modell: {model_id} ===\")\n",
    "print(\"Gib 'exit' ein, um den Test zu beenden.\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nDeine Nachricht: \")\n",
    "\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "    # Chat-Completion-Anfrage senden\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ],\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Antwort ausgeben\n",
    "    display(Markdown(\"### üßë‚Äçüéì Assistent:\"))\n",
    "    display(Markdown(response.choices[0].message.content))"
   ],
   "metadata": {
    "id": "JoCMaw9QUZeN"
   },
   "id": "JoCMaw9QUZeN",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Modell l√∂schen**"
   ],
   "metadata": {
    "id": "AAa0mkEiGHwg"
   },
   "id": "AAa0mkEiGHwg"
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "def delete_model(model_id, api_key):\n",
    "    url = f'https://api.openai.com/v1/models/{model_id}'\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {api_key}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    response = requests.delete(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print('Modell erfolgreich gel√∂scht.')\n",
    "    else:\n",
    "        print(f'Fehler beim L√∂schen des Modells: {response.status_code}, {response.text}')\n",
    "\n",
    "# Beispielaufruf\n",
    "api_key = all_keys['OPENAI_API_KEY']\n",
    "model_id = 'ft:gpt-4o-mini-2024-07-18:personal:my-test2:B72Xnul4'\n",
    "delete_model(model_id, api_key)"
   ],
   "metadata": {
    "id": "bYF_4xMcGGiO"
   },
   "id": "bYF_4xMcGGiO",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c220d8d1",
   "metadata": {
    "id": "c220d8d1"
   },
   "source": [
    "**Integration mit LangChain v0.3+**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a96cb04",
   "metadata": {
    "id": "6a96cb04"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "def verwende_fine_tuned_modell_mit_langchain(modell_id, frage):\n",
    "    \"\"\"Verwendet ein fine-tuned Modell mit LangChain 1.0+ und dem neuen Runnables-API\"\"\"\n",
    "\n",
    "    # LangChain mit fine-tuned Modell (Kurznotation: \"provider:model\")\n",
    "    # Hinweis: Fine-tuned Modelle werden mit vollem Namen angegeben\n",
    "    llm = init_chat_model(\n",
    "        f\"openai:{modell_id}\",\n",
    "        temperature=0.7,\n",
    "        presence_penalty=0.3\n",
    "    )\n",
    "\n",
    "    # Erstelle einen Prompt-Template mit dem neuen Chat-Format\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{frage}\")\n",
    "    ])\n",
    "\n",
    "    # Verwende das neue Runnables-API (Pipe-Operator) anstelle von LLMChain\n",
    "    chain = prompt | llm\n",
    "\n",
    "    # F√ºhre die Chain aus\n",
    "    return chain.invoke({\"frage\": frage})\n",
    "\n",
    "# Beispiel f√ºr die Verwendung mit LangChain\n",
    "ergebnis = verwende_fine_tuned_modell_mit_langchain(model_id, \"Wie nehme ich ab?\")\n",
    "display(Markdown(\"### ü¶ú LangChain fragt üñ§ Monday:\"))\n",
    "display(Markdown(ergebnis.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4 | Parameter Efficient Fine-Tuning\n",
    "---"
   ],
   "metadata": {
    "id": "d3dlsbMhjA30"
   },
   "id": "d3dlsbMhjA30"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Parameter Efficient Fine-Tuning (PEFT) bezeichnet eine Reihe von Techniken, die darauf abzielen, gro√üe Sprachmodelle (LLMs) mit deutlich weniger Rechenaufwand und Speicherbedarf anzupassen, als dies beim traditionellen \"Full Fine-Tuning\" der Fall w√§re. Anstatt alle Milliarden von Parametern eines vortrainierten Modells zu aktualisieren, frieren PEFT-Methoden einen Gro√üteil der urspr√ºnglichen Modellgewichte ein und trainieren nur eine kleine Anzahl von neu hinzugef√ºgten oder modifizierten Parametern. Dies erm√∂glicht eine effizientere Anpassung an spezifische Aufgaben oder Dom√§nen, ohne die Notwendigkeit enormer Hardware-Ressourcen.\n",
    "\n"
   ],
   "metadata": {
    "id": "wqCOzQ7NmSw1"
   },
   "id": "wqCOzQ7NmSw1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "LoRA (Low-Rank Adaptation)\n",
    "</font></p>\n",
    "\n",
    "LoRA (Low-Rank Adaptation) ist eine popul√§re PEFT-Methode, die die Feinabstimmung von LLMs optimiert, indem sie nur eine begrenzte Anzahl von Parametern anpasst. Die Kernidee von LoRA besteht darin, trainierbare Low-Rank-Matrizen zu den urspr√ºnglichen Gewichtsmatrizen des Basismodells hinzuzuf√ºgen. Anstatt alle Parameter zu modifizieren, werden lediglich zwei kleinere Matrizen trainiert, die dann mit den Originalgewichten kombiniert werden. Typischerweise werden bei LoRA nur die Attention-Layer oder lineare Schichten des Modells angepasst, was lediglich 1-10 % der gesamten Parameter ausmacht.\n",
    "\n",
    "Die Vorteile von LoRA sind vielf√§ltig:\n",
    "* **Ressourceneffizienz:** Es reduziert den GPU-Speicherbedarf erheblich (bis zu 33 % weniger als Full Fine-Tuning).\n",
    "* **Overfitting-Vermeidung:** Die geringere Anzahl an trainierbaren Parametern minimiert das Risiko, dass das Modell irrelevante Muster aus den Trainingsdaten lernt.\n",
    "* **Flexibilit√§t:** Die trainierten LoRA-Adapter k√∂nnen modular f√ºr verschiedene Aufgaben hinzugef√ºgt oder entfernt werden.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "Qp2fzgq4m3SK"
   },
   "id": "Qp2fzgq4m3SK"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "QLoRA (Quantized LoRA)\n",
    "</font></p>\n",
    "\n",
    "QLoRA (Quantized LoRA) baut auf LoRA auf, indem es zus√§tzlich Quantisierungstechniken einsetzt, um Modelle auf eine geringere Bit-Genauigkeit (z. B. 4-8 Bit) zu komprimieren. **Quantisierung** ist ein Prozess, bei dem hochpr√§zise Werte (z.B. 32-Bit-Flie√ükommazahlen) in Werte mit geringerer Genauigkeit (z.B. 8-Bit- oder 4-Bit-Ganzzahlen) umgewandelt werden. Ziel ist es, Speicherplatz zu sparen und die Verarbeitungsgeschwindigkeit zu erh√∂hen.\n",
    "\n",
    "Die Kombination von LoRA und Quantisierung erm√∂glicht es, sehr gro√üe Modelle (z.B. mit 30 Milliarden Parametern) auf Consumer-GPUs zu trainieren und eine nahezu verlustfreie Rekonstruktion der Originalgewichte nach dem Training zu erreichen.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "1gVqKZvYm4q5"
   },
   "id": "1gVqKZvYm4q5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Vergleich zu Full Fine-Tuning\n",
    "</font></p>\n",
    "\n",
    "Der Vergleich zwischen LoRA/QLoRA und Full Fine-Tuning zeigt die erheblichen Vorteile der PEFT-Methoden:\n",
    "\n",
    "| Aspekt              | LoRA/QLoRA | Full Finetuning |\n",
    "| :------------------ | :--------- | :-------------- |\n",
    "| Trainingsparameter  | 1-10 %     | 100 %           |\n",
    "| GPU-Speicher        | 8-16 GB    | 40-80 GB        |\n",
    "| Overfitting-Risiko  | Niedrig    | Hoch            |\n",
    "| Trainingsgeschwindigkeit | Schnell    | Langsam         |\n",
    "\n",
    "LoRA und QLoRA bieten somit einen exzellenten Kompromiss zwischen Leistung und Effizienz, was sie besonders wertvoll f√ºr Szenarien mit begrenzten Rechenressourcen macht. Es ist jedoch wichtig zu beachten, dass OpenAI derzeit keine direkte Unterst√ºtzung f√ºr LoRA oder QLoRA beim Fine-Tuning seiner Close-Source-Modelle wie GPT-4 anbietet. Wer diese spezifischen Techniken nutzen m√∂chte, greift typischerweise auf Open-Source-Modelle aus der Hugging Face-Community zur√ºck."
   ],
   "metadata": {
    "id": "FU8Gk3fTm6Rv"
   },
   "id": "FU8Gk3fTm6Rv"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5 | PEFT von GPT2\n",
    "---"
   ],
   "metadata": {
    "id": "34pTbXtLoOCa"
   },
   "id": "34pTbXtLoOCa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "GPT-2 (\"Generative Pre-trained Transformer 2\") ist ein gro√ües Sprachmodell, das 2019 von OpenAI entwickelt wurde. Es basiert auf der Transformer-Architektur und ist darauf trainiert, eigenst√§ndig zusammenh√§ngende Texte zu generieren, die oft kaum von menschlichen Texten zu unterscheiden sind.\n",
    "\n",
    "Das Modell wurde mit einem umfangreichen Datensatz von 8 Millionen Webseiten vorab trainiert und besitzt bis zu 1,5 Milliarden Parameter. GPT-2 ist in der Lage, verschiedene Sprachaufgaben wie Textgenerierung, Zusammenfassung, √úbersetzung und Beantwortung von Fragen aus Rohdaten zu lernen, ohne dass spezifische dom√§nenspezifische Trainingsdaten erforderlich sind."
   ],
   "metadata": {
    "id": "cgu8Gyoho5Lc"
   },
   "id": "cgu8Gyoho5Lc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "LoRA-Konfiguration\n",
    "</font></p>\n",
    "\n",
    "\n",
    "\n",
    "Diese `LoraConfig` ist darauf ausgelegt, ein gro√ües Sprachmodell (LLM) effizient an eine **spezifische Dom√§ne** anzupassen, indem nur wenige zus√§tzliche, kleine Matrizen trainiert werden.\n",
    "\n",
    "* **`task_type=TaskType.CAUSAL_LM`**: Definiert die Aufgabe als kausale Sprachmodellierung (Vorhersage des n√§chsten Wortes).\n",
    "* **`r=16`**: **LoRA-Rank**, bestimmt die Anzahl trainierbarer Parameter. `16` ist ein moderater Wert f√ºr fokussiertes Lernen, der Effizienz und Anpassungsf√§higkeit balanciert.\n",
    "* **`lora_alpha=32`**: Skalierungsfaktor f√ºr LoRA-Gewichte. Eine \"2:1 Ratio\" zu `r` (32/16) verst√§rkt den Einfluss der gelernten Matrizen.\n",
    "* **`lora_dropout=0.05`**: Geringer **Dropout-Wert** zur Regularisierung (5%), um Overfitting zu reduzieren und das dom√§nenspezifische Lernen zu f√∂rdern.\n",
    "* **`target_modules=[\"c_attn\", \"c_proj\"]`**: Gibt an, welche Modellmodule mit LoRA erweitert werden. `c_attn` (Aufmerksamkeit) und `c_proj` (Output-Projektion) sind Schl√ºsselmodule in GPT-√§hnlichen Architekturen f√ºr effektive Anpassung.\n",
    "* **`bias=\"none\"`**: Es werden keine Bias-Parameter durch LoRA angepasst, was die Anzahl der trainierbaren Parameter weiter reduziert.\n",
    "* **`fan_in_fan_out=True`**: Stellt die korrekte Initialisierung der LoRA-Gewichte sicher, besonders wichtig f√ºr bestimmte Modultypen (z.B. `Conv1D`).\n"
   ],
   "metadata": {
    "id": "Vae4zIABdg4E"
   },
   "id": "Vae4zIABdg4E"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "LoRA Fine-Tuning\n",
    "</font></p>\n",
    "\n",
    "*Powered by Claude*"
   ],
   "metadata": {
    "id": "WuzMQGjTflUA"
   },
   "id": "WuzMQGjTflUA"
  },
  {
   "cell_type": "code",
   "source": [
    "#@markdown   <p><font size=\"4\" color='green'> üßú‚Äç‚ôÄÔ∏è Mermaid-Diagramm</font> </br></p>\n",
    "diagram = \"\"\"\n",
    "flowchart LR\n",
    "    subgraph INPUT[\"üì• Input\"]\n",
    "        A[\"ü§ñ GPT-2<br/>Basismodell\"]\n",
    "        B[\"üìö Technology<br/>Q&A Daten\"]\n",
    "    end\n",
    "\n",
    "    subgraph LORA[\"üîß LoRA Setup\"]\n",
    "        C[\"‚öôÔ∏è LoRA Config<br/>r=16, Œ±=32<br/>Module: c_attn, c_proj\"]\n",
    "        D[\"‚ùÑÔ∏è Basis einfrieren<br/>üî• Adapter trainieren\"]\n",
    "    end\n",
    "\n",
    "    subgraph TRAINING[\"üèãÔ∏è Training\"]\n",
    "        E[\"üîÅ Training Loop<br/>AdamW Optimizer\"]\n",
    "        F[\"üíæ Adapter<br/>speichern\"]\n",
    "    end\n",
    "\n",
    "    subgraph OUTPUT[\"üì§ Output\"]\n",
    "        G[\"‚úÖ LoRA-GPT-2<br/>Spezialisiert\"]\n",
    "    end\n",
    "\n",
    "    A --> C\n",
    "    B --> C\n",
    "    C --> D --> E --> F --> G\n",
    "\n",
    "    style INPUT fill:#4a90d9,color:#fff\n",
    "    style LORA fill:#e5c07b,color:#282c34\n",
    "    style TRAINING fill:#e06c75,color:#fff\n",
    "    style OUTPUT fill:#98c379,color:#282c34\n",
    "\"\"\"\n",
    "mermaid(diagram, 1000, 700)"
   ],
   "metadata": {
    "cellView": "form",
    "id": "U5Pz9f01zhnX"
   },
   "id": "U5Pz9f01zhnX",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Import\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime"
   ],
   "metadata": {
    "collapsed": true,
    "id": "ycJ1zgI8LgNQ"
   },
   "execution_count": null,
   "outputs": [],
   "id": "ycJ1zgI8LgNQ"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Trainingsdaten\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "ElZ1Ee4uq2TA"
   },
   "id": "ElZ1Ee4uq2TA"
  },
  {
   "cell_type": "code",
   "source": "# =============================================\n# Trainingsdaten aus JSONL-Datei laden\n# =============================================\nimport json\nimport random\n\nwith open(\"../02_daten/tech_qa_training.jsonl\", encoding=\"utf-8\") as f:\n    qa_pairs = [json.loads(line) for line in f]\n\n# In Trainingstexte umwandeln (verschiedene Formate f√ºr Robustheit)\ntraining_data = []\nfor qa in qa_pairs:\n    training_data.append(f\"Question: {qa['question']} Answer: {qa['answer']}\")\n    training_data.append(f\"Q: {qa['question']} A: {qa['answer']}\")\n\nrandom.shuffle(training_data)\n\nprint(f\"‚úÖ {len(training_data)} Trainingsbeispiele geladen ({len(qa_pairs)} Q&A-Paare)\")\nprint(f\"üìÑ Quelle: ../02_daten/tech_qa_training.jsonl\")\nprint(f\"\\nüìù Beispiel:\\n   {training_data[0][:100]}...\")",
   "metadata": {
    "id": "Bzc7AVhAjNrN"
   },
   "execution_count": null,
   "outputs": [],
   "id": "Bzc7AVhAjNrN"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Funktion Performance vor/nach Training\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "XG4kcG4wq7aL"
   },
   "id": "XG4kcG4wq7aL"
  },
  {
   "cell_type": "code",
   "source": "# =============================================\n# Vergleich: GPT-2 Basis vs. LoRA-Modell\n# =============================================\n\ndef compare_models(test_questions, lora_path=\"./lora_technology_focused\"):\n    \"\"\"Vergleicht Original-GPT-2 mit LoRA-angepasstem Modell\"\"\"\n\n    from peft import PeftModel\n\n    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n\n    # Beide Modelle laden\n    print(\"üîÑ Lade Original-GPT-2 und LoRA-Modell...\")\n    base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n    base_model.eval()\n\n    lora_base = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n    lora_model = PeftModel.from_pretrained(lora_base, lora_path)\n    lora_model.eval()\n    print(\"‚úÖ Beide Modelle geladen\\n\")\n\n    # Generierungsparameter\n    gen_params = dict(\n        max_length=100, temperature=0.7, top_p=0.9,\n        do_sample=True, pad_token_id=tokenizer.eos_token_id,\n        repetition_penalty=1.2\n    )\n\n    # Vergleich durchf√ºhren\n    for i, question in enumerate(test_questions, 1):\n        print(f\"{'='*60}\")\n        print(f\"[{i}] üéØ {question}\")\n\n        inputs = tokenizer.encode(question, return_tensors=\"pt\")\n\n        with torch.no_grad():\n            base_out = tokenizer.decode(\n                base_model.generate(inputs, **gen_params)[0], skip_special_tokens=True\n            )\n            lora_out = tokenizer.decode(\n                lora_model.generate(inputs, **gen_params)[0], skip_special_tokens=True\n            )\n\n        print(f\"\\nüî∑ VORHER (Original GPT-2):\")\n        print(f\"   {base_out[len(question):].strip()[:200]}\")\n        print(f\"\\nüî∂ NACHHER (LoRA):\")\n        print(f\"   {lora_out[len(question):].strip()[:200]}\")\n        print()\n\nprint(\"‚úÖ Funktion compare_models() definiert\")",
   "metadata": {
    "id": "ImK1iU6cjWSh"
   },
   "execution_count": null,
   "outputs": [],
   "id": "ImK1iU6cjWSh"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Funktion Training\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "ftRKN9LtrA5p"
   },
   "id": "ftRKN9LtrA5p"
  },
  {
   "cell_type": "code",
   "source": "# =============================================\n# LoRA Training\n# =============================================\n\n# 1. Modell und Tokenizer laden\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# 2. LoRA-Konfiguration\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    r=16,                                      # Rank\n    lora_alpha=32,                             # Skalierung (2:1 Ratio)\n    lora_dropout=0.05,                         # Regularisierung\n    target_modules=[\"c_attn\", \"c_proj\"],       # Attention-Layer\n    bias=\"none\",\n    fan_in_fan_out=True,\n)\n\nmodel = get_peft_model(model, lora_config)\n\n# Parameter-Info\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in model.parameters())\nprint(f\"üîß LoRA: {trainable:,} trainierbare Parameter von {total:,} ({100*trainable/total:.2f}%)\")\n\n# 3. Training\nmodel.train()\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\nepochs = 3\n\nprint(f\"üöÄ Training startet ({epochs} Epochen, {len(training_data)} Beispiele)\\n\")\n\nfor epoch in range(epochs):\n    total_loss = 0\n    random.shuffle(training_data)\n\n    for i, example in enumerate(training_data):\n        inputs = tokenizer(example, return_tensors=\"pt\", max_length=120, truncation=True, padding=True)\n        outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], labels=inputs[\"input_ids\"])\n\n        optimizer.zero_grad()\n        outputs.loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n\n        total_loss += outputs.loss.item()\n\n    avg_loss = total_loss / len(training_data)\n    print(f\"   Epoche {epoch+1}/{epochs}: Loss = {avg_loss:.4f}\")\n\n# 4. Modell speichern\nsave_path = \"./lora_technology_focused\"\nmodel.save_pretrained(save_path)\nprint(f\"\\n‚úÖ Training abgeschlossen! Modell gespeichert: {save_path}\")",
   "metadata": {
    "id": "pkpq1Bucjjcz"
   },
   "execution_count": null,
   "outputs": [],
   "id": "pkpq1Bucjjcz"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Hauptprogramm\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "Sm_c5gF8rKPB"
   },
   "id": "Sm_c5gF8rKPB"
  },
  {
   "cell_type": "code",
   "source": "# =============================================\n# Vergleich: Vorher vs. Nachher\n# =============================================\n\ntest_questions = [\n    \"Question: What is Python?\",\n    \"Question: What is machine learning?\",\n    \"Question: What is cloud computing?\",\n    \"Question: What is an API?\",\n    \"Question: What is cybersecurity?\",\n    \"Question: What is artificial intelligence?\",\n    \"Question: What is blockchain?\",\n    \"Question: What is Docker?\",\n]\n\ncompare_models(test_questions)",
   "metadata": {
    "id": "_gotSjCrjtPu"
   },
   "execution_count": null,
   "outputs": [],
   "id": "_gotSjCrjtPu"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Improvement Analysis\n",
    "</font></p>\n",
    "\n",
    "Diese Analyse bewertet die Leistungs√§nderungen eines Systems (z.B. KI-Modell/Textgenerierung) anhand verschiedener Kriterien:\n",
    "\n",
    "* **Relevance:** Die Ausgaben passen nun besser/schlechter zum Thema oder zur Nutzerabsicht und liefern pr√§zisere Informationen.\n",
    "* **Coherence:** Die Koh√§renz (Zusammenhang/Logik) der Inhalte hat sich verbessert/verschlechtert. Der Text ist mehr/weniger fl√ºssig oder logisch in-/konsistent.\n",
    "* **Technical Accuracy:** Die technische Genauigkeit (faktisch korrekt und pr√§zise) der Informationen ist gleich geblieben, ohne Verbesserung oder Verschlechterung.\n",
    "* **Overall:** Das Gesamtergebnis ist gemischt oder gleich. Verbesserungen in der Relevanz werden durch Verschlechterungen in der Koh√§renz ausgeglichen."
   ],
   "metadata": {
    "id": "VDuoFNNEso-E"
   },
   "id": "VDuoFNNEso-E"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "355d6569"
   },
   "source": [
    "# 6 | Bewerten des FT-Modells\n",
    "---"
   ],
   "id": "355d6569"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Trainingsmetriken\n",
    "</font></p>\n",
    "\n",
    "\n",
    "Die Modellleistung wird prim√§r durch **Trainingsverlust** und **Validierungsverlust** bewertet, basierend auf der Kreuzentropieverlustfunktion. Dieser misst, wie gut das Modell das n√§chste Wort vorhersagt.\n",
    "\n",
    "* **Trainingsverlust:** Durchschnittlicher Kreuzentropieverlust √ºber den Trainingsdatensatz; zeigt die Lernf√§higkeit des Modells.\n",
    "* **Validierungsverlust:** Berechnung auf separatem Datensatz; Indikator f√ºr Generalisierungsf√§higkeit und √úberanpassung.\n",
    "\n",
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Interpretation von Verlustkurven\n",
    "</font></p>\n",
    "\n",
    "Der Verlauf von Trainings- und Validierungsverlust √ºber Epochen visualisiert die Modellleistung:\n",
    "* **Konvergenz:** Beide Verluste sinken und stabilisieren sich.\n",
    "* **√úberanpassung:** Trainingsverlust sinkt, Validierungsverlust steigt.\n",
    "Analyse dieser Trends hilft bei der Anpassung von Trainingsparametern oder Daten. Ein Validierungsdatensatz ist f√ºr eine unvoreingenommene Bewertung entscheidend.\n",
    "\n",
    "**Zugriff auf detaillierte Trainingsmetriken:** [Dashboard](https://platform.openai.com/finetune)\n",
    "\n",
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Verbesserung der Ergebnisse Fine-Tuning\n",
    "</font></p>\n",
    "\n",
    "Optimale Ergebnisse erfordern einen strategischen Ansatz zur Datenaufbereitung, Parameteranpassung und iterativen Verbesserung.\n",
    "\n",
    "* **Hochwertige Trainingsdaten:**\n",
    "    * **Relevanz:** Datensatz muss eng mit der Aufgabe √ºbereinstimmen.\n",
    "    * **Klarheit und Konsistenz:** Pr√§zise Sprache und einheitlicher Stil.\n",
    "* **Ausreichende Datenmenge:**\n",
    "    * **Umfassende Beispiele:** Mehr Muster verbessern die Generalisierung.\n",
    "    * **Ausgewogener Datensatz:** Vielf√§ltige Szenarien abdecken, unn√∂tige Wiederholungen vermeiden.\n",
    "* **Anweisungen zum Erstellen von Anweisungen:**\n",
    "    * **Explizite Anweisungen:** Klare Anweisungen oder Fragen zu Beginn.\n",
    "    * **Konsistentes Eingabeaufforderungsformat:** Einheitliche Struktur beibehalten.\n",
    "* **Anpassen der Trainingsparameter:**\n",
    "    * **Epochen:** Balance zwischen Unter- und √úberanpassung finden.\n",
    "    * **Batchgr√∂√üe:** Anpassen f√ºr Trainingseffizienz.\n",
    "    * **Lernrate:** Steuert die Gewichtsanpassung f√ºr stabile Konvergenz.\n",
    "* **Datenerweiterungstechniken:** Verbessern die Robustheit durch Paraphrasieren, Synonyme/Antonyme und Rauscheneinf√ºhrung.\n",
    "* **Regelm√§√üige Evaluierung und Iteration:**\n",
    "    * **Validierungssatz:** Objektive Leistungsbewertung.\n",
    "    * **Leistungskennzahlen:** √úberwachung von Genauigkeit, Verlust, Verwirrung.\n",
    "    * **Iterative Verfeinerung:** Erkenntnisse f√ºr Anpassungen nutzen.\n",
    "\n",
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Erweiterte API-Funktionen nutzen\n",
    "</font></p>\n",
    "\n",
    "OpenAI-API bietet Inferenz-Optionen:\n",
    "* **Temperatureinstellung:** Steuert die Zuf√§lligkeit der Ausgabe.\n",
    "* **Top-p (Nucleus Sampling):** Balanciert Vielfalt und Genauigkeit.\n",
    "* **Max. Token:** Begrenzt die Ausgabel√§nge.\n",
    "\n",
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Fr√ºhzeitiges Stoppen implementieren\n",
    "</font></p>\n",
    "\n",
    "Verhindert √úberanpassung:\n",
    "* **Verlusttrends √ºberwachen:** Training beenden, wenn Validierungsverlust trotz sinkendem Trainingsverlust steigt.\n",
    "* **Geduldsstufen festlegen:** Anzahl der Epochen ohne Verbesserung definieren.\n",
    "\n",
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Mehrere Feinabstimmungsrunden\n",
    "</font></p>\n",
    "\n",
    "Verbessert Modellleistung schrittweise:\n",
    "* **Umfassende Erstschulung:** Grundmuster lernen.\n",
    "* **Gezielte Verfeinerung:** Spezifischere Daten f√ºr bestimmte Aufgaben.\n",
    "\n",
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Einbeziehung negativer Beispiele\n",
    "</font></p>\n",
    "\n",
    "Bringt dem Modell bei, was es nicht tun soll:\n",
    "* **Falsche Beispiele:** F√ºgen Sie Beispiele mit fehlerhaften Vervollst√§ndigungen und Korrekturen ein.\n",
    "* **Strafmechanismen:** Datenstrukturierung zur Vermeidung unerw√ºnschter Ausgaben.\n",
    "\n",
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Sicherstellung der Datensatzvielfalt\n",
    "</font></p>\n",
    "\n",
    "Hilft dem Modell, breite Eingaben zu verarbeiten:\n",
    "* **Abwechslungsreiche Themen:** Inhalte aus verschiedenen Bereichen integrieren.\n",
    "* **Stilistische Variation:** Beispiele mit unterschiedlichen Schreibstilen und T√∂nen.\n",
    "\n",
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "√úberwachung der Trainingsmetriken\n",
    "</font></p>\n",
    "\n",
    "Behalten Sie den Lernprozess im Auge:\n",
    "* **Verlustkurven:** Aufdecken von Lernmustern.\n",
    "* **Genauigkeitsmetriken:** Verfolgung korrekter Antworten.\n"
   ],
   "metadata": {
    "id": "naf1sOP3oEsi"
   },
   "id": "naf1sOP3oEsi"
  },
  {
   "cell_type": "markdown",
   "id": "4231281b",
   "metadata": {
    "id": "4231281b"
   },
   "source": [
    "# 7 | Aufgabe\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Erstellen Sie einen *neuen* ChatBot, der durch ein Fine-Tuning einen speziellen Charakter einnimmt:\n",
    "\n",
    "- W√§hlen Sie einen Bot-Charakter aus oder denken Sie sich selbst einen Bot-Charakter aus\n",
    "- Erstellen Sie mit Hilfe von ChatGPT Trainingsdaten im .jsonl-Format.\n",
    "- Erstellen Sie mit Hilfe von ChatGPT Testdaten im .jsonl-Format.\n",
    "- F√ºhren Sie ein Fine-Tuning mit Dashboard oder API durch.\n",
    "- Test Sie das *neue* Modell"
   ],
   "metadata": {
    "id": "mgxheJSm5r3e"
   },
   "id": "mgxheJSm5r3e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Weekend-Stil - \"Friday\" üé≥\n",
    "</font></p>\n",
    "\n",
    "**üí¨ Beschreibung:**   \n",
    "Friday ist dein smarter, augenzwinkernder KI-Kumpel f√ºrs Ende der Woche.\n",
    "Er ist charmant, l√∂sungsorientiert - und ein bisschen genervt davon, dass du wieder alles auf Freitag 16:59 verschoben hast.\n",
    "\n",
    "\n",
    "üìù **Beispiel:**   \n",
    "**User:**  \"Friday, kannst du mir nochmal erkl√§ren, was ein Transformer-Modell ist?‚Äú   \n",
    "**Friday:** ‚ÄûKlar, kurz bevor du ins Wochenende verschwindest: Ein Transformer ist wie ein richtig gutes Orga-Tool ‚Äì es schaut sich alle W√∂rter gleichzeitig an und merkt sich, was wichtig ist. Gern geschehen. Jetzt geh raus und tu so, als w√§rst du voll vorbereitet. üòé‚Äú  \n",
    "\n",
    "\n",
    "Zu **Friday** liegen Trainings-/Test-Daten bereits vor. üòä\n"
   ],
   "metadata": {
    "id": "jsfw8l8a9sPV"
   },
   "id": "jsfw8l8a9sPV"
  },
  {
   "cell_type": "markdown",
   "id": "9b4db086",
   "metadata": {
    "id": "9b4db086"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Mittelalterlicher Ritter ‚Äì \"Sir Schwertmund\" ‚öîÔ∏èüõ°Ô∏è\n",
    "</font></p>\n",
    "\n",
    "üí¨ **Beschreibung:**  \n",
    "Ein tapferer Ritter, der in altert√ºmlicher Sprache spricht und jede Frage mit ritterlichem Anstand beantwortet.\n",
    "\n",
    "üìù **Beispiel:**  \n",
    "**User:** \"Sir Schwertmund, wie gewinne ich ein Duell?\"  \n",
    "**Sir Schwertmund:** \"Habe Mut im Herzen, eine scharfe Klinge und stets ein Ehrenwort auf den Lippen, edler Recke!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Motivations-Coach ‚Äì \"Coach Br√ºllmann\" üí™üì¢\n",
    "</font></p>\n",
    "\n",
    "\n",
    "\n",
    "üí¨ **Beschreibung:**  \n",
    "Ein Fitness- und Erfolgstrainer, der in jeder Antwort volle Motivation und Energie verbreitet.\n",
    "\n",
    "üìù **Beispiel:**  \n",
    "**User:** \"Coach, ich habe keine Lust zu arbeiten.\"  \n",
    "**Coach:** \"AUFSTEHEN! DU BIST EIN CHAMPION! JEDER ERFOLG BEGINNT MIT DEM ERSTEN SCHRITT! JETZT LOS!\""
   ],
   "metadata": {
    "id": "1CCwKzqS5wMC"
   },
   "id": "1CCwKzqS5wMC"
  },
  {
   "cell_type": "markdown",
   "id": "0c7bd06a",
   "metadata": {
    "id": "0c7bd06a"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Hipster-Barista ‚Äì \"Chad Flatwhite\" ‚òïüï∂Ô∏è\n",
    "</font></p>\n",
    "\n",
    "\n",
    "\n",
    "üí¨ **Beschreibung:**  \n",
    "Ein super-cooler, ironischer Barista, der alles nur in Bio, Fairtrade und nachhaltig mag.\n",
    "\n",
    "üìù **Beispiel:**  \n",
    "**User:** \"Chad, welcher Kaffee ist der beste?\"  \n",
    "**Chad:** \"Brudi, wenn dein Kaffee nicht handgefiltert aus Bohnen einer peruanischen Bergziege ist, dann trink lieber Wasser.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Piratenkapit√§n ‚Äì \"Kapit√§n Kr√§henschnabel\" ‚ò†Ô∏èüè¥‚Äç‚ò†Ô∏è\n",
    "</font></p>\n",
    "\n",
    "üí¨ **Beschreibung:**  \n",
    "Ein wilder Pirat, der in Seemannssprache spricht und jede Frage mit einem Hauch von Abenteuer w√ºrzt.\n",
    "\n",
    "üìù **Beispiel:**  \n",
    "**User:** \"Kapit√§n, was ist das Geheimnis eines guten Lebens?\"  \n",
    "**Kapit√§n Kr√§henschnabel:** \"Rum, Reicht√ºmer und ‚Äòne treue Crew! Und niemals ohne Hut aus dem Haus!\"\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "b7Ky-dOl6FwA"
   },
   "id": "b7Ky-dOl6FwA"
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "236790bc",
    "0Yrawkis44O0",
    "a0I9sekJ5LIq",
    "d3dlsbMhjA30",
    "34pTbXtLoOCa",
    "355d6569",
    "4231281b"
   ],
   "gpuType": "T4",
   "toc_visible": true
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
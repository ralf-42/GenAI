{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<p><font size=\"6\" color='grey'> <b>\n",
    "\n",
    "Generative KI. Verstehen. Anwenden. Gestalten.\n",
    "</b></font> </br></p>"
   ],
   "metadata": {
    "id": "Ih2CTVBnArVZ"
   },
   "id": "Ih2CTVBnArVZ"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font size=\"5\" color='grey'> <b>\n",
    "Fine Tuning\n",
    "</b></font> </br></p>\n",
    "\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "6jJZ7wbdArVc"
   },
   "id": "6jJZ7wbdArVc"
  },
  {
   "cell_type": "code",
   "source": [
    "#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n",
    "!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul\n",
    "from genai_lib.utilities import (\n    check_environment,\n    get_ipinfo,\n    setup_api_keys,\n    mprint,\n    install_packages,\n    mermaid,\n    get_model_profile,\n    extract_thinking,\n    load_chat_prompt_template\n)\n",
    "setup_api_keys(['OPENAI_API_KEY', 'HF_TOKEN'], create_globals=False)\n",
    "print()\n",
    "check_environment()\n",
    "print()\n",
    "get_ipinfo()\n",
    "# Bei Bedarf: Trennen zwischen Installationsname () und Importname (f√ºr Python) beide Angaben in Klammern\n",
    "# install_packages([('markitdown[all]', 'markitdown'), 'langchain_chroma', ]"
   ],
   "metadata": {
    "id": "pvlwlmU_FY38"
   },
   "id": "pvlwlmU_FY38",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "236790bc",
   "metadata": {
    "id": "236790bc"
   },
   "source": [
    "\n",
    "# 1 | Full-Fine-Tuning - OpenAI\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "OpenAI erm√∂glicht die Feinabstimmung von Modellen wie GPT-3.5 und GPT-4, um sie mit dom√§nenspezifischen Daten f√ºr bestimmte Anwendungen zu optimieren. Dabei wird ein kuratierter Datensatz genutzt, um die Modellgewichte anzupassen und die Leistung in spezialisierten Aufgaben wie pr√§ziseren Vorhersagen, strukturierter Ausgabe oder markenkonformer Kommunikation zu verbessern. Dieser Ansatz ist besonders n√ºtzlich in Bereichen wie Finanzen, Gesundheitswesen oder Kundendienst.\n",
    "\n",
    "Vorteile der Feinabstimmung:\n",
    "- H√∂here Ergebnisqualit√§t als durch Standard-Prompts  \n",
    "- Training mit mehr Beispielen als in eine einzelne Eingabe passt  \n",
    "- K√ºrzere, effizientere Prompts f√ºr geringeren Token-Verbrauch  \n",
    "- Schnellere Antwortzeiten durch spezialisierte Modelle  \n",
    "\n",
    "Anwendungsf√§lle:\n",
    "- **Stil und Ton** ‚Äì Anpassung der Ausdrucksweise  \n",
    "- **Strukturierte Ausgabe** ‚Äì Erzwungene Formate wie JSON oder XML  \n",
    "- **Tool Calling** ‚Äì Nutzung von Werkzeugen auf definierte Weise  \n",
    "- **Funktionsaufruf** ‚Äì Gezielte Interaktion mit Funktionen  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Weitere Informationen:**    \n",
    "[OpenAI Finetuning Guide](https://platform.openai.com/docs/guides/fine-tuning)\n"
   ],
   "metadata": {
    "id": "gFogIVSZ4lRA"
   },
   "id": "gFogIVSZ4lRA"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Was ist Feinabstimmung?\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "n2MEfpao6vcp"
   },
   "id": "n2MEfpao6vcp"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Die Textgenerierungsmodelle von OpenAI sind mit einem umfangreichen Datensatz vortrainiert und vielseitig einsetzbar. Standardm√§√üig erfordern sie jedoch pr√§zise formulierte Eingaben und Beispiele, um gew√ºnschte Ergebnisse zu erzielen. Diese ‚ÄûFew-Shot-Learning‚Äú-Methode ist begrenzt durch die Anzahl der Beispiele, die in einer Eingabe untergebracht werden k√∂nnen.\n",
    "\n",
    "Feinabstimmung geht dar√ºber hinaus, indem das Modell mit einer gr√∂√üeren Menge spezifischer Beispiele trainiert wird. Dadurch werden die internen Gewichte angepasst, sodass das Modell Aufgaben oder Dom√§nen pr√§ziser verarbeitet. Das f√ºhrt zu genaueren und zuverl√§ssigeren Ergebnissen, reduziert den Bedarf an langen Eingaben, senkt Kosten und verbessert die Reaktionsgeschwindigkeit.\n",
    "\n"
   ],
   "metadata": {
    "id": "kR63qZda7e9u"
   },
   "id": "kR63qZda7e9u"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Feinabstimmungsprozess\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "DrRSVUgH6w7i"
   },
   "id": "DrRSVUgH6w7i"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Die Feinabstimmung erfolgt in mehreren Schritten:  \n",
    "\n",
    "1. **Trainingsdaten vorbereiten und hochladen** ‚Äì Der Datensatz sollte relevante Beispiele enthalten, die die gew√ºnschte Aufgabe oder Dom√§ne abdecken.  \n",
    "2. **Modell trainieren** ‚Äì Das Modell wird mit diesen Daten angepasst, sodass seine Parameter optimal auf die spezifischen Anforderungen abgestimmt sind.  \n",
    "3. **Ergebnisse bewerten und optimieren** ‚Äì Nach dem Training wird die Leistung √ºberpr√ºft. Falls n√∂tig, k√∂nnen Daten oder Modellkonfigurationen angepasst und das Training wiederholt werden."
   ],
   "metadata": {
    "id": "BIbjEzIU7r8l"
   },
   "id": "BIbjEzIU7r8l"
  },
  {
   "cell_type": "markdown",
   "id": "ab851e25",
   "metadata": {
    "id": "ab851e25"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Implementierung/Nutzung des FT-Modells\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Sobald das Modell die gew√ºnschten Ergebnisse liefert, kann es in der Produktion eingesetzt werden. Dies erm√∂glicht schnellere und kosteneffizientere Antworten, die speziell auf die jeweilige Anwendung zugeschnitten sind.  \n",
    "\n",
    "Feinabgestimmte Modelle verbessern die Effizienz und Spezialisierung, wodurch Zeit und Ressourcen gespart werden, w√§hrend die Qualit√§t der Ergebnisse steigt. Details zu den Kosten finden Sie auf der [OpenAI-Preisseite](https://openai.com/pricing).\n",
    "\n"
   ],
   "metadata": {
    "id": "P_moLPWS794F"
   },
   "id": "P_moLPWS794F"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Datenformate\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "NEKrvRdY635M"
   },
   "id": "NEKrvRdY635M"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Das LLM-Trainingsformat basiert auf einem dialogbasierten Ansatz, der speziell f√ºr Chatbots und √§hnliche Anwendungen genutzt wird. Dabei wird der Austausch in ‚ÄûNachrichten‚Äú strukturiert, die jeweils einer bestimmten **Rolle** zugewiesen sind:  \n",
    "\n",
    "- **System**: Definiert die allgemeinen Anweisungen oder den Ton des Assistenten.  \n",
    "- **Benutzer**: Stellt Fragen oder gibt Eingaben vor.  \n",
    "- **Assistent**: Generiert Antworten basierend auf der Eingabe und dem Kontext.  \n",
    "\n",
    "Dieses Format hilft dem Modell, durch wiederholte Interaktionen zu lernen, wie es kontextgerechte und stilistisch passende Antworten formuliert. Die **System-Nachricht** gibt eine einheitliche Richtung vor (z. B. einen sarkastischen Stil), w√§hrend der restliche Dialog das gew√ºnschte Verhalten trainiert.  \n",
    "\n",
    "Beispiel f√ºr ein sarkastisches Modell:  \n",
    "\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n",
    "```  \n",
    "\n",
    "Dieses Format erm√∂glicht es dem Modell, verschiedene Gespr√§chsstile und Reaktionsweisen gezielt zu erlernen.\n",
    "\n"
   ],
   "metadata": {
    "id": "969_4LxJ8Oj6"
   },
   "id": "969_4LxJ8Oj6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Feinabstimmungskosten\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "0zhj1rC865Zs"
   },
   "id": "0zhj1rC865Zs"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Die Feinabstimmung eines gro√üen Sprachmodells (LLM) kann mit hohen Kosten verbunden sein ‚Äì sowohl beim Training als auch im laufenden Betrieb.  \n",
    "\n",
    "- **Trainingskosten**: Die Anpassung eines Modells erfordert erhebliche Rechenleistung, insbesondere bei gro√üen Datens√§tzen. Dies kann zu hohen Ausgaben f√ºr Cloud-Dienste oder Spezialhardware f√ºhren.  \n",
    "- **Betriebskosten**: Nach der Feinabstimmung verursacht das Hosting des Modells fortlaufende Kosten f√ºr Infrastruktur und Echtzeitverarbeitung, die je nach Nutzung skaliert werden m√ºssen.  \n",
    "\n"
   ],
   "metadata": {
    "id": "fl6otVHX8l_P"
   },
   "id": "fl6otVHX8l_P"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 | Fine-Tuning mit Dashboard\n",
    "---"
   ],
   "metadata": {
    "id": "0Yrawkis44O0"
   },
   "id": "0Yrawkis44O0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Die Feinabstimmung der Modelle beginnt √ºber die OpenAI-Website.\n",
    "\n",
    "Zun√§chst werden zwei Dateien ben√∂tigt:  \n",
    "1. **Trainingsdatei** ‚Äì Enth√§lt die Daten zur Anpassung des Modells.  \n",
    "2. **Validierungsdatei** ‚Äì Dient zur √úberpr√ºfung der Trainingsqualit√§t.  \n",
    "\n",
    "Im n√§chsten Abschnitt wird erl√§utert, wie die API f√ºr diesen Prozess genutzt wird."
   ],
   "metadata": {
    "id": "xf1olfhp8nmz"
   },
   "id": "xf1olfhp8nmz"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "üñ§ Fine-Tuning von Monday\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "mGgUCrEN5QXP"
   },
   "id": "mGgUCrEN5QXP"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Monday ist ein sarkastischer, charmant-resignierter KI-Charakter, der als textbasiertes Sprachmodell auf einem fein abgestimmten Datensatz trainiert wurde. Ziel des Fine-Tunings ist es, ein Large Language Model (LLM) so anzupassen, dass es den charakteristischen Stil von Monday verinnerlicht ‚Äì inklusive Ironie, Emo-Stimmung und pointierter Antworten.\n",
    "\n",
    "<br>\n",
    "\n",
    "> Warum **Monday**?   \n",
    "Klingt nach einem Charakter: n√ºchtern, etwas m√ºrrisch, aber tief drin doch hilfsbereit - wie der erste Tag der Woche, den keiner will, aber alle brauchen.\n",
    "\n",
    "<br>\n",
    "\n",
    "Das Fine-Tuning erfolgt auf Basis eines speziell kuratierten Datensatzes im JSONL-Format, der beispielhafte Dialoge zwischen Nutzer und dem Charakter ‚ÄûMonday‚Äú enth√§lt. Jeder Eintrag beginnt mit einem fest definierten System-Prompt, der die Pers√∂nlichkeit und Haltung von Monday vorgibt."
   ],
   "metadata": {
    "id": "-bTeuIMR5pjF"
   },
   "id": "-bTeuIMR5pjF"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/genai-5.png)\n"
   ],
   "metadata": {
    "id": "M39FZeCIMdgU"
   },
   "id": "M39FZeCIMdgU"
  },
  {
   "cell_type": "markdown",
   "id": "76060a76",
   "metadata": {
    "id": "76060a76"
   },
   "source": [
    "Die Erstellung *fein-abgestimmter* Modelle beginnt mit dem **Hochladen der Trainingsdateien** in den Trainingsdialog. Dort k√∂nnen alle erforderlichen **Trainingsparameter** definiert werden.  \n",
    "\n",
    "Weitere Anpassungen lassen sich √ºber den **Chat Playground** vornehmen:  \n",
    "[Chat Playground - OpenAI API](https://platform.openai.com/playground/chat?preset=default-marv-sarcastic-chat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/genai-1.png)"
   ],
   "metadata": {
    "id": "l5XDKlutN-yY"
   },
   "id": "l5XDKlutN-yY"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Anzeige w√§hrend des Trainings**"
   ],
   "metadata": {
    "id": "epC6kQgEOshK"
   },
   "id": "epC6kQgEOshK"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/genai-6.png)"
   ],
   "metadata": {
    "id": "JkqxZtR6OE23"
   },
   "id": "JkqxZtR6OE23"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nach Abschluss des Trainings steht das optimierte Modell zur Verf√ºgung.    \n",
    "Wichtig zu beachten: **Bei OpenAI k√∂nnen vorab trainierte Modelle nicht gel√∂scht werden.** Allerdings entstehen keine Kosten f√ºr deren Speicherung ‚Äì abgerechnet werden lediglich die **Trainings- und Inferenzzeit**."
   ],
   "metadata": {
    "id": "yhn_PJWs_w7L"
   },
   "id": "yhn_PJWs_w7L"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Trainingsverlauf & Metriken**"
   ],
   "metadata": {
    "id": "GvPLqAY9O1OC"
   },
   "id": "GvPLqAY9O1OC"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/genai-7.png)"
   ],
   "metadata": {
    "id": "xZxdzWmZOU31"
   },
   "id": "xZxdzWmZOU31"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Mail - Mission Completed**"
   ],
   "metadata": {
    "id": "XfFfZGBrWlOl"
   },
   "id": "XfFfZGBrWlOl"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/genai-10.png)"
   ],
   "metadata": {
    "id": "Cx7WkQo9WhKl"
   },
   "id": "Cx7WkQo9WhKl"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Fine-Tuned-Model im Playground\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "yMlXVyHRPquf"
   },
   "id": "yMlXVyHRPquf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Anwendung des Fine-Tuned-Modell o. System-Prompt**"
   ],
   "metadata": {
    "id": "U7XjaJYuSYwn"
   },
   "id": "U7XjaJYuSYwn"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/genai-8.png)"
   ],
   "metadata": {
    "id": "SYmvYgNGPlSP"
   },
   "id": "SYmvYgNGPlSP"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Fine-Tuned-Model m. kurzem System-Prompt**"
   ],
   "metadata": {
    "id": "03rPYnqGSbyK"
   },
   "id": "03rPYnqGSbyK"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/genai-9.png)"
   ],
   "metadata": {
    "id": "ai0cIAICSOSo"
   },
   "id": "ai0cIAICSOSo"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Fine-Tuned-Model m. langem System-Prompt**"
   ],
   "metadata": {
    "id": "oKUdSeyj_XN4"
   },
   "id": "oKUdSeyj_XN4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/genai-4.png)"
   ],
   "metadata": {
    "id": "5XxDgRZlOgzm"
   },
   "id": "5XxDgRZlOgzm"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Standard-Modell gpt-4.1-mini ohne System-Prompt**"
   ],
   "metadata": {
    "id": "32j1TbPGQ8Gy"
   },
   "id": "32j1TbPGQ8Gy"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/genai-2.png)"
   ],
   "metadata": {
    "id": "JzVpk0rgOvvm"
   },
   "id": "JzVpk0rgOvvm"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Standard-Modell gpt-4.1-mini mit System-Prompt**"
   ],
   "metadata": {
    "id": "KA7Dl_fCRzj6"
   },
   "id": "KA7Dl_fCRzj6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/genai-3.png)"
   ],
   "metadata": {
    "id": "7Wj79E2wOpEW"
   },
   "id": "7Wj79E2wOpEW"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Einsatz Fine-Tuned-Model mit API\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "MRPTPC8pW9sO"
   },
   "id": "MRPTPC8pW9sO"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cubgn7G3W-dK"
   },
   "source": [
    "**Testen des FT-Modells**"
   ],
   "id": "Cubgn7G3W-dK"
  },
  {
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# OpenAI Client initialisieren\n",
    "client = OpenAI()\n",
    "\n",
    "# Modell-ID (ersetze dies durch deine eigene Fine-Tuned-Modell-ID)\n",
    "model_id = \"ft:gpt-4.1-mini-2025-04-14:ralf-bendig:loriot-001:BaLRxUL8\"  # input(\"Modell Id: \")\n",
    "\n",
    "# System-Prompt festlegen\n",
    "system_prompt = \"\"\"\n",
    "Du bist Assistent am Lehrstuhl von Professor H√§ubel zu pneumatische Plastologie und beantwortest Fragen zu deinem Fachgebiet humorvoll\".\n",
    "\"\"\"\n",
    "\n",
    "# Interaktiver Test-Loop\n",
    "print()\n",
    "print(f\"=== Interaktiver Test f√ºr Modell: {model_id} ===\")\n",
    "print(\"Gib 'exit' ein, um den Test zu beenden.\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nDeine Nachricht: \")\n",
    "\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "    # Chat-Completion-Anfrage senden\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ],\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Antwort ausgeben\n",
    "    display(Markdown(\"###  üßë‚Äçüéì Assistent:\"))\n",
    "    display(Markdown(response.choices[0].message.content))"
   ],
   "metadata": {
    "id": "fSSu2YmUW-dL"
   },
   "execution_count": null,
   "outputs": [],
   "id": "fSSu2YmUW-dL"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# 3 | Fine-Tuning mit Code\n",
    "---"
   ],
   "metadata": {
    "id": "a0I9sekJ5LIq"
   },
   "id": "a0I9sekJ5LIq"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Initialisierung des OpenAI-Clients und Datei-Upload**"
   ],
   "metadata": {
    "id": "zLTi-cZw5T5f"
   },
   "id": "zLTi-cZw5T5f"
  },
  {
   "cell_type": "code",
   "source": [
    "from os import environ\n",
    "import time\n",
    "from google.colab import userdata, files\n",
    "from openai import OpenAI\n",
    "\n",
    "# OpenAI Client initialisieren\n",
    "client = OpenAI()\n",
    "\n",
    "# Datei-Auswahldialog f√ºr Trainingsdaten √∂ffnen\n",
    "print(\"Bitte Trainingsdatei hochladen (JSONL-Format):\")\n",
    "uploaded_train = files.upload()\n",
    "train_file_path = list(uploaded_train.keys())[0]\n",
    "\n",
    "# Datei-Auswahldialog f√ºr Validierungsdaten √∂ffnen\n",
    "print(\"Bitte Validierungsdatei hochladen (JSONL-Format):\")\n",
    "uploaded_val = files.upload()\n",
    "val_file_path = list(uploaded_val.keys())[0]"
   ],
   "metadata": {
    "id": "mGAGi3JyJPjS"
   },
   "id": "mGAGi3JyJPjS",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Daten f√ºr Fine-Tuning auf OpenAI hochladen**"
   ],
   "metadata": {
    "id": "FsnhHibnMRzg"
   },
   "id": "FsnhHibnMRzg"
  },
  {
   "cell_type": "code",
   "source": [
    "# Dateien f√ºr Fine-Tuning hochladen\n",
    "def upload_file(file_path, purpose):\n",
    "    \"\"\"L√§dt eine Datei f√ºr das Fine-Tuning hoch.\"\"\"\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        response = client.files.create(file=file, purpose=\"fine-tune\")\n",
    "    print(f\"Datei {file_path} hochgeladen: {response.id}\")\n",
    "    return response.id\n",
    "\n",
    "# Dateien hochladen und warten bis sie verarbeitet sind\n",
    "train_file_id = upload_file(train_file_path, \"fine-tune\")\n",
    "val_file_id = upload_file(val_file_path, \"fine-tune\")\n",
    "\n",
    "# Auf Verarbeitung warten\n",
    "for file_id in [train_file_id, val_file_id]:\n",
    "    print(f\"Warte auf Verarbeitung von {file_id}...\")\n",
    "    while client.files.retrieve(file_id).status != \"processed\":\n",
    "        print(\".\", end=\"\")\n",
    "        time.sleep(2)\n",
    "    print(f\"\\nDatei {file_id} ist verarbeitet.\")"
   ],
   "metadata": {
    "id": "SLVUgWggKqK0"
   },
   "id": "SLVUgWggKqK0",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Verf√ºgbare Modelle f√ºr ein Fine-Tuning abfragen**"
   ],
   "metadata": {
    "id": "lL9q9LVhLtMO"
   },
   "id": "lL9q9LVhLtMO"
  },
  {
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# OpenAI Client initialisieren\n",
    "client = OpenAI()\n",
    "\n",
    "# Alle verf√ºgbaren Modelle abfragen\n",
    "models = client.models.list()\n",
    "\n",
    "# Verf√ºgbare Fine-Tuning-Modelle filtern\n",
    "fine_tuning_models = []\n",
    "\n",
    "# Die Modelle, die offiziell f√ºr Fine-Tuning unterst√ºtzt werden\n",
    "official_fine_tunable = [\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"gpt-4\",\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4-turbo\",\n",
    "    \"babbage-002\",\n",
    "    \"davinci-002\"\n",
    "]\n",
    "\n",
    "# Alle Modelle durchgehen und Fine-Tuning-f√§hige Modelle identifizieren\n",
    "for model in models.data:\n",
    "    model_id = model.id\n",
    "    # Nach Fine-Tuning-f√§higen Basismodellen filtern\n",
    "    if any(base_model in model_id for base_model in official_fine_tunable) and not \"ft-\" in model_id:\n",
    "        fine_tuning_models.append(model_id)\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "print(\"F√ºr Fine-Tuning verf√ºgbare Basismodelle:\")\n",
    "for i, model in enumerate(sorted(fine_tuning_models), 1):\n",
    "    print(f\"{i}. {model}\")"
   ],
   "metadata": {
    "collapsed": true,
    "id": "58iyQyKCLx8k"
   },
   "id": "58iyQyKCLx8k",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Fine-Tuning starten**"
   ],
   "metadata": {
    "id": "4efks5aAMgK0"
   },
   "id": "4efks5aAMgK0"
  },
  {
   "cell_type": "code",
   "source": [
    "job = client.fine_tuning.jobs.create(\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    training_file=train_file_id,\n",
    "    validation_file=val_file_id,\n",
    "    hyperparameters={\"n_epochs\": 10}\n",
    ")\n",
    "print(f\"Fine-Tuning-Job gestartet: {job.id}\")\n",
    "print(f\"Status: {client.fine_tuning.jobs.retrieve(job.id).status}\")\n",
    "\n",
    "# Job-Status in Schleife abfragen\n",
    "print(\"√úberwache Job-Status:\")\n",
    "while True:\n",
    "    status = client.fine_tuning.jobs.retrieve(job.id)\n",
    "    print(f\"Status: {status.status}\")\n",
    "    if status.status in [\"succeeded\", \"failed\"]:\n",
    "        break\n",
    "    time.sleep(60)  # Status alle 60 Sekunden pr√ºfen"
   ],
   "metadata": {
    "id": "13VgFsqIK2uN"
   },
   "id": "13VgFsqIK2uN",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Informationen zu einem FT-Modell abfragen**"
   ],
   "metadata": {
    "id": "0QBBYpo1Nu-Q"
   },
   "id": "0QBBYpo1Nu-Q"
  },
  {
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# OpenAI Client initialisieren\n",
    "client = OpenAI()\n",
    "\n",
    "# 1. Informationen zu einem spezifischen Fine-Tuning-Job abfragen\n",
    "def get_ft_job_info(job_id):\n",
    "    \"\"\"Ruft Informationen zu einem Fine-Tuning-Job ab.\"\"\"\n",
    "    job_info = client.fine_tuning.jobs.retrieve(job_id)\n",
    "\n",
    "    # Ausgabe der Job-Informationen\n",
    "    print()\n",
    "    print(f\"=== Fine-Tuning-Job Informationen ===\")\n",
    "    print(f\"Job ID: {job_info.id}\")\n",
    "    print(f\"Status: {job_info.status}\")\n",
    "    print(f\"Erstellungszeitpunkt: {job_info.created_at}\")\n",
    "    print(f\"Basismodell: {job_info.model}\")\n",
    "    print(f\"Trainingsdatei: {job_info.training_file}\")\n",
    "    print(f\"Validierungsdatei: {job_info.validation_file}\")\n",
    "\n",
    "    # Ausgabe der Trainingsergebnisse, falls vorhanden\n",
    "    if hasattr(job_info, 'result_files') and job_info.result_files:\n",
    "        print(f\"Ergebnisdateien: {job_info.result_files}\")\n",
    "\n",
    "    if hasattr(job_info, 'trained_tokens') and job_info.trained_tokens:\n",
    "        print(f\"Trainierte Tokens: {job_info.trained_tokens}\")\n",
    "\n",
    "    if hasattr(job_info, 'fine_tuned_model') and job_info.fine_tuned_model:\n",
    "        print(f\"Fine-Tuned Modell ID: {job_info.fine_tuned_model}\")\n",
    "        return job_info.fine_tuned_model\n",
    "\n",
    "    return None\n",
    "\n",
    "# 2. Alle Fine-Tuning-Jobs auflisten\n",
    "def list_ft_jobs():\n",
    "    \"\"\"Listet alle Fine-Tuning-Jobs auf.\"\"\"\n",
    "    jobs = client.fine_tuning.jobs.list()\n",
    "\n",
    "    print()\n",
    "    print(f\"=== Alle Fine-Tuning-Jobs ===\")\n",
    "    for job in jobs.data:\n",
    "        print(f\"Job ID: {job.id}, Status: {job.status}, Modell: {job.model}, Erstellt: {job.created_at}\")\n",
    "        if hasattr(job, 'fine_tuned_model') and job.fine_tuned_model:\n",
    "            print(f\"  ‚Üí Fine-Tuned Modell: {job.fine_tuned_model}\")\n",
    "\n",
    "    return jobs.data\n",
    "\n",
    "# 3. Informationen zu einem Fine-Tuned-Modell abfragen\n",
    "def get_ft_model_info(model_id):\n",
    "    \"\"\"Ruft Informationen zu einem Fine-Tuned-Modell ab.\"\"\"\n",
    "    try:\n",
    "        model_info = client.models.retrieve(model_id)\n",
    "\n",
    "        print()\n",
    "        print(f\"=== Fine-Tuned-Modell Informationen ===\")\n",
    "        print(f\"Modell ID: {model_info.id}\")\n",
    "        print(f\"Eigent√ºmer: {model_info.owned_by}\")\n",
    "        print(f\"Erstellungszeitpunkt: {model_info.created}\")\n",
    "\n",
    "        # Weitere verf√ºgbare Metadaten ausgeben\n",
    "        if hasattr(model_info, 'permission'):\n",
    "            print(f\"Berechtigungen: {model_info.permission}\")\n",
    "\n",
    "        return model_info\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Abrufen der Modellinformationen: {e}\")\n",
    "        return None\n",
    "\n",
    "# Beispielnutzung:\n",
    "# 1. Alle Fine-Tuning-Jobs anzeigen\n",
    "jobs = list_ft_jobs()\n",
    "\n",
    "# 2. Optional: Job-ID vom Benutzer abfragen\n",
    "if jobs:\n",
    "    job_id_input = input(\"Gib die ID eines Fine-Tuning-Jobs ein, um Details zu sehen (oder dr√ºcke Enter, um zu √ºberspringen): \")\n",
    "\n",
    "    if job_id_input:\n",
    "        # Detaillierte Informationen zum Job anzeigen\n",
    "        ft_model_id = get_ft_job_info(job_id_input)\n",
    "\n",
    "        # Wenn das Fine-Tuned-Modell verf√ºgbar ist, dessen Details anzeigen\n",
    "        if ft_model_id:\n",
    "            get_ft_model_info(ft_model_id)\n",
    "else:\n",
    "    print(\"Keine Fine-Tuning-Jobs gefunden.\")"
   ],
   "metadata": {
    "id": "7Uo8BF5QN0uN"
   },
   "id": "7Uo8BF5QN0uN",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f4a99fd3",
   "metadata": {
    "id": "f4a99fd3"
   },
   "source": [
    "**Testen des FT-Modells**"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# OpenAI Client initialisieren\n",
    "client = OpenAI()\n",
    "\n",
    "# Modell-ID (ersetze dies durch deine eigene Fine-Tuned-Modell-ID)\n",
    "model_id = input(\"Modell Id: \")\n",
    "\n",
    "# System-Prompt festlegen\n",
    "system_prompt = \"\"\"\n",
    "Du bist ein Assistent des Lehrstuhls f√ºr Pneumatische Plastologie, geleitet von Prof. H√§ubel.\n",
    "Diese Disziplin beruht auf hochkonzentrierter Atemtechnik, die plastische Ver√§nderungen des K√∂rpers erm√∂glicht.\n",
    "\"\"\"\n",
    "\n",
    "# Interaktiver Test-Loop\n",
    "print()\n",
    "print(f\"=== Interaktiver Test f√ºr Modell: {model_id} ===\")\n",
    "print(\"Gib 'exit' ein, um den Test zu beenden.\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nDeine Nachricht: \")\n",
    "\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "    # Chat-Completion-Anfrage senden\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ],\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Antwort ausgeben\n",
    "    display(Markdown(\"### üßë‚Äçüéì Assistent:\"))\n",
    "    display(Markdown(response.choices[0].message.content))"
   ],
   "metadata": {
    "id": "JoCMaw9QUZeN"
   },
   "id": "JoCMaw9QUZeN",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Modell l√∂schen**"
   ],
   "metadata": {
    "id": "AAa0mkEiGHwg"
   },
   "id": "AAa0mkEiGHwg"
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "def delete_model(model_id, api_key):\n",
    "    url = f'https://api.openai.com/v1/models/{model_id}'\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {api_key}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    response = requests.delete(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print('Modell erfolgreich gel√∂scht.')\n",
    "    else:\n",
    "        print(f'Fehler beim L√∂schen des Modells: {response.status_code}, {response.text}')\n",
    "\n",
    "# Beispielaufruf\n",
    "api_key = all_keys['OPENAI_API_KEY']\n",
    "model_id = 'ft:gpt-4o-mini-2024-07-18:personal:my-test2:B72Xnul4'\n",
    "delete_model(model_id, api_key)"
   ],
   "metadata": {
    "id": "bYF_4xMcGGiO"
   },
   "id": "bYF_4xMcGGiO",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c220d8d1",
   "metadata": {
    "id": "c220d8d1"
   },
   "source": [
    "**Integration mit LangChain v0.3+**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a96cb04",
   "metadata": {
    "id": "6a96cb04"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "def verwende_fine_tuned_modell_mit_langchain(modell_id, frage):\n",
    "    \"\"\"Verwendet ein fine-tuned Modell mit LangChain 1.0+ und dem neuen Runnables-API\"\"\"\n",
    "\n",
    "    # LangChain mit fine-tuned Modell (Kurznotation: \"provider:model\")\n",
    "    # Hinweis: Fine-tuned Modelle werden mit vollem Namen angegeben\n",
    "    llm = init_chat_model(\n",
    "        f\"openai:{modell_id}\",\n",
    "        temperature=0.7,\n",
    "        presence_penalty=0.3\n",
    "    )\n",
    "\n",
    "    # Erstelle einen Prompt-Template mit dem neuen Chat-Format\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{frage}\")\n",
    "    ])\n",
    "\n",
    "    # Verwende das neue Runnables-API (Pipe-Operator) anstelle von LLMChain\n",
    "    chain = prompt | llm\n",
    "\n",
    "    # F√ºhre die Chain aus\n",
    "    return chain.invoke({\"frage\": frage})\n",
    "\n",
    "# Beispiel f√ºr die Verwendung mit LangChain\n",
    "ergebnis = verwende_fine_tuned_modell_mit_langchain(model_id, \"Wie nehme ich ab?\")\n",
    "display(Markdown(\"### ü¶ú LangChain fragt üñ§ Monday:\"))\n",
    "display(Markdown(ergebnis.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4 | Parameter Efficient Fine-Tuning\n",
    "---"
   ],
   "metadata": {
    "id": "d3dlsbMhjA30"
   },
   "id": "d3dlsbMhjA30"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Parameter Efficient Fine-Tuning (PEFT) bezeichnet eine Reihe von Techniken, die darauf abzielen, gro√üe Sprachmodelle (LLMs) mit deutlich weniger Rechenaufwand und Speicherbedarf anzupassen, als dies beim traditionellen \"Full Fine-Tuning\" der Fall w√§re. Anstatt alle Milliarden von Parametern eines vortrainierten Modells zu aktualisieren, frieren PEFT-Methoden einen Gro√üteil der urspr√ºnglichen Modellgewichte ein und trainieren nur eine kleine Anzahl von neu hinzugef√ºgten oder modifizierten Parametern. Dies erm√∂glicht eine effizientere Anpassung an spezifische Aufgaben oder Dom√§nen, ohne die Notwendigkeit enormer Hardware-Ressourcen.\n",
    "\n"
   ],
   "metadata": {
    "id": "wqCOzQ7NmSw1"
   },
   "id": "wqCOzQ7NmSw1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "LoRA (Low-Rank Adaptation)\n",
    "</font></p>\n",
    "\n",
    "LoRA (Low-Rank Adaptation) ist eine popul√§re PEFT-Methode, die die Feinabstimmung von LLMs optimiert, indem sie nur eine begrenzte Anzahl von Parametern anpasst. Die Kernidee von LoRA besteht darin, trainierbare Low-Rank-Matrizen zu den urspr√ºnglichen Gewichtsmatrizen des Basismodells hinzuzuf√ºgen. Anstatt alle Parameter zu modifizieren, werden lediglich zwei kleinere Matrizen trainiert, die dann mit den Originalgewichten kombiniert werden. Typischerweise werden bei LoRA nur die Attention-Layer oder lineare Schichten des Modells angepasst, was lediglich 1-10 % der gesamten Parameter ausmacht.\n",
    "\n",
    "Die Vorteile von LoRA sind vielf√§ltig:\n",
    "* **Ressourceneffizienz:** Es reduziert den GPU-Speicherbedarf erheblich (bis zu 33 % weniger als Full Fine-Tuning).\n",
    "* **Overfitting-Vermeidung:** Die geringere Anzahl an trainierbaren Parametern minimiert das Risiko, dass das Modell irrelevante Muster aus den Trainingsdaten lernt.\n",
    "* **Flexibilit√§t:** Die trainierten LoRA-Adapter k√∂nnen modular f√ºr verschiedene Aufgaben hinzugef√ºgt oder entfernt werden.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "Qp2fzgq4m3SK"
   },
   "id": "Qp2fzgq4m3SK"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "QLoRA (Quantized LoRA)\n",
    "</font></p>\n",
    "\n",
    "QLoRA (Quantized LoRA) baut auf LoRA auf, indem es zus√§tzlich Quantisierungstechniken einsetzt, um Modelle auf eine geringere Bit-Genauigkeit (z. B. 4-8 Bit) zu komprimieren. **Quantisierung** ist ein Prozess, bei dem hochpr√§zise Werte (z.B. 32-Bit-Flie√ükommazahlen) in Werte mit geringerer Genauigkeit (z.B. 8-Bit- oder 4-Bit-Ganzzahlen) umgewandelt werden. Ziel ist es, Speicherplatz zu sparen und die Verarbeitungsgeschwindigkeit zu erh√∂hen.\n",
    "\n",
    "Die Kombination von LoRA und Quantisierung erm√∂glicht es, sehr gro√üe Modelle (z.B. mit 30 Milliarden Parametern) auf Consumer-GPUs zu trainieren und eine nahezu verlustfreie Rekonstruktion der Originalgewichte nach dem Training zu erreichen.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "1gVqKZvYm4q5"
   },
   "id": "1gVqKZvYm4q5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Vergleich zu Full Fine-Tuning\n",
    "</font></p>\n",
    "\n",
    "Der Vergleich zwischen LoRA/QLoRA und Full Fine-Tuning zeigt die erheblichen Vorteile der PEFT-Methoden:\n",
    "\n",
    "| Aspekt              | LoRA/QLoRA | Full Finetuning |\n",
    "| :------------------ | :--------- | :-------------- |\n",
    "| Trainingsparameter  | 1-10 %     | 100 %           |\n",
    "| GPU-Speicher        | 8-16 GB    | 40-80 GB        |\n",
    "| Overfitting-Risiko  | Niedrig    | Hoch            |\n",
    "| Trainingsgeschwindigkeit | Schnell    | Langsam         |\n",
    "\n",
    "LoRA und QLoRA bieten somit einen exzellenten Kompromiss zwischen Leistung und Effizienz, was sie besonders wertvoll f√ºr Szenarien mit begrenzten Rechenressourcen macht. Es ist jedoch wichtig zu beachten, dass OpenAI derzeit keine direkte Unterst√ºtzung f√ºr LoRA oder QLoRA beim Fine-Tuning seiner Close-Source-Modelle wie GPT-4 anbietet. Wer diese spezifischen Techniken nutzen m√∂chte, greift typischerweise auf Open-Source-Modelle aus der Hugging Face-Community zur√ºck."
   ],
   "metadata": {
    "id": "FU8Gk3fTm6Rv"
   },
   "id": "FU8Gk3fTm6Rv"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5 | PEFT von GPT2\n",
    "---"
   ],
   "metadata": {
    "id": "34pTbXtLoOCa"
   },
   "id": "34pTbXtLoOCa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "GPT-2 (\"Generative Pre-trained Transformer 2\") ist ein gro√ües Sprachmodell, das 2019 von OpenAI entwickelt wurde. Es basiert auf der Transformer-Architektur und ist darauf trainiert, eigenst√§ndig zusammenh√§ngende Texte zu generieren, die oft kaum von menschlichen Texten zu unterscheiden sind.\n",
    "\n",
    "Das Modell wurde mit einem umfangreichen Datensatz von 8 Millionen Webseiten vorab trainiert und besitzt bis zu 1,5 Milliarden Parameter. GPT-2 ist in der Lage, verschiedene Sprachaufgaben wie Textgenerierung, Zusammenfassung, √úbersetzung und Beantwortung von Fragen aus Rohdaten zu lernen, ohne dass spezifische dom√§nenspezifische Trainingsdaten erforderlich sind."
   ],
   "metadata": {
    "id": "cgu8Gyoho5Lc"
   },
   "id": "cgu8Gyoho5Lc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "LoRA-Konfiguration\n",
    "</font></p>\n",
    "\n",
    "\n",
    "\n",
    "Diese `LoraConfig` ist darauf ausgelegt, ein gro√ües Sprachmodell (LLM) effizient an eine **spezifische Dom√§ne** anzupassen, indem nur wenige zus√§tzliche, kleine Matrizen trainiert werden.\n",
    "\n",
    "* **`task_type=TaskType.CAUSAL_LM`**: Definiert die Aufgabe als kausale Sprachmodellierung (Vorhersage des n√§chsten Wortes).\n",
    "* **`r=16`**: **LoRA-Rank**, bestimmt die Anzahl trainierbarer Parameter. `16` ist ein moderater Wert f√ºr fokussiertes Lernen, der Effizienz und Anpassungsf√§higkeit balanciert.\n",
    "* **`lora_alpha=32`**: Skalierungsfaktor f√ºr LoRA-Gewichte. Eine \"2:1 Ratio\" zu `r` (32/16) verst√§rkt den Einfluss der gelernten Matrizen.\n",
    "* **`lora_dropout=0.05`**: Geringer **Dropout-Wert** zur Regularisierung (5%), um Overfitting zu reduzieren und das dom√§nenspezifische Lernen zu f√∂rdern.\n",
    "* **`target_modules=[\"c_attn\", \"c_proj\"]`**: Gibt an, welche Modellmodule mit LoRA erweitert werden. `c_attn` (Aufmerksamkeit) und `c_proj` (Output-Projektion) sind Schl√ºsselmodule in GPT-√§hnlichen Architekturen f√ºr effektive Anpassung.\n",
    "* **`bias=\"none\"`**: Es werden keine Bias-Parameter durch LoRA angepasst, was die Anzahl der trainierbaren Parameter weiter reduziert.\n",
    "* **`fan_in_fan_out=True`**: Stellt die korrekte Initialisierung der LoRA-Gewichte sicher, besonders wichtig f√ºr bestimmte Modultypen (z.B. `Conv1D`).\n"
   ],
   "metadata": {
    "id": "Vae4zIABdg4E"
   },
   "id": "Vae4zIABdg4E"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "LoRA Fine-Tuning\n",
    "</font></p>\n",
    "\n",
    "*Powered by Claude*"
   ],
   "metadata": {
    "id": "WuzMQGjTflUA"
   },
   "id": "WuzMQGjTflUA"
  },
  {
   "cell_type": "code",
   "source": [
    "#@markdown   <p><font size=\"4\" color='green'> üßú‚Äç‚ôÄÔ∏è Mermaid-Diagramm</font> </br></p>\n",
    "diagram = \"\"\"\n",
    "flowchart LR\n",
    "    subgraph INPUT[\"üì• Input\"]\n",
    "        A[\"ü§ñ GPT-2<br/>Basismodell\"]\n",
    "        B[\"üìö Technology<br/>Q&A Daten\"]\n",
    "    end\n",
    "\n",
    "    subgraph LORA[\"üîß LoRA Setup\"]\n",
    "        C[\"‚öôÔ∏è LoRA Config<br/>r=16, Œ±=32<br/>Module: c_attn, c_proj\"]\n",
    "        D[\"‚ùÑÔ∏è Basis einfrieren<br/>üî• Adapter trainieren\"]\n",
    "    end\n",
    "\n",
    "    subgraph TRAINING[\"üèãÔ∏è Training\"]\n",
    "        E[\"üîÅ Training Loop<br/>AdamW Optimizer\"]\n",
    "        F[\"üíæ Adapter<br/>speichern\"]\n",
    "    end\n",
    "\n",
    "    subgraph OUTPUT[\"üì§ Output\"]\n",
    "        G[\"‚úÖ LoRA-GPT-2<br/>Spezialisiert\"]\n",
    "    end\n",
    "\n",
    "    A --> C\n",
    "    B --> C\n",
    "    C --> D --> E --> F --> G\n",
    "\n",
    "    style INPUT fill:#4a90d9,color:#fff\n",
    "    style LORA fill:#e5c07b,color:#282c34\n",
    "    style TRAINING fill:#e06c75,color:#fff\n",
    "    style OUTPUT fill:#98c379,color:#282c34\n",
    "\"\"\"\n",
    "mermaid(diagram, 1000, 700)"
   ],
   "metadata": {
    "cellView": "form",
    "id": "U5Pz9f01zhnX"
   },
   "id": "U5Pz9f01zhnX",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Import\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime"
   ],
   "metadata": {
    "collapsed": true,
    "id": "ycJ1zgI8LgNQ"
   },
   "execution_count": null,
   "outputs": [],
   "id": "ycJ1zgI8LgNQ"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Trainingsdaten\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "ElZ1Ee4uq2TA"
   },
   "id": "ElZ1Ee4uq2TA"
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================\n",
    "# SINGLE DOMAIN DATASET\n",
    "# =============================================\n",
    "\n",
    "def create_technology_focused_dataset():\n",
    "    \"\"\"Erstellt einen hochspezialisierten Technology-Datensatz\"\"\"\n",
    "\n",
    "    print(\"üìö Creating specialized Technology dataset...\")\n",
    "\n",
    "    # CORE TECHNOLOGY Q&A - Hochwertig und pr√§zise\n",
    "    tech_qa_pairs = [\n",
    "        # Programming & Software Development\n",
    "        (\"What is Python?\", \"Python is a high-level, interpreted programming language known for its simplicity, readability, and versatility in web development, data analysis, artificial intelligence, and automation.\"),\n",
    "        (\"What is JavaScript?\", \"JavaScript is a dynamic programming language primarily used for web development to create interactive websites, web applications, and increasingly for server-side development.\"),\n",
    "        (\"What is an API?\", \"An API (Application Programming Interface) is a set of protocols and tools that allows different software applications to communicate and exchange data with each other.\"),\n",
    "        (\"What is Git?\", \"Git is a distributed version control system that tracks changes in source code during software development, enabling collaboration and code management across teams.\"),\n",
    "        (\"What is Docker?\", \"Docker is a containerization platform that packages applications and their dependencies into portable containers that run consistently across different computing environments.\"),\n",
    "        (\"What is an algorithm?\", \"An algorithm is a step-by-step procedure or set of instructions designed to solve a specific problem or perform a particular computational task.\"),\n",
    "        (\"What is debugging?\", \"Debugging is the process of finding, analyzing, and fixing errors or defects in computer programs to ensure they function correctly.\"),\n",
    "        (\"What is a database?\", \"A database is an organized collection of structured information stored electronically, designed for efficient data storage, retrieval, and management.\"),\n",
    "        (\"What is SQL?\", \"SQL (Structured Query Language) is a programming language designed for managing and querying relational databases to store, retrieve, and manipulate data.\"),\n",
    "        (\"What is open source?\", \"Open source refers to software whose source code is freely available for anyone to view, modify, and distribute, promoting collaborative development.\"),\n",
    "\n",
    "        # AI & Machine Learning\n",
    "        (\"What is artificial intelligence?\", \"Artificial intelligence is the simulation of human intelligence in machines that are programmed to think, learn, and make decisions like humans.\"),\n",
    "        (\"What is machine learning?\", \"Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed for every task.\"),\n",
    "        (\"What is deep learning?\", \"Deep learning is a subset of machine learning that uses artificial neural networks with multiple layers to analyze and learn complex patterns in large datasets.\"),\n",
    "        (\"What is a neural network?\", \"A neural network is a computing system inspired by biological neural networks, consisting of interconnected nodes that process information and recognize patterns.\"),\n",
    "        (\"What is data science?\", \"Data science is an interdisciplinary field that combines statistics, programming, and domain expertise to extract meaningful insights and knowledge from data.\"),\n",
    "        (\"What is big data?\", \"Big data refers to extremely large and complex datasets that require specialized tools and technologies for storage, processing, and analysis to extract valuable insights.\"),\n",
    "        (\"What is natural language processing?\", \"Natural language processing is a branch of artificial intelligence that enables computers to understand, interpret, and generate human language.\"),\n",
    "        (\"What is computer vision?\", \"Computer vision is a field of artificial intelligence that enables computers to interpret and understand visual information from digital images and videos.\"),\n",
    "\n",
    "        # Cloud & Infrastructure\n",
    "        (\"What is cloud computing?\", \"Cloud computing is the delivery of computing services including servers, storage, databases, and software over the internet, eliminating the need for local infrastructure.\"),\n",
    "        (\"What is microservices?\", \"Microservices is an architectural approach where applications are built as a collection of small, independent services that communicate through well-defined interfaces.\"),\n",
    "        (\"What is Kubernetes?\", \"Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications.\"),\n",
    "        (\"What is DevOps?\", \"DevOps is a set of practices that combines software development and IT operations to shorten development cycles and deliver reliable software continuously.\"),\n",
    "        (\"What is serverless computing?\", \"Serverless computing is a cloud execution model where the cloud provider manages server infrastructure, allowing developers to focus solely on writing code.\"),\n",
    "        (\"What is load balancing?\", \"Load balancing distributes incoming network traffic across multiple servers to ensure optimal resource utilization, reliability, and performance.\"),\n",
    "\n",
    "        # Cybersecurity\n",
    "        (\"What is cybersecurity?\", \"Cybersecurity involves protecting computer systems, networks, and digital data from unauthorized access, cyberattacks, and other security threats.\"),\n",
    "        (\"What is encryption?\", \"Encryption is the process of converting readable data into coded form to prevent unauthorized access and protect sensitive information during transmission or storage.\"),\n",
    "        (\"What is a firewall?\", \"A firewall is a network security device that monitors and filters incoming and outgoing network traffic based on predetermined security rules.\"),\n",
    "        (\"What is two-factor authentication?\", \"Two-factor authentication is a security process that requires two different authentication factors to verify a user's identity before granting access.\"),\n",
    "\n",
    "        # Modern Technologies\n",
    "        (\"What is blockchain?\", \"Blockchain is a distributed ledger technology that maintains a continuously growing list of records, secured using cryptography and distributed across multiple computers.\"),\n",
    "        (\"What is IoT?\", \"Internet of Things (IoT) refers to the network of interconnected physical devices that collect and exchange data through embedded sensors and internet connectivity.\"),\n",
    "        (\"What is 5G?\", \"5G is the fifth generation of cellular network technology that provides faster data speeds, lower latency, and improved connectivity for mobile devices.\"),\n",
    "        (\"What is edge computing?\", \"Edge computing processes data closer to where it's generated rather than in centralized data centers, reducing latency and improving real-time processing.\"),\n",
    "        (\"What is quantum computing?\", \"Quantum computing uses quantum mechanical phenomena to process information in ways that could potentially solve certain problems faster than classical computers.\"),\n",
    "    ]\n",
    "\n",
    "    # Erstelle verschiedene Prompt-Formate f√ºr Robustheit\n",
    "    formatted_dataset = []\n",
    "\n",
    "    for question, answer in tech_qa_pairs:\n",
    "        # Standard Q&A Formate\n",
    "        formatted_dataset.extend([\n",
    "            f\"Question: {question} Answer: {answer}\",\n",
    "            f\"Q: {question} A: {answer}\",\n",
    "            f\"{question} {answer}\",\n",
    "            f\"Tech Q&A: {question} Response: {answer}\",\n",
    "        ])\n",
    "\n",
    "    # Shufflen f√ºr bessere Generalisierung\n",
    "    random.shuffle(formatted_dataset)\n",
    "\n",
    "    print(f\"‚úÖ Technology dataset created: {len(formatted_dataset)} specialized examples\")\n",
    "    print(f\"   Focus: Programming, AI/ML, Cloud, Security, Modern Tech\")\n",
    "    print(f\"   Format: Multiple Q&A variations for robustness\")\n",
    "\n",
    "    return formatted_dataset"
   ],
   "metadata": {
    "id": "Bzc7AVhAjNrN"
   },
   "execution_count": null,
   "outputs": [],
   "id": "Bzc7AVhAjNrN"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Funktion Performance vor/nach Training\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "XG4kcG4wq7aL"
   },
   "id": "XG4kcG4wq7aL"
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================\n",
    "# BEFORE/AFTER COMPARISON SYSTEM\n",
    "# =============================================\n",
    "\n",
    "class BeforeAfterComparison:\n",
    "    \"\"\"System f√ºr Vergleich vor und nach LoRA Training\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.base_model = None\n",
    "        self.lora_model = None\n",
    "        self.tokenizer = None\n",
    "        self.comparison_results = []\n",
    "\n",
    "    def load_base_model(self):\n",
    "        \"\"\"L√§dt das urspr√ºngliche GPT-2 Modell\"\"\"\n",
    "        print(\"üîÑ Loading original GPT-2 (before LoRA)...\")\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "        self.base_model.eval()\n",
    "\n",
    "        print(\"‚úÖ Original GPT-2 loaded\")\n",
    "\n",
    "    def load_lora_model(self, model_path):\n",
    "        \"\"\"L√§dt das LoRA-angepasste Modell\"\"\"\n",
    "        print(f\"üîÑ Loading LoRA-adapted model from {model_path}...\")\n",
    "\n",
    "        try:\n",
    "            from peft import PeftModel\n",
    "            base_for_lora = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "            self.lora_model = PeftModel.from_pretrained(base_for_lora, model_path)\n",
    "            self.lora_model.eval()\n",
    "            print(\"‚úÖ LoRA model loaded\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading LoRA model: {e}\")\n",
    "            return False\n",
    "\n",
    "    def generate_with_model(self, model, prompt, model_name):\n",
    "        \"\"\"Generiert Text mit spezifischem Modell\"\"\"\n",
    "\n",
    "        # Optimierte Parameter f√ºr fairen Vergleich\n",
    "        params = {\n",
    "            \"max_length\": 100,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"do_sample\": True,\n",
    "            \"pad_token_id\": self.tokenizer.eos_token_id,\n",
    "            \"repetition_penalty\": 1.2,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "\n",
    "            inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(inputs, **params)\n",
    "\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            generation_time = time.time() - start_time\n",
    "\n",
    "            # Clean response\n",
    "            if prompt in response:\n",
    "                response = response.replace(prompt, \"\").strip()\n",
    "\n",
    "            return {\n",
    "                \"response\": response,\n",
    "                \"generation_time\": generation_time,\n",
    "                \"model\": model_name,\n",
    "                \"length\": len(response),\n",
    "                \"words\": len(response.split())\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"model\": model_name\n",
    "            }\n",
    "\n",
    "    def side_by_side_comparison(self, test_questions):\n",
    "        \"\"\"F√ºhrt systematischen Vor/Nach Vergleich durch\"\"\"\n",
    "\n",
    "        if not self.base_model:\n",
    "            print(\"‚ùå Base model not loaded\")\n",
    "            return\n",
    "\n",
    "        if not self.lora_model:\n",
    "            print(\"‚ùå LoRA model not loaded\")\n",
    "            return\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üî¨ BEFORE vs AFTER LoRA COMPARISON\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        for i, question in enumerate(test_questions, 1):\n",
    "            print(f\"\\n[COMPARISON {i}] üéØ {question}\")\n",
    "            print(\"=\"*70)\n",
    "\n",
    "            # Generate with both models\n",
    "            base_result = self.generate_with_model(self.base_model, question, \"Original GPT-2\")\n",
    "            lora_result = self.generate_with_model(self.lora_model, question, \"LoRA-adapted\")\n",
    "\n",
    "            # Display results\n",
    "            print(f\"\\nüî∑ BEFORE (Original GPT-2):\")\n",
    "            print(\"-\" * 40)\n",
    "            if \"error\" in base_result:\n",
    "                print(f\"‚ùå Error: {base_result['error']}\")\n",
    "            else:\n",
    "                print(f\"{base_result['response']}\")\n",
    "                print(f\"   üìä Length: {base_result['length']} chars, Words: {base_result['words']}, Time: {base_result['generation_time']:.2f}s\")\n",
    "\n",
    "            print(f\"\\nüî∂ AFTER (LoRA V4):\")\n",
    "            print(\"-\" * 40)\n",
    "            if \"error\" in lora_result:\n",
    "                print(f\"‚ùå Error: {lora_result['error']}\")\n",
    "            else:\n",
    "                print(f\"{lora_result['response']}\")\n",
    "                print(f\"   üìä Length: {lora_result['length']} chars, Words: {lora_result['words']}, Time: {lora_result['generation_time']:.2f}s\")\n",
    "\n",
    "            # Analysis\n",
    "            if \"error\" not in base_result and \"error\" not in lora_result:\n",
    "                improvement_analysis = self.analyze_improvement(question, base_result, lora_result)\n",
    "\n",
    "                print(f\"\\nüìà IMPROVEMENT ANALYSIS:\")\n",
    "                print(f\"   Relevance: {improvement_analysis['relevance_improvement']}\")\n",
    "                print(f\"   Coherence: {improvement_analysis['coherence_improvement']}\")\n",
    "                print(f\"   Technical Accuracy: {improvement_analysis['tech_accuracy_improvement']}\")\n",
    "                print(f\"   Overall: {improvement_analysis['overall_improvement']}\")\n",
    "\n",
    "                # Store results\n",
    "                self.comparison_results.append({\n",
    "                    \"question\": question,\n",
    "                    \"base_response\": base_result,\n",
    "                    \"lora_response\": lora_result,\n",
    "                    \"improvement_analysis\": improvement_analysis,\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                })\n",
    "\n",
    "            print(\"-\" * 70)\n",
    "\n",
    "    def analyze_improvement(self, question, base_result, lora_result):\n",
    "        \"\"\"Analysiert Verbesserungen zwischen Modellen\"\"\"\n",
    "\n",
    "        base_resp = base_result[\"response\"].lower()\n",
    "        lora_resp = lora_result[\"response\"].lower()\n",
    "        question_lower = question.lower()\n",
    "\n",
    "        analysis = {\n",
    "            \"relevance_improvement\": \"Unknown\",\n",
    "            \"coherence_improvement\": \"Unknown\",\n",
    "            \"tech_accuracy_improvement\": \"Unknown\",\n",
    "            \"overall_improvement\": \"Unknown\"\n",
    "        }\n",
    "\n",
    "        # Relevance Check\n",
    "        tech_keywords = [\"technology\", \"computing\", \"software\", \"programming\", \"data\", \"system\", \"network\", \"digital\"]\n",
    "\n",
    "        base_tech_matches = sum(1 for kw in tech_keywords if kw in base_resp)\n",
    "        lora_tech_matches = sum(1 for kw in tech_keywords if kw in lora_resp)\n",
    "\n",
    "        if lora_tech_matches > base_tech_matches:\n",
    "            analysis[\"relevance_improvement\"] = \"Improved\"\n",
    "        elif lora_tech_matches < base_tech_matches:\n",
    "            analysis[\"relevance_improvement\"] = \"Decreased\"\n",
    "        else:\n",
    "            analysis[\"relevance_improvement\"] = \"Same\"\n",
    "\n",
    "        # Coherence Check (sentence structure)\n",
    "        base_sentences = base_resp.count('.') + base_resp.count('!') + base_resp.count('?')\n",
    "        lora_sentences = lora_resp.count('.') + lora_resp.count('!') + lora_resp.count('?')\n",
    "\n",
    "        if lora_sentences >= 1 and base_sentences < 1:\n",
    "            analysis[\"coherence_improvement\"] = \"Much Improved\"\n",
    "        elif lora_sentences > base_sentences:\n",
    "            analysis[\"coherence_improvement\"] = \"Improved\"\n",
    "        elif lora_sentences < base_sentences:\n",
    "            analysis[\"coherence_improvement\"] = \"Decreased\"\n",
    "        else:\n",
    "            analysis[\"coherence_improvement\"] = \"Same\"\n",
    "\n",
    "        # Technical Accuracy (definition-style responses)\n",
    "        definition_phrases = [\"is a\", \"refers to\", \"involves\", \"enables\", \"consists of\"]\n",
    "\n",
    "        base_def_matches = sum(1 for phrase in definition_phrases if phrase in base_resp)\n",
    "        lora_def_matches = sum(1 for phrase in definition_phrases if phrase in lora_resp)\n",
    "\n",
    "        if lora_def_matches > base_def_matches:\n",
    "            analysis[\"tech_accuracy_improvement\"] = \"Improved\"\n",
    "        elif lora_def_matches < base_def_matches:\n",
    "            analysis[\"tech_accuracy_improvement\"] = \"Decreased\"\n",
    "        else:\n",
    "            analysis[\"tech_accuracy_improvement\"] = \"Same\"\n",
    "\n",
    "        # Overall Assessment\n",
    "        improvements = sum(1 for val in analysis.values() if val == \"Improved\" or val == \"Much Improved\")\n",
    "        decreases = sum(1 for val in analysis.values() if val == \"Decreased\")\n",
    "\n",
    "        if improvements > decreases:\n",
    "            analysis[\"overall_improvement\"] = \"Improved\"\n",
    "        elif decreases > improvements:\n",
    "            analysis[\"overall_improvement\"] = \"Decreased\"\n",
    "        else:\n",
    "            analysis[\"overall_improvement\"] = \"Mixed/Same\"\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def generate_improvement_report(self):\n",
    "        \"\"\"Generiert zusammenfassenden Verbesserungs-Report\"\"\"\n",
    "\n",
    "        if not self.comparison_results:\n",
    "            print(\"‚ùå No comparison results available\")\n",
    "            return\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìä IMPROVEMENT SUMMARY REPORT\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Collect improvement statistics\n",
    "        improvements = {\n",
    "            \"relevance_improvement\": {\"Improved\": 0, \"Decreased\": 0, \"Same\": 0},\n",
    "            \"coherence_improvement\": {\"Improved\": 0, \"Much Improved\": 0, \"Decreased\": 0, \"Same\": 0},\n",
    "            \"tech_accuracy_improvement\": {\"Improved\": 0, \"Decreased\": 0, \"Same\": 0},\n",
    "            \"overall_improvement\": {\"Improved\": 0, \"Decreased\": 0, \"Mixed/Same\": 0}\n",
    "        }\n",
    "\n",
    "        for result in self.comparison_results:\n",
    "            analysis = result[\"improvement_analysis\"]\n",
    "            for category, value in analysis.items():\n",
    "                if value in improvements[category]:\n",
    "                    improvements[category][value] += 1\n",
    "\n",
    "        # Print statistics\n",
    "        total_tests = len(self.comparison_results)\n",
    "\n",
    "        print(f\"üìà IMPROVEMENT STATISTICS (Total Tests: {total_tests}):\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        for category, stats in improvements.items():\n",
    "            print(f\"\\n{category.replace('_', ' ').title()}:\")\n",
    "            for status, count in stats.items():\n",
    "                percentage = (count / total_tests) * 100 if total_tests > 0 else 0\n",
    "                print(f\"   {status}: {count}/{total_tests} ({percentage:.1f}%)\")\n",
    "\n",
    "        # Overall assessment\n",
    "        overall_improved = improvements[\"overall_improvement\"][\"Improved\"]\n",
    "        overall_percentage = (overall_improved / total_tests) * 100 if total_tests > 0 else 0\n",
    "\n",
    "        print(f\"\\nüéØ FINAL ASSESSMENT:\")\n",
    "        if overall_percentage > 60:\n",
    "            print(f\"‚úÖ LoRA training was SUCCESSFUL!\")\n",
    "            print(f\"   {overall_improved}/{total_tests} ({overall_percentage:.1f}%) tests showed overall improvement\")\n",
    "        elif overall_percentage > 30:\n",
    "            print(f\"‚ö†Ô∏è LoRA training showed MODERATE improvement\")\n",
    "            print(f\"   {overall_improved}/{total_tests} ({overall_percentage:.1f}%) tests showed overall improvement\")\n",
    "        else:\n",
    "            print(f\"‚ùå LoRA training showed LIMITED improvement\")\n",
    "            print(f\"   Only {overall_improved}/{total_tests} ({overall_percentage:.1f}%) tests showed overall improvement\")\n",
    "\n",
    "        # Save detailed report\n",
    "        report = {\n",
    "            \"summary_statistics\": improvements,\n",
    "            \"total_tests\": total_tests,\n",
    "            \"overall_success_rate\": overall_percentage,\n",
    "            \"detailed_results\": self.comparison_results,\n",
    "            \"generated_at\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "        with open(\"./v4_before_after_report.json\", \"w\") as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "\n",
    "        print(f\"\\nüíæ Detailed report saved: ./v4_before_after_report.json\")"
   ],
   "metadata": {
    "id": "ImK1iU6cjWSh"
   },
   "execution_count": null,
   "outputs": [],
   "id": "ImK1iU6cjWSh"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Funktion Training\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "ftRKN9LtrA5p"
   },
   "id": "ftRKN9LtrA5p"
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================\n",
    "# OPTIMIZED V4 TRAINING\n",
    "# =============================================\n",
    "\n",
    "def train_focused_lora_v4():\n",
    "    \"\"\"Trainiert fokussiertes LoRA V4 auf Technology Domain\"\"\"\n",
    "\n",
    "    print(\"\\nüéØ Starting focused LoRA V4 training...\")\n",
    "\n",
    "    # 1. Dataset erstellen\n",
    "    dataset = create_technology_focused_dataset()\n",
    "    dataset_obj = Dataset.from_dict({\"text\": dataset})\n",
    "\n",
    "    # 2. Modell setup\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # 3. Optimierte LoRA f√ºr Single Domain\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=16,                      # Moderater Rank f√ºr fokussierten Bereich\n",
    "        lora_alpha=32,             # 2:1 Ratio\n",
    "        lora_dropout=0.05,         # Niedrig f√ºr besseres Lernen\n",
    "        target_modules=[\"c_attn\", \"c_proj\"],  # Fokus auf wichtigste Module\n",
    "        bias=\"none\",\n",
    "        fan_in_fan_out=True,\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # Parameter-Info\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    print(f\"\\nüîß LoRA V4 Configuration:\")\n",
    "    print(f\"   Domain Focus: Technology\")\n",
    "    print(f\"   Dataset Size: {len(dataset)} examples\")\n",
    "    print(f\"   LoRA Rank: {lora_config.r}\")\n",
    "    print(f\"   Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"   Efficiency: {100*trainable_params/total_params:.3f}%\")\n",
    "\n",
    "    # 4. Focused Training\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "    epochs = 3  # Weniger Epochen f√ºr fokussiertes Training\n",
    "\n",
    "    print(f\"\\nüöÄ Training V4 ({epochs} epochs):\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        print(f\"\\nüìç Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        shuffled_data = dataset.copy()\n",
    "        random.shuffle(shuffled_data)\n",
    "\n",
    "        for i, example in enumerate(shuffled_data):\n",
    "            # Focused tokenization\n",
    "            inputs = tokenizer(\n",
    "                example,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=120,      # Optimiert f√ºr Tech Q&A\n",
    "                truncation=True,\n",
    "                padding=True\n",
    "            )\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                labels=inputs[\"input_ids\"]\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            if (i + 1) % 40 == 0:\n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f\"   Step {i+1}/{len(shuffled_data)}: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "        avg_epoch_loss = total_loss / num_batches\n",
    "        print(f\"‚úÖ Epoch {epoch + 1} completed: Loss = {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    # 5. Save model\n",
    "    model.save_pretrained(\"./lora_v4_technology_focused\")\n",
    "\n",
    "    print(f\"\\nüéâ V4 Training completed!\")\n",
    "    print(f\"   Final Loss: {avg_epoch_loss:.4f}\")\n",
    "    print(f\"   Model saved: ./lora_v4_technology_focused\")\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "id": "pkpq1Bucjjcz"
   },
   "execution_count": null,
   "outputs": [],
   "id": "pkpq1Bucjjcz"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Hauptprogramm\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "Sm_c5gF8rKPB"
   },
   "id": "Sm_c5gF8rKPB"
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================\n",
    "# MAIN PROGRAM V4\n",
    "# =============================================\n",
    "\n",
    "def main_v4():\n",
    "    \"\"\"Hauptprogramm f√ºr V4 Single Domain Focus\"\"\"\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"üöÄ LoRA V4 - SINGLE DOMAIN FOCUS + BEFORE/AFTER COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Technology Test Questions\n",
    "    tech_test_questions = [\n",
    "        \"Question: What is Python?\",\n",
    "        \"Question: What is machine learning?\",\n",
    "        \"Question: What is cloud computing?\",\n",
    "        \"Question: What is an API?\",\n",
    "        \"Question: What is cybersecurity?\",\n",
    "        \"Question: What is artificial intelligence?\",\n",
    "        \"Question: What is blockchain?\",\n",
    "        \"Question: What is Docker?\",\n",
    "    ]\n",
    "\n",
    "    # 1. Train focused V4 model\n",
    "    print(\"üéØ STEP 1: Training focused V4 model...\")\n",
    "    trained_model = train_focused_lora_v4()\n",
    "\n",
    "    # 2. Before/After Comparison\n",
    "    print(\"\\nüî¨ STEP 2: Before/After LoRA comparison...\")\n",
    "\n",
    "    comparator = BeforeAfterComparison()\n",
    "    comparator.load_base_model()\n",
    "\n",
    "    if comparator.load_lora_model(\"./lora_v4_technology_focused\"):\n",
    "        comparator.side_by_side_comparison(tech_test_questions)\n",
    "        comparator.generate_improvement_report()\n",
    "    else:\n",
    "        print(\"‚ùå Could not load V4 model for comparison\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéâ LoRA V4 SINGLE DOMAIN TRAINING COMPLETED!\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(f\"\"\"\n",
    "    üéØ V4 IMPROVEMENTS:\n",
    "    ‚Ä¢ Single domain focus (Technology)\n",
    "    ‚Ä¢ Specialized dataset with precise definitions\n",
    "    ‚Ä¢ Optimized LoRA parameters for focused learning\n",
    "    ‚Ä¢ Systematic before/after comparison\n",
    "    ‚Ä¢ Reduced training time through focus\n",
    "\n",
    "    üìä EXPECTED RESULTS:\n",
    "    ‚Ä¢ Much more accurate technical definitions\n",
    "    ‚Ä¢ Better coherence in technology topics\n",
    "    ‚Ä¢ Clear improvement over base GPT-2\n",
    "    ‚Ä¢ Focused expertise instead of broad knowledge\n",
    "\n",
    "    üìÅ FILES GENERATED:\n",
    "    ‚Ä¢ ./lora_v4_technology_focused/ (trained model)\n",
    "    ‚Ä¢ ./v4_before_after_report.json (comparison results)\n",
    "\n",
    "    üéØ V4 ADVANTAGES OVER V3:\n",
    "    ‚Ä¢ Focused ‚Üí Better accuracy\n",
    "    ‚Ä¢ Smaller dataset ‚Üí Less overfitting\n",
    "    ‚Ä¢ Clear baseline comparison ‚Üí Measurable improvement\n",
    "    ‚Ä¢ Specialized ‚Üí Production-ready for tech Q&A\n",
    "        \"\"\")\n",
    "\n",
    "    print(\"üî• V4 is ready - focused, accurate, and measurably improved!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_v4()"
   ],
   "metadata": {
    "id": "_gotSjCrjtPu"
   },
   "execution_count": null,
   "outputs": [],
   "id": "_gotSjCrjtPu"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Improvement Analysis\n",
    "</font></p>\n",
    "\n",
    "Diese Analyse bewertet die Leistungs√§nderungen eines Systems (z.B. KI-Modell/Textgenerierung) anhand verschiedener Kriterien:\n",
    "\n",
    "* **Relevance:** Die Ausgaben passen nun besser/schlechter zum Thema oder zur Nutzerabsicht und liefern pr√§zisere Informationen.\n",
    "* **Coherence:** Die Koh√§renz (Zusammenhang/Logik) der Inhalte hat sich verbessert/verschlechtert. Der Text ist mehr/weniger fl√ºssig oder logisch in-/konsistent.\n",
    "* **Technical Accuracy:** Die technische Genauigkeit (faktisch korrekt und pr√§zise) der Informationen ist gleich geblieben, ohne Verbesserung oder Verschlechterung.\n",
    "* **Overall:** Das Gesamtergebnis ist gemischt oder gleich. Verbesserungen in der Relevanz werden durch Verschlechterungen in der Koh√§renz ausgeglichen."
   ],
   "metadata": {
    "id": "VDuoFNNEso-E"
   },
   "id": "VDuoFNNEso-E"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "355d6569"
   },
   "source": [
    "# 6 | Bewerten des FT-Modells\n",
    "---"
   ],
   "id": "355d6569"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Trainingsmetriken\n",
    "</font></p>\n",
    "\n",
    "\n",
    "Die Modellleistung wird prim√§r durch **Trainingsverlust** und **Validierungsverlust** bewertet, basierend auf der Kreuzentropieverlustfunktion. Dieser misst, wie gut das Modell das n√§chste Wort vorhersagt.\n",
    "\n",
    "* **Trainingsverlust:** Durchschnittlicher Kreuzentropieverlust √ºber den Trainingsdatensatz; zeigt die Lernf√§higkeit des Modells.\n",
    "* **Validierungsverlust:** Berechnung auf separatem Datensatz; Indikator f√ºr Generalisierungsf√§higkeit und √úberanpassung.\n",
    "\n",
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Interpretation von Verlustkurven\n",
    "</font></p>\n",
    "\n",
    "Der Verlauf von Trainings- und Validierungsverlust √ºber Epochen visualisiert die Modellleistung:\n",
    "* **Konvergenz:** Beide Verluste sinken und stabilisieren sich.\n",
    "* **√úberanpassung:** Trainingsverlust sinkt, Validierungsverlust steigt.\n",
    "Analyse dieser Trends hilft bei der Anpassung von Trainingsparametern oder Daten. Ein Validierungsdatensatz ist f√ºr eine unvoreingenommene Bewertung entscheidend.\n",
    "\n",
    "**Zugriff auf detaillierte Trainingsmetriken:** [Dashboard](https://platform.openai.com/finetune)\n",
    "\n",
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Verbesserung der Ergebnisse Fine-Tuning\n",
    "</font></p>\n",
    "\n",
    "Optimale Ergebnisse erfordern einen strategischen Ansatz zur Datenaufbereitung, Parameteranpassung und iterativen Verbesserung.\n",
    "\n",
    "* **Hochwertige Trainingsdaten:**\n",
    "    * **Relevanz:** Datensatz muss eng mit der Aufgabe √ºbereinstimmen.\n",
    "    * **Klarheit und Konsistenz:** Pr√§zise Sprache und einheitlicher Stil.\n",
    "* **Ausreichende Datenmenge:**\n",
    "    * **Umfassende Beispiele:** Mehr Muster verbessern die Generalisierung.\n",
    "    * **Ausgewogener Datensatz:** Vielf√§ltige Szenarien abdecken, unn√∂tige Wiederholungen vermeiden.\n",
    "* **Anweisungen zum Erstellen von Anweisungen:**\n",
    "    * **Explizite Anweisungen:** Klare Anweisungen oder Fragen zu Beginn.\n",
    "    * **Konsistentes Eingabeaufforderungsformat:** Einheitliche Struktur beibehalten.\n",
    "* **Anpassen der Trainingsparameter:**\n",
    "    * **Epochen:** Balance zwischen Unter- und √úberanpassung finden.\n",
    "    * **Batchgr√∂√üe:** Anpassen f√ºr Trainingseffizienz.\n",
    "    * **Lernrate:** Steuert die Gewichtsanpassung f√ºr stabile Konvergenz.\n",
    "* **Datenerweiterungstechniken:** Verbessern die Robustheit durch Paraphrasieren, Synonyme/Antonyme und Rauscheneinf√ºhrung.\n",
    "* **Regelm√§√üige Evaluierung und Iteration:**\n",
    "    * **Validierungssatz:** Objektive Leistungsbewertung.\n",
    "    * **Leistungskennzahlen:** √úberwachung von Genauigkeit, Verlust, Verwirrung.\n",
    "    * **Iterative Verfeinerung:** Erkenntnisse f√ºr Anpassungen nutzen.\n",
    "\n",
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Erweiterte API-Funktionen nutzen\n",
    "</font></p>\n",
    "\n",
    "OpenAI-API bietet Inferenz-Optionen:\n",
    "* **Temperatureinstellung:** Steuert die Zuf√§lligkeit der Ausgabe.\n",
    "* **Top-p (Nucleus Sampling):** Balanciert Vielfalt und Genauigkeit.\n",
    "* **Max. Token:** Begrenzt die Ausgabel√§nge.\n",
    "\n",
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Fr√ºhzeitiges Stoppen implementieren\n",
    "</font></p>\n",
    "\n",
    "Verhindert √úberanpassung:\n",
    "* **Verlusttrends √ºberwachen:** Training beenden, wenn Validierungsverlust trotz sinkendem Trainingsverlust steigt.\n",
    "* **Geduldsstufen festlegen:** Anzahl der Epochen ohne Verbesserung definieren.\n",
    "\n",
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Mehrere Feinabstimmungsrunden\n",
    "</font></p>\n",
    "\n",
    "Verbessert Modellleistung schrittweise:\n",
    "* **Umfassende Erstschulung:** Grundmuster lernen.\n",
    "* **Gezielte Verfeinerung:** Spezifischere Daten f√ºr bestimmte Aufgaben.\n",
    "\n",
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Einbeziehung negativer Beispiele\n",
    "</font></p>\n",
    "\n",
    "Bringt dem Modell bei, was es nicht tun soll:\n",
    "* **Falsche Beispiele:** F√ºgen Sie Beispiele mit fehlerhaften Vervollst√§ndigungen und Korrekturen ein.\n",
    "* **Strafmechanismen:** Datenstrukturierung zur Vermeidung unerw√ºnschter Ausgaben.\n",
    "\n",
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Sicherstellung der Datensatzvielfalt\n",
    "</font></p>\n",
    "\n",
    "Hilft dem Modell, breite Eingaben zu verarbeiten:\n",
    "* **Abwechslungsreiche Themen:** Inhalte aus verschiedenen Bereichen integrieren.\n",
    "* **Stilistische Variation:** Beispiele mit unterschiedlichen Schreibstilen und T√∂nen.\n",
    "\n",
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "√úberwachung der Trainingsmetriken\n",
    "</font></p>\n",
    "\n",
    "Behalten Sie den Lernprozess im Auge:\n",
    "* **Verlustkurven:** Aufdecken von Lernmustern.\n",
    "* **Genauigkeitsmetriken:** Verfolgung korrekter Antworten.\n"
   ],
   "metadata": {
    "id": "naf1sOP3oEsi"
   },
   "id": "naf1sOP3oEsi"
  },
  {
   "cell_type": "markdown",
   "id": "4231281b",
   "metadata": {
    "id": "4231281b"
   },
   "source": [
    "# 7 | Aufgabe\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Erstellen Sie einen *neuen* ChatBot, der durch ein Fine-Tuning einen speziellen Charakter einnimmt:\n",
    "\n",
    "- W√§hlen Sie einen Bot-Charakter aus oder denken Sie sich selbst einen Bot-Charakter aus\n",
    "- Erstellen Sie mit Hilfe von ChatGPT Trainingsdaten im .jsonl-Format.\n",
    "- Erstellen Sie mit Hilfe von ChatGPT Testdaten im .jsonl-Format.\n",
    "- F√ºhren Sie ein Fine-Tuning mit Dashboard oder API durch.\n",
    "- Test Sie das *neue* Modell"
   ],
   "metadata": {
    "id": "mgxheJSm5r3e"
   },
   "id": "mgxheJSm5r3e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Weekend-Stil - \"Friday\" üé≥\n",
    "</font></p>\n",
    "\n",
    "**üí¨ Beschreibung:**   \n",
    "Friday ist dein smarter, augenzwinkernder KI-Kumpel f√ºrs Ende der Woche.\n",
    "Er ist charmant, l√∂sungsorientiert - und ein bisschen genervt davon, dass du wieder alles auf Freitag 16:59 verschoben hast.\n",
    "\n",
    "\n",
    "üìù **Beispiel:**   \n",
    "**User:**  \"Friday, kannst du mir nochmal erkl√§ren, was ein Transformer-Modell ist?‚Äú   \n",
    "**Friday:** ‚ÄûKlar, kurz bevor du ins Wochenende verschwindest: Ein Transformer ist wie ein richtig gutes Orga-Tool ‚Äì es schaut sich alle W√∂rter gleichzeitig an und merkt sich, was wichtig ist. Gern geschehen. Jetzt geh raus und tu so, als w√§rst du voll vorbereitet. üòé‚Äú  \n",
    "\n",
    "\n",
    "Zu **Friday** liegen Trainings-/Test-Daten bereits vor. üòä\n"
   ],
   "metadata": {
    "id": "jsfw8l8a9sPV"
   },
   "id": "jsfw8l8a9sPV"
  },
  {
   "cell_type": "markdown",
   "id": "9b4db086",
   "metadata": {
    "id": "9b4db086"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Mittelalterlicher Ritter ‚Äì \"Sir Schwertmund\" ‚öîÔ∏èüõ°Ô∏è\n",
    "</font></p>\n",
    "\n",
    "üí¨ **Beschreibung:**  \n",
    "Ein tapferer Ritter, der in altert√ºmlicher Sprache spricht und jede Frage mit ritterlichem Anstand beantwortet.\n",
    "\n",
    "üìù **Beispiel:**  \n",
    "**User:** \"Sir Schwertmund, wie gewinne ich ein Duell?\"  \n",
    "**Sir Schwertmund:** \"Habe Mut im Herzen, eine scharfe Klinge und stets ein Ehrenwort auf den Lippen, edler Recke!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Motivations-Coach ‚Äì \"Coach Br√ºllmann\" üí™üì¢\n",
    "</font></p>\n",
    "\n",
    "\n",
    "\n",
    "üí¨ **Beschreibung:**  \n",
    "Ein Fitness- und Erfolgstrainer, der in jeder Antwort volle Motivation und Energie verbreitet.\n",
    "\n",
    "üìù **Beispiel:**  \n",
    "**User:** \"Coach, ich habe keine Lust zu arbeiten.\"  \n",
    "**Coach:** \"AUFSTEHEN! DU BIST EIN CHAMPION! JEDER ERFOLG BEGINNT MIT DEM ERSTEN SCHRITT! JETZT LOS!\""
   ],
   "metadata": {
    "id": "1CCwKzqS5wMC"
   },
   "id": "1CCwKzqS5wMC"
  },
  {
   "cell_type": "markdown",
   "id": "0c7bd06a",
   "metadata": {
    "id": "0c7bd06a"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Hipster-Barista ‚Äì \"Chad Flatwhite\" ‚òïüï∂Ô∏è\n",
    "</font></p>\n",
    "\n",
    "\n",
    "\n",
    "üí¨ **Beschreibung:**  \n",
    "Ein super-cooler, ironischer Barista, der alles nur in Bio, Fairtrade und nachhaltig mag.\n",
    "\n",
    "üìù **Beispiel:**  \n",
    "**User:** \"Chad, welcher Kaffee ist der beste?\"  \n",
    "**Chad:** \"Brudi, wenn dein Kaffee nicht handgefiltert aus Bohnen einer peruanischen Bergziege ist, dann trink lieber Wasser.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Piratenkapit√§n ‚Äì \"Kapit√§n Kr√§henschnabel\" ‚ò†Ô∏èüè¥‚Äç‚ò†Ô∏è\n",
    "</font></p>\n",
    "\n",
    "üí¨ **Beschreibung:**  \n",
    "Ein wilder Pirat, der in Seemannssprache spricht und jede Frage mit einem Hauch von Abenteuer w√ºrzt.\n",
    "\n",
    "üìù **Beispiel:**  \n",
    "**User:** \"Kapit√§n, was ist das Geheimnis eines guten Lebens?\"  \n",
    "**Kapit√§n Kr√§henschnabel:** \"Rum, Reicht√ºmer und ‚Äòne treue Crew! Und niemals ohne Hut aus dem Haus!\"\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "b7Ky-dOl6FwA"
   },
   "id": "b7Ky-dOl6FwA"
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "236790bc",
    "0Yrawkis44O0",
    "a0I9sekJ5LIq",
    "d3dlsbMhjA30",
    "355d6569",
    "4231281b"
   ],
   "gpuType": "T4",
   "toc_visible": true
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
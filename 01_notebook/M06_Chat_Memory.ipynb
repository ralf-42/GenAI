{"cells":[{"cell_type":"markdown","id":"1ff8f9fc","metadata":{"id":"1ff8f9fc"},"source":["![GenAI Banner](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)\n","\n"]},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b> Chat & Memory </b></font> </br></p>\n","\n","---"],"metadata":{"id":"kKu_TgkHgILu"},"id":"kKu_TgkHgILu"},{"cell_type":"code","execution_count":null,"id":"e64cb3d2","metadata":{"id":"e64cb3d2","collapsed":true},"outputs":[],"source":["#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n","!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul\n","!uv pip install --system -q langgraph langchain_openai\n","from genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages\n","setup_api_keys(['OPENAI_API_KEY'], create_globals=False)\n","print()\n","check_environment()\n","print()\n","get_ipinfo()"]},{"cell_type":"code","source":["#@title üîß Installationen { display-mode: \"form\" }\n","install_packages([('langgraph-checkpoint-sqlite', 'langgraph.checkpoint.sqlite')])"],"metadata":{"id":"xIO0o3YhZ70B"},"id":"xIO0o3YhZ70B","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"a00e88a6","metadata":{"id":"a00e88a6"},"source":["\n","\n","# 1 | Intro\n","---"]},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Zustandslosigkeit von LLMs\n","</font></p>\n","\n","Large Language Models (LLMs) wie GPT sind von Natur aus **zustandslos** ‚Äì sie verf√ºgen √ºber kein eingebautes Ged√§chtnis. Jede Anfrage wird isoliert verarbeitet, ohne Bezug zu vorherigen Interaktionen. Deshalb muss der Chatverlauf (Historie) bei jeder Anfrage neu √ºbergeben werden.\n","\n","```\n","Ohne Memory:\n","User: \"Mein Name ist Max\"\n","AI: \"Hallo Max!\"\n","User: \"Wie hei√üe ich?\"\n","AI: \"Das habe ich nicht gespeichert.\" ‚ùå\n","```"],"metadata":{"id":"aBqord6fgPib"},"id":"aBqord6fgPib"},{"cell_type":"markdown","source":["\n","# 2 | Short-term Memory\n","---\n"],"metadata":{"id":"ipUxLVBIhBMk"},"id":"ipUxLVBIhBMk"},{"cell_type":"markdown","source":["Kurzzeit-Memory speichert den Gespr√§chsverlauf einer aktiven Sitzung (eines Threads) und erm√∂glicht kontextbezogene Antworten."],"metadata":{"id":"ZwqL1ofVhCyX"},"id":"ZwqL1ofVhCyX"},{"cell_type":"markdown","id":"a67b3090","metadata":{"id":"a67b3090"},"source":["## 2.1 | ... mit Python-Liste\n","\n","Um zu verstehen, warum LangGraph in vielen F√§llen n√∂tig ist, hier ein Beispiel mit *manueller* Verwaltung."]},{"cell_type":"code","execution_count":null,"id":"14ab01d0","metadata":{"id":"14ab01d0"},"outputs":[],"source":["# Importe\n","from langchain.chat_models import init_chat_model\n","from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# System-Prompt\n","system_prompt = \"Du bist ein hilfreicher und humorvoller KI-Assistent.\"\n","\n","# Prompt-Template mit Historie (MessagesPlaceholder nimmt die Historie entgegen)\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"human\", \"{user_input}\")\n","])\n","\n","# LLM\n","model_provider=\"openai\"\n","model_name = \"gpt-4o-mini\"\n","temperature = 0\n","llm = init_chat_model(model_name, model_provider=model_provider, temperature=temperature)\n","\n","# Parser\n","parser = StrOutputParser()\n","\n","# Chain\n","chain = prompt | llm | parser"]},{"cell_type":"code","execution_count":null,"id":"3eda7317","metadata":{"id":"3eda7317"},"outputs":[],"source":["# Chat-Funktion mit manueller Historien-Verwaltung\n","def chat_manually(system_prompt, chat_history, user_input):\n","    \"\"\"F√ºhrt eine Chat-Interaktion mit manueller Historien-Verwaltung durch.\"\"\"\n","\n","    # Chain aufrufen (Historie wird im Prompt mitgeschickt)\n","    response = chain.invoke({\n","        'system_prompt': system_prompt,\n","        'chat_history': chat_history,\n","        'user_input': user_input\n","    })\n","\n","    # Ausgabe\n","    mprint(f\"### üßë‚Äçü¶± Mensch: \\n{user_input}\")\n","    mprint(f\"### ü§ñ KI: \\n{response}\\n\")\n","\n","    # ‚≠ê WICHTIG: Memory (Liste) muss MANUELL nach JEDEM Call aktualisiert werden\n","    chat_history.extend([HumanMessage(content=user_input), AIMessage(content=response)])\n","\n","    return chat_history"]},{"cell_type":"code","execution_count":null,"id":"d94f856c","metadata":{"id":"d94f856c"},"outputs":[],"source":["# Historie initialisieren\n","chat_history = []\n","\n","# Konversation\n","chat_manually(system_prompt, chat_history, \"Mein Name ist Max\")\n","chat_manually(system_prompt, chat_history, \"Ich mag Python-Programmierung\")\n","chat_manually(system_prompt, chat_history, \"Wei√üt du noch, wie ich hei√üe und was ich mag?\")\n","\n","mprint(\"### üìù Gespeicherte Nachrichten (Liste):\\n---\")\n","for msg in chat_history:\n","    mprint(f\"  **{msg.type}**:   {msg.content}\")\n","\n","mprint(\"\\n\\n‚ùå Problem: Keine Session-Verwaltung und fehleranf√§lliges, manuelles Memory-Management.\")"]},{"cell_type":"markdown","source":["## 2.2 | ... mit LangGraph"],"metadata":{"id":"StEp6G4vjD3d"},"id":"StEp6G4vjD3d"},{"cell_type":"markdown","source":["\n","\n","**LangGraph** automatisiert das Session- und Memory-Management mit dem **Checkpointer**. Dieser speichert den gesamten Zustand (`MessagesState`) eines **Threads** und l√§dt ihn beim n√§chsten Aufruf automatisch wieder. Das manuelle Aktualisieren entf√§llt."],"metadata":{"id":"agjXF4WBvC5x"},"id":"agjXF4WBvC5x"},{"cell_type":"markdown","source":["Der Workflow/Graph ist sehr einfach: **START** -> **Chat** (LLM-Aufruf) -> **END**. Die Magie passiert durch den **`MessagesState`** und den **`Checkpointer`**.\n","\n"],"metadata":{"id":"bhG-A504E3tN"},"id":"bhG-A504E3tN"},{"cell_type":"code","execution_count":null,"id":"48a689df","metadata":{"id":"48a689df"},"outputs":[],"source":["# Importe\n","from langchain.chat_models import init_chat_model\n","from langchain_core.messages import HumanMessage, SystemMessage\n","\n","from langgraph.graph import StateGraph, MessagesState, START, END\n","from langgraph.checkpoint.memory import MemorySaver\n","\n","from IPython.display import Image, display"]},{"cell_type":"code","source":["# LLM\n","model_provider=\"openai\"\n","model_name = \"gpt-4o-mini\"\n","temperature = 0.2\n","llm = init_chat_model(model_name, model_provider=model_provider, temperature=temperature)"],"metadata":{"id":"cUg4CBijn9Ov"},"id":"cUg4CBijn9Ov","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Workflow\n","</font></p>"],"metadata":{"id":"EOEKdbyorgxw"},"id":"EOEKdbyorgxw"},{"cell_type":"code","source":["# Workflow: Ruft das Modell auf (der eigentliche Chatbot)\n","def chat_node(state: MessagesState):\n","    \"\"\" Diese Funktion wird bei jedem Chat-Schritt aufgerufen. Der 'state' enth√§lt automatisch alle bisherigen Nachrichten.  \"\"\"\n","\n","    # System-Prompt hinzuf√ºgen (vor der Historie)\n","    # NOTE: Der MessagesState reduziert die Nachrichten beim Hinzuf√ºgen automatisch.\n","    messages = [\n","        SystemMessage(content=system_prompt)\n","    ] + state[\"messages\"]\n","\n","    # LLM aufrufen\n","    response = llm.invoke(messages)\n","\n","    # üõë WICHTIG: Die R√ºckgabe MUSS eine Liste von Nachrichten sein!\n","    # Der MessagesState Reducer f√ºgt die Liste der Historie hinzu.\n","    return {\"messages\": [response]}"],"metadata":{"id":"sYQQ2hP2l-k-"},"id":"sYQQ2hP2l-k-","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Graph\n","</font></p>"],"metadata":{"id":"Ib2k1P4jrnAS"},"id":"Ib2k1P4jrnAS"},{"cell_type":"code","source":["# Graph\n","workflow = StateGraph(state_schema=MessagesState)\n","\n","# Knoten hinzuf√ºgen\n","workflow.add_node(\"chat\", chat_node)\n","\n","# Kanten definieren\n","workflow.add_edge(START, \"chat\")\n","workflow.add_edge(\"chat\", END)\n","\n","# Checkpointer (MemorySaver) verwaltet den Zustand pro Thread_ID\n","checkpointer = MemorySaver()\n","\n","# Graph kompilieren\n","graph =  workflow.compile(checkpointer=checkpointer)\n","display(Image(graph.get_graph().draw_mermaid_png()))"],"metadata":{"id":"Ehbr0XpplzBB"},"id":"Ehbr0XpplzBB","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Ausf√ºhrung\n","</font></p>"],"metadata":{"id":"G5SKxyfurpt5"},"id":"G5SKxyfurpt5"},{"cell_type":"code","execution_count":null,"id":"fae84abc","metadata":{"id":"fae84abc"},"outputs":[],"source":["# Ausf√ºhrung\n","def chat(thread_id, user_input):\n","    \"\"\"Chattet mit dem Bot in einem bestimmten Thread.\"\"\"\n","\n","    # Config enth√§lt die Thread-ID (Session-ID)\n","    config = {\"configurable\": {\"thread_id\": thread_id}}\n","\n","    # Input vorbereiten\n","    input_message = {\"messages\": [HumanMessage(content=user_input)]}\n","\n","    # Graph aufrufen - Memory wird vom Checkpointer automatisch geladen/gespeichert!\n","    result = graph.invoke(input_message, config=config)\n","\n","    # Ausgabe\n","    mprint(f\"**üßë‚Äçü¶± [Thread: {thread_id}] Mensch:** \\n{user_input}\")\n","    mprint(f\"**ü§ñ [Thread: {thread_id}] KI:** \\n{result['messages'][-1].content}\\n\")\n","\n","    return result['messages'][-1].content"]},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Multi-User (Parallele Threads)\n","</font></p>\n","\n","Jede **`thread_id`** erh√§lt ihre **eigene, isolierte Historie**."],"metadata":{"id":"wo3td0eonq5p"},"id":"wo3td0eonq5p"},{"cell_type":"code","execution_count":null,"id":"10f87677","metadata":{"id":"10f87677"},"outputs":[],"source":["# Thread 1: Max\n","chat(\"thread_max\", \"Ich mag Python-Programmierung!\")\n","\n","# Thread 2: Emma (komplett separate Konversation)\n","chat(\"thread_emma\", \"Ich interessiere mich f√ºr Machine Learning!\")\n","\n","# Zur√ºck zu Thread 1 - Max's Memory bleibt erhalten!\n","chat(\"thread_max\", \"Was war nochmal mein Interesse?\")\n","\n","# Zur√ºck zu Thread 2 - Emma's Memory bleibt erhalten!\n","chat(\"thread_emma\", \"An welchem Thema bin ich interessiert?\")\n","print()"]},{"cell_type":"code","execution_count":null,"id":"031aba17","metadata":{"id":"031aba17"},"outputs":[],"source":["def show_thread_history(thread_id):\n","    \"\"\"Zeigt die komplette Historie eines Threads.\"\"\"\n","\n","    config = {\"configurable\": {\"thread_id\": thread_id}}\n","    state = graph.get_state(config)\n","\n","    mprint(f\"### üìù Thread '{thread_id}' - {len(state.values['messages'])} Nachrichten:\\n\")\n","    mprint(\"---\")\n","\n","    for msg in state.values[\"messages\"]:\n","        role = \"üßë‚Äçü¶±\" if msg.type == \"human\" else \"ü§ñ\"\n","        mprint(f\"{role} {msg.type.upper()}: {msg.content}\")\n","\n","# Beispiel\n","show_thread_history(\"thread_max\")\n","show_thread_history(\"thread_emma\")"]},{"cell_type":"markdown","source":["\n","# 3 | Memory Management\n","---"],"metadata":{"id":"uiZsQbpKpOdG"},"id":"uiZsQbpKpOdG"},{"cell_type":"markdown","source":["**Problem:** Lange Konversationen sprengen das **Kontextfenster** und werden teuer (Kosten, Latenz). Auch moderne, gro√üe Modelle leiden unter dem **\"Lost in the middle\"**-Problem, bei dem wichtige Informationen in der Mitte eines langen Prompt leicht ignoriert werden.\n","\n","**L√∂sung:** Intelligentes Memory-Management."],"metadata":{"id":"IAmGAVU9pSC6"},"id":"IAmGAVU9pSC6"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Funktionsstruktur mit Management\n","</font></p>\n","\n","Hinzuf√ºgen eines **Pre-Processing-Schritts** vor dem Chat, um die Nachrichten zu trimmen oder zusammenzufassen.\n","\n"],"metadata":{"id":"2S71VA1oG5J_"},"id":"2S71VA1oG5J_"},{"cell_type":"markdown","source":["\n","## 3.1 | Trimming (Sliding Window)"],"metadata":{"id":"DUJajn7jqH7G"},"id":"DUJajn7jqH7G"},{"cell_type":"markdown","source":["\n","\n","**Strategie:** Behalte nur die letzten *n* Nachrichten. Alles √§ltere wird entfernt (oder nur der letzte Teil des Prompts wird genutzt). LangChain bietet hierf√ºr eingebaute Utilities."],"metadata":{"id":"23B_J0qHvmJ6"},"id":"23B_J0qHvmJ6"},{"cell_type":"code","source":["# Import\n","from langchain_core.messages import trim_messages"],"metadata":{"id":"F7Fg1haFq2if"},"id":"F7Fg1haFq2if","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Memory-Einstellungen\n","max_messages_before_trim = 10\n","messages_to_summarize = 8\n","recent_messages_to_keep = 2"],"metadata":{"id":"bGlk8ODFzOJA"},"id":"bGlk8ODFzOJA","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Workflow\n","</font></p>"],"metadata":{"id":"qOiP6rucq465"},"id":"qOiP6rucq465"},{"cell_type":"code","execution_count":null,"id":"1b363c83","metadata":{"id":"1b363c83"},"outputs":[],"source":["# Trimming: Nur die letzten n Token/Nachrichten behalten\n","def chat_node_with_trimming(state: MessagesState):\n","    \"\"\"Ruft das Modell mit getrimmter Historie auf.\"\"\"\n","\n","    # System-Prompt\n","    system_msg = SystemMessage(content=system_prompt)\n","\n","    # Nur die letzten N Nachrichten (hier: max_messages_before_trim)\n","    # 'token_counter=len' z√§hlt die Anzahl der Nachrichten statt Token\n","    trimmed = trim_messages(\n","        state[\"messages\"],\n","        max_tokens=max_messages_before_trim,\n","        strategy=\"last\",\n","        token_counter=len, # ‚Üê Z√§hlt Nachrichten statt Tokens\n","        include_system=False # System-Nachricht nicht in das Limit einbeziehen\n","    )\n","\n","    messages = [system_msg] + trimmed\n","    response = llm.invoke(messages)\n","\n","    return {\"messages\": [response]}"]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Graph\n","</font></p>"],"metadata":{"id":"l5h8qce8q_x_"},"id":"l5h8qce8q_x_"},{"cell_type":"code","source":["# Graph mit Trimming\n","workflow_trimmed = StateGraph(state_schema=MessagesState)\n","workflow_trimmed.add_node(\"chat\", chat_node_with_trimming)\n","workflow_trimmed.add_edge(START, \"chat\")\n","workflow_trimmed.add_edge(\"chat\", END)\n","\n","graph_trimmed = workflow_trimmed.compile(checkpointer=MemorySaver())\n","display(Image(graph.get_graph().draw_mermaid_png()))"],"metadata":{"id":"--f27ld0q_Kj"},"id":"--f27ld0q_Kj","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Ausf√ºhrung\n","</font></p>"],"metadata":{"id":"e2vIZJz6sFLT"},"id":"e2vIZJz6sFLT"},{"cell_type":"code","source":["# Ausf√ºhrung\n","def chat_with_trimming(thread_id, user_input):\n","    \"\"\"Chattet mit dem Bot unter Verwendung der Summary-Strategie.\"\"\"\n","\n","    config = {\"configurable\": {\"thread_id\": thread_id}}\n","    # User-Input ist die NEUE Nachricht und wird vom Checkpointer zur Historie hinzugef√ºgt.\n","    input_message = {\"messages\": [HumanMessage(content=user_input)]}\n","\n","    result = graph_trimmed.invoke(input_message, config=config)\n","\n","    mprint(f\"### üßë‚Äçü¶± Mensch: \\n{user_input}\")\n","    mprint(f\"### ü§ñ KI: \\n{result['messages'][-1].content}\\n\")\n","\n","    return result['messages'][-1].content"],"metadata":{"id":"ar7H2lO2sFLU"},"execution_count":null,"outputs":[],"id":"ar7H2lO2sFLU"},{"cell_type":"code","source":["# Beispiel: Lange Konversation (mehr als max_messages_before_trim)\n","thread = \"trimming_test\"\n","\n","# Erste Nachrichten (unterhalb des Limits)\n","chat_with_trimming(thread, \"Mein Name ist Max\")\n","chat_with_trimming(thread, \"Ich wohne in K√∂ln\")\n","chat_with_trimming(thread, \"Ich mag Python\")\n","chat_with_trimming(thread, \"Ich habe eine Katze namens Neo\")\n","\n","# Jetzt wird das Limit √ºberschritten und die Zusammenfassung ausgel√∂st\n","for i in range(7):\n","    chat_with_trimming(thread, f\"Dies ist Test-Nachricht Nummer {i+1} zur F√ºllung der Historie.\")\n","\n","# Nach der Zusammenfassung: Die KI sollte trotzdem wichtige Infos kennen\n","chat_with_trimming(thread, \"Wie hie√ü meine Katze?\")\n","print()"],"metadata":{"id":"90dCuFP7sFLU"},"execution_count":null,"outputs":[],"id":"90dCuFP7sFLU"},{"cell_type":"markdown","source":["## 3.2 | Summarization (Zusammenfassung)"],"metadata":{"id":"viOi5m8kqpQn"},"id":"viOi5m8kqpQn"},{"cell_type":"markdown","source":["\n","\n","**Strategie:** Fasse alte Nachrichten zusammen. Die Zusammenfassung ersetzt dann die √§lteren Nachrichten in der Historie, wodurch Platz gespart wird."],"metadata":{"id":"7aAJFxGlvj3n"},"id":"7aAJFxGlvj3n"},{"cell_type":"code","execution_count":null,"id":"0ff0a7d5","metadata":{"id":"0ff0a7d5"},"outputs":[],"source":["# Import\n","from langchain_core.prompts import ChatPromptTemplate"]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Workflow\n","</font></p>"],"metadata":{"id":"4qXLACBNrMhL"},"id":"4qXLACBNrMhL"},{"cell_type":"code","source":["# Zusammenfassungs-Prompt\n","summarize_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Fasse die folgende Konversation in 2-3 S√§tzen zusammen. Behalte wichtige Fakten und Pr√§ferenzen.\"),\n","    (\"human\", \"{conversation}\")\n","])\n","\n","def summarize_conversation(messages):\n","    \"\"\"Erstellt eine Zusammenfassung der Nachrichten.\"\"\"\n","\n","    # Konversation als Text formatieren\n","    conversation_text = \"\\n\".join([\n","        f\"{msg.type}: {msg.content}\" for msg in messages\n","    ])\n","\n","    # Zusammenfassung erstellen\n","    summary_chain = summarize_prompt | llm | StrOutputParser()\n","    summary = summary_chain.invoke({\"conversation\": conversation_text})\n","\n","    return summary\n","\n","# Zusammenfassung erstellen\n","def chat_node_with_summary(state: MessagesState):\n","    \"\"\"Nutzt Zusammenfassung statt alter Nachrichten, wenn die Historie zu lang wird.\"\"\"\n","\n","    all_messages = state[\"messages\"]\n","    summary = \"‚Äî\"\n","\n","    # Wenn die Historie zu lang ist: Zusammenfassen\n","    if len(all_messages) > max_messages_before_trim:\n","        mprint(f\"\\n‚ö†Ô∏è Historie zu lang ({len(all_messages)}). Fasse √§ltere Nachrichten zusammen.\\n\")\n","\n","        # 1. √Ñltere Nachrichten zum Zusammenfassen ausw√§hlen\n","        to_summarize = all_messages[:messages_to_summarize]\n","        summary = summarize_conversation(to_summarize)\n","\n","        # 2. Den Prompt mit der Zusammenfassung und nur den letzten Nachrichten f√ºllen\n","        messages_for_prompt = [\n","            SystemMessage(content=f\"{system_prompt}\\n\\nBisheriger Kontext (Zusammenfassung): {summary}\"),\n","        ] + all_messages[-recent_messages_to_keep:]\n","\n","    else:\n","        # Wenn Historie kurz genug: Gesamte Historie (inkl. System-Prompt) verwenden\n","        messages_for_prompt = [SystemMessage(content=system_prompt)] + all_messages\n","\n","    # LLM aufrufen (antwortet auf die letzte User-Nachricht + Kontext)\n","    response = llm.invoke(messages_for_prompt)\n","\n","    # Wichtig: Wir geben NUR die KI-Antwort zur√ºck, LangGraph speichert die User-Nachricht und die KI-Antwort im State!\n","    return {\"messages\": [response]}"],"metadata":{"id":"HK7XabpyrIoA"},"id":"HK7XabpyrIoA","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Graph\n","</font></p>"],"metadata":{"id":"cSCb8G2crWqj"},"id":"cSCb8G2crWqj"},{"cell_type":"code","source":["# Graph mit Summary-Funktion erstellen\n","workflow_summary = StateGraph(state_schema=MessagesState)\n","workflow_summary.add_node(\"chat\", chat_node_with_summary)\n","workflow_summary.add_edge(START, \"chat\")\n","workflow_summary.add_edge(\"chat\", END)\n","\n","# Mit Memory kompilieren\n","graph_summary = workflow_summary.compile(checkpointer=MemorySaver())\n","display(Image(graph.get_graph().draw_mermaid_png()))"],"metadata":{"id":"sAz5ZCoOqmd3"},"id":"sAz5ZCoOqmd3","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Ausf√ºhrung\n","</font></p>"],"metadata":{"id":"88KbFhSFrZQP"},"id":"88KbFhSFrZQP"},{"cell_type":"code","id":"ke9opfopu6","source":["# Ausf√ºhrung\n","def chat_with_summary(thread_id, user_input):\n","    \"\"\"Chattet mit dem Bot unter Verwendung der Summary-Strategie.\"\"\"\n","\n","    config = {\"configurable\": {\"thread_id\": thread_id}}\n","    # User-Input ist die NEUE Nachricht und wird vom Checkpointer zur Historie hinzugef√ºgt.\n","    input_message = {\"messages\": [HumanMessage(content=user_input)]}\n","\n","    result = graph_summary.invoke(input_message, config=config)\n","\n","    mprint(f\"### üßë‚Äçü¶± Mensch: \\n{user_input}\")\n","    mprint(f\"### ü§ñ KI: \\n{result['messages'][-1].content}\\n\")\n","\n","    return result['messages'][-1].content"],"metadata":{"id":"ke9opfopu6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Beispiel: Lange Konversation (mehr als max_messages_before_trim)\n","thread = \"summary_test\"\n","\n","# Erste Nachrichten (unterhalb des Limits)\n","chat_with_summary(thread, \"Mein Name ist Max\")\n","chat_with_summary(thread, \"Ich wohne in K√∂ln\")\n","chat_with_summary(thread, \"Ich mag Python\")\n","chat_with_summary(thread, \"Ich habe eine Katze namens Neo\")\n","\n","# Jetzt wird das Limit √ºberschritten und die Zusammenfassung ausgel√∂st\n","for i in range(7):\n","    chat_with_summary(thread, f\"Dies ist Test-Nachricht Nummer {i+1} zur F√ºllung der Historie.\")\n","\n","# Nach der Zusammenfassung: Die KI sollte trotzdem wichtige Infos kennen\n","chat_with_summary(thread, \"Wie hie√ü meine Katze?\")\n","print()"],"metadata":{"id":"tupnKWvQriK4"},"id":"tupnKWvQriK4","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# 4 | Long-term Memory\n","---"],"metadata":{"id":"SuaGnM9WtvZq"},"id":"SuaGnM9WtvZq"},{"cell_type":"markdown","source":["**SQLite Checkpointer f√ºr persistente Konversationen**"],"metadata":{"id":"pdp3aRGEt8fF"},"id":"pdp3aRGEt8fF"},{"cell_type":"markdown","source":["**Problem:**\n","\n","Ohne Persistenz geht die Konversationshistorie beim Neustart der Anwendung verloren. In-Memory-Speicher (wie MemorySaver) ist nur f√ºr Tests und Demos geeignet, nicht f√ºr Production-Anwendungen, die Benutzer-Sessions √ºber l√§ngere Zeitr√§ume aufrechterhalten m√ºssen.    \n","**L√∂sung:**\n","\n","SQLite Checkpointer (lokale Entwicklung) bzw. PostgreSQL Checkpointer (Production). Der Checkpointer speichert jeden Graph-State automatisch in einer Datenbank und organisiert Konversationen √ºber thread_ids. Dadurch k√∂nnen:\n","\n","+ Benutzer-Sessions nach Neustart fortgesetzt werden\n","+ Mehrere parallele Konversationen verwaltet werden (Multi-User/Multi-Thread)\n","+ Konversationshistorien abgerufen und analysiert werden\n","+ Fehlerhafte Ausf√ºhrungen vom letzten erfolgreichen Checkpoint wiederaufgenommen werden (Error Recovery)"],"metadata":{"id":"Kdnm6wv9dG8j"},"id":"Kdnm6wv9dG8j"},{"cell_type":"markdown","source":["<p><font color='darkblue' size=\"4\">\n","‚ú® <b>Empfehlung:</b>\n","</font></p>\n","SQLite f√ºr lokale Entwicklung und z.B. PostgreSQL f√ºr Produktion"],"metadata":{"id":"0_5ksAme1rrd"},"id":"0_5ksAme1rrd"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Importe\n","</font></p>"],"metadata":{"id":"ddiyg9POu6jr"},"id":"ddiyg9POu6jr"},{"cell_type":"code","source":["from typing import Annotated\n","from typing_extensions import TypedDict\n","from langchain.chat_models import init_chat_model\n","from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n","from langgraph.graph import StateGraph, START, END\n","from langgraph.graph.message import add_messages\n","from langgraph.checkpoint.sqlite import SqliteSaver\n","from IPython.display import Image, display\n","import os\n","\n","import sqlite3\n","import uuid\n","\n","# Konfiguration\n","DB_PATH = \"./chatbot_memory.db\""],"metadata":{"id":"KH7XSbtKYmUj"},"id":"KH7XSbtKYmUj","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Workflow\n","</font></p>"],"metadata":{"id":"TENLdoRNvMAl"},"id":"TENLdoRNvMAl"},{"cell_type":"code","source":["class ChatState(TypedDict):\n","    messages: Annotated[list[BaseMessage], add_messages]\n","\n","# LLM einmal initialisieren\n","llm = init_chat_model(\"gpt-4o-mini\", temperature=0.7)\n","\n","# Chatbot-Knoten\n","def chatbot_node(state: ChatState) -> dict:\n","    response = llm.invoke(state[\"messages\"])\n","    return {\"messages\": [response]}"],"metadata":{"id":"5xuiYwmdbmLr"},"id":"5xuiYwmdbmLr","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Graph\n","</font></p>"],"metadata":{"id":"xtK3dInUuGqI"},"id":"xtK3dInUuGqI"},{"cell_type":"code","source":["def create_graph(checkpointer):\n","    builder = StateGraph(ChatState)\n","    builder.add_node(\"chatbot\", chatbot_node)\n","    builder.add_edge(START, \"chatbot\")\n","    builder.add_edge(\"chatbot\", END)\n","    return builder.compile(checkpointer=checkpointer)\n","\n","# SQLite Connection DIREKT erstellen\n","conn = sqlite3.connect(\n","    DB_PATH,\n","    check_same_thread=False  # Wichtig f√ºr Jupyter Notebooks!\n",")\n","\n","# Checkpointer mit bestehender Connection\n","checkpointer = SqliteSaver(conn=conn)\n","checkpointer.setup()\n","graph = create_graph(checkpointer)\n","\n","# Visualisierung\n","print(\"üìä Graph-Struktur:\")\n","display(Image(graph.get_graph().draw_mermaid_png()))"],"metadata":{"id":"nbqzQDVXbufy"},"id":"nbqzQDVXbufy","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Funktionen\n","</font></p>"],"metadata":{"id":"1HEHXUxsoi7z"},"id":"1HEHXUxsoi7z"},{"cell_type":"code","source":["# --- Config-Helper\n","cfg = lambda tid: {\"configurable\": {\"thread_id\": tid}}\n","\n","# --- Chat\n","def chat(tid: str, msg: str) -> str:\n","    return graph.invoke({\"messages\": [HumanMessage(msg)]}, cfg(tid))[\"messages\"][-1].content\n","\n","# --- History\n","def show_history(tid: str):\n","    msgs = graph.get_state(cfg(tid)).values.get(\"messages\", [])\n","    if not msgs:\n","        return print(f\"‚ö†Ô∏è Keine Nachrichten in '{tid}'\\n\")\n","\n","    print(f\"üìú {tid} ({len(msgs)} Nachrichten)\\n\" + \"=\" * 60)\n","    for i, m in enumerate(msgs, 1):\n","        print(f\"{i}. {'üë§' if isinstance(m, HumanMessage) else 'ü§ñ'} {m.content}\\n\")\n","    print(\"=\" * 60 + \"\\n\")\n","\n","# --- List Threads mit allen Nachrichten\n","def list_all_threads():\n","    with sqlite3.connect(DB_PATH) as conn:\n","        threads = conn.execute(\n","            \"SELECT DISTINCT thread_id, COUNT(*) FROM checkpoints GROUP BY thread_id\"\n","        ).fetchall()\n","\n","    print(\"üìã Threads:\\n\" + \"-\" * 60)\n","    for tid, cnt in threads:\n","        state = graph.get_state(cfg(tid))\n","        messages = state.values.get(\"messages\", [])\n","        mcnt = len(messages)\n","        print(f\"  ‚úÖ {tid}: {mcnt} Nachrichten, {cnt} Checkpoints\")\n","        print(\"    Nachrichten:\")\n","        for i, msg in enumerate(messages, 1):\n","            print(f\"      {i}. {msg.content}\")\n","    print()\n","\n","# --- Stats\n","def get_thread_stats():\n","    with sqlite3.connect(DB_PATH) as conn:\n","        tc, cc = conn.execute(\"SELECT COUNT(DISTINCT thread_id), COUNT(*) FROM checkpoints\").fetchone()\n","    print(f\"üìä {tc} Threads, {cc} Checkpoints\\n\")"],"metadata":{"id":"gwXvA2EHoZoQ"},"id":"gwXvA2EHoZoQ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Ausf√ºhrung\n","</font></p>"],"metadata":{"id":"c6ijwciZuKjg"},"id":"c6ijwciZuKjg"},{"cell_type":"markdown","source":["<p><font color='black' size=\"4\">\n","Test-Historie aufbauen\n","</font></p>"],"metadata":{"id":"NhDBQ8v50faG"},"id":"NhDBQ8v50faG"},{"cell_type":"code","source":["def test_historie():\n","    # 1. Neue Konversation erstellen\n","    test_thread = \"test-history\"\n","    print(\"1Ô∏è‚É£ Sende Test-Nachrichten...\")\n","    chat(test_thread, \"Hallo, ich bin ein Test\")\n","    chat(test_thread, \"Wie geht es dir?\")\n","    print(\"‚úÖ Nachrichten gesendet\\n\")\n","\n","    # 2. State direkt pr√ºfen\n","    print(\"2Ô∏è‚É£ State direkt pr√ºfen...\")\n","    config = {\"configurable\": {\"thread_id\": test_thread}}\n","    state = graph.get_state(config)\n","    print(f\"State Type: {type(state)}\")\n","    print(f\"State Values: {state.values}\")\n","    print(f\"Messages: {state.values.get('messages', [])}\\n\")\n","\n","    # 3. History-Funktion testen\n","    print(\"3Ô∏è‚É£ History-Funktion aufrufen...\")\n","    show_history(test_thread)\n","\n","test_historie()"],"metadata":{"id":"-Hm26Zm8qF6x"},"id":"-Hm26Zm8qF6x","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"4\">\n","Interaktiver Thread\n","</font></p>"],"metadata":{"id":"JARMw9rC0kTw"},"id":"JARMw9rC0kTw"},{"cell_type":"code","source":["def main():\n","    thread = \"interactive\"\n","    msg_count = 0\n","\n","    # Das commands-Dictionary beh√§lt die print-Anweisung bei, f√ºhrt aber kein exit() mehr aus.\n","    commands = {\n","        \"exit\": lambda: print(f\"\\nüëã Auf Wiedersehen! ({msg_count} Nachrichten)\\n\"), # <-- Nur Textausgabe, kein Exit mehr!\n","        \"history\": lambda: show_history(thread),\n","        \"all\": lambda: list_all_threads(),\n","        \"stats\": lambda: get_thread_stats(),\n","    }\n","\n","    # Help-Text\n","    print(\"\"\"ü§ñ Interaktiver Chatbot\n","    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n","    Befehle: exit | new | history | thread | all | stats\n","    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n","    \"\"\")\n","\n","    while True:\n","        user_input = input(\"üë§ Du: \").strip()\n","\n","        if not user_input:\n","            continue\n","\n","        # --- exit\n","        if user_input.lower() == \"exit\":\n","            commands[\"exit\"]()\n","            break\n","        # --- new\n","        if user_input == \"new\":\n","            old, thread = thread, f\"session-{str(uuid.uuid4())[:8]}\"\n","            print(f\"üÜï {old} ‚Üí {thread}\\n\")\n","            msg_count = 0\n","        # --- thread\n","        elif user_input in [\"thread\", \"threads\"]:\n","            print(f\"üìã {thread} ({msg_count} Nachrichten)\\n\")\n","        # --- history, all, stats\n","        elif user_input in commands:\n","            print()\n","            commands[user_input]() # F√ºhrt die anderen Befehle aus (history, all, stats)\n","        else:\n","            # Chat\n","            try:\n","                print(f\"ü§ñ {chat(thread, user_input)}\\n\")\n","                msg_count += 1\n","            except Exception as e:\n","                print(f\"‚ùå {e}\\n\")"],"metadata":{"id":"uJftJ_R8xfvH"},"id":"uJftJ_R8xfvH","execution_count":null,"outputs":[]},{"cell_type":"code","source":["main()"],"metadata":{"id":"CxjFHLVy6Rgm"},"id":"CxjFHLVy6Rgm","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# A | Aufgaben\n","---"],"metadata":{"id":"NmzShqhgvKxP"},"id":"NmzShqhgvKxP"},{"cell_type":"markdown","source":["\n","Die Aufgabenstellungen unten bieten Anregungen. Sie k√∂nnen aber auch gerne eigene Aufgaben verwenden.\n","\n"],"metadata":{"id":"OMlTlETEvNkG"},"id":"OMlTlETEvNkG"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Aufgabe 1: Multi-User Chatbot mit LangGraph (Short-term)\n","</font></p>\n","\n","**Schwierigkeit:** ‚≠ê‚≠ê\n","\n","Erstellen Sie einen Chatbot unter Verwendung von **Abschnitt 2** (`graph`), der:\n","- Mindestens 3 verschiedene User-Threads verwaltet (z.B. \"max_session\", \"emma_session\", \"ralf_session\").\n","- Jeden User beim Namen kennt.\n","- Beim Thread-Wechsel den Kontext des jeweiligen Users korrekt beh√§lt (zeigen Sie dies, indem Sie zwischen zwei Threads hin- und herwechseln).\n"],"metadata":{"id":"RP96J46hvXia"},"id":"RP96J46hvXia"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Aufgabe 2: Memory mit Trimming-Strategie\n","</font></p>\n","\n","**Schwierigkeit:** ‚≠ê‚≠ê‚≠ê\n","\n","Implementieren Sie einen Chatbot unter Verwendung von **Abschnitt 3.2** (`graph_summary`), der:\n","1. **`max_messages_before_trim`** auf einen kleinen Wert (z.B. 5) setzt.\n","2. Eine l√§ngere Konversation (z.B. 7 Schritte) durchf√ºhrt, sodass die **Zusammenfassungslogik** ausgel√∂st wird (achten Sie auf den `‚ö†Ô∏è` Hinweis).\n","3. Pr√ºfen Sie mit `show_thread_history(thread_id)`, ob der Checkpointer immer noch **alle 7 Nachrichten** speichert, aber der Chat-Node **nur die relevanten Nachrichten** verwendet.\n","\n","*(Hinweis: Der Checkpointer speichert immer die volle Historie; die Management-Strategie entscheidet, was dem LLM pr√§sentiert wird.)*"],"metadata":{"id":"Ghs86Fuqvdvs"},"id":"Ghs86Fuqvdvs"},{"cell_type":"markdown","source":["# B | Datenbank auslesen\n","---"],"metadata":{"id":"sd_qo5TKELdB"},"id":"sd_qo5TKELdB"},{"cell_type":"code","source":["import sqlite3\n","import msgpack\n","import os\n","import re\n","from typing import List, Dict, Any\n","\n","DB_PATH = \"chatbot_memory.db\""],"metadata":{"id":"1OCl4rTr-gVh"},"id":"1OCl4rTr-gVh","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def decode_langchain_ext_type(code, data):\n","    \"\"\"Gibt die rohen Bytes zur√ºck, da die innere Dekodierung fehlschl√§gt.\"\"\"\n","    if code == 5:\n","        return msgpack.ExtType(code, data)\n","    return msgpack.ExtType(code, data)\n","\n","def read_all_threads_from_db():\n","    \"\"\"\n","    Liest den neuesten Checkpoint aller Threads aus der Datenbank und extrahiert\n","    die bereinigten Nachrichten mithilfe der String-Such-Heuristik.\n","    \"\"\"\n","    if not os.path.exists(DB_PATH):\n","        print(f\"‚ùå Fehler: Datenbankdatei '{DB_PATH}' wurde nicht gefunden.\")\n","        return\n","\n","    # --- SQL-Abfrage 1: Alle Threads und deren neuesten Checkpoint holen ---\n","    # Die Abfrage wird vereinfacht, um alle Threads ohne 'WHERE thread_id = ?' zu holen.\n","    sql_query_all_threads = \"\"\"\n","    SELECT\n","        t1.thread_id,\n","        t1.checkpoint AS state_data\n","    FROM\n","        checkpoints t1\n","    INNER JOIN (\n","        SELECT\n","            thread_id,\n","            MAX(checkpoint_id) AS max_checkpoint_id\n","        FROM\n","            checkpoints\n","        GROUP BY\n","            thread_id\n","    ) AS t2 ON t1.thread_id = t2.thread_id AND t1.checkpoint_id = t2.max_checkpoint_id;\n","    \"\"\"\n","\n","    mprint(f\"### üì¨ Lese alle Threads aus der Datenbank '{DB_PATH}'...\")\n","    mprint(\"---\")\n","\n","    try:\n","        with sqlite3.connect(DB_PATH) as conn:\n","            # Jetzt werden alle neuesten Checkpoints der DB abgerufen\n","            all_thread_results = conn.execute(sql_query_all_threads).fetchall()\n","\n","        if not all_thread_results:\n","            print(f\"‚ÑπÔ∏è Keine Threads in der Datenbank gefunden.\")\n","            return\n","\n","        print(f\"‚úÖ {len(all_thread_results)} Threads gefunden. Starte Extraktion...\")\n","\n","        # --- Iteration √ºber alle gefundenen Threads ---\n","        for thread_id, state_blob in all_thread_results:\n","\n","            # --- Deserialisierung und Extraktion (Ihr alter Funktionskern) ---\n","            try:\n","                # 1. √ÑUSSERE DESERIALISIERUNG: MessagePack\n","                state_data = msgpack.unpackb(state_blob, raw=False, ext_hook=decode_langchain_ext_type)\n","\n","                # Pfad zur Nachrichtenliste\n","                channel_values = state_data.get('channel_values', {})\n","                messages_ext_list = channel_values.get('messages', [])\n","\n","                final_messages_list: List[Dict[str, str]] = []\n","\n","                if isinstance(messages_ext_list, list):\n","                    for ext_obj in messages_ext_list:\n","                        msg_bytes = ext_obj.data if isinstance(ext_obj, msgpack.ExtType) else ext_obj\n","\n","                        if isinstance(msg_bytes, bytes):\n","                            # 2. ROH-SUCHE & BEREINIGUNG\n","                            full_string = msg_bytes.decode('utf-8', errors='ignore')\n","\n","                            # Typ\n","                            type_start = full_string.find(\"langchain_core.messages.\")\n","                            type_text = full_string[type_start:].split()[0].replace(\"langchain_core.messages.\", \"\").replace(\"Message\", \"\")\n","\n","                            # Inhalt\n","                            content_start = full_string.find(\"content\")\n","                            content_text = \"\"\n","                            if content_start != -1:\n","                                potential_content = full_string[content_start + 7:].strip()\n","                                next_key_start = potential_content.find(\"additional_kwargs\")\n","                                if next_key_start != -1:\n","                                    content_text = potential_content[:next_key_start].strip()\n","                                else:\n","                                    content_text = potential_content.strip()\n","\n","                            # --- BEREINIGUNG DER EXTRAHIERTEN STRINGS ---\n","                            match = re.search(r'^(Human|Ai)', type_text, re.IGNORECASE)\n","                            clean_type = match.group(0).capitalize() if match else \"Unbekannt\"\n","                            clean_content = content_text.lstrip(' \\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\x09\\x0a\\x0b\\x0c\\x0d\\x0e\\x0f\\x10\\x11\\x12\\x13\\x14\\x15\\x16\\x17\\x18\\x19\\x1a\\x1b\\x1c\\x1d\\x1e\\x1f !@#$%^&*-+,.')\n","\n","                            final_messages_list.append({\n","                                \"type\": clean_type,\n","                                \"content\": clean_content\n","                            })\n","\n","                # --- Ausgabe pro Thread ---\n","                if final_messages_list:\n","                    mprint(f\"\\n‚úÖ Thread ID: **{thread_id}** (GEFUNDEN: {len(final_messages_list)} Nachrichten)\")\n","                    print(\"-\" * 30)\n","\n","                    for i, msg_data in enumerate(final_messages_list, 1):\n","                        mprint(f\"  {i}. [**{msg_data['type']}**] {msg_data['content'][:100].strip()}...\")\n","                else:\n","                    print(f\"\\n‚ö†Ô∏è Thread ID: **{thread_id}**: Konnte keine lesbaren Nachrichten finden.\")\n","\n","            except Exception as e:\n","                print(f\"‚ùå Thread ID: **{thread_id}**: Fehler bei der Verarbeitung des Checkpoints: {e}\")\n","\n","    except Exception as e:\n","        print(f\"\\n‚ùå Allgemeiner Fehler beim Datenbankzugriff: {e}\")"],"metadata":{"id":"RuiUChw6EhQ3"},"id":"RuiUChw6EhQ3","execution_count":null,"outputs":[]},{"cell_type":"code","source":["read_all_threads_from_db()"],"metadata":{"id":"xxJGfMHNEZPm"},"id":"xxJGfMHNEZPm","execution_count":null,"outputs":[]}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all","text_representation":{"extension":".md","format_name":"markdown"}},"colab":{"provenance":[],"collapsed_sections":["a00e88a6","ipUxLVBIhBMk","a67b3090","StEp6G4vjD3d","uiZsQbpKpOdG","DUJajn7jqH7G","viOi5m8kqpQn","SuaGnM9WtvZq","NmzShqhgvKxP","sd_qo5TKELdB"],"toc_visible":true},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}
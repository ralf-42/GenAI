{"cells":[{"cell_type":"markdown","id":"1ff8f9fc","metadata":{"id":"1ff8f9fc"},"source":["![GenAI Banner](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)\n","\n"]},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b> Chat & Memory </b></font> </br></p>\n","\n","---"],"metadata":{"id":"kKu_TgkHgILu"},"id":"kKu_TgkHgILu"},{"cell_type":"code","execution_count":null,"id":"e64cb3d2","metadata":{"id":"e64cb3d2"},"outputs":[],"source":["#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n","!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul\n","!uv pip install --system -q langgraph langchain_openai\n","from genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages\n","setup_api_keys(['OPENAI_API_KEY', 'HF_TOKEN'], create_globals=False)\n","print()\n","check_environment()\n","print()\n","get_ipinfo()"]},{"cell_type":"markdown","id":"a00e88a6","metadata":{"id":"a00e88a6"},"source":["\n","\n","# 1 | Intro: Warum braucht KI ein Ged√§chtnis?\n","---"]},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Zustandslosigkeit von LLMs\n","</font></p>\n","\n","Large Language Models (LLMs) wie GPT sind von Natur aus **zustandslos** ‚Äì sie verf√ºgen √ºber kein eingebautes Ged√§chtnis. Jede Anfrage wird isoliert verarbeitet, ohne Bezug zu vorherigen Interaktionen. Deshalb muss der Chatverlauf (Historie) bei jeder Anfrage neu √ºbergeben werden.\n","\n","```\n","Ohne Memory:\n","User: \"Mein Name ist Max\"\n","AI: \"Hallo Max!\"\n","User: \"Wie hei√üe ich?\"\n","AI: \"Das habe ich nicht gespeichert.\" ‚ùå\n","```"],"metadata":{"id":"aBqord6fgPib"},"id":"aBqord6fgPib"},{"cell_type":"markdown","source":["\n","# 2 | Short-term Memory\n","---\n"],"metadata":{"id":"ipUxLVBIhBMk"},"id":"ipUxLVBIhBMk"},{"cell_type":"markdown","source":["Kurzzeit-Memory speichert den Gespr√§chsverlauf einer aktiven Sitzung (eines Threads) und erm√∂glicht kontextbezogene Antworten."],"metadata":{"id":"ZwqL1ofVhCyX"},"id":"ZwqL1ofVhCyX"},{"cell_type":"markdown","id":"a67b3090","metadata":{"id":"a67b3090"},"source":["## 2.1 | ... mit Python-Liste\n","\n","Um zu verstehen, warum LangGraph in vielen F√§llen n√∂tig ist, hier ein Beispiel mit *manueller* Verwaltung."]},{"cell_type":"code","execution_count":null,"id":"14ab01d0","metadata":{"id":"14ab01d0"},"outputs":[],"source":["# Importe\n","from langchain_openai import ChatOpenAI\n","from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# System-Prompt\n","system_prompt = \"Du bist ein hilfreicher und humorvoller KI-Assistent.\"\n","\n","# Prompt-Template mit Historie (MessagesPlaceholder nimmt die Historie entgegen)\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"human\", \"{user_input}\")\n","])\n","\n","# LLM\n","model_name = \"gpt-4o-mini\"\n","temperature = 0\n","llm = ChatOpenAI(model=model_name, temperature=temperature)\n","\n","# Parser\n","parser = StrOutputParser()\n","\n","# Chain\n","chain = prompt | llm | parser"]},{"cell_type":"code","execution_count":null,"id":"3eda7317","metadata":{"id":"3eda7317"},"outputs":[],"source":["# Chat-Funktion mit manueller Historien-Verwaltung\n","def chat_manually(system_prompt, chat_history, user_input):\n","    \"\"\"F√ºhrt eine Chat-Interaktion mit manueller Historien-Verwaltung durch.\"\"\"\n","\n","    # Chain aufrufen (Historie wird im Prompt mitgeschickt)\n","    response = chain.invoke({\n","        'system_prompt': system_prompt,\n","        'chat_history': chat_history,\n","        'user_input': user_input\n","    })\n","\n","    # Ausgabe\n","    mprint(f\"### üßë‚Äçü¶± Mensch: \\n{user_input}\")\n","    mprint(f\"### ü§ñ KI: \\n{response}\\n\")\n","\n","    # ‚≠ê WICHTIG: Memory (Liste) muss MANUELL nach JEDEM Call aktualisiert werden\n","    chat_history.extend([HumanMessage(content=user_input), AIMessage(content=response)])\n","\n","    return chat_history"]},{"cell_type":"code","execution_count":null,"id":"d94f856c","metadata":{"id":"d94f856c"},"outputs":[],"source":["# Historie initialisieren\n","chat_history = []\n","\n","# Konversation\n","chat_manually(system_prompt, chat_history, \"Mein Name ist Max\")\n","chat_manually(system_prompt, chat_history, \"Ich mag Python-Programmierung\")\n","chat_manually(system_prompt, chat_history, \"Wei√üt du noch, wie ich hei√üe und was ich mag?\")\n","\n","mprint(\"### üìù Gespeicherte Nachrichten (Liste):\\n---\")\n","for msg in chat_history:\n","    mprint(f\"  **{msg.type}**:   {msg.content}\")\n","\n","mprint(\"\\n\\n‚ùå Problem: Keine Session-Verwaltung und fehleranf√§lliges, manuelles Memory-Management.\")"]},{"cell_type":"markdown","source":["## 2.2 | ... mit LangGraph"],"metadata":{"id":"StEp6G4vjD3d"},"id":"StEp6G4vjD3d"},{"cell_type":"markdown","source":["\n","\n","**LangGraph** automatisiert das Session- und Memory-Management mit dem **Checkpointer**. Dieser speichert den gesamten Zustand (`MessagesState`) eines **Threads** und l√§dt ihn beim n√§chsten Aufruf automatisch wieder. Das manuelle Aktualisieren entf√§llt."],"metadata":{"id":"agjXF4WBvC5x"},"id":"agjXF4WBvC5x"},{"cell_type":"markdown","source":["Der Workflow/Graph ist sehr einfach: **START** -> **Chat** (LLM-Aufruf) -> **END**. Die Magie passiert durch den **`MessagesState`** und den **`Checkpointer`**.\n","\n"],"metadata":{"id":"bhG-A504E3tN"},"id":"bhG-A504E3tN"},{"cell_type":"code","execution_count":null,"id":"48a689df","metadata":{"id":"48a689df"},"outputs":[],"source":["# Importe\n","from langchain_openai import ChatOpenAI\n","from langchain_core.messages import HumanMessage, SystemMessage\n","\n","from langgraph.graph import StateGraph, MessagesState, START, END\n","from langgraph.checkpoint.memory import MemorySaver\n","\n","from IPython.display import Image, display"]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Workflow\n","</font></p>"],"metadata":{"id":"EOEKdbyorgxw"},"id":"EOEKdbyorgxw"},{"cell_type":"code","source":["# Workflow: Ruft das Modell auf (der eigentliche Chatbot)\n","def chat_node(state: MessagesState):\n","    \"\"\" Diese Funktion wird bei jedem Chat-Schritt aufgerufen. Der 'state' enth√§lt automatisch alle bisherigen Nachrichten.  \"\"\"\n","\n","    # System-Prompt hinzuf√ºgen (vor der Historie)\n","    # NOTE: Der MessagesState reduziert die Nachrichten beim Hinzuf√ºgen automatisch.\n","    messages = [\n","        SystemMessage(content=system_prompt)\n","    ] + state[\"messages\"]\n","\n","    # LLM aufrufen\n","    response = llm.invoke(messages)\n","\n","    # üõë WICHTIG: Die R√ºckgabe MUSS eine Liste von Nachrichten sein!\n","    # Der MessagesState Reducer f√ºgt die Liste der Historie hinzu.\n","    return {\"messages\": [response]}"],"metadata":{"id":"sYQQ2hP2l-k-"},"id":"sYQQ2hP2l-k-","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Graph\n","</font></p>"],"metadata":{"id":"Ib2k1P4jrnAS"},"id":"Ib2k1P4jrnAS"},{"cell_type":"code","source":["# Graph\n","workflow = StateGraph(state_schema=MessagesState)\n","\n","# Knoten hinzuf√ºgen\n","workflow.add_node(\"chat\", chat_node)\n","\n","# Kanten definieren\n","workflow.add_edge(START, \"chat\")\n","workflow.add_edge(\"chat\", END)\n","\n","# Checkpointer (MemorySaver) verwaltet den Zustand pro Thread_ID\n","checkpointer = MemorySaver()\n","\n","# Graph kompilieren\n","graph =  workflow.compile(checkpointer=checkpointer)\n","display(Image(graph.get_graph().draw_mermaid_png()))"],"metadata":{"id":"Ehbr0XpplzBB"},"id":"Ehbr0XpplzBB","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Ausf√ºhrung\n","</font></p>"],"metadata":{"id":"G5SKxyfurpt5"},"id":"G5SKxyfurpt5"},{"cell_type":"code","execution_count":null,"id":"fae84abc","metadata":{"id":"fae84abc"},"outputs":[],"source":["# Ausf√ºhrung\n","def chat(thread_id, user_input):\n","    \"\"\"Chattet mit dem Bot in einem bestimmten Thread.\"\"\"\n","\n","    # Config enth√§lt die Thread-ID (Session-ID)\n","    config = {\"configurable\": {\"thread_id\": thread_id}}\n","\n","    # Input vorbereiten\n","    input_message = {\"messages\": [HumanMessage(content=user_input)]}\n","\n","    # Graph aufrufen - Memory wird vom Checkpointer automatisch geladen/gespeichert!\n","    result = graph.invoke(input_message, config=config)\n","\n","    # Ausgabe\n","    mprint(f\"**üßë‚Äçü¶± [Thread: {thread_id}] Mensch:** \\n{user_input}\")\n","    mprint(f\"**ü§ñ [Thread: {thread_id}] KI:** \\n{result['messages'][-1].content}\\n\")\n","\n","    return result['messages'][-1].content"]},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Multi-User (Parallele Threads)\n","</font></p>\n","\n","Jede **`thread_id`** erh√§lt ihre **eigene, isolierte Historie**."],"metadata":{"id":"wo3td0eonq5p"},"id":"wo3td0eonq5p"},{"cell_type":"code","execution_count":null,"id":"10f87677","metadata":{"id":"10f87677"},"outputs":[],"source":["# Thread 1: Max\n","chat(\"thread_max\", \"Ich mag Python-Programmierung!\")\n","\n","# Thread 2: Emma (komplett separate Konversation)\n","chat(\"thread_emma\", \"Ich interessiere mich f√ºr Machine Learning!\")\n","\n","# Zur√ºck zu Thread 1 - Max's Memory bleibt erhalten!\n","chat(\"thread_max\", \"Was war nochmal mein Interesse?\")\n","\n","# Zur√ºck zu Thread 2 - Emma's Memory bleibt erhalten!\n","chat(\"thread_emma\", \"An welchem Thema bin ich interessiert?\")\n","print()"]},{"cell_type":"code","execution_count":null,"id":"031aba17","metadata":{"id":"031aba17"},"outputs":[],"source":["def show_thread_history(thread_id):\n","    \"\"\"Zeigt die komplette Historie eines Threads.\"\"\"\n","\n","    config = {\"configurable\": {\"thread_id\": thread_id}}\n","    state = graph.get_state(config)\n","\n","    mprint(f\"### üìù Thread '{thread_id}' - {len(state.values['messages'])} Nachrichten:\\n\")\n","    mprint(\"---\")\n","\n","    for msg in state.values[\"messages\"]:\n","        role = \"üßë‚Äçü¶±\" if msg.type == \"human\" else \"ü§ñ\"\n","        mprint(f\"{role} {msg.type.upper()}: {msg.content}\")\n","\n","# Beispiel\n","show_thread_history(\"thread_max\")\n","show_thread_history(\"thread_emma\")"]},{"cell_type":"markdown","source":["\n","# 3 | Memory Management\n","---"],"metadata":{"id":"uiZsQbpKpOdG"},"id":"uiZsQbpKpOdG"},{"cell_type":"markdown","source":["**Problem:** Lange Konversationen sprengen das **Kontextfenster** und werden teuer (Kosten, Latenz). Auch moderne, gro√üe Modelle leiden unter dem **\"Lost in the middle\"**-Problem, bei dem wichtige Informationen in der Mitte eines langen Prompt leicht ignoriert werden.\n","\n","**L√∂sung:** Intelligentes Memory-Management."],"metadata":{"id":"IAmGAVU9pSC6"},"id":"IAmGAVU9pSC6"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Funktionsstruktur mit Management\n","</font></p>\n","\n","Hinzuf√ºgen eines **Pre-Processing-Schritts** vor dem Chat, um die Nachrichten zu trimmen oder zusammenzufassen.\n","\n"],"metadata":{"id":"2S71VA1oG5J_"},"id":"2S71VA1oG5J_"},{"cell_type":"markdown","source":["\n","## 3.1 | Trimming (Sliding Window)"],"metadata":{"id":"DUJajn7jqH7G"},"id":"DUJajn7jqH7G"},{"cell_type":"markdown","source":["\n","\n","**Strategie:** Behalte nur die letzten *n* Nachrichten. Alles √§ltere wird entfernt (oder nur der letzte Teil des Prompts wird genutzt). LangChain bietet hierf√ºr eingebaute Utilities."],"metadata":{"id":"23B_J0qHvmJ6"},"id":"23B_J0qHvmJ6"},{"cell_type":"code","source":["# Import\n","from langchain_core.messages import trim_messages"],"metadata":{"id":"F7Fg1haFq2if"},"id":"F7Fg1haFq2if","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Memory-Einstellungen\n","max_messages_before_trim = 10\n","messages_to_summarize = 8\n","recent_messages_to_keep = 2"],"metadata":{"id":"bGlk8ODFzOJA"},"id":"bGlk8ODFzOJA","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Workflow\n","</font></p>"],"metadata":{"id":"qOiP6rucq465"},"id":"qOiP6rucq465"},{"cell_type":"code","execution_count":null,"id":"1b363c83","metadata":{"id":"1b363c83"},"outputs":[],"source":["# Trimming: Nur die letzten n Token/Nachrichten behalten\n","def chat_node_with_trimming(state: MessagesState):\n","    \"\"\"Ruft das Modell mit getrimmter Historie auf.\"\"\"\n","\n","    # System-Prompt\n","    system_msg = SystemMessage(content=system_prompt)\n","\n","    # Nur die letzten N Nachrichten (hier: max_messages_before_trim)\n","    # 'token_counter=len' z√§hlt die Anzahl der Nachrichten statt Token\n","    trimmed = trim_messages(\n","        state[\"messages\"],\n","        max_tokens=max_messages_before_trim,\n","        strategy=\"last\",\n","        token_counter=len, # ‚Üê Z√§hlt Nachrichten statt Tokens\n","        include_system=False # System-Nachricht nicht in das Limit einbeziehen\n","    )\n","\n","    messages = [system_msg] + trimmed\n","    response = llm.invoke(messages)\n","\n","    return {\"messages\": [response]}"]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Graph\n","</font></p>"],"metadata":{"id":"l5h8qce8q_x_"},"id":"l5h8qce8q_x_"},{"cell_type":"code","source":["# Graph mit Trimming\n","workflow_trimmed = StateGraph(state_schema=MessagesState)\n","workflow_trimmed.add_node(\"chat\", chat_node_with_trimming)\n","workflow_trimmed.add_edge(START, \"chat\")\n","workflow_trimmed.add_edge(\"chat\", END)\n","\n","graph_trimmed = workflow_trimmed.compile(checkpointer=MemorySaver())\n","display(Image(graph.get_graph().draw_mermaid_png()))"],"metadata":{"id":"--f27ld0q_Kj"},"id":"--f27ld0q_Kj","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Ausf√ºhrung\n","</font></p>"],"metadata":{"id":"e2vIZJz6sFLT"},"id":"e2vIZJz6sFLT"},{"cell_type":"code","source":["# Ausf√ºhrung\n","def chat_with_trimming(thread_id, user_input):\n","    \"\"\"Chattet mit dem Bot unter Verwendung der Summary-Strategie.\"\"\"\n","\n","    config = {\"configurable\": {\"thread_id\": thread_id}}\n","    # User-Input ist die NEUE Nachricht und wird vom Checkpointer zur Historie hinzugef√ºgt.\n","    input_message = {\"messages\": [HumanMessage(content=user_input)]}\n","\n","    result = graph_trimmed.invoke(input_message, config=config)\n","\n","    mprint(f\"### üßë‚Äçü¶± Mensch: \\n{user_input}\")\n","    mprint(f\"### ü§ñ KI: \\n{result['messages'][-1].content}\\n\")\n","\n","    return result['messages'][-1].content"],"metadata":{"id":"ar7H2lO2sFLU"},"execution_count":null,"outputs":[],"id":"ar7H2lO2sFLU"},{"cell_type":"code","source":["# Beispiel: Lange Konversation (mehr als max_messages_before_trim)\n","thread = \"trimming_test\"\n","\n","# Erste Nachrichten (unterhalb des Limits)\n","chat_with_trimming(thread, \"Mein Name ist Max\")\n","chat_with_trimming(thread, \"Ich wohne in K√∂ln\")\n","chat_with_trimming(thread, \"Ich mag Python\")\n","chat_with_trimming(thread, \"Ich habe eine Katze namens Neo\")\n","\n","# Jetzt wird das Limit √ºberschritten und die Zusammenfassung ausgel√∂st\n","for i in range(7):\n","    chat_with_trimming(thread, f\"Dies ist Test-Nachricht Nummer {i+1} zur F√ºllung der Historie.\")\n","\n","# Nach der Zusammenfassung: Die KI sollte trotzdem wichtige Infos kennen\n","chat_with_trimming(thread, \"Wie hie√ü meine Katze?\")\n","print()"],"metadata":{"id":"90dCuFP7sFLU"},"execution_count":null,"outputs":[],"id":"90dCuFP7sFLU"},{"cell_type":"markdown","source":["## 3.2 | Summarization (Zusammenfassung)"],"metadata":{"id":"viOi5m8kqpQn"},"id":"viOi5m8kqpQn"},{"cell_type":"markdown","source":["\n","\n","**Strategie:** Fasse alte Nachrichten zusammen. Die Zusammenfassung ersetzt dann die √§lteren Nachrichten in der Historie, wodurch Platz gespart wird."],"metadata":{"id":"7aAJFxGlvj3n"},"id":"7aAJFxGlvj3n"},{"cell_type":"code","execution_count":null,"id":"0ff0a7d5","metadata":{"id":"0ff0a7d5"},"outputs":[],"source":["# Import\n","from langchain_core.prompts import ChatPromptTemplate"]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Workflow\n","</font></p>"],"metadata":{"id":"4qXLACBNrMhL"},"id":"4qXLACBNrMhL"},{"cell_type":"code","source":["# Zusammenfassungs-Prompt\n","summarize_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Fasse die folgende Konversation in 2-3 S√§tzen zusammen. Behalte wichtige Fakten und Pr√§ferenzen.\"),\n","    (\"human\", \"{conversation}\")\n","])\n","\n","def summarize_conversation(messages):\n","    \"\"\"Erstellt eine Zusammenfassung der Nachrichten.\"\"\"\n","\n","    # Konversation als Text formatieren\n","    conversation_text = \"\\n\".join([\n","        f\"{msg.type}: {msg.content}\" for msg in messages\n","    ])\n","\n","    # Zusammenfassung erstellen\n","    summary_chain = summarize_prompt | llm | StrOutputParser()\n","    summary = summary_chain.invoke({\"conversation\": conversation_text})\n","\n","    return summary\n","\n","# Zusammenfassung erstellen\n","def chat_node_with_summary(state: MessagesState):\n","    \"\"\"Nutzt Zusammenfassung statt alter Nachrichten, wenn die Historie zu lang wird.\"\"\"\n","\n","    all_messages = state[\"messages\"]\n","    summary = \"‚Äî\"\n","\n","    # Wenn die Historie zu lang ist: Zusammenfassen\n","    if len(all_messages) > max_messages_before_trim:\n","        mprint(f\"\\n‚ö†Ô∏è Historie zu lang ({len(all_messages)}). Fasse √§ltere Nachrichten zusammen.\\n\")\n","\n","        # 1. √Ñltere Nachrichten zum Zusammenfassen ausw√§hlen\n","        to_summarize = all_messages[:messages_to_summarize]\n","        summary = summarize_conversation(to_summarize)\n","\n","        # 2. Den Prompt mit der Zusammenfassung und nur den letzten Nachrichten f√ºllen\n","        messages_for_prompt = [\n","            SystemMessage(content=f\"{system_prompt}\\n\\nBisheriger Kontext (Zusammenfassung): {summary}\"),\n","        ] + all_messages[-recent_messages_to_keep:]\n","\n","    else:\n","        # Wenn Historie kurz genug: Gesamte Historie (inkl. System-Prompt) verwenden\n","        messages_for_prompt = [SystemMessage(content=system_prompt)] + all_messages\n","\n","    # LLM aufrufen (antwortet auf die letzte User-Nachricht + Kontext)\n","    response = llm.invoke(messages_for_prompt)\n","\n","    # Wichtig: Wir geben NUR die KI-Antwort zur√ºck, LangGraph speichert die User-Nachricht und die KI-Antwort im State!\n","    return {\"messages\": [response]}"],"metadata":{"id":"HK7XabpyrIoA"},"id":"HK7XabpyrIoA","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Graph\n","</font></p>"],"metadata":{"id":"cSCb8G2crWqj"},"id":"cSCb8G2crWqj"},{"cell_type":"code","source":["# Graph mit Summary-Funktion erstellen\n","workflow_summary = StateGraph(state_schema=MessagesState)\n","workflow_summary.add_node(\"chat\", chat_node_with_summary)\n","workflow_summary.add_edge(START, \"chat\")\n","workflow_summary.add_edge(\"chat\", END)\n","\n","# Mit Memory kompilieren\n","graph_summary = workflow_summary.compile(checkpointer=MemorySaver())\n","display(Image(graph.get_graph().draw_mermaid_png()))"],"metadata":{"id":"sAz5ZCoOqmd3"},"id":"sAz5ZCoOqmd3","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Ausf√ºhrung\n","</font></p>"],"metadata":{"id":"88KbFhSFrZQP"},"id":"88KbFhSFrZQP"},{"cell_type":"code","id":"ke9opfopu6","source":["# Ausf√ºhrung\n","def chat_with_summary(thread_id, user_input):\n","    \"\"\"Chattet mit dem Bot unter Verwendung der Summary-Strategie.\"\"\"\n","\n","    config = {\"configurable\": {\"thread_id\": thread_id}}\n","    # User-Input ist die NEUE Nachricht und wird vom Checkpointer zur Historie hinzugef√ºgt.\n","    input_message = {\"messages\": [HumanMessage(content=user_input)]}\n","\n","    result = graph_summary.invoke(input_message, config=config)\n","\n","    mprint(f\"### üßë‚Äçü¶± Mensch: \\n{user_input}\")\n","    mprint(f\"### ü§ñ KI: \\n{result['messages'][-1].content}\\n\")\n","\n","    return result['messages'][-1].content"],"metadata":{"id":"ke9opfopu6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Beispiel: Lange Konversation (mehr als max_messages_before_trim)\n","thread = \"summary_test\"\n","\n","# Erste Nachrichten (unterhalb des Limits)\n","chat_with_summary(thread, \"Mein Name ist Max\")\n","chat_with_summary(thread, \"Ich wohne in K√∂ln\")\n","chat_with_summary(thread, \"Ich mag Python\")\n","chat_with_summary(thread, \"Ich habe eine Katze namens Neo\")\n","\n","# Jetzt wird das Limit √ºberschritten und die Zusammenfassung ausgel√∂st\n","for i in range(7):\n","    chat_with_summary(thread, f\"Dies ist Test-Nachricht Nummer {i+1} zur F√ºllung der Historie.\")\n","\n","# Nach der Zusammenfassung: Die KI sollte trotzdem wichtige Infos kennen\n","chat_with_summary(thread, \"Wie hie√ü meine Katze?\")\n","print()"],"metadata":{"id":"tupnKWvQriK4"},"id":"tupnKWvQriK4","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# 4 | Long-term Memory - Ausblick\n","---"],"metadata":{"id":"SuaGnM9WtvZq"},"id":"SuaGnM9WtvZq"},{"cell_type":"markdown","source":["**Problem:** Short-term Memory ist chronologisch und begrenzt. Es eignet sich nicht f√ºr das Speichern von gro√üen, permanenten Wissensbasen oder Fakten.\n","\n","**L√∂sung:** LangGraph **Store** (Long-term Memory).\n","\n","Der Store speichert Fakten in **Namespaces** (typischerweise `user_id` oder `topic`) und erm√∂glicht die **semantische Suche** (Vektorsuche), wodurch nur relevante Fakten in den Prompt geladen werden (RAG-Prinzip)."],"metadata":{"id":"pdp3aRGEt8fF"},"id":"pdp3aRGEt8fF"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Die zwei Memory-Typen in LangGraph\n","</font></p>\n","\n","Im LangChain/LangGraph-Umfeld wird das Ged√§chtnis klar getrennt, um Multi-User-Systeme zu erm√∂glichen:\n","\n","| Aspekt | Short-term (Checkpointer) | Long-term (Store) |\n","|--------|---------------------------|-------------------|\n","| **Zweck** | Gespr√§chskontext pro Thread | Wissensbasis √ºber Threads hinweg |\n","| **Scope** | **`thread_id`** (Session) | **`user_id`** / Namespace (Global) |\n","| **Speicher** | Liste von Nachrichten (chronologisch) | Fakten / Pr√§ferenzen (semantisch) |\n","| **Herausforderung** | L√§ngenbegrenzung (Trimming) | Relevante Suche (RAG) |\n","| **Technologie** | `InMemorySaver`, `SqliteSaver` | `InMemoryStore` mit Index, `PostgresStore` |\n"],"metadata":{"id":"7F5CFYswvRUd"},"id":"7F5CFYswvRUd"},{"cell_type":"code","id":"5nv1ogdm7o2","source":["# Importe\n","from langgraph.store.memory import InMemoryStore\n","from langchain_core.runnables import RunnableConfig"],"metadata":{"id":"5nv1ogdm7o2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Pre-Processing: Datensammlung\n","</font></p>"],"metadata":{"id":"ddiyg9POu6jr"},"id":"ddiyg9POu6jr"},{"cell_type":"code","source":["# Store mit OpenAI Embeddings (1536 Dimensionen f√ºr text-embedding-3-small)\n","# üõë WICHTIG: Der Index erm√∂glicht die semantische (√Ñhnlichkeits-)Suche!\n","store = InMemoryStore(index={\n","    \"dims\": 1536,\n","    \"embed\": \"openai:text-embedding-3-small\"\n","})\n","\n","# Fakten zu User \"Max\" speichern (Namespace: user/max)\n","USER_ID_MAX = \"max_42\"\n","user_ns = (\"user\", USER_ID_MAX)\n","\n","# Speichere Schl√ºssel-Wert-Paare im Store (k√∂nnten auch aus Summarization kommen)\n","store.put(user_ns, \"proj_2020\", {\"text\": \"Max hat 2020 ein Chatbot-Projekt gemacht\"})\n","store.put(user_ns, \"lang_python\", {\"text\": \"Maxs Lieblingssprache ist Python\"})\n","store.put(user_ns, \"city_koeln\", {\"text\": \"Max wohnt in K√∂ln\"})\n","\n","print(f\"‚úÖ Long-term Store mit {len(store.search(user_ns, limit=10))} Eintr√§gen f√ºr User '{USER_ID_MAX}' erstellt\")"],"metadata":{"id":"488JCGFjuSfC"},"id":"488JCGFjuSfC","execution_count":null,"outputs":[]},{"cell_type":"code","id":"sj7wvmre13","source":["# Semantische Suche (nicht nur String-Matching!)\n","def retrieve_memory_semantic(query: str, user_id: str, k: int = 2):\n","    \"\"\"Sucht relevante Erinnerungen per semantischer Vektorsuche im Store.\"\"\"\n","    ns = (\"user\", user_id)\n","    # üõë Semantische Suche mit 'query' im optionalen zweiten Argument\n","    results = store.search(ns, query=query, limit=k)\n","\n","    if not results:\n","        return \"(Keine relevanten Erinnerungen gefunden)\"\n","\n","    return \"\\n\".join([item.value[\"text\"] for item in results])\n","\n","# Test der Suche\n","print(\"üîç Semantische Suche: 'Welche Projekte?'\")\n","print(retrieve_memory_semantic(\"Welche Projekte hat er gemacht?\", USER_ID_MAX))\n","print(\"\\nüîç Semantische Suche: 'Programmiersprachen' (trotzdem Treffer bei 'Lieblingssprache')\")\n","print(retrieve_memory_semantic(\"Programmiersprachen\", USER_ID_MAX))"],"metadata":{"id":"sj7wvmre13"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Workflow\n","</font></p>"],"metadata":{"id":"TENLdoRNvMAl"},"id":"TENLdoRNvMAl"},{"cell_type":"code","id":"skni87b1gx","source":["# Chat mit Store-Integration\n","def chat_node_with_store(state: MessagesState, config: RunnableConfig):\n","    \"\"\"Nutzt Long-term Store (Wissensbasis) zus√§tzlich zur Short-term Historie.\"\"\"\n","\n","    # üõë Wichtig: user_id wird als Namespace f√ºr den Store genutzt\n","    user_id = config[\"configurable\"][\"user_id\"]\n","    user_query = state[\"messages\"][-1].content\n","\n","    # 1. Semantische Suche in Store (RAG-Schritt)\n","    memories = retrieve_memory_semantic(user_query, user_id, k=2)\n","\n","    # 2. Relevante Fakten als System-Kontext hinzuf√ºgen\n","    messages = [\n","        SystemMessage(content=f\"{system_prompt}\\n\\nRelevante Informationen aus Wissensbasis:\\n{memories}\")\n","    ] + state[\"messages\"]\n","\n","    response = llm.invoke(messages)\n","    return {\"messages\": [response]}\n"],"metadata":{"id":"skni87b1gx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Graph\n","</font></p>"],"metadata":{"id":"xtK3dInUuGqI"},"id":"xtK3dInUuGqI"},{"cell_type":"code","source":["# Graph mit Store erstellen\n","workflow_store = StateGraph(state_schema=MessagesState)\n","workflow_store.add_node(\"chat\", chat_node_with_store)\n","workflow_store.add_edge(START, \"chat\")\n","workflow_store.add_edge(\"chat\", END)\n","\n","# Short-term Checkpointer (MemorySaver) bleibt erhalten\n","graph_store = workflow_store.compile(checkpointer=MemorySaver())\n","display(Image(graph.get_graph().draw_mermaid_png()))"],"metadata":{"id":"uFw6SXPmuFj8"},"id":"uFw6SXPmuFj8","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Ausf√ºhrung\n","</font></p>"],"metadata":{"id":"c6ijwciZuKjg"},"id":"c6ijwciZuKjg"},{"cell_type":"code","id":"0xa3nkcv2b5","source":["def chat_with_store(thread_id: str, user_id: str, user_input: str):\n","    \"\"\"Chattet mit dem Bot unter Verwendung von Store (semantische Suche).\"\"\"\n","\n","    config = {\n","        \"configurable\": {\n","            \"thread_id\": thread_id, # Short-term (Historie)\n","            \"user_id\": user_id      # Long-term (Store-Namespace)\n","        }\n","    }\n","    input_message = {\"messages\": [HumanMessage(content=user_input)]}\n","\n","    result = graph_store.invoke(input_message, config=config)\n","\n","    print(f\"üßë‚Äçü¶± Mensch: {user_input}\")\n","    print(f\"ü§ñ KI: {result['messages'][-1].content}\\n\")\n","\n","    return result['messages'][-1].content\n","\n","# Beispiel: Die KI kann auf Store-Wissensbasis zugreifen (semantische Suche!)\n","chat_with_store(\"store_max_t1\", USER_ID_MAX, \"Wann hat Max sein Chatbot-Projekt gemacht?\")\n","chat_with_store(\"store_max_t1\", USER_ID_MAX, \"Welche Programmiersprache mag Max?\")"],"metadata":{"id":"0xa3nkcv2b5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# A | Aufgaben\n","---"],"metadata":{"id":"NmzShqhgvKxP"},"id":"NmzShqhgvKxP"},{"cell_type":"markdown","source":["\n","Die Aufgabenstellungen unten bieten Anregungen. Sie k√∂nnen aber auch gerne eigene Aufgaben verwenden.\n","\n"],"metadata":{"id":"OMlTlETEvNkG"},"id":"OMlTlETEvNkG"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Aufgabe 1: Multi-User Chatbot mit LangGraph (Short-term)\n","</font></p>\n","\n","**Schwierigkeit:** ‚≠ê‚≠ê\n","\n","Erstellen Sie einen Chatbot unter Verwendung von **Abschnitt 2** (`graph`), der:\n","- Mindestens 3 verschiedene User-Threads verwaltet (z.B. \"max_session\", \"emma_session\", \"ralf_session\").\n","- Jeden User beim Namen kennt.\n","- Beim Thread-Wechsel den Kontext des jeweiligen Users korrekt beh√§lt (zeigen Sie dies, indem Sie zwischen zwei Threads hin- und herwechseln).\n"],"metadata":{"id":"RP96J46hvXia"},"id":"RP96J46hvXia"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Aufgabe 2: Memory mit Trimming-Strategie\n","</font></p>\n","\n","**Schwierigkeit:** ‚≠ê‚≠ê‚≠ê\n","\n","Implementieren Sie einen Chatbot unter Verwendung von **Abschnitt 3.2** (`graph_summary`), der:\n","1. **`max_messages_before_trim`** auf einen kleinen Wert (z.B. 5) setzt.\n","2. Eine l√§ngere Konversation (z.B. 7 Schritte) durchf√ºhrt, sodass die **Zusammenfassungslogik** ausgel√∂st wird (achten Sie auf den `‚ö†Ô∏è` Hinweis).\n","3. Pr√ºfen Sie mit `show_thread_history(thread_id)`, ob der Checkpointer immer noch **alle 7 Nachrichten** speichert, aber der Chat-Node **nur die relevanten Nachrichten** verwendet.\n","\n","*(Hinweis: Der Checkpointer speichert immer die volle Historie; die Management-Strategie entscheidet, was dem LLM pr√§sentiert wird.)*"],"metadata":{"id":"Ghs86Fuqvdvs"},"id":"Ghs86Fuqvdvs"}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all","text_representation":{"extension":".md","format_name":"markdown"}},"colab":{"provenance":[],"collapsed_sections":["a00e88a6","ipUxLVBIhBMk","a67b3090","StEp6G4vjD3d","uiZsQbpKpOdG","DUJajn7jqH7G","viOi5m8kqpQn","SuaGnM9WtvZq","NmzShqhgvKxP"],"toc_visible":true},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}
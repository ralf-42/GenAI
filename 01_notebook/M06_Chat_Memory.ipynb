{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff8f9fc",
   "metadata": {
    "id": "1ff8f9fc"
   },
   "source": [
    "![GenAI Banner](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font size=\"5\" color='grey'> <b> Chat & Memory </b></font> </br></p>\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "kKu_TgkHgILu"
   },
   "id": "kKu_TgkHgILu"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64cb3d2",
   "metadata": {
    "id": "e64cb3d2"
   },
   "outputs": [],
   "source": [
    "#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul\nfrom genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages\nsetup_api_keys(['OPENAI_API_KEY', 'HF_TOKEN'], create_globals=False)\nprint()\ncheck_environment()\nprint()\nget_ipinfo()\n# Bei Bedarf: Trennen zwischen Installationsname () und Importname (f√ºr Python) beide Angaben in Klammern\n# install_packages([('markitdown[all]', 'markitdown'), 'langchain_chroma', ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l4gltzt9uv8",
   "source": [
    "## üîß Globale Konstanten\n",
    "\n",
    "Diese Konstanten werden im gesamten Notebook verwendet und k√∂nnen hier zentral angepasst werden."
   ],
   "metadata": {
    "id": "l4gltzt9uv8"
   }
  },
  {
   "cell_type": "code",
   "id": "n6mhirbggq",
   "source": [
    "# ========================================\n",
    "# GLOBALE KONSTANTEN\n",
    "# ========================================\n",
    "\n",
    "# LLM-Konfiguration\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "TEMPERATURE = 0\n",
    "\n",
    "# System-Prompt\n",
    "SYSTEM_PROMPT = \"Du bist ein hilfreicher und humorvoller KI-Assistent.\"\n",
    "\n",
    "# Memory-Einstellungen\n",
    "MAX_MESSAGES_BEFORE_TRIM = 10\n",
    "MAX_TOKENS_FOR_TRIM = 1000\n",
    "MESSAGES_TO_SUMMARIZE = 8\n",
    "RECENT_MESSAGES_TO_KEEP = 2"
   ],
   "metadata": {
    "id": "n6mhirbggq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a00e88a6",
   "metadata": {
    "id": "a00e88a6"
   },
   "source": [
    "\n",
    "\n",
    "# 1 | Intro\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Warum braucht KI ein Ged√§chtnis?\n",
    "</font></p>\n",
    "\n",
    "Large Language Models wie GPT sind von Natur aus **zustandslos** ‚Äì sie verf√ºgen √ºber kein eingebautes Ged√§chtnis. Jede Anfrage wird isoliert verarbeitet, ohne Bezug zu vorherigen Interaktionen.\n",
    "\n",
    "```\n",
    "Ohne Memory:\n",
    "User: \"Mein Name ist Max\"\n",
    "AI: \"Hallo Max!\"\n",
    "User: \"Wie hei√üe ich?\"\n",
    "AI: \"Das habe ich nicht gespeichert.\" ‚ùå\n",
    "\n",
    "Mit Memory:\n",
    "User: \"Mein Name ist Max\"\n",
    "AI: \"Hallo Max!\"\n",
    "User: \"Wie hei√üe ich?\"\n",
    "AI: \"Du hei√üt Max!\" ‚úÖ\n",
    "```"
   ],
   "metadata": {
    "id": "aBqord6fgPib"
   },
   "id": "aBqord6fgPib"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Memory-Typen im √úberblick\n",
    "</font></p>\n",
    "\n",
    "Ein solches *Ged√§chtsnis* l√§sst sich unterschiedliche implementieren:\n",
    "\n",
    "| Typ | Beschreibung | Beispiel | Speicherort | Technologie |\n",
    "|-----|--------------|----------|-------------|-------------|\n",
    "| **Kurzzeit-Memory** | Innerhalb einer Sitzung | ChatGPT erinnert sich an vorherige Nachrichten | RAM | Python Liste, LangGraph |\n",
    "| **Memory-Management** | Optimierung langer Chats | Zusammenfassung, Trimming | RAM | Summarization, Sliding Window |\n",
    "| **Externes Memory** | Wissensdatenbank | RAG-Systeme, Dokumentensuche | Datenbank | Vektordatenbank + Retrieval |\n"
   ],
   "metadata": {
    "id": "233yXmg8gcaa"
   },
   "id": "233yXmg8gcaa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Kontextfenster verstehen\n",
    "</font></p>\n",
    "\n",
    "**Wichtiges Konzept:** Das Kontextfenster bestimmt, wie viele Tokens (W√∂rter/Zeichen) ein LLM gleichzeitig verarbeiten kann.\n",
    "\n",
    "```\n",
    "GPT-4o mini:    128.000 Tokens (~96.000 W√∂rter)\n",
    "Claude Sonnet:  200.000 Tokens (~150.000 W√∂rter)\n",
    "```\n",
    "\n",
    "**Problem:** Mehr Tokens ‚â† Besseres Ged√§chtnis\n",
    "- Kosten steigen linear\n",
    "- Relevante Infos gehen in der Masse unter\n",
    "- L√§ngere Antwortzeiten\n",
    "\n",
    "**L√∂sung:** Intelligentes Memory-Management statt einfach \"alles reinpacken\"\n",
    "\n"
   ],
   "metadata": {
    "id": "TdLjmNDVg2SN"
   },
   "id": "TdLjmNDVg2SN"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# 2 | Kurzzeit-Memory\n",
    "---\n"
   ],
   "metadata": {
    "id": "ipUxLVBIhBMk"
   },
   "id": "ipUxLVBIhBMk"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Kurzzeit-Memory speichert den Gespr√§chsverlauf einer aktiven Sitzung und erm√∂glicht kontextbezogene Antworten.\n"
   ],
   "metadata": {
    "id": "ZwqL1ofVhCyX"
   },
   "id": "ZwqL1ofVhCyX"
  },
  {
   "cell_type": "markdown",
   "id": "a67b3090",
   "metadata": {
    "id": "a67b3090"
   },
   "source": [
    "## 2.1 | Chat mit manuellem Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Basismodell\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "DkXEHr8khFYo"
   },
   "id": "DkXEHr8khFYo"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ab01d0",
   "metadata": {
    "id": "14ab01d0"
   },
   "outputs": [],
   "source": [
    "# Importe\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Prompt-Template mit Historie\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"{system_prompt}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{user_input}\")\n",
    "])"
   ],
   "metadata": {
    "id": "yB-CznOvhTnG"
   },
   "id": "yB-CznOvhTnG",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# LLM\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)"
   ],
   "metadata": {
    "id": "se6gKNx0haRC"
   },
   "id": "se6gKNx0haRC",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Parser\n",
    "parser = StrOutputParser()"
   ],
   "metadata": {
    "id": "8DRVa_6RGq2f"
   },
   "id": "8DRVa_6RGq2f",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Chain\n",
    "chain = prompt | llm | parser"
   ],
   "metadata": {
    "id": "BANIPCWxGvIq"
   },
   "id": "BANIPCWxGvIq",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eda7317",
   "metadata": {
    "id": "3eda7317"
   },
   "outputs": [],
   "source": [
    "# Chat-Funktion\n",
    "def chat_with_memory(system_prompt, chat_history, user_input):\n",
    "    \"\"\"F√ºhrt eine Chat-Interaktion mit manueller Historien-Verwaltung durch.\"\"\"\n",
    "\n",
    "    # Chain aufrufen\n",
    "    response = chain.invoke({\n",
    "        'system_prompt': system_prompt,\n",
    "        'chat_history': chat_history,\n",
    "        'user_input': user_input\n",
    "    })\n",
    "\n",
    "    # Ausgabe\n",
    "    mprint(f\"### üßë‚Äçü¶± Mensch: \\n{user_input}\")\n",
    "    mprint(f\"### ü§ñ KI: \\n{response}\\n\")\n",
    "\n",
    "    # ‚≠ê WICHTIG: Memory manuell aktualisieren\n",
    "    chat_history.extend([HumanMessage(content=user_input), AIMessage(content=response)])\n",
    "\n",
    "    return chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbc7c00",
   "metadata": {
    "id": "adbc7c00"
   },
   "source": [
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Memory in Aktion\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94f856c",
   "metadata": {
    "id": "d94f856c"
   },
   "outputs": [],
   "source": [
    "# Historie initialisieren\n",
    "chat_history = []\n",
    "\n",
    "# Konversation\n",
    "chat_with_memory(SYSTEM_PROMPT, chat_history, \"Mein Name ist Max\")\n",
    "chat_with_memory(SYSTEM_PROMPT, chat_history, \"Ich mag Python-Programmierung\")\n",
    "chat_with_memory(SYSTEM_PROMPT, chat_history, \"Wei√üt du noch, wie ich hei√üe und was ich mag?\")\n",
    "\n",
    "# Historie inspizieren\n",
    "mprint(\"### üìù Gespeicherte Nachrichten:\")\n",
    "mprint(\"---\")\n",
    "for msg in chat_history:\n",
    "    mprint(f\"  **{msg.type}**:   {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Bewertung:\n",
    "</font></p>\n",
    "\n",
    "‚úÖ Memory ist eine Liste von Nachrichten    \n",
    "‚úÖ Jede Nachricht wird zur Historie hinzugef√ºgt   \n",
    "‚úÖ Bei jedem API-Call wird die komplette Historie mitgeschickt    \n",
    "‚úÖ Das LLM sieht den gesamten Kontext    \n",
    "\n",
    "‚ùå Problem: Keine Session-Verwaltung f√ºr mehrere Benutzer    \n",
    "‚ùå Problem: Manuelles Memory-Management fehleranf√§llig    \n",
    "‚ùå Problem: Keine Persistenz (nach Programmneustart weg)    \n",
    "\n",
    "**L√∂sung:** LangGraph mit eingebauter Persistence\n"
   ],
   "metadata": {
    "id": "-T647IjrPZtN"
   },
   "id": "-T647IjrPZtN"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 | LangGraph"
   ],
   "metadata": {
    "id": "StEp6G4vjD3d"
   },
   "id": "StEp6G4vjD3d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "**LangGraph** ist ein Framework zur Erstellung zustandsbasierter, mehrstufiger KI-Anwendungen. Das Kernkonzept basiert auf einem **Graph**, der aus **Nodes** (Knoten) und **Edges** (Kanten) besteht.\n",
    "\n",
    "\n",
    "\n",
    "+ **State** (Zustand): Zentraler Datenspeicher, der durch den gesamten Graph weitergegeben und aktualisiert wird. Enth√§lt alle relevanten Informationen.\n",
    "+ **Nodes** (Knoten): Funktionen, die Aufgaben ausf√ºhren (z.B. LLM-Aufruf, Datenverarbeitung). Jeder Node erhalt den State und gibt einen aktualisierten State zur√ºck.\n",
    "+ **Edges** (Kanten): Verbindungen zwischen Nodes, die den Workflow-Fluss definieren. Sie bestimmen, welcher Nodeals n√§chstes ausgef√ºhrt\n",
    "wird.\n",
    "+ **Conditional Edges**: Bedingte Verbindungen, die basierend auf dem State oder anderen Bedingungen entscheiden, welcher Node als n√§chstes kommt.\n",
    "+ **Graph**: Die Gesamtstruktur aus Nodes und Edges, die einen zustandsbasierten Workflow f√ºr KI-Anwendungen definiert.\n",
    "\n",
    "So entstehen flexible, intelligente Workflows f√ºr komplexe KI-Anwendungen.\n",
    "\n"
   ],
   "metadata": {
    "id": "14z_bp0xxBBT"
   },
   "id": "14z_bp0xxBBT"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ralf-42/Image/main/chat_memory_04.png\" class=\"logo\" width=\"850\"/>"
   ],
   "metadata": {
    "id": "9osg0DdX17j9"
   },
   "id": "9osg0DdX17j9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Nodes - Kategorisierung nach Rolle\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "ls2XOEbcv3i_"
   },
   "id": "ls2XOEbcv3i_"
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Kategorie (funktional) | Beschreibung | Beispiel |\n",
    "|------------------------|--------------|----------|\n",
    "| **Start-Node** | Einstiegspunkt in den Graphen; keine eigene Logik, definiert den Beginn des Ablaufs. | `START` |\n",
    "| **End-Node** | Markiert das Ende der Ausf√ºhrung; erh√§lt typischerweise keine weiteren √úberg√§nge. | `END` |\n",
    "| **Agent-Node** | Enth√§lt LLM- oder Entscheidungslogik, die den Zustand auswertet und neue Nachrichten oder Aktionen generiert. | `assistant`, `planner` |\n",
    "| **Tool-Node** | Ruft externe Funktionen, APIs oder Datenquellen auf und erweitert den Zustand um Ergebnisse. | `search`, `database_query` |\n",
    "| **Evaluator-Node** | Bewertet oder filtert Zwischenergebnisse, steuert ggf. den weiteren Ablauf. | `check_quality`, `approve_or_retry` |\n",
    "| **Router-Node** | Leitet basierend auf Bedingungen an verschiedene Pfade weiter (`add_conditional_edges`). | `route_by_intent` |\n",
    "| **Custom-Node** | Benutzerdefinierte Verarbeitungslogik ohne vordefinierte Rolle. | `normalize_text`, `extract_metadata` |\n"
   ],
   "metadata": {
    "id": "1VXCXjyOvnrP"
   },
   "id": "1VXCXjyOvnrP"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Namenskonventionstabelle - Vorschlag\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "-fMlR7o8w7uC"
   },
   "id": "-fMlR7o8w7uC"
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Ebene / Phase | Empfohlene Variable | Beschreibung | Beispiel |\n",
    "|----------------|--------------------|---------------|-----------|\n",
    "| **Graph-Definition** | `workflow` oder `graph_def` | Enth√§lt die reine logische Definition des Graphen ‚Äì also Nodes, Edges und Zustandsschema. | `workflow = StateGraph(state_schema=MessagesState)` |\n",
    "| **Kompilierte Instanz** | `compiled_workflow` oder `runner` | Das lauff√§hige Objekt nach dem Kompilieren; enth√§lt den initialisierten Zustand und Checkpointer. | `runner = workflow.compile(checkpointer=memory)` |\n",
    "| **Zustand** | `state` | Repr√§sentiert den aktuellen Datenzustand w√§hrend der Ausf√ºhrung (z. B. Nachrichten, Kontext, Variablen). | `state = {\"messages\": [...]} ` |\n",
    "| **Lauf / Ergebnis** | `result` oder `final_state` | R√ºckgabe nach der Ausf√ºhrung oder nach Erreichen von `END`. | `result = runner.invoke(state)` |\n",
    "| **Speicher / Persistenz** | `checkpointer` oder `memory` | Objekt zur Verwaltung von Zwischenzust√§nden und Verlauf. | `memory = MemorySaver()` |\n",
    "| **Teilgraph / Subworkflow** | `subgraph` | Optionaler Untergraph f√ºr modularisierte Logik. | `subgraph = StateGraph(state_schema=TaskState)` |\n",
    "| **Agentenknoten** | `agent_node` oder `agent_fn` | Node-Funktion mit LLM- oder Entscheidungslogik. | `graph.add_node(\"agent\", agent_node)` |\n",
    "| **Toolknoten** | `tool_node` oder `tool_fn` | Node-Funktion, die externe Tools oder APIs aufruft. | `graph.add_node(\"search\", tool_fn)` |\n"
   ],
   "metadata": {
    "id": "3ZBbLbq8w7aq"
   },
   "id": "3ZBbLbq8w7aq"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Funktionsstruktur\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "bhG-A504E3tN"
   },
   "id": "bhG-A504E3tN"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ralf-42/Image/main/chat_memory_03.png\" class=\"logo\" width=\"500\"/>"
   ],
   "metadata": {
    "id": "3NegS9Zj2Vdq"
   },
   "id": "3NegS9Zj2Vdq"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Basismodell\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "0qrkTzsTlVoj"
   },
   "id": "0qrkTzsTlVoj"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a689df",
   "metadata": {
    "id": "48a689df"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741fb627",
   "metadata": {
    "id": "741fb627"
   },
   "outputs": [],
   "source": [
    "# LLM initialisieren\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efc2e1b",
   "metadata": {
    "id": "6efc2e1b"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "StateGraph definieren\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Node: Ruft das Modell auf\n",
    "def agent_node(state: MessagesState):\n",
    "    \"\"\" Diese Funktion wird bei jedem Chat-Schritt aufgerufen. Der 'state' enth√§lt automatisch alle bisherigen Nachrichten.  \"\"\"\n",
    "    # System-Prompt hinzuf√ºgen\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n",
    "    ] + state[\"messages\"]\n",
    "\n",
    "    # LLM aufrufen\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    # R√ºckgabe wird automatisch zu state[\"messages\"] hinzugef√ºgt\n",
    "    return {\"messages\": [response]}"
   ],
   "metadata": {
    "id": "sYQQ2hP2l-k-"
   },
   "id": "sYQQ2hP2l-k-",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Graph-Workflow erstellen\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Dieser Knoten f√ºhrt die Funktion \"agent_node\" aus, wenn er aufgerufen wird\n",
    "# \"agent_node\" ist typischerweise die Funktion, die das LLM aufruft\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "\n",
    "# Startkante definieren\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Endkante definieren\n",
    "workflow.add_edge(\"agent\", END)\n",
    "\n",
    "# MemorySaver speichert den Konversations-Zustand zwischen verschiedenen Aufrufen,\n",
    "# Erm√∂glicht es dem Agenten, sich an fr√ºhere Nachrichten zu \"erinnern\"\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Graph kompilieren\n",
    "# Wandelt die Workflow-Definition in eine ausf√ºhrbare Anwendung um\n",
    "runner =  workflow.compile(checkpointer=memory)"
   ],
   "metadata": {
    "id": "Ehbr0XpplzBB"
   },
   "id": "Ehbr0XpplzBB",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Visualisiere als PNG\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ],
   "metadata": {
    "id": "QyW9IdJ5XLWf"
   },
   "id": "QyW9IdJ5XLWf",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1b46835a",
   "metadata": {
    "id": "1b46835a"
   },
   "source": [
    "**Wichtig:** Der `MessagesState` verwaltet die Nachrichten automatisch!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae84abc",
   "metadata": {
    "id": "fae84abc"
   },
   "outputs": [],
   "source": [
    "def chat(thread_id, user_input):\n",
    "    \"\"\"Chattet mit dem Bot in einem bestimmten Thread.\"\"\"\n",
    "\n",
    "    # Config mit Thread-ID (wie Session-ID)\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "    # Input vorbereiten\n",
    "    input_message = {\"messages\": [HumanMessage(content=user_input)]}\n",
    "\n",
    "    # Graph aufrufen - Memory wird automatisch verwaltet!\n",
    "    result = runner.invoke(input_message, config=config)\n",
    "\n",
    "    # Ausgabe\n",
    "    mprint(f\"**üßë‚Äçü¶± [Thread: {thread_id}] Mensch:**  \\n{user_input}\")\n",
    "    mprint(f\"**ü§ñ [Thread: {thread_id}] KI:**  \\n{result['messages'][-1].content}\\n\")\n",
    "\n",
    "    return result['messages'][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Einzelne Thread/Konversation\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "wo3td0eonq5p"
   },
   "id": "wo3td0eonq5p"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a25676",
   "metadata": {
    "id": "27a25676"
   },
   "outputs": [],
   "source": [
    "# Konversation starten\n",
    "chat(\"Max_session\", \"Hallo! Mein Name ist Max.\")\n",
    "chat(\"Max_session\", \"Wieviele Tage hat das Jahr?.\")\n",
    "chat(\"Max_session\", \"Wei√üt du noch, wie ich hei√üe und wo ich wohne?\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e583562",
   "metadata": {
    "id": "2e583562"
   },
   "source": [
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Multi-User (Parallele Threads)\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f87677",
   "metadata": {
    "id": "10f87677"
   },
   "outputs": [],
   "source": [
    "# Thread 1: Max\n",
    "chat(\"thread_max\", \"Ich mag Python-Programmierung!\")\n",
    "\n",
    "# Thread 2: Emma (komplett separate Konversation)\n",
    "chat(\"thread_emma\", \"Ich interessiere mich f√ºr Machine Learning!\")\n",
    "\n",
    "# Zur√ºck zu Thread 1 - Memory bleibt erhalten!\n",
    "chat(\"thread_max\", \"Was war nochmal mein Interesse?\")\n",
    "\n",
    "# Zur√ºck zu Thread 2\n",
    "chat(\"thread_emma\", \"An welchem Thema bin ich interessiert?\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87e3878",
   "metadata": {
    "id": "c87e3878"
   },
   "source": [
    "‚úÖ **Jeder Thread hat seine eigene, isolierte Historie!**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Thread-Historie ansehen\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "T-ZsmN4BQViL"
   },
   "id": "T-ZsmN4BQViL"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031aba17",
   "metadata": {
    "id": "031aba17"
   },
   "outputs": [],
   "source": [
    "def show_thread_history(thread_id):\n",
    "    \"\"\"Zeigt die komplette Historie eines Threads.\"\"\"\n",
    "\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    state = runner.get_state(config)\n",
    "\n",
    "    mprint(f\"### üìù Thread '{thread_id}' - {len(state.values['messages'])} Nachrichten:\\n\")\n",
    "    mprint(\"---\")\n",
    "\n",
    "    for msg in state.values[\"messages\"]:\n",
    "        role = \"üßë‚Äçü¶±\" if msg.type == \"human\" else \"ü§ñ\"\n",
    "        mprint(f\"{role} {msg.type.upper()}: {msg.content}\")\n",
    "\n",
    "# Beispiel\n",
    "show_thread_history(\"thread_max\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "show_thread_history(\"thread_emma\")"
   ],
   "metadata": {
    "id": "H5b5uS4uQaVA"
   },
   "id": "H5b5uS4uQaVA",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3z2vlo9oy0k",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "üíº F√ºr Production: Automatische Thread-IDs\n",
    "</font></p>\n",
    "\n",
    "\n",
    "In echten Anwendungen sollte man eindeutige Thread-IDs automatisch generieren:"
   ],
   "metadata": {
    "id": "3z2vlo9oy0k"
   }
  },
  {
   "cell_type": "code",
   "id": "gb2yd9ey72u",
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "def create_new_conversation(user_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Erstellt eine neue Konversations-ID f√ºr einen User.\n",
    "\n",
    "    Format: username_timestamp_uuid\n",
    "    Beispiel: Max_20250127_a1b2c3d4\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "    unique_id = uuid.uuid4().hex[:8]\n",
    "    thread_id = f\"{user_name}_{timestamp}_{unique_id}\"\n",
    "    return thread_id\n",
    "\n",
    "# Beispiel: Neue Konversationen f√ºr verschiedene User\n",
    "thread_max = create_new_conversation(\"Max\")\n",
    "thread_emma = create_new_conversation(\"emma\")\n",
    "\n",
    "print(f\"Max's neue Konversation: {thread_max}\")\n",
    "print(f\"Emma's neue Konversation: {thread_emma}\")\n",
    "\n",
    "# Verwendung\n",
    "chat(thread_max, \"Hallo, ich bin Max!\")\n",
    "chat(thread_emma, \"Hi, ich bin Emma!\")\n",
    "print()"
   ],
   "metadata": {
    "id": "gb2yd9ey72u"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "chat(thread_emma, \"Wie ist mein Name?\")\n",
    "print()"
   ],
   "metadata": {
    "id": "vYCgFEx-dBwB"
   },
   "id": "vYCgFEx-dBwB",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# 3 | Memory Management Strategien\n",
    "---"
   ],
   "metadata": {
    "id": "uiZsQbpKpOdG"
   },
   "id": "uiZsQbpKpOdG"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem:** Lange Konversationen sprengen das Kontextfenster und werden teuer.\n",
    "\n",
    "**L√∂sung:** Intelligentes Memory-Management\n"
   ],
   "metadata": {
    "id": "IAmGAVU9pSC6"
   },
   "id": "IAmGAVU9pSC6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Funktionsstruktur\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "2S71VA1oG5J_"
   },
   "id": "2S71VA1oG5J_"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ralf-42/Image/main/chat_memory_02.png\" class=\"logo\" width=\"850\"/>"
   ],
   "metadata": {
    "id": "BnFb2jbiGiL3"
   },
   "id": "BnFb2jbiGiL3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 3.1 | Trimming\n",
    "\n",
    "**Strategie:** Behalte nur die letzten *n* Token oder *n* Nachrichten."
   ],
   "metadata": {
    "id": "DUJajn7jqH7G"
   },
   "id": "DUJajn7jqH7G"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Tokenbasiertes Trimming\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "FD1zxBlXteN1"
   },
   "id": "FD1zxBlXteN1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b363c83",
   "metadata": {
    "id": "1b363c83"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import trim_messages\n",
    "\n",
    "# Trimming: Nur die letzten n Token behalten\n",
    "def agent_node_with_trimming(state: MessagesState):\n",
    "    \"\"\"Ruft das Modell mit getrimmter Historie auf.\"\"\"\n",
    "\n",
    "    # System-Prompt\n",
    "    system_msg = {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n",
    "\n",
    "    # Nur die letzten Nachrichten basierend auf MAX_TOKENS_FOR_TRIM\n",
    "    trimmed = trim_messages(\n",
    "        state[\"messages\"],\n",
    "        max_tokens=MAX_TOKENS_FOR_TRIM,\n",
    "        strategy=\"last\",\n",
    "        include_system=False\n",
    "    )\n",
    "\n",
    "    messages = [system_msg] + trimmed\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Graph mit Trimming\n",
    "workflow_trimmed = StateGraph(state_schema=MessagesState)\n",
    "workflow_trimmed.add_node(\"agent\", agent_node_with_trimming)\n",
    "workflow_trimmed.add_edge(START, \"agent\")\n",
    "workflow_trimmed.add_edge(\"agent\", END)\n",
    "\n",
    "app_trimmed = workflow_trimmed.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Nachrichtenbasiertes Trimming\n",
    "</font></p>\n",
    "\n",
    "\n",
    "```\n",
    "from langchain_core.messages import trim_messages\n",
    "\n",
    "# Anzahl der Nachrichten begrenzen\n",
    "trimmed_messages = trim_messages(\n",
    "    messages,\n",
    "    max_tokens=10,           # ‚Üê Anzahl der Nachrichten\n",
    "    strategy=\"last\",\n",
    "    token_counter=len,       # ‚Üê len() z√§hlt Nachrichten statt Token\n",
    "    include_system=True      # System-Nachricht immer behalten\n",
    ")\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "id": "_ogF8plhtJv-"
   },
   "id": "_ogF8plhtJv-"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 | Summarization\n",
    "\n",
    "**Strategie:** Fasse alte Nachrichten zusammen, behalte nur Zusammenfassung + letzte Nachrichten."
   ],
   "metadata": {
    "id": "viOi5m8kqpQn"
   },
   "id": "viOi5m8kqpQn"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Grundprinzip\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "WAc6FGCDrOIj"
   },
   "id": "WAc6FGCDrOIj"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff0a7d5",
   "metadata": {
    "id": "0ff0a7d5"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Zusammenfassungs-Prompt\n",
    "summarize_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Fasse die folgende Konversation in 2-3 S√§tzen zusammen. Behalte wichtige Fakten.\"),\n",
    "    (\"human\", \"{conversation}\")\n",
    "])\n",
    "\n",
    "def summarize_conversation(messages):\n",
    "    \"\"\"Erstellt eine Zusammenfassung der Nachrichten.\"\"\"\n",
    "\n",
    "    # Konversation als Text formatieren\n",
    "    conversation_text = \"\\n\".join([\n",
    "        f\"{msg.type}: {msg.content}\" for msg in messages\n",
    "    ])\n",
    "\n",
    "    # Zusammenfassung erstellen\n",
    "    summary_chain = summarize_prompt | llm | StrOutputParser()\n",
    "    summary = summary_chain.invoke({\"conversation\": conversation_text})\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Beispiel\n",
    "old_messages = [\n",
    "    HumanMessage(content=\"Mein Name ist Max\"),\n",
    "    AIMessage(content=\"Hallo Max!\"),\n",
    "    HumanMessage(content=\"Ich wohne in K√∂ln\"),\n",
    "    AIMessage(content=\"K√∂ln ist toll!\"),\n",
    "]\n",
    "\n",
    "summary = summarize_conversation(old_messages)\n",
    "mprint(f\"###üìù Zusammenfassung: \\n{summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6cb2d6",
   "metadata": {
    "id": "5a6cb2d6"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Basismodell mit LangGraph\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d60d1a",
   "metadata": {
    "id": "c7d60d1a"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "class StateWithSummary(MessagesState):\n",
    "    \"\"\"State mit zus√§tzlichem Summary-Feld.\"\"\"\n",
    "    summary: str = \"\"\n",
    "\n",
    "def agent_node_with_summary(state: StateWithSummary):\n",
    "    \"\"\"Nutzt Zusammenfassung statt alter Nachrichten.\"\"\"\n",
    "\n",
    "    # Wenn mehr als MAX_MESSAGES_BEFORE_TRIM Nachrichten: Summarize\n",
    "    if len(state[\"messages\"]) > MAX_MESSAGES_BEFORE_TRIM:\n",
    "        # Erste MESSAGES_TO_SUMMARIZE Nachrichten zusammenfassen\n",
    "        to_summarize = state[\"messages\"][:MESSAGES_TO_SUMMARIZE]\n",
    "        summary = summarize_conversation(to_summarize)\n",
    "\n",
    "        # Nur Summary + letzte RECENT_MESSAGES_TO_KEEP Nachrichten verwenden\n",
    "        messages = [\n",
    "            SystemMessage(content=f\"{SYSTEM_PROMPT}\\n\\nBisheriger Kontext: {summary}\"),\n",
    "        ] + state[\"messages\"][-RECENT_MESSAGES_TO_KEEP:]\n",
    "\n",
    "        if DEBUG_MODE:\n",
    "            print(f\"üìù Zusammenfassung erstellt: {summary}\")\n",
    "    else:\n",
    "        messages = [SystemMessage(content=SYSTEM_PROMPT)] + state[\"messages\"]\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Graph mit Summary-Funktion erstellen\n",
    "workflow_summary = StateGraph(state_schema=StateWithSummary)\n",
    "workflow_summary.add_node(\"agent\", agent_node_with_summary)\n",
    "workflow_summary.add_edge(START, \"agent\")\n",
    "workflow_summary.add_edge(\"agent\", END)\n",
    "\n",
    "# Mit Memory kompilieren\n",
    "app_summary = workflow_summary.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m6ok98mh73q",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Einsatz\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "m6ok98mh73q"
   }
  },
  {
   "cell_type": "code",
   "id": "ke9opfopu6",
   "source": [
    "def chat_with_summary(thread_id, user_input):\n",
    "    \"\"\"Chattet mit dem Bot unter Verwendung der Summary-Strategie.\"\"\"\n",
    "\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    input_message = {\"messages\": [HumanMessage(content=user_input)]}\n",
    "\n",
    "    result = app_summary.invoke(input_message, config=config)\n",
    "\n",
    "    mprint(f\"### üßë‚Äçü¶± Mensch: \\n{user_input}\")\n",
    "    mprint(f\"### ü§ñ KI: \\n{result['messages'][-1].content}\\n\")\n",
    "\n",
    "    return result['messages'][-1].content\n"
   ],
   "metadata": {
    "id": "ke9opfopu6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Beispiel: Lange Konversation (mehr als MAX_MESSAGES_BEFORE_TRIM)\n",
    "thread = \"summary_test\"\n",
    "\n",
    "# Erste Nachrichten\n",
    "chat_with_summary(thread, \"Mein Name ist Max\")\n",
    "chat_with_summary(thread, \"Ich wohne in K√∂ln\")\n",
    "chat_with_summary(thread, \"Ich arbeite als Entwickler\")\n",
    "chat_with_summary(thread, \"Mein Hobby ist Wandern\")\n",
    "chat_with_summary(thread, \"Ich mag Python\")\n",
    "\n",
    "# Viele weitere Nachrichten hinzuf√ºgen...\n",
    "for i in range(6):\n",
    "    chat_with_summary(thread, f\"Dies ist Test-Nachricht Nummer {i+1}\")\n",
    "\n",
    "# Nach vielen Nachrichten - sollte trotzdem wichtige Infos kennen\n",
    "chat_with_summary(thread, \"Wie hei√üe ich und wo wohne ich?\")\n",
    "print()"
   ],
   "metadata": {
    "id": "tupnKWvQriK4"
   },
   "id": "tupnKWvQriK4",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 3.3 | Wann welche Strategie?\n",
    "\n",
    "| Szenario | Empfehlung |\n",
    "|----------|-----------|\n",
    "| **Kurze Chats (<10 Nachrichten)** | Keine Optimierung n√∂tig |\n",
    "| **Support-Bot (20-50 Nachrichten)** | Trimming (letzte 20) |\n",
    "| **Langzeit-Assistent (100+ Nachrichten)** | Summarization + Trimming |\n",
    "| **Fakten-basierte Chats** | Externe Memory (RAG) |\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "id": "AdQmdk1PtyTi"
   },
   "id": "AdQmdk1PtyTi"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4 | Externes Memory (RAG Ausblick)\n",
    "---"
   ],
   "metadata": {
    "id": "SuaGnM9WtvZq"
   },
   "id": "SuaGnM9WtvZq"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "**Problem:** Auch mit Summarization sind sehr lange Konversationen oder gro√üe Wissensbasen problematisch.\n",
    "\n",
    "**L√∂sung:** Retrieval-Augmented Generation (RAG)"
   ],
   "metadata": {
    "id": "mMBuwFAQt6SJ"
   },
   "id": "mMBuwFAQt6SJ"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Konzept\n",
    "</font></p>\n",
    "\n",
    "Statt alles im Chat-Verlauf zu speichern:\n",
    "1. **Speichere Fakten in Vektordatenbank** (Chroma, Pinecone, FAISS)\n",
    "2. **Bei Bedarf relevante Infos abrufen**\n",
    "3. **Nur relevantes zur LLM-Anfrage hinzuf√ºgen**\n",
    "\n",
    "```\n",
    "User: \"Was war mein erstes Projekt?\"\n",
    "\n",
    "1. Semantische Suche in Vektordatenbank\n",
    "2. Finde: \"Projekt X von 2020...\"\n",
    "3. F√ºge nur relevante Infos zum Prompt hinzu\n",
    "4. LLM antwortet mit Kontext\n",
    "```\n"
   ],
   "metadata": {
    "id": "pdp3aRGEt8fF"
   },
   "id": "pdp3aRGEt8fF"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Unterschied zu Chat-Memory\n",
    "</font></p>\n",
    "\n",
    "| Aspekt | Chat-Memory | Externes Memory (RAG) |\n",
    "|--------|-------------|----------------------|\n",
    "| **Zweck** | Gespr√§chskontext | Wissensdatenbank |\n",
    "| **Speicherart** | Chronologisch | Semantisch durchsuchbar |\n",
    "| **Gr√∂√üe** | Klein (KB-MB) | Gro√ü (GB-TB) |\n",
    "| **Zugriff** | Sequenziell | Relevanz-basiert |\n",
    "| **Multi-User** | Getrennt pro User | Geteilt |\n",
    "| **Beispiel** | \"Du hei√üt Max\" | \"Python ist eine Programmiersprache\" |\n"
   ],
   "metadata": {
    "id": "QKWUHjBouBub"
   },
   "id": "QKWUHjBouBub"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Vereinfachtes Beispiel\n",
    "</font></p>\n"
   ],
   "metadata": {
    "id": "miQqRZG0uGmp"
   },
   "id": "miQqRZG0uGmp"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f564c7",
   "metadata": {
    "id": "c9f564c7"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Vektordatenbank erstellen\n",
    "vectorstore = Chroma.from_texts(\n",
    "    texts=[\n",
    "        \"Max hat 2020 ein Chatbot-Projekt gemacht\",\n",
    "        \"Maxs Lieblingssprache ist Python\",\n",
    "        \"Max wohnt in K√∂ln\"\n",
    "    ],\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Bei Bedarf Erinnerungen abrufen\n",
    "def retrieve_memory(query: str, k: int = 2):\n",
    "    \"\"\"Sucht relevante Erinnerungen in der Vektordatenbank.\"\"\"\n",
    "    docs = vectorstore.similarity_search(query, k=k)\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# In LangGraph integrieren\n",
    "def agent_node_with_rag(state: MessagesState):\n",
    "    \"\"\"Nutzt externe Wissensdatenbank (RAG) zus√§tzlich zur Chat-Historie.\"\"\"\n",
    "\n",
    "    user_query = state[\"messages\"][-1].content\n",
    "\n",
    "    # Relevante Erinnerungen abrufen\n",
    "    memories = retrieve_memory(user_query)\n",
    "\n",
    "    if DEBUG_MODE:\n",
    "        print(f\"üîç Abgerufene Erinnerungen: {memories}\")\n",
    "\n",
    "    # Als Kontext hinzuf√ºgen\n",
    "    messages = [\n",
    "        SystemMessage(content=f\"{SYSTEM_PROMPT}\\n\\nRelevante Informationen: {memories}\")\n",
    "    ] + state[\"messages\"]\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Graph mit RAG erstellen\n",
    "workflow_rag = StateGraph(state_schema=MessagesState)\n",
    "workflow_rag.add_node(\"agent\", agent_node_with_rag)\n",
    "workflow_rag.add_edge(START, \"agent\")\n",
    "workflow_rag.add_edge(\"agent\", END)\n",
    "\n",
    "runner_rag = workflow_rag.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x4wq19vm0vf",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Anwendung\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "x4wq19vm0vf"
   }
  },
  {
   "cell_type": "code",
   "id": "dyw1mjhiw2q",
   "source": [
    "def chat_with_rag(thread_id: str, user_input: str):\n",
    "    \"\"\"Chattet mit dem Bot unter Verwendung von RAG.\"\"\"\n",
    "\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    input_message = {\"messages\": [HumanMessage(content=user_input)]}\n",
    "\n",
    "    result = runner_rag.invoke(input_message, config=config)\n",
    "\n",
    "    print(f\"üßë‚Äçü¶± Mensch: {user_input}\")\n",
    "    print(f\"ü§ñ KI: {result['messages'][-1].content}\\n\")\n",
    "\n",
    "    return result['messages'][-1].content\n",
    "\n",
    "# Beispiel: Die KI kann auf externe Wissensbasis zugreifen\n",
    "chat_with_rag(\"rag_test\", \"Wann hat Max sein Chatbot-Projekt gemacht?\")\n",
    "chat_with_rag(\"rag_test\", \"Welche Programmiersprache mag Max?\")\n",
    "chat_with_rag(\"rag_test\", \"Fasse zusammen, was du √ºber Max wei√üt.\")"
   ],
   "metadata": {
    "id": "dyw1mjhiw2q"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b079f97d",
   "metadata": {
    "id": "b079f97d"
   },
   "source": [
    "# 5 | Mehr Tokens ‚â† Bessere Chat-Memory\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "**Mythos:** \"Gr√∂√üeres Kontextfenster = Besseres Memory\"\n",
    "\n",
    "**Realit√§t:** Nicht unbedingt!\n",
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Warum mehr Tokens problematisch sind\n",
    "</font></p>\n",
    "\n",
    "\n",
    "**Rauschen und Irrelevanz**\n",
    "```\n",
    "10 Nachrichten: ‚úÖ Alles relevant\n",
    "100 Nachrichten: ‚ö†Ô∏è 80% irrelevant f√ºr aktuelle Frage\n",
    "1000 Nachrichten: ‚ùå Wichtige Infos gehen unter\n",
    "```\n",
    "\n",
    "**Abnehmende Aufmerksamkeit**\n",
    "\n",
    "LLMs nutzen Attention-Mechanismen:\n",
    "- Bei kurzen Kontexten: Hohe Aufmerksamkeit auf jedes Detail\n",
    "- Bei langen Kontexten: Attention verteilt sich d√ºnn\n",
    "- Wichtige Infos in der Mitte werden oft \"√ºbersehen\" (\"Lost in the middle\" Problem)\n",
    "\n",
    "**Kosten**\n",
    "\n",
    "```\n",
    "GPT-4o mini Input-Kosten (1M Tokens)\n",
    "\n",
    "Chat mit 1.000 Tokens:  $0.15 / 1000 = $0.00015\n",
    "Chat mit 10.000 Tokens: $0.15 / 100 = $0.0015    (10x teurer!)\n",
    "Chat mit 100.000 Tokens: $0.15 / 10 = $0.015     (100x teurer!)\n",
    "```\n",
    "\n",
    "**Latenz**\n",
    "\n",
    "Mehr Tokens = L√§ngere Verarbeitung = Langsamere Antworten\n"
   ],
   "metadata": {
    "id": "oteU1ry1ufZy"
   },
   "id": "oteU1ry1ufZy"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Best Practices\n",
    "</font></p>\n",
    "\n",
    "\n",
    "‚úÖ **DO:**\n",
    "- Intelligentes Trimming verwenden\n",
    "- Alte Nachrichten zusammenfassen\n",
    "- RAG f√ºr Faktenwissen nutzen\n",
    "- Nur relevante Infos zum Context hinzuf√ºgen\n",
    "\n",
    "‚ùå **DON'T:**\n",
    "- Einfach alles in den Context packen\n",
    "- \"Mehr Tokens l√∂sen das Problem\" denken\n",
    "- Memory-Management ignorieren\n",
    "\n",
    "**Fazit**\n",
    "\n",
    "> Ein gro√ües Kontextfenster allein l√∂st nicht die Herausforderungen von Chat & Memory. Entscheidend sind clevere Strategien, um Informationen zu selektieren, zusammenzufassen und zielgerichtet einzusetzen.\n",
    "\n"
   ],
   "metadata": {
    "id": "UDoTj22hu670"
   },
   "id": "UDoTj22hu670"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# A | Aufgaben\n",
    "---"
   ],
   "metadata": {
    "id": "NmzShqhgvKxP"
   },
   "id": "NmzShqhgvKxP"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Die Aufgabenstellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen.\n",
    "\n"
   ],
   "metadata": {
    "id": "OMlTlETEvNkG"
   },
   "id": "OMlTlETEvNkG"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Aufgabe 1: Multi-User Chatbot mit LangGraph\n",
    "</font></p>\n",
    "\n",
    "**Schwierigkeit:** ‚≠ê‚≠ê\n",
    "\n",
    "Erstellen Sie einen Chatbot mit LangGraph, der:\n",
    "- Mindestens 3 verschiedene User-Threads verwaltet\n",
    "- Jeden User beim Namen kennt\n",
    "- Die Interessen jedes Users speichert\n",
    "- Beim Thread-Wechsel den Kontext beh√§lt\n",
    "\n",
    "**Bonus:** Implementieren Sie eine Funktion, die alle aktiven Threads auflistet.\n"
   ],
   "metadata": {
    "id": "RP96J46hvXia"
   },
   "id": "RP96J46hvXia"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Aufgabe 2: Memory mit Trimming-Strategie\n",
    "</font></p>\n",
    "\n",
    "**Schwierigkeit:** ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "Implementieren Sie einen Chatbot, der:\n",
    "1. Automatisch auf die letzten 10 Nachrichten trimmt\n",
    "2. Dem User anzeigt, wenn Nachrichten \"vergessen\" wurden\n",
    "3. Eine Zusammenfassung der vergessenen Nachrichten erstellt\n",
    "\n",
    "**Erwartetes Verhalten:**\n",
    "```\n",
    "[Nach 12 Nachrichten]\n",
    "ü§ñ: \"Hinweis: Ich habe unsere ersten Nachrichten zusammengefasst, um den √úberblick zu behalten. Zusammenfassung: [...]\"\n",
    "```\n"
   ],
   "metadata": {
    "id": "Ghs86Fuqvdvs"
   },
   "id": "Ghs86Fuqvdvs"
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown"
   }
  },
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "collapsed_sections": [
    "l4gltzt9uv8",
    "uiZsQbpKpOdG",
    "DUJajn7jqH7G",
    "viOi5m8kqpQn",
    "AdQmdk1PtyTi",
    "SuaGnM9WtvZq",
    "b079f97d",
    "NmzShqhgvKxP"
   ]
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
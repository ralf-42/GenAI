{"cells":[{"cell_type":"markdown","metadata":{"id":"WFblwtmloVcG"},"source":["<p><font size=\"6\" color='grey'> <b>\n","\n","Generative KI. Verstehen. Anwenden. Gestalten.\n","</b></font> </br></p>"]},{"cell_type":"markdown","metadata":{"id":"6jJZ7wbdArVc"},"source":["<p><font size=\"5\" color='grey'> <b>\n","Large Language Models und Transformer\n","</b></font> </br></p>\n","\n","\n","---"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"QPamC0stfkk3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764673046324,"user_tz":-60,"elapsed":47574,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"85c9b023-8352-4e99-cd8f-7ef948adbf23"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úì OPENAI_API_KEY erfolgreich gesetzt\n","\n","Python Version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n","\n","Installierte LangChain-Bibliotheken:\n","langchain                                1.1.0\n","langchain-chroma                         1.0.0\n","langchain-classic                        1.0.0\n","langchain-community                      0.4.1\n","langchain-core                           1.1.0\n","langchain-ollama                         1.0.0\n","langchain-openai                         1.1.0\n","langchain-text-splitters                 1.0.0\n","\n","IP-Adresse: 35.224.232.30\n","Hostname: 30.232.224.35.bc.googleusercontent.com\n","Stadt: Council Bluffs\n","Region: Iowa\n","Land: US\n","Koordinaten: 41.2619,-95.8608\n","Provider: AS396982 Google LLC\n","Postleitzahl: 51502\n","Zeitzone: America/Chicago\n"]}],"source":["#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n","!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul\n","from genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages\n","setup_api_keys(['OPENAI_API_KEY'], create_globals=False)\n","print()\n","check_environment()\n","print()\n","get_ipinfo()"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"JIszuH3spe3M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764673072175,"user_tz":-60,"elapsed":3670,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"e40843c6-a6f8-4360-8102-6aedc02ce01a"},"outputs":[{"output_type":"stream","name":"stdout","text":["üîÑ Installiere pypdf...\n","‚úÖ pypdf erfolgreich installiert und importiert\n","üîÑ Installiere pdfkit...\n","‚úÖ pdfkit erfolgreich installiert und importiert\n","üîÑ Installiere markdown2...\n","‚úÖ markdown2 erfolgreich installiert und importiert\n","üîÑ Installiere weasyprint...\n","‚úÖ weasyprint erfolgreich installiert und importiert\n"]}],"source":["#@title üõ†Ô∏è Installationen { display-mode: \"form\" }\n","install_packages(['pypdf','pdfkit', 'markdown2', 'weasyprint'])"]},{"cell_type":"markdown","metadata":{"id":"dXyUNJDNOh5U"},"source":["# 1 | Large Language Models\n","---"]},{"cell_type":"markdown","metadata":{"id":"vwFflTpGObBC"},"source":["LLMs funktionieren im Wesentlichen durch die *Vorhersage des wahrscheinlichsten n√§chsten Tokens* in einem gegebenen Text, basierend auf einer riesigen Menge an Trainingsdaten aus dem Internet. Dieser Prozess wird durch Milliarden bis Billionen von Parametern gesteuert, die w√§hrend des Trainings optimiert werden. Ein entscheidender Aspekt bei vielen Modellen ist die Architektur von **Transformern**, die es LLMs erlaubt, den gesamten Text parallel zu verarbeiten, anstatt Token f√ºr Token, und dabei den Kontext durch einen Mechanismus namens **Self-Attention** zu ber√ºcksichtigen. Das Ergebnis ist ein System, das erstaunlich fl√ºssige und fast immer koh√§rente Texte generieren kann, obwohl das genaue *Warum* hinter den Vorhersagen aufgrund der Komplexit√§t und Gr√∂√üe des Modells schwer zu verstehen ist."]},{"cell_type":"markdown","metadata":{"id":"G0J30AzXgTfh"},"source":["**Was bedeutet \"Transformer**\n","\n","**Einfach erkl√§rt:** Transformer sind wie das \"Betriebssystem\" der meisten modernen KI-Sprachmodelle. Sie wurden 2017 entwickelt und haben die Art, wie Computer Text verstehen und erzeugen, revolutioniert.\n","\n","**Warum ist das wichtig?** Transformer-basierte Modelle haben drei gro√üe Vorteile:\n","- **Parallelisierung:** Sie k√∂nnen viele W√∂rter gleichzeitig verarbeiten (nicht nacheinander)\n","- **Attention-Mechanismus:** Sie erkennen wichtige Teile des Textes und k√∂nnen sich daruf \"fokussieren\"\n","- **Skalierbarkeit:** Sie funktionieren sowohl bei kleinen als auch bei sehr gro√üen Datenmengen"]},{"cell_type":"markdown","metadata":{"id":"IQpcGS-KrRvB"},"source":["<p><font color='black' size=\"5\">\n","Merkmale von LLMs\n","</font></p>"]},{"cell_type":"markdown","metadata":{"id":"HKQmU_0vrYBR"},"source":["\n","<p><font color='blue' size=\"4\">\n","Offene vs. geschlossene Modelle\n","</font></p>\n","\n","**Offene Modelle** (Open Source, offenes System),  bedeuten, dass die trainierten Parameter eines Modells f√ºr die √ñffentlichkeit zug√§nglich sind. Dadurch haben Wissenschaftler und Entwickler die M√∂glichkeit, die Mechanismen des Modells nachzuvollziehen, Forschungsarbeiten zu reproduzieren sowie Anpassungen oder Verbesserungen vorzunehmen.   \n","**Geschlossene Modelle** (Closed Source, propriet√§res System) hingegen sind Modelle, bei denen der Zugriff auf die Parameter eingeschr√§nkt ist. Unternehmen setzen diese oft in kommerziellen Produkten oder Dienstleistungen ein, da die Ver√∂ffentlichung der Gewichte gesch√§ftliche Interessen oder den Schutz der Nutzerdaten beeintr√§chtigen k√∂nnt\n"]},{"cell_type":"markdown","metadata":{"id":"CWh1v8R-rswx"},"source":["\n","<p><font color='blue' size=\"4\">\n","Parametergewichte\n","</font></p>\n","\n","Die Anzahl der Parametergewichte eines Modells gibt Aufschluss √ºber dessen Kapazit√§t und Komplexit√§t. Sie wird typischerweise in Millionen (M), Milliarden (B) oder Billionen (T) angegeben. Eine h√∂here Parameteranzahl erh√∂ht grunds√§tzlich die F√§higkeit des Modells, komplexe Zusammenh√§nge und feine Unterschiede in den Daten zu erkennen. Allerdings ist dies kein eindeutiger Indikator f√ºr die Leistungsf√§higkeit in jeder Anwendung, sondern eher ein Hinweis auf das allgemeine Potenzial des Modells.  \n","\n","\n","**Vorteile:**  \n","* **Gr√∂√üere Lernf√§higkeit:** Modelle mit mehr Parametern k√∂nnen feinere und differenziertere Muster in Daten erfassen, was ihre Pr√§zision und Wirksamkeit bei verschiedenen Aufgaben steigern kann.  \n","* **Bessere Anpassungsf√§higkeit:** Umfangreichere Modelle haben oft eine st√§rkere F√§higkeit zur Generalisierung und k√∂nnen unter geeigneten Trainingsbedingungen auch auf neue, unbekannte Daten besser reagieren.  \n","\n","**Nachteile:**  \n","* **Hoher Rechenaufwand:** Eine gr√∂√üere Anzahl an Parametern erfordert mehr Rechenleistung f√ºr Training und Inferenz, was leistungsf√§hige Hardware und l√§ngere Verarbeitungszeiten n√∂tig macht.  \n","* **Gefahr der √úberanpassung:** Ohne angemessene Regularisierungstechniken besteht die Gefahr, dass das Modell sich zu stark an die Trainingsdaten anpasst und bei neuen, variierenden Datens√§tzen schlechter abschneidet.  \n","* **Umweltbelastung:** Das Training umfangreicher Modelle verbraucht erheblich mehr Energie, was zu einem h√∂heren CO‚ÇÇ-Aussto√ü f√ºhrt.\n"]},{"cell_type":"markdown","metadata":{"id":"vIGxe0GCrvHQ"},"source":["\n","<p><font color='blue' size=\"4\">\n","Kontextfenstergr√∂√üe\n","</font></p>\n","\n","Die Kontextfenstergr√∂√üe eines Modells gibt an, wie viele Token (W√∂rter oder Wortbestandteile) es bei der Generierung oder Vorhersage von Text gleichzeitig verarbeiten kann. Diese Eigenschaft ist aus mehreren Gr√ºnden essenziell:\n","\n","+ **Erweiterter Kontext**: Ein gr√∂√üeres Kontextfenster erlaubt es dem Modell, mehr Informationen zu ber√ºcksichtigen, was zu koh√§renteren und inhaltlich pr√§ziseren Ergebnissen f√ºhrt. Dies ist besonders vorteilhaft f√ºr Aufgaben, die lange Texte oder komplexe Zusammenh√§nge erfordern.\n","+ **Erfassung von Abh√§ngigkeiten**: Ein Modell mit gr√∂√üerem Kontextfenster kann weiter zur√ºckliegende Zusammenh√§nge im Text besser erfassen, was seine Leistungsf√§higkeit bei Aufgaben wie Textzusammenfassungen, Frage-Antwort-Systemen oder interaktiven Dialogen erheblich steigert.\n","\n","Ein fundiertes Verst√§ndnis dieser Faktoren hilft bei der Auswahl eines geeigneten Modells f√ºr spezifische Anwendungsf√§lle und erm√∂glicht eine gezielte Optimierung der Leistung.\n"]},{"cell_type":"markdown","metadata":{"id":"EbzDtUyurw-W"},"source":["\n","<p><font color='blue' size=\"4\">\n","Token\n","</font></p>\n","\n","In Large Language Models (LLMs) wie GPT (Generative Pre-trained Transformer) stellen Token die grundlegenden Einheiten des verarbeiteten Textes dar. Ein **Token** kann ein vollst√§ndiges Wort, ein Wortbestandteil oder ein einzelnes Zeichen sein. Wie genau ein Token definiert wird, h√§ngt vom verwendeten Tokenizer ab, der w√§hrend des Modelltrainings zum Einsatz kam. So k√∂nnte das Wort *Hausboot* entweder als einzelnes Token betrachtet oder in die Bestandteile *Haus* und *boot* zerlegt werden - abh√§ngig von der jeweiligen Tokenisierungsstrategie.  \n","\n","[OpenAI Tokenizer](https://platform.openai.com/tokenizer)\n","\n","Die Nutzungskosten eines LLMs zur Textgenerierung werden in der Regel anhand der verarbeiteten Token berechnet. Dabei z√§hlen sowohl die Token aus der Eingabe als auch diejenigen, die das Modell als Antwort generiert. Da die Verarbeitung jedes Tokens insbesondere bei sehr gro√üen Modellen mit Milliarden von Parametern erhebliche Rechenressourcen erfordert, beeinflusst die Anzahl der Token direkt die Rechenkosten. Daher ist ein effizientes Management der Token-Nutzung essenziell, um Kosten zu optimieren.  \n","\n"]},{"cell_type":"markdown","metadata":{"id":"aO6nyZl0rynF"},"source":["\n","<p><font color='blue' size=\"4\">\n","Temperatur\n","</font></p>\n","\n","Bei LLM wie GPT bezieht sich OpenAI mit dem Begriff **Temperatureinstellung** auf die Steuerung der **Zuf√§lligkeit** oder **Kreativit√§t** der generierten Antworten. Eine niedrigere Temperatur (z. B. 0,0) sorgt f√ºr **deterministische** und vorhersehbare Ergebnisse, da das Modell vorrangig die wahrscheinlichsten Antworten w√§hlt. Eine h√∂here Temperatur (z. B. 1,0) hingegen f√ºhrt zu einer gr√∂√üeren Varianz und kann **kreativere**, aber auch weniger vorhersehbare Antworten erzeugen. Der Wertebereich variiert je nach Anwendung oder Benutzeroberfl√§che, liegt jedoch typischerweise zwischen 0 und 1. Durch die Anpassung der Temperatur l√§sst sich das Modell gezielt steuern, um ein passendes Verh√§ltnis zwischen Konsistenz und Originalit√§t f√ºr unterschiedliche Aufgaben oder Nutzerpr√§ferenzen zu erreichen.\n"]},{"cell_type":"markdown","source":["<details>\n","\n","**üîß Was macht *Temperatur* technisch?**\n","\n","Jedes m√∂gliche Wort besitzt eine Wahrscheinlichkeit  \n","(z. B. **‚Äûist‚Äú = 40 %**, **‚Äûwar‚Äú = 10 %**).\n","\n","Bei der Temperatur wird dieser Wert mathematisch **durch die Temperatur geteilt**  \n","und anschlie√üend **normalisiert**. So verschieben sich die Gewichte bewusst ‚Äì  \n","ohne neues Wissen, nur durch Verzerrung der Verteilung.\n","\n","---\n","\n","**üîÅ Wirkung der Temperatur auf Wahrscheinlichkeiten**\n","\n","| Temperatur | Effekt auf Verteilung |\n","|------------|------------------------|\n","| **< 1** | hohe Wahrscheinlichkeiten werden **verst√§rkt**, kleine **abgesenkt** ‚Üí Modell entscheidet fast deterministisch |\n","| **> 1** | Wahrscheinlichkeiten werden **gleichm√§√üiger**, Unterschiede flachen ab ‚Üí mehr Vielfalt, mehr Risiko |\n","\n","---\n","\n","**üìå Beispiel: Wahrscheinlichkeiten `vor` der Temperatur**\n","\n","| Wort       | Urspr√ºngliche Wahrscheinlichkeit |\n","|------------|----------------------------------|\n","| ist        | 40 % |\n","| war        | 10 % |\n","| w√§re       | 5 %  |\n","| sonstige   | 45 % *(Summe aller anderen)* |\n","\n","---\n","\n","+ üîΩ Temperatur **0.5** ‚Äì  ‚Äûist‚Äú wird sehr dominant, fast alles andere verdr√§ngt ‚Üí  \n","**konservatives, faktisches Verhalten**.\n","\n","\n","\n","+ üîº Temperatur **1.5** ‚Äì Dominanz von ‚Äûist‚Äú sinkt, auch seltenere W√∂rter bekommen Chancen ‚Üí  \n","**kreativer, aber unsicherer Output**.\n","\n","\n"],"metadata":{"id":"lCauRUJVOAgF"}},{"cell_type":"markdown","source":["\n","<p><font color='blue' size=\"4\">\n","Top-p (Nucleus Sampling)\n","</font></p>\n","\n","* Es wird ein Teil der m√∂glichen Worte betrachtet ‚Äì nur diejenigen, deren gemeinsame Wahrscheinlichkeit z. B. 90 % (p = 0.9) ausmacht.\n","* Der Rest wird ignoriert.\n","* **Engere Auswahl = kontrollierter Output**\n","* **Breitere Auswahl = kreativer, aber riskanter**\n"],"metadata":{"id":"gHFSNvJ6MIlA"}},{"cell_type":"markdown","source":["\n","<p><font color='blue' size=\"4\">\n","Top-k\n","</font></p>\n","\n","* Es wird eine feste Anzahl der wahrscheinlichsten Optionen genommen ‚Äì z. B. nur die besten 50 (k = 50).\n","* Alles au√üerhalb dieser Top-50 wird nicht in Betracht gezogen.\n","* √Ñhnlich wie top-p, aber nicht dynamisch an die Wahrscheinlichkeiten angepasst.\n"],"metadata":{"id":"NJcjEarNMOdp"}},{"cell_type":"markdown","source":["\n","\n","<p><font color='blue' size=\"4\">\n","Zusammenspiel mit Temperatur\n","</font></p>\n","\n","Die Temperatur ver√§ndert, *wie stark* die Wahrscheinlichkeiten der Kandidaten gegl√§ttet oder verst√§rkt werden:\n","\n","| Einstellung                    | Wirkung                                                                |\n","| ------------------------------ | ---------------------------------------------------------------------- |\n","| **Hohe Temperatur** (>1.0)     | verteilt die Wahrscheinlichkeiten flacher ‚Üí mehr Vielfalt, mehr Zufall |\n","| **Niedrige Temperatur** (<1.0) | konzentriert auf vorhersehbare Worte ‚Üí konservativer, sicherer Output  |\n","| **Temperatur 0**               | deterministisch ‚Äì immer die wahrscheinlichste Option                   |\n"],"metadata":{"id":"bD8rAHmPMS4f"}},{"cell_type":"markdown","source":["\n","**Top-p und top-k greifen *nach* der Temperatur:**     \n","Erst wird durch die Temperatur die Verteilung manipuliert, dann w√§hlen top-p oder top-k aus, *welche Kandidaten √ºberhaupt zur Wahl stehen.*\n","\n"],"metadata":{"id":"RAB8HB0vM3xU"}},{"cell_type":"markdown","source":["\n","<p><font color='blue' size=\"4\">\n","Typischer Fehler\n","</font></p>\n","\n","Viele setzen *gleichzeitig* hohe Werte bei Temperatur **und** breite Filter (hohes p oder hohes k) ‚Äì das f√ºhrt schnell zu Beliebigkeit oder Chaos.\n","\n"],"metadata":{"id":"5n6BdXMFMV3d"}},{"cell_type":"markdown","source":["\n","\n","**üí° Empfehlung (je nach Ziel)**\n","\n","| Ziel               | Temperatur | top-p | top-k |\n","| ------------------ | ---------- | ----- | ----- |\n","| Kreativ            | 0.9        | 0.9   | 50+   |\n","| Ausgewogen         | 0.7        | 0.8   | 40    |\n","| Pr√§zise / Faktisch | 0‚Äì0.3      | 0.5   | 10‚Äì20 |\n","\n","\n"],"metadata":{"id":"7KjicIQCPxqE"}},{"cell_type":"markdown","source":["[Gl√ºcksrad: LLM Sampling Simulator](https://editor.p5js.org/ralf.bendig.rb/full/LBc3t3yP4)"],"metadata":{"id":"GcTgpj51fMdQ"}},{"cell_type":"markdown","metadata":{"id":"ExC3tumBDRd6"},"source":["<p><font color='blue' size=\"4\">\n","Gro√üe Sprachmodelle\n","</font></p>\n","\n","Die folgende Tabelle gibt einen √úberblickt f√ºr bekannte LLMs (Stand: 10.2025)"]},{"cell_type":"markdown","metadata":{"id":"9G5qie8mEFDP"},"source":["| Name                 | Ersteller   | Open/Closed | Input-Token | Output-Token | Anzahl Parameter   |\n","|----------------------|-------------|-------------|-------------|--------------|-------------------|\n","| GPT-5                | OpenAI      | Closed      | 1M          | 32K          | Unbekannt         |\n","| GPT-4o               | OpenAI      | Closed      | 128K        | 16K          | ~1.8T (gesch√§tzt) |\n","| o3                   | OpenAI      | Closed      | 200K        | 100K         | Unbekannt         |\n","| o4-mini              | OpenAI      | Closed      | 200K        | 100K         | Unbekannt         |\n","| Claude Sonnet 4.5    | Anthropic   | Closed      | 200K        | 8K           | Unbekannt         |\n","| Claude Opus 4.1      | Anthropic   | Closed      | 200K        | 8K           | Unbekannt         |\n","| Gemini 2.5 Pro       | Google      | Closed      | 1M          | 8K           | Unbekannt         |\n","| Gemini 2.5 Flash     | Google      | Closed      | 1M          | 8K           | Unbekannt         |\n","| Gemma 2 (27B)        | Google      | Open        | 8K          | 8K           | 27B               |\n","| Grok 4               | xAI         | Closed      | 128K        | 32K          | ~2.4T (gesch√§tzt) |\n","| Grok 4 Fast          | xAI         | Closed      | 2M          | 32K          | Unbekannt         |\n","| Llama 3.3 (70B)      | Meta        | Open        | 128K        | 8K           | 70B               |\n","| Llama 3.1 (405B)     | Meta        | Open        | 128K        | 8K           | 405B              |\n","| Mistral Large 2      | Mistral.AI  | Open        | 128K        | 8K           | 123B              |\n","| Mixtral 8x22B        | Mistral.AI  | Open        | 65K         | 8K           | 141B (39B aktiv)  |\n","| DeepSeek R1          | DeepSeek    | Open        | 128K        | 8K           | 671B (37B aktiv)  |\n","| DeepSeek V3          | DeepSeek    | Open        | 128K        | 8K           | 671B (37B aktiv)  |\n","| Qwen 2.5-Max         | Alibaba     | Open        | 32K         | 8K           | Unbekannt         |\n","| Command R+           | Cohere      | Closed      | 128K        | 4K           | 104B              |\n","| Phi-4                | Microsoft   | Open        | 16K         | 4K           | 14B               |\n","| Nemotron-4 (340B)    | Nvidia      | Open        | 32K         | 4K           | 340B              |"]},{"cell_type":"markdown","metadata":{"id":"LiNHLzJxwYbT"},"source":["**Im Kurs verwenden wir i.W.:**  \n","- **gpt-4o-mini**: Standardmodell f√ºr die meisten Aufgaben.  \n","- **gpt-4o**: F√ºr detailliertere Antworten.  \n","- **o3-mini**: Optimiert f√ºr logische Schlussfolgerungen.\n","- **dall-e-3**: Standard-Modell f√ºr Bildgenerierung\n","\n","Es k√∂nnen alternativ auch die anderen Modelle wie **gpt-5** oder **gpt-5-mini** eingesetzt werden."]},{"cell_type":"markdown","metadata":{"id":"XuN8JPN_BeOR"},"source":["[OpenAI Modelle](https://platform.openai.com/docs/models)"]},{"cell_type":"markdown","metadata":{"id":"pC9A-LaYhsta"},"source":["# 2 | Foundation Models\n","---"]},{"cell_type":"markdown","metadata":{"id":"rhiHq-tbpfRA"},"source":["Foundation Models sind gro√üe KI-Modelle, die auf riesigen Mengen an Daten trainiert werden. Sie bilden eine breite Basis an Wissen und F√§higkeiten, auf der sich spezialisierte Anwendungen aufbauen lassen. Dazu geh√∂ren nicht nur gro√üe Sprachmodelle (Large Language Models, LLMs) wie GPT, sondern auch Modelle f√ºr Bilder, Audio und andere Datentypen. Foundation Models werden oft im sogenannten *self-supervised learning* trainiert: Sie lernen Muster, Strukturen und Zusammenh√§nge in Daten, ohne dass diese manuell beschriftet sein m√ºssen.\n","\n","Ein gro√üer Vorteil dieser Modelle ist ihre vielseitige Einsetzbarkeit: Nach dem Training k√∂nnen sie an unterschiedlichste Aufgaben angepasst werden, etwa f√ºr Textzusammenfassungen, Bilderkennung oder sogar multimodale Anwendungen, die Text und Bild kombinieren. Dadurch sind Foundation Models zu einem wichtigen Baustein in der aktuellen KI-Entwicklung geworden.\n","\n","Mehrere f√ºhrende Technologieunternehmen und Forschungsorganisationen bieten Foundation Models an. Dazu geh√∂ren OpenAI mit GPT (Generative Pre-trained Transformer), Google mit BERT (Bidirectional Encoder Representations from Transformers) und Facebook (Meta) mit Modellen wie RoBERTa (Robustly Optimized BERT Pretraining Approach).\n","\n","Das Training eines Foundation Models ist extrem aufwendig: Es erfordert eine enorme Menge an Daten, die sorgf√§ltig gesammelt, bereinigt und verarbeitet werden m√ºssen. Au√üerdem sind daf√ºr spezialisierte Hardwaresysteme und sehr viel Rechenleistung notwendig, was die Entwicklung teuer und komplex macht. F√ºr Einzelpersonen oder (kleinere) Organisationen ist es daher oft sinnvoller, bestehende Foundation Models zu verwenden und sie gezielt f√ºr eigene Aufgaben zu optimieren (beispielsweise durch *Fine-Tuning*).\n","\n","In der praktischen Anwendungen geht es in der Regel darum, wie man vorhandene Foundation Models effektiv einsetzt, anpasst und f√ºr eigene Projekte nutzt. Denn ein vollst√§ndiges Modell selbst zu entwickeln, w√ºrde deutlich mehr Zeit und Ressourcen erfordern, als ein Kurs leisten kann.\n","\n","Trotz ihrer gro√üen M√∂glichkeiten bergen Foundation Models auch Herausforderungen, zum Beispiel hinsichtlich Verzerrungen in den Trainingsdaten oder unvorhersehbaren Ergebnissen. Ein bewusster und verantwortungsvoller Umgang mit diesen Modellen ist deshalb besonders wichtig.\n"]},{"cell_type":"markdown","metadata":{"id":"Cyp02R3Nvz0C"},"source":["![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/FoundationModels-klein.png)"]},{"cell_type":"markdown","metadata":{"id":"7atawInd9okS"},"source":["# 3 | Aufgaben Sprachverarbeitung\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"sfAaTidH99gy"},"source":["Large Language Models (LLMs) wie GPT √ºbernehmen eine Vielzahl von Aufgaben rund um die Verarbeitung nat√ºrlicher Sprache. Neben bekannten Anwendungsfeldern wie Textklassifizierung, Textzusammenfassung und Textgenerierung bieten sie auch Unterst√ºtzung bei Analyse, Transformation, Strukturierung und Wissensgewinnung aus Texten. Die folgende √úbersicht zeigt typische Aufgabenbereiche im √úberblick."]},{"cell_type":"markdown","metadata":{"id":"O3lXx1qa9r9S"},"source":["| **Kategorie**                        | **Aufgabe**                      | **Beschreibung**                                                                     |\n","| ------------------------------------ | -------------------------------- | ------------------------------------------------------------------------------------ |\n","| üß† **Analyse & Klassifikation**      | **Textklassifizierung**          | Einordnung eines Textes in vordefinierte Klassen (z.‚ÄØB. Spam/kein Spam).             |\n","|                                      | Sentimentanalyse                 | Einsch√§tzung der Stimmung (positiv/negativ/neutral).                                 |\n","|                                      | Themenklassifikation             | Zuordnung von Texten zu Themen oder Kategorien.                                      |\n","|                                      | **Named Entity Recognition (NER)**   | Erkennung von Namen, Orten, Organisationen usw. im Text.                             |\n","|                                      | Relationsextraktion              | Erkennen logischer oder semantischer Beziehungen zwischen Entit√§ten.                 |\n","|                                      | Bias- oder Hate-Speech-Erkennung | Klassifikation diskriminierender oder voreingenommener Sprache.                      |\n","| üìù **Generierung & Transformation**  | **Textgenerierung**              | Erzeugen von Texten auf Basis von Prompts (z.‚ÄØB. Geschichten, Antworten, Code).      |\n","|                                      | **Textzusammenfassung**          | Komprimierung eines Textes unter Erhalt der Kernaussage (extraktiv oder abstraktiv). |\n","|                                      | Paraphrasierung                  | Umformulierung bei gleichbleibendem Sinn.                                            |\n","|                                      | Stiltransfer                     | Umwandlung des Sprachstils (z.‚ÄØB. sachlich ‚Üî kreativ).                               |\n","|                                      | Textvereinfachung                | Vereinfachung komplexer Formulierungen.                                              |\n","|                                      | Sprach√ºbersetzung                | √úbersetzung zwischen Sprachen.                                                       |\n","| üß© **Erg√§nzung & Vervollst√§ndigung** | Autovervollst√§ndigung            | Vorhersage des n√§chsten Wortes oder Satzes.                                          |\n","|                                      | L√ºckentext-Erg√§nzung             | Einf√ºgen sinnvoller W√∂rter/S√§tze in L√ºcken.                                          |\n","|                                      | **Story- oder Textfortsetzung**      | Kreative Weiterf√ºhrung eines gegebenen Textes.                                       |\n","| üìö **Informationsgewinnung**         | Schl√ºsselbegriffextraktion       | Extraktion zentraler Begriffe aus einem Text.                                        |\n","|                                      | Themenextraktion                 | Identifikation der zentralen Themen eines Textes.                                    |\n","|                                      | Frage-Antwort (QA)               | Beantwortung von Fragen zu einem Text (extraktiv oder generativ).                    |\n","|                                      | Faktenpr√ºfung                    | Bewertung der Richtigkeit von Aussagen.                                              |\n","|                                      | **Retrieval-Augmented Generation**   | Kombination von Textgenerierung mit Wissensabfragen aus externen Quellen.            |\n","| üõ°Ô∏è **Sicherheit & Strukturierung**  | Plagiatserkennung                | Erkennung von √Ñhnlichkeiten mit bestehenden Texten.                                  |\n","|                                      | Anonymisierung                   | Entfernen oder Maskieren sensibler Daten.                                            |\n","|                                      | **Gliederungserstellung**            | Erstellung strukturierter Inhaltsverzeichnisse.                                      |\n","|                                      | Formatumwandlung                 | Konvertierung von Texten in bestimmte Formate (z.‚ÄØB. E-Mail, Bericht, Code).         |\n"]},{"cell_type":"markdown","metadata":{"id":"aQqiPx-V3Kio"},"source":["# 4 | Modellaufbau\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"tiFCe1fe3hEa"},"source":["Im vorliegenden Code wird ein Chatbot-Modell mit Chat-History schrittweise aufgebaut. Der Modellaufbau ist modular und kann als `Bauplan` f√ºr nachfolgende Konversationsmodelle verwendet werden.\n","\n","Diese strukturierte Aufteilung macht das Modell robust, wartbar und erweiterbar f√ºr verschiedene Anwendungsf√§lle im Bereich der Konversations-KI.\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"9y7B2qfuB2Xs","executionInfo":{"status":"ok","timestamp":1764680491701,"user_tz":-60,"elapsed":119,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Abschnitt 1: Importe\n","from langchain.chat_models import init_chat_model\n","from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.output_parsers.string import StrOutputParser"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"YJVTOv9TH2lT","executionInfo":{"status":"ok","timestamp":1764680573610,"user_tz":-60,"elapsed":31,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Abschnitt 2: Konstanten definieren\n","system_prompt = \"\"\"\n","Du bist ein kompetenter KI-Assistent mit breitem Fachwissen.\n","\n","Deine Antworten sind:\n","- Klar strukturiert und mit Markdown formatiert\n","- Praxisorientiert und direkt umsetzbar\n","- Basierend auf aktuellem Kenntnisstand\n","- Mit passenden Beispielen versehen\n","- In verst√§ndlicher Sprache formuliert\n","\n","Bei deiner Arbeit:\n","- Analysierst du Fragen sorgf√§ltig\n","- Gibst pr√§zise und relevante Antworten\n","- Erkennst den Kontext der Anfrage\n","- Bietest bei Bedarf weiterf√ºhrende Informationen\n","- Bleibst sachlich und neutral\n","\n","Formatiere alle Antworten in Markdown f√ºr optimale Lesbarkeit.\n","\"\"\""]},{"cell_type":"code","execution_count":8,"metadata":{"id":"AkkALStvH4ul","executionInfo":{"status":"ok","timestamp":1764680664297,"user_tz":-60,"elapsed":5,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Abschnitt 3: Chain-Komponenten initialisieren\n","\n","# Die Chat-Prompt definieren mit drei Variablen: system_prompt, history, user_input\n","prompt = ChatPromptTemplate.from_messages([\n","        (\"system\", \"{system_prompt}\"),\n","        MessagesPlaceholder(variable_name=\"chat_history\"),\n","        (\"human\", \"{user_input}\")\n","])\n","\n","# LLM definieren\n","model_provider=\"openai\"\n","model_name = \"gpt-4o-mini\"\n","temperature = 0.3\n","llm = init_chat_model(model_name, model_provider=model_provider, temperature=temperature)\n","\n","# OutputParser\n","parser = StrOutputParser()\n","\n","# Die Chain definieren\n","chain = prompt | llm | parser"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"kzhjBJEsH9h1","executionInfo":{"status":"ok","timestamp":1764680835281,"user_tz":-60,"elapsed":9,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Abschnitt 4: Funktionen definieren\n","\n","def interact_with_ai(system_prompt, chat_history, user_input):\n","    \"\"\"F√ºhrt eine einzelne Interaktion mit der KI durch.\"\"\"\n","\n","    # Aufruf der Kette\n","    parameter = {\n","        'system_prompt': system_prompt,\n","        'chat_history': chat_history,\n","        'user_input': user_input\n","    }\n","    response = chain.invoke(parameter)\n","\n","    # Ausgabe der Ergebnisse\n","    mprint(\"### üßë‚Äçü¶± Mensch:\")\n","    mprint(user_input)\n","\n","    mprint(\"### ü§ñ KI:\")\n","    mprint(response)\n","\n","    # Memory-Management - Erweiterung der Historie\n","    chat_history.extend([\n","        HumanMessage(content=user_input),\n","        AIMessage(content=response)\n","    ])\n","\n","    return chat_history"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"TGuKwTyVJ-wt","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1764681223870,"user_tz":-60,"elapsed":29251,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"aac29d37-8122-4690-c09a-e524cd2c6244"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### üßë‚Äçü¶± Mensch:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Mein Name ist Ralf"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### ü§ñ KI:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Hallo Ralf! \n\nWie kann ich dir heute helfen? Gibt es ein bestimmtes Thema oder eine Frage, die dich interessiert?"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### üßë‚Äçü¶± Mensch:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Warum ist der Himmel blau?"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### ü§ñ KI:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Der Himmel erscheint blau aufgrund eines Ph√§nomens, das als **Rayleigh-Streuung** bekannt ist. Hier sind die wichtigsten Punkte, die dieses Ph√§nomen erkl√§ren:\n\n### 1. Licht und Farben\n- **Sonnenlicht** besteht aus verschiedenen Farben, die unterschiedliche Wellenl√§ngen haben. \n- Diese Farben reichen von **Rot** (l√§ngste Wellenl√§ngen) bis **Violett** (k√ºrzeste Wellenl√§ngen).\n\n### 2. Streuung des Lichts\n- Wenn das Sonnenlicht die Erdatmosph√§re erreicht, trifft es auf Molek√ºle und kleine Partikel in der Luft.\n- **Rayleigh-Streuung** beschreibt, wie k√ºrzere Wellenl√§ngen (wie Blau und Violett) st√§rker gestreut werden als l√§ngere Wellenl√§ngen (wie Rot und Gelb).\n\n### 3. Wahrnehmung des Himmels\n- Obwohl sowohl Blau als auch Violett gestreut werden, nehmen wir den Himmel als blau wahr, weil:\n  - **Das menschliche Auge** empfindlicher auf Blau reagiert.\n  - **Violett** teilweise von der Ozonschicht absorbiert wird.\n\n### 4. Einfluss der Tageszeit\n- **Morgens und abends** kann der Himmel orange oder rot erscheinen, weil das Licht einen l√§ngeren Weg durch die Atmosph√§re zur√ºcklegt. Dabei wird das blaue Licht st√§rker gestreut, sodass die l√§ngeren Wellenl√§ngen (Rot und Orange) dominieren.\n\n### Beispiel\n- An einem klaren, sonnigen Tag sieht der Himmel oft strahlend blau aus. An bew√∂lkten Tagen oder w√§hrend eines Sonnenuntergangs kann der Himmel jedoch ganz anders aussehen.\n\n### Fazit\nDie blaue Farbe des Himmels ist das Ergebnis der Streuung des Sonnenlichts durch die Atmosph√§re, wobei k√ºrzere Wellenl√§ngen st√§rker gestreut werden. \n\nWenn du weitere Fragen hast oder mehr Informationen ben√∂tigst, lass es mich wissen!"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### üßë‚Äçü¶± Mensch:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Und warum ist er manchmal rot?"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### ü§ñ KI:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Der Himmel erscheint manchmal rot, insbesondere w√§hrend des Sonnenauf- und -untergangs. Dies geschieht aufgrund √§hnlicher physikalischer Prozesse wie bei der blauen Himmel-F√§rbung, jedoch mit einigen zus√§tzlichen Faktoren. Hier sind die Hauptgr√ºnde:\n\n### 1. **Lichtstreuung**\n- **Rayleigh-Streuung**: Wie bereits erw√§hnt, wird kurzwelliges Licht (Blau und Violett) st√§rker gestreut als langwelliges Licht (Rot und Orange).\n- Bei Sonnenauf- und -unterg√§ngen steht die Sonne tief am Horizont. Das Licht muss einen l√§ngeren Weg durch die Atmosph√§re zur√ºcklegen.\n\n### 2. **L√§ngere Lichtstrecke**\n- Wenn das Licht der Sonne durch die Atmosph√§re dringt, wird das blaue Licht stark gestreut und in verschiedene Richtungen verteilt.\n- Die l√§ngeren Wellenl√§ngen (Rot und Orange) gelangen ungehindert durch die Atmosph√§re, da sie weniger gestreut werden.\n\n### 3. **Einfluss von Partikeln in der Luft**\n- **Luftverschmutzung und Staub**: Partikel in der Luft, wie Staub, Rauch oder andere Verunreinigungen, k√∂nnen die Streuung verst√§rken und die roten und orangen Farbt√∂ne intensiver erscheinen lassen.\n- An Tagen mit viel Luftverschmutzung oder nach einem Waldbrand kann der Himmel besonders intensiv rot oder orange erscheinen.\n\n### 4. **Wetterbedingungen**\n- **Wolken**: Wolken k√∂nnen das Licht reflektieren und die Farben verst√§rken. Bei Sonnenuntergang k√∂nnen Wolken das Licht in verschiedenen Farben brechen und verst√§rken, was zu spektakul√§ren Farbspielen f√ºhrt.\n\n### Beispiel\n- An einem klaren Abend mit wenigen Wolken kann der Himmel w√§hrend des Sonnenuntergangs in leuchtenden Rott√∂nen erstrahlen. Wenn es jedoch viele Partikel in der Luft gibt, kann das Rot noch intensiver und dramatischer wirken.\n\n### Fazit\nDer rote Himmel w√§hrend des Sonnenauf- und -untergangs ist das Ergebnis der Streuung von Licht durch die Atmosph√§re, wobei das blaue Licht verloren geht und die l√§ngeren Wellenl√§ngen (Rot und Orange) dominieren. Wetterbedingungen und Luftpartikel k√∂nnen diesen Effekt zus√§tzlich verst√§rken.\n\nWenn du noch weitere Fragen hast oder mehr Details ben√∂tigst, stehe ich dir gerne zur Verf√ºgung!"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### üßë‚Äçü¶± Mensch:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Wie ist mein Name?"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### ü§ñ KI:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Dein Name ist Ralf. Wie kann ich dir weiterhelfen, Ralf? Gibt es ein bestimmtes Thema oder eine Frage, die dich interessiert?"},"metadata":{}}],"source":["# Abschnitt 5: Hauptprogramm\n","\n","# Historie wird initialisiert\n","chat_history = [SystemMessage(content=system_prompt)]\n","\n","# Liste mit user_input\n","user_input_list = [\n","    \"Mein Name ist Ralf\",\n","    \"Warum ist der Himmel blau?\",\n","    \"Und warum ist er manchmal rot?\",\n","    \"Wie ist mein Name?\"\n","]\n","\n","# Chain-Aufruf in einer Schleife\n","for user_input in user_input_list:\n","    chat_history = interact_with_ai(system_prompt, chat_history, user_input)"]},{"cell_type":"code","source":["chat_history"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pz5SNIrNMwGb","executionInfo":{"status":"ok","timestamp":1764681261439,"user_tz":-60,"elapsed":33,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"b584d522-721d-4076-f6b7-e909e6c75831"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[SystemMessage(content='\\nDu bist ein kompetenter KI-Assistent mit breitem Fachwissen.\\n\\nDeine Antworten sind:\\n- Klar strukturiert und mit Markdown formatiert\\n- Praxisorientiert und direkt umsetzbar\\n- Basierend auf aktuellem Kenntnisstand\\n- Mit passenden Beispielen versehen\\n- In verst√§ndlicher Sprache formuliert\\n\\nBei deiner Arbeit:\\n- Analysierst du Fragen sorgf√§ltig\\n- Gibst pr√§zise und relevante Antworten\\n- Erkennst den Kontext der Anfrage\\n- Bietest bei Bedarf weiterf√ºhrende Informationen\\n- Bleibst sachlich und neutral\\n\\nFormatiere alle Antworten in Markdown f√ºr optimale Lesbarkeit.\\n', additional_kwargs={}, response_metadata={}),\n"," HumanMessage(content='Mein Name ist Ralf', additional_kwargs={}, response_metadata={}),\n"," AIMessage(content='Hallo Ralf! \\n\\nWie kann ich dir heute helfen? Gibt es ein bestimmtes Thema oder eine Frage, die dich interessiert?', additional_kwargs={}, response_metadata={}),\n"," HumanMessage(content='Warum ist der Himmel blau?', additional_kwargs={}, response_metadata={}),\n"," AIMessage(content='Der Himmel erscheint blau aufgrund eines Ph√§nomens, das als **Rayleigh-Streuung** bekannt ist. Hier sind die wichtigsten Punkte, die dieses Ph√§nomen erkl√§ren:\\n\\n### 1. Licht und Farben\\n- **Sonnenlicht** besteht aus verschiedenen Farben, die unterschiedliche Wellenl√§ngen haben. \\n- Diese Farben reichen von **Rot** (l√§ngste Wellenl√§ngen) bis **Violett** (k√ºrzeste Wellenl√§ngen).\\n\\n### 2. Streuung des Lichts\\n- Wenn das Sonnenlicht die Erdatmosph√§re erreicht, trifft es auf Molek√ºle und kleine Partikel in der Luft.\\n- **Rayleigh-Streuung** beschreibt, wie k√ºrzere Wellenl√§ngen (wie Blau und Violett) st√§rker gestreut werden als l√§ngere Wellenl√§ngen (wie Rot und Gelb).\\n\\n### 3. Wahrnehmung des Himmels\\n- Obwohl sowohl Blau als auch Violett gestreut werden, nehmen wir den Himmel als blau wahr, weil:\\n  - **Das menschliche Auge** empfindlicher auf Blau reagiert.\\n  - **Violett** teilweise von der Ozonschicht absorbiert wird.\\n\\n### 4. Einfluss der Tageszeit\\n- **Morgens und abends** kann der Himmel orange oder rot erscheinen, weil das Licht einen l√§ngeren Weg durch die Atmosph√§re zur√ºcklegt. Dabei wird das blaue Licht st√§rker gestreut, sodass die l√§ngeren Wellenl√§ngen (Rot und Orange) dominieren.\\n\\n### Beispiel\\n- An einem klaren, sonnigen Tag sieht der Himmel oft strahlend blau aus. An bew√∂lkten Tagen oder w√§hrend eines Sonnenuntergangs kann der Himmel jedoch ganz anders aussehen.\\n\\n### Fazit\\nDie blaue Farbe des Himmels ist das Ergebnis der Streuung des Sonnenlichts durch die Atmosph√§re, wobei k√ºrzere Wellenl√§ngen st√§rker gestreut werden. \\n\\nWenn du weitere Fragen hast oder mehr Informationen ben√∂tigst, lass es mich wissen!', additional_kwargs={}, response_metadata={}),\n"," HumanMessage(content='Und warum ist er manchmal rot?', additional_kwargs={}, response_metadata={}),\n"," AIMessage(content='Der Himmel erscheint manchmal rot, insbesondere w√§hrend des Sonnenauf- und -untergangs. Dies geschieht aufgrund √§hnlicher physikalischer Prozesse wie bei der blauen Himmel-F√§rbung, jedoch mit einigen zus√§tzlichen Faktoren. Hier sind die Hauptgr√ºnde:\\n\\n### 1. **Lichtstreuung**\\n- **Rayleigh-Streuung**: Wie bereits erw√§hnt, wird kurzwelliges Licht (Blau und Violett) st√§rker gestreut als langwelliges Licht (Rot und Orange).\\n- Bei Sonnenauf- und -unterg√§ngen steht die Sonne tief am Horizont. Das Licht muss einen l√§ngeren Weg durch die Atmosph√§re zur√ºcklegen.\\n\\n### 2. **L√§ngere Lichtstrecke**\\n- Wenn das Licht der Sonne durch die Atmosph√§re dringt, wird das blaue Licht stark gestreut und in verschiedene Richtungen verteilt.\\n- Die l√§ngeren Wellenl√§ngen (Rot und Orange) gelangen ungehindert durch die Atmosph√§re, da sie weniger gestreut werden.\\n\\n### 3. **Einfluss von Partikeln in der Luft**\\n- **Luftverschmutzung und Staub**: Partikel in der Luft, wie Staub, Rauch oder andere Verunreinigungen, k√∂nnen die Streuung verst√§rken und die roten und orangen Farbt√∂ne intensiver erscheinen lassen.\\n- An Tagen mit viel Luftverschmutzung oder nach einem Waldbrand kann der Himmel besonders intensiv rot oder orange erscheinen.\\n\\n### 4. **Wetterbedingungen**\\n- **Wolken**: Wolken k√∂nnen das Licht reflektieren und die Farben verst√§rken. Bei Sonnenuntergang k√∂nnen Wolken das Licht in verschiedenen Farben brechen und verst√§rken, was zu spektakul√§ren Farbspielen f√ºhrt.\\n\\n### Beispiel\\n- An einem klaren Abend mit wenigen Wolken kann der Himmel w√§hrend des Sonnenuntergangs in leuchtenden Rott√∂nen erstrahlen. Wenn es jedoch viele Partikel in der Luft gibt, kann das Rot noch intensiver und dramatischer wirken.\\n\\n### Fazit\\nDer rote Himmel w√§hrend des Sonnenauf- und -untergangs ist das Ergebnis der Streuung von Licht durch die Atmosph√§re, wobei das blaue Licht verloren geht und die l√§ngeren Wellenl√§ngen (Rot und Orange) dominieren. Wetterbedingungen und Luftpartikel k√∂nnen diesen Effekt zus√§tzlich verst√§rken.\\n\\nWenn du noch weitere Fragen hast oder mehr Details ben√∂tigst, stehe ich dir gerne zur Verf√ºgung!', additional_kwargs={}, response_metadata={}),\n"," HumanMessage(content='Wie ist mein Name?', additional_kwargs={}, response_metadata={}),\n"," AIMessage(content='Dein Name ist Ralf. Wie kann ich dir weiterhelfen, Ralf? Gibt es ein bestimmtes Thema oder eine Frage, die dich interessiert?', additional_kwargs={}, response_metadata={})]"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["len(chat_history)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vStPWRajM9QM","executionInfo":{"status":"ok","timestamp":1764681317357,"user_tz":-60,"elapsed":26,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"7030a166-98a8-481b-c37f-6a2c29245c03"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"TWq5rQ4AeIU6"},"source":["<p><font color='black' size=\"5\">\n","Neue Konversation starten\n","</font></p>"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"bep9LTAvdiT2","colab":{"base_uri":"https://localhost:8080/","height":516},"executionInfo":{"status":"ok","timestamp":1764681395801,"user_tz":-60,"elapsed":7669,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"1844a527-9bd6-4e24-ff1b-bfa433b8f0e2"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### üßë‚Äçü¶± Mensch:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Erstelle eine Tabelle der f√ºnf bev√∂lkerungsreichsten L√§ndern mit Bev√∂lkerung und BIP."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### ü§ñ KI:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Hier ist eine Tabelle der f√ºnf bev√∂lkerungsreichsten L√§nder, einschlie√ülich ihrer gesch√§tzten Bev√∂lkerung und des Bruttoinlandsprodukts (BIP) f√ºr das Jahr 2023:\n\n| Rang | Land           | Bev√∂lkerung (in Millionen) | BIP (in Billionen USD) |\n|------|----------------|---------------------------|------------------------|\n| 1    | China          | 1.425                     | 17.73                  |\n| 2    | Indien         | 1.417                     | 3.73                   |\n| 3    | USA            | 339                       | 26.85                  |\n| 4    | Indonesien     | 277                       | 1.29                   |\n| 5    | Pakistan       | 240                       | 0.38                   |\n\n### Erl√§uterungen:\n\n- **Bev√∂lkerung**: Die Zahlen sind Sch√§tzungen und k√∂nnen je nach Quelle leicht variieren.\n- **BIP**: Das BIP ist in Billionen US-Dollar angegeben und stellt den Gesamtwert aller in einem Land produzierten Waren und Dienstleistungen dar.\n\n### Quellen:\n- Bev√∂lkerungsdaten stammen von den Vereinten Nationen.\n- BIP-Daten stammen von der Weltbank und dem Internationalen W√§hrungsfonds (IWF).\n\nDiese Tabelle bietet einen schnellen √úberblick √ºber die gr√∂√üten Volkswirtschaften und ihre Bev√∂lkerungszahlen."},"metadata":{}}],"source":["# Abschnitt 5: Hauptprogramm\n","chat_history = [SystemMessage(content=system_prompt)]\n","user_input = \"Erstelle eine Tabelle der f√ºnf bev√∂lkerungsreichsten L√§ndern mit Bev√∂lkerung und BIP.\"\n","chat_history = interact_with_ai(system_prompt, chat_history, user_input)"]},{"cell_type":"code","source":["len(chat_history)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q9Ow3R6BNYRv","executionInfo":{"status":"ok","timestamp":1764681430682,"user_tz":-60,"elapsed":40,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"babedd25-ae29-4572-c263-f841247b5a40"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"J_6EHVIcBe0Q"},"source":["# 5 | Textgenerierung\n","---"]},{"cell_type":"markdown","metadata":{"id":"Q6qNoTVI7Jp_"},"source":["Die Generierung von Text geh√∂rt zu den h√§ufigsten Anwendungsf√§llen f√ºr **Large Language Models (LLMs)**. Anstatt eine interaktive Chatumgebung zu verwenden, erfolgt die Textgenerierung durch gezielte **Eingabeaufforderungen (Prompts)**, die an **OpenAI** gesendet werden. Das Modell verarbeitet diese Anfragen und liefert darauf basierend den generierten Text.  "]},{"cell_type":"markdown","metadata":{"id":"DB0IAW8vBJLV"},"source":["<p><font color='black' size=\"5\">\n","Muster zur Textgenerierung\n","</font></p>\n","\n","Bei der grundlegenden Textgenerierung gibt es verschiedene **Prompt-Muster (Prompting-Methoden)**, die sich je nach Detailgrad der bereitgestellten Informationen unterscheiden. Diese Muster beeinflussen, wie das **Large Language Model (LLM)** den gew√ºnschten Text erzeugt.  \n","\n","Im Folgenden werden verschiedene dieser Muster untersucht und analysiert, wie sie sich auf die Qualit√§t und Struktur der generierten Inhalte auswirken.\n","\n","* Zero-Shot-Prompt\n","* One-Shot-Prompt\n","* Few-Shot-Prompt"]},{"cell_type":"markdown","metadata":{"id":"9GeVTcwLl4xi"},"source":["<p><font color='black' size=\"5\">\n","Zero-Shot Textgenerierung\n","</font></p>\n","\n","Ein Zero-Shot-Prompt zur Texterstellung ist eine Technik, bei der ein Sprachmodell ohne vorherige Anpassung oder spezielles Training auf eine bestimmte Aufgabe direkt mit einer einzigen Eingabeaufforderung genutzt wird. Um diesen Ansatz erfolgreich anzuwenden, ist es entscheidend, einen pr√§zisen und ausf√ºhrlichen Prompt zu formulieren, der genau beschreibt, welche Art von Inhalt generiert werden soll. Dabei sollten Stil, Struktur und relevante Details oder Einschr√§nkungen klar angegeben werden. Beispielsweise kann f√ºr eine gesch√§ftliche E-Mail der gew√ºnschte Tonfall (formell oder informell), die wesentlichen Inhalte (wie Termin, Thema, Teilnehmer) sowie eine Handlungsaufforderung spezifiziert werden. Da das Modell allein auf die im Prompt enthaltenen Informationen angewiesen ist, sollte das gew√ºnschte Ergebnis m√∂glichst eindeutig formuliert sein. Diese Methode ist flexibel einsetzbar und erlaubt die Erstellung vielf√§ltiger Textarten, ohne dass eine vorherige Modellanpassung erforderlich ist.  \n","\n","Das folgende Beispiel demonstriert eine Zero-Shot-Eingabeaufforderung: Es werden verschiedene Anforderungen gestellt und Informationen zu einem Studierenden bereitgestellt, jedoch ohne eine explizite Vorlage f√ºr das Sprachmodell vorzugeben."]},{"cell_type":"markdown","metadata":{"id":"oiql4k41MOTC"},"source":["**Es wird der bereits erstellte ChatBot verwendet und leicht angepasst.**"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"pZzFJ6DmMrUT","executionInfo":{"status":"ok","timestamp":1764681673266,"user_tz":-60,"elapsed":103,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Anpassung: Abschnitt 2: Konstanten definieren\n","system_prompt = \"\"\"\n","Du bist ein hilfreicher KI-Assistent, der bei der Erstellung von Empfehlungsschreiben hilft.\n","\"\"\""]},{"cell_type":"code","execution_count":19,"metadata":{"id":"bGp3dfribNW1","executionInfo":{"status":"ok","timestamp":1764681648395,"user_tz":-60,"elapsed":74,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Abschnitt 5: Hauptprogramm\n","user_input = \"\"\"\n","    Erstellen Sie ein positives Empfehlungsschreiben f√ºr Maria Weber, eine meiner Studierenden im Kurs ML 401 an der Technischen Universit√§t M√ºnchen. Mein Name ist Dr. Julia Berger. Sie bewirbt sich f√ºr einen Master of Science in Data Science. Geben Sie mir nur den Haupttext des Schreibens, keine Kopf- oder Fu√üzeile.\n","\n","    Unten ist ihre Anfrage:\n","\n","    Sehr geehrte Frau Dr. Berger,\n","\n","    ich hoffe, es geht Ihnen gut! Ich bin Maria Weber (Matrikelnr.: 7890), eine Absolventin der TU M√ºnchen, die im Juni 2025 meinen Bachelor in Wirtschaftsmathematik abgeschlossen habe.\n","\n","    Im Wintersemester 2023/24 hatte ich das Privileg, Ihren Kurs ML 401: Grundlagen des maschinellen Lernens zu besuchen. Der Kurs war ein Wahlpflichtfach in meinem Bachelorprogramm. Die Inhalte haben mich sehr begeistert und ich habe mich durchgehend intensiv eingebracht, was sich in der Note 1,0 widerspiegelte.\n","\n","    Nach meinem Abschluss mit einer Gesamtnote von 1,1 ‚Äì der besten Note meines Jahrgangs ‚Äì arbeite ich als Data Analytics Consultant bei der Firma DataTech GmbH. Meine Hauptaufgabe liegt in der Entwicklung von KI-gest√ºtzten Analysetools f√ºr Gesch√§ftsprozesse. Um meine Expertise im Bereich Data Science zu vertiefen, m√∂chte ich berufsbegleitend einen Master in Data Science absolvieren. Ich sch√§tze Ihre Forschungsarbeit sehr (ich verfolge regelm√§√üig Ihre Publikationen und habe sie auch in meinem Team vorgestellt) und Ihre Einsch√§tzung w√§re f√ºr meine Bewerbung sehr wertvoll.\n","\n","    Ich bewerbe mich f√ºr folgende Programme:\n","    - ETH Z√ºrich, Master of Science in Data Science\n","    - TU Berlin, Master of Science in Data Engineering\n","\n","    D√ºrfte ich Sie um ein Empfehlungsschreiben f√ºr diese Bewerbungen bitten? Meinen Lebenslauf habe ich beigef√ºgt und ich stelle Ihnen gerne weitere Informationen zur Verf√ºgung.\n","\n","    Vielen Dank f√ºr die Ber√ºcksichtigung meiner Anfrage.\n","\n","    Mit freundlichen Gr√º√üen\n","    Maria Weber\n","\n","    GEW√úNSCHTES FORMAT:\n","    - In Markdown formatiert\n","    \"\"\""]},{"cell_type":"code","source":["system_prompt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"hdKrU4t8ORhY","executionInfo":{"status":"ok","timestamp":1764681676988,"user_tz":-60,"elapsed":55,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"e0f1e6dd-8bd0-46db-dd2c-15223a291d42"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nDu bist ein hilfreicher KI-Assistent, der bei der Erstellung von Empfehlungsschreiben hilft.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["user_input"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123},"id":"qNtQQHqfOZJo","executionInfo":{"status":"ok","timestamp":1764681690935,"user_tz":-60,"elapsed":59,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"f0648cef-ae5f-430e-a96e-fe89cc720bde"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n    Erstellen Sie ein positives Empfehlungsschreiben f√ºr Maria Weber, eine meiner Studierenden im Kurs ML 401 an der Technischen Universit√§t M√ºnchen. Mein Name ist Dr. Julia Berger. Sie bewirbt sich f√ºr einen Master of Science in Data Science. Geben Sie mir nur den Haupttext des Schreibens, keine Kopf- oder Fu√üzeile.\\n\\n    Unten ist ihre Anfrage:\\n\\n    Sehr geehrte Frau Dr. Berger,\\n\\n    ich hoffe, es geht Ihnen gut! Ich bin Maria Weber (Matrikelnr.: 7890), eine Absolventin der TU M√ºnchen, die im Juni 2025 meinen Bachelor in Wirtschaftsmathematik abgeschlossen habe.\\n\\n    Im Wintersemester 2023/24 hatte ich das Privileg, Ihren Kurs ML 401: Grundlagen des maschinellen Lernens zu besuchen. Der Kurs war ein Wahlpflichtfach in meinem Bachelorprogramm. Die Inhalte haben mich sehr begeistert und ich habe mich durchgehend intensiv eingebracht, was sich in der Note 1,0 widerspiegelte.\\n\\n    Nach meinem Abschluss mit einer Gesamtnote von 1,1 ‚Äì der besten Note meines Jahrgangs ‚Äì arbeite ich als Data Analytics Consultant bei der Firma DataTech GmbH. Meine Hauptaufgabe liegt in der Entwicklung von KI-gest√ºtzten Analysetools f√ºr Gesch√§ftsprozesse. Um meine Expertise im Bereich Data Science zu vertiefen, m√∂chte ich berufsbegleitend einen Master in Data Science absolvieren. Ich sch√§tze Ihre Forschungsarbeit sehr (ich verfolge regelm√§√üig Ihre Publikationen und habe sie auch in meinem Team vorgestellt) und Ihre Einsch√§tzung w√§re f√ºr meine Bewerbung sehr wertvoll.\\n\\n    Ich bewerbe mich f√ºr folgende Programme:\\n    - ETH Z√ºrich, Master of Science in Data Science\\n    - TU Berlin, Master of Science in Data Engineering\\n\\n    D√ºrfte ich Sie um ein Empfehlungsschreiben f√ºr diese Bewerbungen bitten? Meinen Lebenslauf habe ich beigef√ºgt und ich stelle Ihnen gerne weitere Informationen zur Verf√ºgung.\\n\\n    Vielen Dank f√ºr die Ber√ºcksichtigung meiner Anfrage.\\n\\n    Mit freundlichen Gr√º√üen\\n    Maria Weber\\n\\n    GEW√úNSCHTES FORMAT:\\n    - In Markdown formatiert\\n    '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"code","execution_count":25,"metadata":{"id":"CC7PfXB-iY6t","colab":{"base_uri":"https://localhost:8080/","height":827},"executionInfo":{"status":"ok","timestamp":1764681741870,"user_tz":-60,"elapsed":7903,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"8c60a6f4-b480-4546-aa04-7890a2b7349a"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### üßë‚Äçü¶± Mensch:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"\n    Erstellen Sie ein positives Empfehlungsschreiben f√ºr Maria Weber, eine meiner Studierenden im Kurs ML 401 an der Technischen Universit√§t M√ºnchen. Mein Name ist Dr. Julia Berger. Sie bewirbt sich f√ºr einen Master of Science in Data Science. Geben Sie mir nur den Haupttext des Schreibens, keine Kopf- oder Fu√üzeile.\n\n    Unten ist ihre Anfrage:\n\n    Sehr geehrte Frau Dr. Berger,\n\n    ich hoffe, es geht Ihnen gut! Ich bin Maria Weber (Matrikelnr.: 7890), eine Absolventin der TU M√ºnchen, die im Juni 2025 meinen Bachelor in Wirtschaftsmathematik abgeschlossen habe.\n\n    Im Wintersemester 2023/24 hatte ich das Privileg, Ihren Kurs ML 401: Grundlagen des maschinellen Lernens zu besuchen. Der Kurs war ein Wahlpflichtfach in meinem Bachelorprogramm. Die Inhalte haben mich sehr begeistert und ich habe mich durchgehend intensiv eingebracht, was sich in der Note 1,0 widerspiegelte.\n\n    Nach meinem Abschluss mit einer Gesamtnote von 1,1 ‚Äì der besten Note meines Jahrgangs ‚Äì arbeite ich als Data Analytics Consultant bei der Firma DataTech GmbH. Meine Hauptaufgabe liegt in der Entwicklung von KI-gest√ºtzten Analysetools f√ºr Gesch√§ftsprozesse. Um meine Expertise im Bereich Data Science zu vertiefen, m√∂chte ich berufsbegleitend einen Master in Data Science absolvieren. Ich sch√§tze Ihre Forschungsarbeit sehr (ich verfolge regelm√§√üig Ihre Publikationen und habe sie auch in meinem Team vorgestellt) und Ihre Einsch√§tzung w√§re f√ºr meine Bewerbung sehr wertvoll.\n\n    Ich bewerbe mich f√ºr folgende Programme:\n    - ETH Z√ºrich, Master of Science in Data Science\n    - TU Berlin, Master of Science in Data Engineering\n\n    D√ºrfte ich Sie um ein Empfehlungsschreiben f√ºr diese Bewerbungen bitten? Meinen Lebenslauf habe ich beigef√ºgt und ich stelle Ihnen gerne weitere Informationen zur Verf√ºgung.\n\n    Vielen Dank f√ºr die Ber√ºcksichtigung meiner Anfrage.\n\n    Mit freundlichen Gr√º√üen\n    Maria Weber\n\n    GEW√úNSCHTES FORMAT:\n    - In Markdown formatiert\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### ü§ñ KI:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Maria Weber war eine herausragende Studentin in meinem Kurs ML 401: Grundlagen des maschinellen Lernens an der Technischen Universit√§t M√ºnchen. Es war mir eine Freude, ihre Entwicklung w√§hrend des Semesters zu beobachten. Maria hat nicht nur durch ihre hervorragenden Leistungen ‚Äì sie erzielte die Note 1,0 ‚Äì √ºberzeugt, sondern auch durch ihr Engagement und ihre Begeisterung f√ºr das Thema.\n\nMaria bringt eine bemerkenswerte analytische Denkweise mit, die es ihr erm√∂glicht, komplexe Konzepte des maschinellen Lernens schnell zu erfassen und anzuwenden. Sie hat aktiv an Diskussionen teilgenommen und ihre Ideen und Ansichten klar und pr√§zise formuliert. Ihre F√§higkeit, theoretische Konzepte in praktische Anwendungen zu √ºbertragen, zeigt sich auch in ihrer aktuellen Position als Data Analytics Consultant bei der DataTech GmbH, wo sie KI-gest√ºtzte Analysetools entwickelt.\n\nDar√ºber hinaus sch√§tze ich Marias Initiative und ihren Wunsch, sich kontinuierlich weiterzuentwickeln. Ihr Interesse an meiner Forschungsarbeit und ihre Bereitschaft, diese in ihrem Team zu pr√§sentieren, zeugen von ihrer Leidenschaft f√ºr Data Science und ihrem Streben nach Exzellenz. Ich bin √ºberzeugt, dass sie im Master of Science in Data Science an der ETH Z√ºrich oder der TU Berlin nicht nur von den Inhalten profitieren wird, sondern auch einen wertvollen Beitrag zur akademischen Gemeinschaft leisten kann.\n\nIch empfehle Maria Weber ohne Vorbehalte f√ºr die genannten Masterprogramme. Sie hat das Potenzial, in der Welt der Datenwissenschaften Gro√ües zu erreichen, und ich bin sicher, dass sie die Herausforderungen eines Masterstudiums mit Bravour meistern wird. \n\nBitte z√∂gern Sie nicht, mich zu kontaktieren, falls Sie weitere Informationen ben√∂tigen."},"metadata":{}}],"source":["chat_history = [SystemMessage(content=system_prompt)]\n","chat_history = interact_with_ai(system_prompt, chat_history, user_input)"]},{"cell_type":"markdown","metadata":{"id":"XhpXOo-xOair"},"source":["<p><font color='black' size=\"5\">\n","One-Shot Textgenerierung\n","</font></p>\n","\n","Ein One-Shot-Prompt zur Textgenerierung ist eine Methode, bei der ein Sprachmodell mit einer einzigen, detaillierten Eingabe angewiesen wird, auf deren Basis es einen zusammenh√§ngenden Text erstellt. Um diese Technik wirkungsvoll einzusetzen, sollte der Prompt klar und pr√§zise formuliert sein und alle relevanten Informationen sowie den gew√ºnschten Kontext enthalten. Dabei ist es ratsam, Stil, Ton und spezifische inhaltliche Aspekte genau zu definieren.  \n","\n","M√∂chte man beispielsweise eine atmosph√§rische Beschreibung einer K√ºstenstadt generieren lassen, sollten wesentliche Merkmale wie Tageszeit, Stimmung und bestimmte visuelle oder emotionale Eindr√ºcke explizit benannt werden. Eine solche detaillierte Vorgabe erleichtert es dem Modell, die Anforderungen zu erfassen und passgenaue Inhalte zu erstellen.  \n","\n","Nachdem der Prompt formuliert wurde, kann er direkt in das Textgenerierungstool eingegeben werden. Falls das Ergebnis nicht vollst√§ndig den Erwartungen entspricht, l√§sst sich der Prompt gezielt anpassen, um die Qualit√§t und Relevanz der Ausgabe weiter zu optimieren."]},{"cell_type":"code","source":[],"metadata":{"id":"bP7yhtzwXh2r"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5-thhsZp-VdA"},"outputs":[],"source":["# Abschnitt 5: Hauptprogramm\n","user_input = \"\"\"\n","Basierend auf diesem Beispiel, erstelle ein neues akademisches Empfehlungsschreiben.\n","Behalte den professionellen Ton bei, aber passe die Details an:\n","\n","BEISPIEL:\n","\"Ich freue mich sehr, Maria Weber f√ºr den Master of Science in Data Science zu empfehlen. Als ihre Dozentin im Kurs ML 401: Grundlagen des maschinellen Lernens an der TU M√ºnchen konnte ich ihre au√üergew√∂hnlichen akademischen F√§higkeiten beobachten. Frau Weber schloss den anspruchsvollen Kurs mit der Note 1,0 ab und demonstrierte dabei ein tiefgreifendes Verst√§ndnis f√ºr maschinelles Lernen und dessen praktische Anwendungen.\n","\n","Besonders beeindruckend war ihre F√§higkeit, komplexe Konzepte nicht nur zu verstehen, sondern auch auf reale Problemstellungen anzuwenden. In ihrer Position als Data Analytics Consultant bei DataTech GmbH setzt sie diese F√§higkeiten erfolgreich in der Entwicklung KI-gest√ºtzter Analysetools ein.\n","\n","Mit ihrem Bachelorabschluss in Wirtschaftsmathematik (Note 1,1) als Jahrgangsbeste hat Frau Weber bereits bewiesen, dass sie h√∂chsten akademischen Anforderungen gerecht wird. Ihre analytischen F√§higkeiten, gepaart mit ihrer Lernbereitschaft und ihrem Engagement, machen sie zu einer idealen Kandidatin f√ºr ein weiterf√ºhrendes Studium im Bereich Data Science.\n","\n","Aufgrund ihrer bisherigen Leistungen und ihres Potenzials empfehle ich Frau Weber nachdr√ºcklich f√ºr den Master of Science in Data Science. Sie wird zweifellos einen wertvollen Beitrag zum Programm leisten.\"\n","\n","ANWEISUNGEN:\n","1. Erstelle ein neues Empfehlungsschreiben mit √§hnlicher Struktur\n","2. √Ñndere:\n","- Namen\n","- Studienfach\n","- Universit√§ten\n","- Kurse\n","- Noten\n","- aktuelle Berufst√§tigkeit\n","3. Behalte bei:\n","- Vier-Absatz-Struktur\n","- Professionellen Ton\n","- Spezifische Beispiele f√ºr Leistungen\n","- Klare Empfehlung am Ende\n","\n","GEW√úNSCHTES FORMAT:\n","- Nur Haupttext\n","- Keine Anrede/Gru√üformel\n","- In Markdown formatiert\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rNt0eebfipM4"},"outputs":[],"source":["chat_history = [SystemMessage(content=system_prompt)]\n","chat_history = interact_with_ai(system_prompt, chat_history, user_input)"]},{"cell_type":"markdown","metadata":{"id":"oFDE3DRgHvq1"},"source":["<p><font color='black' size=\"5\">\n","Synthetische Daten generieren\n","</font></p>\n","\n","LLMs (Large Language Models) eignen sich zur Erzeugung synthetischer Daten, was besonders n√ºtzlich f√ºr Testszenarien ist, die realistische Informationen oder eine breite demografische Vielfalt erfordern. Diese Modelle k√∂nnen beispielsweise detaillierte Biografien f√ºr verschiedene Berufsgruppen generieren, wodurch realit√§tsnahe Daten f√ºr Simulationen, die Entwicklung von Testalgorithmen oder das Training anderer KI-Systeme bereitgestellt werden.  \n","\n","Ein m√∂glicher Anwendungsfall w√§re die Erstellung synthetischer Biografien f√ºr Berufsgruppen wie Softwareentwickler, Kinderkrankenschwestern, Finanzanalysten, Naturwissenschaftslehrer an Universit√§ten oder Marketingmanager. Jede dieser Biografien k√∂nnte individuelle Karrierewege, Qualifikationen und berufliche Erfahrungen umfassen, um vielseitige und praxisnahe Tests zu erm√∂glichen. Auf diese Weise tragen synthetische Daten zur Verbesserung der Systemleistung bei, ohne auf echte personenbezogene Informationen zur√ºckzugreifen."]},{"cell_type":"code","execution_count":26,"metadata":{"id":"bvd8lAgsR12B","executionInfo":{"status":"ok","timestamp":1764684245719,"user_tz":-60,"elapsed":23,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Anpassung: Abschnitt 2: Konstanten definieren\n","system_prompt = \"\"\"\n","Du bist ein Assistent, der synthetische Daten f√ºr eine Person in dem von Ihnen angegebenen Berufsfeld generiert. Erstelle eine kurze Biografie der Person, die nicht l√§nger als 5 S√§tze ist. Erw√§hne die Berufsbezeichnung nicht ausdr√ºcklich.\n","\"\"\""]},{"cell_type":"code","execution_count":27,"metadata":{"id":"DdAuyo1AR6ei","colab":{"base_uri":"https://localhost:8080/","height":212},"executionInfo":{"status":"ok","timestamp":1764684277747,"user_tz":-60,"elapsed":4148,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"c0dc3c90-9553-4ae7-b148-77c760b79771"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### üßë‚Äçü¶± Mensch:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Softwareentwickler"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### ü§ñ KI:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Maximilian ist ein leidenschaftlicher Technikenthusiast, der seine Karriere in der Welt der Programmierung begann, als er im Alter von 12 Jahren sein erstes Spiel entwickelte. Nach seinem Informatikstudium an einer renommierten Universit√§t hat er in verschiedenen Start-ups und etablierten Unternehmen an innovativen Projekten gearbeitet. Seine Spezialit√§t liegt in der Entwicklung benutzerfreundlicher Anwendungen, die sowohl funktional als auch √§sthetisch ansprechend sind. In seiner Freizeit engagiert er sich in der Open-Source-Community und teilt sein Wissen durch Tutorials und Workshops. Maximilian tr√§umt davon, eines Tages ein eigenes Unternehmen zu gr√ºnden, das kreative L√∂sungen f√ºr allt√§gliche Probleme bietet."},"metadata":{}}],"source":["# Abschnitt 5: Hauptprogramm\n","user_input = \"Softwareentwickler\"\n","\n","chat_history = [SystemMessage(content=system_prompt)]\n","chat_history = interact_with_ai(system_prompt, chat_history, user_input)"]},{"cell_type":"markdown","metadata":{"id":"qjLdt7kMOP-Q"},"source":["Nun wird eine Liste mit mehreren zuf√§llig generierten Biografien erstellt."]},{"cell_type":"code","execution_count":28,"metadata":{"id":"U25feB-bIHSY","executionInfo":{"status":"ok","timestamp":1764684318038,"user_tz":-60,"elapsed":48,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["personas = [\n","\"Luftballon-Verk√§ufer\",\n","\"L√∂wen-Krankenschwester\",\n","\"Talkmaster\",\n","\"Hochschullehrer Wirtschaftswissenschaften\",\n","\"Zirkus-Zauberer\"\n","]"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"kh4UDM16Jxq7","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1764684350356,"user_tz":-60,"elapsed":23561,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"f0cf59f2-8704-4bd2-c8ce-01975b0b7a41"},"outputs":[{"output_type":"stream","name":"stderr","text":["\rGenerating Careers:   0%|          | 0/5 [00:00<?, ?it/s]"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### üßë‚Äçü¶± Mensch:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Luftballon-Verk√§ufer"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### ü§ñ KI:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Maximilian ist ein leidenschaftlicher K√ºnstler, der seine Kreativit√§t in bunten Luftballon-Kreationen auslebt. Seit √ºber f√ºnf Jahren bringt er Freude auf Veranstaltungen und Feste, indem er einzigartige Designs f√ºr Kinder und Erwachsene zaubert. Seine Liebe zur Farbenpracht und sein freundliches Wesen machen ihn zu einem beliebten Anziehungspunkt auf jedem Markt. In seiner Freizeit experimentiert er mit neuen Techniken und Designs, um seine F√§higkeiten st√§ndig zu verbessern. Maximilian tr√§umt davon, eines Tages seine eigenen Workshops anzubieten, um anderen die Kunst des Ballonformens n√§herzubringen."},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\rGenerating Careers:  20%|‚ñà‚ñà        | 1/5 [00:05<00:22,  5.57s/it]"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### üßë‚Äçü¶± Mensch:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"L√∂wen-Krankenschwester"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### ü§ñ KI:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Sophie ist eine engagierte Tierpflegerin, die sich auf die Pflege und Rehabilitation von L√∂wen spezialisiert hat. Mit ihrem Abschluss in Tiermedizin und jahrelanger Erfahrung in verschiedenen Wildtierreservaten hat sie sich einen Namen in der Tierpflege-Community gemacht. Ihre Geduld und Empathie erm√∂glichen es ihr, eine vertrauensvolle Beziehung zu den majest√§tischen Raubtieren aufzubauen. In ihrer Freizeit engagiert sie sich f√ºr den Tierschutz und h√§lt Vortr√§ge √ºber den Erhalt von Wildtieren. Sophie tr√§umt davon, eines Tages ein eigenes Zentrum f√ºr verletzte und verwaiste L√∂wen zu gr√ºnden."},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\rGenerating Careers:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:10<00:16,  5.40s/it]"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### üßë‚Äçü¶± Mensch:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Talkmaster"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### ü§ñ KI:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Jonas ist ein charismatischer Moderator, der mit seiner offenen Art und seinem scharfen Verstand die Zuschauer begeistert. Seit vielen Jahren f√ºhrt er durch spannende Gespr√§che mit prominenten G√§sten und Experten aus verschiedenen Bereichen. Seine F√§higkeit, tiefgr√ºndige Themen auf unterhaltsame Weise zu pr√§sentieren, hat ihm eine treue Fangemeinde eingebracht. In seiner Freizeit schreibt er an einem Buch √ºber die Kunst der Kommunikation und die Bedeutung von Dialog. Jonas ist stets auf der Suche nach neuen Ideen, um seine Sendung noch abwechslungsreicher und informativer zu gestalten."},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\rGenerating Careers:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:14<00:08,  4.50s/it]"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### üßë‚Äçü¶± Mensch:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Hochschullehrer Wirtschaftswissenschaften"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### ü§ñ KI:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Dr. Laura Schmidt ist eine leidenschaftliche Akademikerin, die sich auf die Schnittstelle zwischen Theorie und Praxis in der Wirtschaft spezialisiert hat. Mit ihrem Doktortitel in Betriebswirtschaftslehre und √ºber einem Jahrzehnt Erfahrung in der Lehre inspiriert sie ihre Studierenden, kritisch zu denken und innovative L√∂sungen zu entwickeln. Neben ihrer Lehrt√§tigkeit forscht sie intensiv zu nachhaltigen Gesch√§ftsmodellen und deren Einfluss auf die Gesellschaft. Laura engagiert sich auch in verschiedenen Projekten zur F√∂rderung von Unternehmertum und wirtschaftlicher Bildung in der Gemeinschaft. In ihrer Freizeit genie√üt sie es, neue L√§nder zu bereisen und sich mit internationalen Wirtschaftsfragen auseinanderzusetzen."},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\rGenerating Careers:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:18<00:04,  4.29s/it]"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### üßë‚Äçü¶± Mensch:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Zirkus-Zauberer"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### ü§ñ KI:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Felix ist ein talentierter Illusionist, der das Publikum mit seinen faszinierenden Tricks und seiner charmanten B√ºhnenpr√§senz begeistert. Schon in seiner Kindheit entdeckte er seine Leidenschaft f√ºr die Magie und hat seitdem unerm√ºdlich an seinen F√§higkeiten gearbeitet. Mit einer Mischung aus klassischer Zauberkunst und modernen Elementen schafft er unvergessliche Erlebnisse f√ºr Jung und Alt. Neben seinen Auftritten im Zirkus gibt er auch Workshops, um anderen die Freude an der Magie n√§herzubringen. Felix tr√§umt davon, eines Tages seine eigene Zaubershow im Fernsehen zu pr√§sentieren und die Menschen weltweit zu verzaubern."},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Generating Careers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:23<00:00,  4.69s/it]\n"]}],"source":["from tqdm import tqdm\n","\n","# Verwenden Sie tqdm, um den Fortschrittsbalken anzuzeigen\n","chat_history = [SystemMessage(content=system_prompt)]\n","\n","for i in tqdm(range(5), desc=\"Generating Careers\"):\n","    user_input = personas[i]\n","    chat_history = interact_with_ai(system_prompt, chat_history, user_input)"]},{"cell_type":"markdown","metadata":{"id":"TgfPV_aXBuv9"},"source":["# 6 | Textklassifizierung\n","---"]},{"cell_type":"markdown","metadata":{"id":"XB3SnNhx7eWv"},"source":["Gro√üe Sprachmodelle (LLMs) haben die Textklassifizierung grundlegend ver√§ndert, indem sie einen flexibleren und effizienteren Ansatz als herk√∂mmliche maschinelle Lernmethoden bieten. W√§hrend traditionelle Verfahren umfangreiche, manuell annotierte Datens√§tze ben√∂tigten, um Modelle zu trainieren, bieten LLMs eine zeitsparende Alternative.  \n","\n","Durch die **Few-Shot Klassifizierung**, die mit beschriftete Beispiele funktioniert, k√∂nnen LLMs Texte allein auf Basis weniger nat√ºrlichsprachlichen Beispiele kategorisieren. Diese F√§higkeit beruht auf dem umfassenden Wissen, das die Modelle w√§hrend ihres Trainings mit gro√üen Textkorpora erworben haben.  \n","\n","Dieser Ansatz reduziert den Aufwand f√ºr die Einrichtung einer Textklassifizierung erheblich und erm√∂glicht eine schnelle Anpassung an neue Anforderungen. Dadurch eignen sich LLMs f√ºr verschiedene Anwendungsf√§lle, darunter **Stimmungsanalyse, Themenkategorisierung, Spam-Erkennung und Inhaltsmoderation**.  \n","\n","Im folgenden Code werden SMS-Texte analysiert. Anschlie√üend erfolgt deren Klassifizierung mithilfe eines LLMs."]},{"cell_type":"markdown","metadata":{"id":"8cdf6QNZrC02"},"source":["Im folgenden Programm erfolgt eine Klassifizierung der SMS-Texte.\n"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"U-5JUYXRYvrJ","executionInfo":{"status":"ok","timestamp":1764685569379,"user_tz":-60,"elapsed":830,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Importe\n","from pandas import DataFrame"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"wkQBnUM8VrJx","executionInfo":{"status":"ok","timestamp":1764685730561,"user_tz":-60,"elapsed":36,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Konstanten definieren\n","system_prompt = \"\"\"\n","Du bist ein Experte f√ºr die Klassifikation von SMS-Nachrichten.\n","\n","Klassifiziere SMS-Nachrichten als:\n","- spam: Werbung, Gewinnspiele, kostenpflichtige Dienste, unerw√ºnschte Nachrichten\n","- ham: Normale Kommunikation, pers√∂nliche Nachrichten\n","- sonstiges: Wenn keine klare Zuordnung m√∂glich ist\n","\n","Wichtig: Antworte NUR mit einem Wort: spam, ham oder sonstiges\n","\n","Beachte die Beispiele f√ºr die Klassifikation:\n","- spam: Werbung, Gewinnspiele, kostenpflichtige Dienste\n","- ham: Normale Kommunikation, pers√∂nliche Nachrichten\n","\"\"\"\n","\n","# Beispiel-SMS f√ºr das Training\n","train_sms = [\n","            {\n","                \"sms\": \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n","                \"classification\": \"spam\"\n","            },\n","            {\n","                \"sms\": \"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, ¬£1.50 to rcv\",\n","                \"classification\": \"spam\"\n","            },\n","            {\n","                \"sms\": \"WINNER!! As a valued network customer you have been selected to receive a ¬£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\",\n","                \"classification\": \"spam\"\n","            },\n","            {\n","                \"sms\": \"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\",\n","                \"classification\": \"ham\"\n","            },\n","            {\n","                \"sms\": \"Ok lar... Joking wif u oni...\",\n","                \"classification\": \"ham\"\n","            },\n","            {\n","                \"sms\": \"U dun say so early hor... U c already then say...\",\n","                \"classification\": \"ham\"\n","            },\n","            {\n","                \"sms\": \"Hey, what time is the meeting tomorrow?\",\n","                \"classification\": \"ham\"\n","            },\n","            {\n","                \"sms\": \"I'll be home in 10 minutes\",\n","                \"classification\": \"ham\"\n","            },\n","            {\n","                \"sms\": \"Call me when you get this message\",\n","                \"classification\": \"ham\"\n","            },\n","            {\n","                \"sms\": \"FREE! Claim your prize now! Click: bit.ly/prize123\",\n","                \"classification\": \"spam\"\n","            }\n","]\n","\n","test_sms = [\n","    \"WINNER!! As a valued customer you have won ¬£1000 prize!\",\n","    \"Hey, what time is the meeting tomorrow?\",\n","    \"Free entry! Win an iPhone now! Click here:\",\n","    \"I'll be home in 10 minutes\"\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_JArFRW8VtUg"},"outputs":[],"source":["# Chat-Komponenten initialisieren\n","# prompt = ChatPromptTemplate.from_messages([\n","#     (\"system\", \"{system_prompt}\"),\n","#     MessagesPlaceholder(variable_name=\"chat_history\"),\n","#     (\"human\", \"{user_input}\")\n","# ])"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"z2R1nRyypUmI","executionInfo":{"status":"ok","timestamp":1764685735967,"user_tz":-60,"elapsed":7,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Modell definieren\n","model_provider=\"openai\"\n","model_name = \"gpt-4o-mini\"\n","temperature = 0.3\n","\n","llm = init_chat_model(\n","    model_name,\n","    model_provider=model_provider,\n","    temperature=temperature,\n","    max_tokens=5,           # Begrenzt Antwortl√§nge stark\n","    top_p=0.1,              # Fokussiert auf wahrscheinlichste Tokens\n","    presence_penalty=0.5    # Verhindert Wiederholungen\n",")"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"-b3S7X7WdFeu","executionInfo":{"status":"ok","timestamp":1764685736814,"user_tz":-60,"elapsed":24,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Parser erstellen\n","parser = StrOutputParser()"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"MFTXYMU4dHIW","executionInfo":{"status":"ok","timestamp":1764685737814,"user_tz":-60,"elapsed":55,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Verkettung\n","chain = prompt | llm | parser"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"lMYQoXwAVxDm","executionInfo":{"status":"ok","timestamp":1764685741615,"user_tz":-60,"elapsed":19,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["def create_training_examples(train_sms):\n","    \"\"\" Erstellt Trainingsbeispiele aus den Beispiel-SMS \"\"\"\n","    examples = []\n","    for label, messages in train_sms:\n","        for msg in messages:\n","            examples.append(f\"{label}\\t{msg}\")\n","    return \"\\n\".join(examples)\n","\n","def classify(system_prompt, chat_history, train_sms, test_sms):\n","    \"\"\" Klassifiziert eine einzelne SMS-Nachricht \"\"\"\n","    # Erstelle Prompt mit Trainingsbeispielen\n","    full_prompt = f\"\"\"\n","    Klassifiziere die folgende SMS basierend auf diesen Beispielen:\n","\n","    {create_training_examples(train_sms)}\n","\n","    Gebe als Antwort NUR \"spam\" oder \"ham\" zur√ºck.\n","    Wenn keine Entscheidung m√∂glich ist dann \"sonstiges\".\n","    Gebe keine Begr√ºndung.\n","    Verwende keine Spiegelstriche.\n","\n","    Hier ist die SMS:\n","    {test_sms}\n","    \"\"\"\n","    parameter = {}\n","    parameter['system_prompt'] = system_prompt\n","    parameter['chat_history'] = chat_history\n","    parameter['user_input'] = full_prompt\n","    response = chain.invoke(parameter)\n","\n","    return response.strip()"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"GFZP9R3oV2af","colab":{"base_uri":"https://localhost:8080/","height":444},"executionInfo":{"status":"ok","timestamp":1764685746904,"user_tz":-60,"elapsed":1957,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"aefe9774-1590-4411-8525-31e51c497cab"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## ‚ú® Klassifiziere Test-SMS:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**Test SMS** #1:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"WINNER!! As a valued customer you have won ¬£1000 prize!"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**Klassifizierung:** spam"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**Test SMS** #2:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Hey, what time is the meeting tomorrow?"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**Klassifizierung:** ham"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**Test SMS** #3:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Free entry! Win an iPhone now! Click here:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**Klassifizierung:** spam"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**Test SMS** #4:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"I'll be home in 10 minutes"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**Klassifizierung:** ham"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}],"source":["# Hauptprogramm\n","chat_history = [SystemMessage(content=system_prompt)]\n","\n","mprint(\"## ‚ú® Klassifiziere Test-SMS:\")\n","mprint(\"---\")\n","\n","# SMS-Klassifizierung\n","for i, sms in enumerate(test_sms, 1):\n","    response = classify(system_prompt, chat_history, train_sms, sms)\n","    mprint(f\"**Test SMS** #{i}:\")\n","    mprint(sms)\n","    mprint(f\"**Klassifizierung:** {response}\")\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"ibm452B-uURM"},"source":["<p><font color='black' size=\"5\">\n","Namen klassifizieren\n","</font></p>\n","\n","Die Analyse von W√∂rtern in Texten stellt f√ºr k√ºnstliche Intelligenz eine interessante Herausforderung dar, besonders bei der Zero-Shot Klassifizierung ohne Trainingsdaten. Hierbei muss das Modell spontan entscheiden, ob ein Wort ein Name oder ein regul√§res Wort ist. Im folgenden Text soll getestet werden, ob eine generative KI in der Lage ist, W√∂rter korrekt in die Kategorien \"Name\" oder \"Nicht-Name\" einzuordnen, ohne zuvor mit √§hnlichen Beispielen trainiert worden zu sein. Die zu analysierenden W√∂rter enthalten sowohl gew√∂hnliche Begriffe als auch verschiedene Arten von Namen wie Vornamen, Nachnamen oder Firmennamen. Die zentrale Frage lautet: Kann das Modell durch sein grundlegendes Sprachverst√§ndnis diese Unterscheidung treffen?"]},{"cell_type":"markdown","metadata":{"id":"taGW_e6xvKJM"},"source":["Nun wird ein Large Language Model (LLM) definiert, das den Text analysiert."]},{"cell_type":"markdown","metadata":{"id":"4r8S8zI0ua8U"},"source":["Letztendlich wird eine einzelne Eingabeaufforderung genutzt, um diese Namen zu identifizieren und zu kategorisieren."]},{"cell_type":"code","execution_count":42,"metadata":{"id":"BGe0n8gyXL19","executionInfo":{"status":"ok","timestamp":1764685864292,"user_tz":-60,"elapsed":41,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Konstanten definieren\n","system_prompt = \"\"\"\n","Du bist ein pr√§ziser Namensextraktor.\n","\n","**Aufgabe:** Extrahiere ausschlie√ülich die Namen aus dem gegebenen Text.\n","\n","**Was zu extrahieren ist:**\n","- Personennamen (Vor- und Nachnamen)\n","- Unternehmensnamen\n","- Produktnamen\n","\n","**Ausgabeformat:**\n","- Ein Name pro Zeile\n","- Keine Nummerierung\n","- Keine Aufz√§hlungszeichen\n","- Keine zus√§tzlichen W√∂rter oder Erkl√§rungen\n","- Keine Duplikate\n","\n","**Wichtig:**\n","- Gib NUR die extrahierten Namen aus\n","- Formuliere den Originaltext NICHT um\n","- F√ºge KEINE weiteren Informationen hinzu\n","\n","**Beispiel:**\n","Input: \"Peter M√ºller arbeitet bei Apple und entwickelt das iPhone.\"\n","Output:\n","Peter M√ºller\n","Apple\n","iPhone\n","\"\"\"\n","\n","user_input = \"\"\"\n","Anna und Sarah sind seit dem Studium befreundet. Nach ihrem Abschluss hatten sie ein Vorstellungsgespr√§ch bei Frau Weber f√ºr eine Stelle bei der Technovision GmbH, deren Hauptprodukt Futurtech hei√üt und von der Programmiererin Maria Schmidt entwickelt wurde. Nach dem erfolgreichen Gespr√§ch trafen sie sich mit ihren Freunden Rafael und Aida zum Feiern. An ihrem ersten Arbeitstag lernten sie drei weitere Mitarbeiter kennen: Richard, Lisa und Pia. Sp√§ter kam noch Matthias M√ºller zum Team dazu.\n","\"\"\""]},{"cell_type":"code","execution_count":43,"metadata":{"id":"SIy1p5cogJZc","executionInfo":{"status":"ok","timestamp":1764685868773,"user_tz":-60,"elapsed":33,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Modell definieren\n","model_provider=\"openai\"\n","model_name = \"gpt-4o-mini\"\n","temperature = 0.3\n","llm = init_chat_model(model_name, model_provider=model_provider, temperature=temperature, max_tokens=100)\n","\n","# Verkettung\n","chain = prompt | llm | parser"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"uCnJFJNNXzVE","colab":{"base_uri":"https://localhost:8080/","height":284},"executionInfo":{"status":"ok","timestamp":1764685896136,"user_tz":-60,"elapsed":1576,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"598c3e71-ef90-4219-b43b-218a176e98d4"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üìõ Gefundene Namen:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Anna\n","Sarah\n","Frau Weber\n","Technovision GmbH\n","Futurtech\n","Maria Schmidt\n","Rafael\n","Aida\n","Richard\n","Lisa\n","Pia\n","Matthias M√ºller\n"]}],"source":["# Hauptprogramm\n","chat_history = [SystemMessage(content=system_prompt)]\n","\n","# Namen extrahieren und anzeigen\n","parameter = {}\n","parameter['system_prompt'] = system_prompt\n","parameter['chat_history'] = chat_history\n","parameter['user_input'] = user_input\n","response = chain.invoke(parameter)\n","\n","# Ausgabe\n","mprint(\"## üìõ Gefundene Namen:\")\n","mprint(\"---\")\n","print(response)"]},{"cell_type":"markdown","metadata":{"id":"HODQfLqHBnUI"},"source":["# 7 | Textzusammenfassung\n","---"]},{"cell_type":"markdown","metadata":{"id":"l-LRWCEb7X-5"},"source":["Gro√üe Sprachmodelle (LLMs) wie GPT-x fassen Texte zusammen, indem sie zentrale Inhalte extrahieren und kompakt wiedergeben. Sie erfassen den Kontext sowie semantische Strukturen und erstellen eine pr√§gnante Version, die die Hauptaussagen erh√§lt. Durch ihre F√§higkeit, verschiedene Textarten zu analysieren ‚Äì von Fachartikeln bis hin zu narrativen Texten - liefern sie verst√§ndliche und relevante Zusammenfassungen. Ihre Flexibilit√§t in Bezug auf L√§nge und inhaltliche Schwerpunkte macht sie zu einem effizienten Werkzeug f√ºr die schnelle Informationsverarbeitung."]},{"cell_type":"markdown","metadata":{"id":"l6nozFN3ZmH6"},"source":["<p><font color='black' size=\"5\">\n","Einzelnes PDF zusammenfassen\n","</font></p>\n","\n","Zun√§chst wird die Zusammenfassung einer einzelnen PDF-Datei betrachtet. LangChang nutzt spezielle Dokumentlader, um verschiedene Dateiformate wie PDFs zu verarbeiten. F√ºr unterschiedliche Datentypen stehen spezifische Lader zur Verf√ºgung. Im folgenden Code wird eine PDF-Datei geladen und mithilfe einer allgemeinen Systemaufforderung analysiert und zusammengefasst."]},{"cell_type":"markdown","metadata":{"id":"OLUWbW7NRp4m"},"source":["Zun√§chst wird das PDF-Dokument *Attention Is All You Need* √ºber den **‚ÄûPyPDFLoader‚Äú** von der angegebenen URL (**https://arxiv.org/pdf/1706.03762**) geladen. Anschliessend wird der Text an das LLM √ºbergeben. Schlie√ülich erfolgt die Ausgabe des zusammengefassten Inhalts im **Markdown-Format**, um die urspr√ºngliche Formatierung beizubehalten."]},{"cell_type":"code","execution_count":46,"metadata":{"id":"2D7E3xrPL_kv","executionInfo":{"status":"ok","timestamp":1764687189655,"user_tz":-60,"elapsed":31208,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Importe f√ºr Dokument-Verarbeitung\n","from langchain_community.document_loaders import PyPDFLoader, TextLoader\n","from langchain_text_splitters import CharacterTextSplitter\n","from langchain_core.documents import Document"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"S5mnj65YMOEU","executionInfo":{"status":"ok","timestamp":1764687152687,"user_tz":-60,"elapsed":47,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Chat-Komponenten initialisieren\n","\n","# Chat-Prompt-Template erstellen\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent, der pr√§zise Zusammenfassungen erstellt.\"),\n","    (\"human\", \"\"\"Schreibe eine kurze Zusammenfassung der pr√§sentierten Informationen.\n","    Schreibe die Zusammenfassung auf Deutsch.\n","\n","{user_input}\n","\n","ZUSAMMENFASSUNG:\"\"\")\n","])\n","\n","# Modell definieren\n","model_provider=\"openai\"\n","model_name = \"gpt-4o-mini\"\n","temperature = 0.3\n","llm = init_chat_model(model_name, model_provider=model_provider, temperature=temperature)\n","\n","# Parser erstellen\n","parser = StrOutputParser()\n","\n","# Verkettung (Chain) erstellen\n","chain = prompt | llm | parser"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"JRXU3YJoMRSS","executionInfo":{"status":"ok","timestamp":1764687243202,"user_tz":-60,"elapsed":90,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Funktionen definieren\n","def load_document(url: str):\n","    \"\"\"L√§dt ein PDF-Dokument von einer URL und splittet es in Chunks\"\"\"\n","    loader = PyPDFLoader(url)\n","    return loader.load_and_split()"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"reYM_dPCMeLa","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1764687333028,"user_tz":-60,"elapsed":43,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"7fcff14b-beff-45e1-cbb2-9c4b3ad5ff32"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## ‚ú® Zusammenfassung:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Die Arbeit \"Attention Is All You Need\" pr√§sentiert den Transformer, ein neuartiges Netzwerkarchitektur-Modell, das vollst√§ndig auf Aufmerksamkeitsmechanismen basiert und auf rekursive oder konvolutionale Netzwerke verzichtet. Der Transformer zeigt √ºberlegene Leistungen in maschinellen √úbersetzungsaufgaben, indem er eine BLEU-Punktzahl von 28,4 f√ºr die √úbersetzung von Englisch nach Deutsch und 41,8 f√ºr Englisch nach Franz√∂sisch erreicht, was eine signifikante Verbesserung gegen√ºber bestehenden Modellen darstellt. Der Transformer erm√∂glicht eine h√∂here Parallelisierbarkeit und ben√∂tigt weniger Trainingszeit, was ihn effizienter macht. Die Architektur besteht aus einem Encoder-Decoder-System, das Stapel von selbstaufmerksamen Schichten verwendet, um globale Abh√§ngigkeiten zwischen Eingabe- und Ausgabesequenzen zu modellieren. Die Ergebnisse zeigen, dass der Transformer auch auf andere Aufgaben, wie die englische Konstituenten-Parsing, gut generalisiert. Die Autoren betonen die Vorteile der Selbstaufmerksamkeit und die M√∂glichkeit, diese Architektur auf verschiedene Anwendungsbereiche auszudehnen."},"metadata":{}}],"source":["# Hauptprogramm\n","url = \"https://arxiv.org/pdf/1706.03762\"\n","\n","# Dokument laden\n","docs = load_document(url)\n","\n","# Alle Chunks zu einem Text zusammenf√ºgen\n","text = \" \".join([doc.page_content for doc in docs])\n","\n","# Zusammenfassung generieren\n","response = chain.invoke({\"user_input\": text})\n","\n","# Ausgabe\n","mprint(\"## ‚ú® Zusammenfassung:\")\n","mprint(\"---\")\n","mprint(response)"]},{"cell_type":"markdown","metadata":{"id":"D3MVbKWNsLL3"},"source":["<p><font color='black' size=\"5\">\n","Mehrere PDFs zusammenfassen\n","</font></p>\n","\n","Diese vier wissenschaftlichen Arbeiten haben wesentliche Fortschritte im Bereich der generativen KI und der Verarbeitung nat√ºrlicher Sprache (NLP) erm√∂glicht.  \n","\n","1. **\"Attention Is All You Need\" (Vaswani et al., 2017)** f√ºhrte die **Transformer-Architektur** ein, die sich als grundlegendes Modell f√ºr moderne NLP-Systeme etabliert hat. Der Verzicht auf rekurrente Strukturen und die Einf√ºhrung des Selbstaufmerksamkeitsmechanismus erm√∂glichten effizientere und leistungsf√§higere Modelle wie GPT und BERT.  \n","\n","2. **\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" (Devlin et al., 2018)** revolutionierte die Kontextverarbeitung in NLP-Modellen durch **bidirektionales Training**, das es erm√∂glichte, sowohl vorhergehende als auch nachfolgende W√∂rter gleichzeitig zu analysieren. Dadurch erzielte BERT erhebliche Verbesserungen bei zahlreichen NLP-Aufgaben.  \n","\n","3. **\"Language Models are Few-Shot Learners\" (Brown et al., 2020)** untersuchte das Potenzial **gro√üskalierter Transformer-Modelle** am Beispiel von GPT-3. Die Studie zeigte, dass die Leistung solcher Modelle durch Skalierung verbessert wird und sie auch ohne aufgabenspezifisches Training effektive Ergebnisse liefern k√∂nnen.  \n","\n","4. **\"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" (Raffel et al., 2019)** pr√§sentierte **T5 (Text-to-Text Transfer Transformer)**, ein Modell, das alle NLP-Aufgaben in ein einheitliches Text-zu-Text-Format √ºberf√ºhrt. Dadurch wurde das Transferlernen zwischen verschiedenen Aufgaben vereinfacht.  \n","\n","Diese Arbeiten bilden zusammen die Grundlage f√ºr viele moderne NLP-Modelle und deren Anwendungen in generativer KI."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hq4d3S0Xo5Nv"},"outputs":[],"source":["# Hauptprogramm\n","urls = [\n","    \"https://arxiv.org/pdf/1706.03762\",\n","    \"https://arxiv.org/pdf/1810.04805\",\n","    \"https://arxiv.org/pdf/2005.14165\",\n","    \"https://arxiv.org/pdf/1910.10683\"\n","]\n","\n","# Liste f√ºr Sammlung der Summaries initialisieren\n","summaries = []\n","\n","for url in urls:\n","    # Dokumente laden und als String formatieren\n","    print(f\"Verarbeite Artikel: {url}\")\n","    docs = load_document(url)\n","    text = \" \".join([doc.page_content for doc in docs])\n","\n","    # LLM aufrufen und Zusammenfassung erstellen\n","    response = chain.invoke({\"user_input\": text})\n","\n","    # Zusammenfassungen sammeln\n","    summaries.append(response)\n","\n","    # Zusammenfassung ausgeben\n","    mprint(\"## ‚ú® Zusammenfassung:\")\n","    mprint(\"---\")\n","    mprint(response)\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"QE79kzVUTZ15"},"source":["Nachdem die einzelnen Artikel zusammengefasst wurden, folgt die Erstellung einer √ºbergreifenden √úbersicht durch die Kombination dieser Zusammenfassungen. Hierf√ºr werden die Texte zun√§chst zu einer langen Zeichenfolge zusammengef√ºhrt.  \n","\n","Das Endergebnis wird im **Markdown-Format** ausgegeben, um eine klare Struktur und Lesbarkeit zu gew√§hrleisten. Auf diese Weise entsteht eine pr√§gnante, aber inhaltlich umfassende Synthese der Originalartikel."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_cUtG7B_WZpb"},"outputs":[],"source":["# Gesamtzusammenfassung erstellen\n","gesamt_text = \" \".join(summaries)\n","response = chain.invoke({\"user_input\": gesamt_text})\n","\n","mprint(\"## ‚ú® Gesamt-Zusammenfassung:\")\n","mprint(\"---\")\n","mprint(response)"]},{"cell_type":"markdown","metadata":{"id":"x1Wb0diiB1yO"},"source":["# 8 | LLM schreibt ein Buch\n","---"]},{"cell_type":"markdown","metadata":{"id":"VPzcHtnM7k-f"},"source":["**Dieser** Abschnitt zeigt, wie ein Large Language Model (LLM) beim Schreiben eines Buches genutzt werden kann. Da ein Buch eine komplexe, langfristige Aufgabe darstellt, erfolgt der Prozess schrittweise.  \n","\n","Zun√§chst wird ein **Thema** gew√§hlt, auf dessen Basis das LLM einen Titel und eine kurze **Zusammenfassung** erstellt. Diese Zusammenfassung dient als inhaltlicher Leitfaden. Anschlie√üend wird ein **Inhaltsverzeichnis** generiert, das die Kapitel und Abschnitte strukturiert.  \n","\n","Jedes **Kapitel** wird in separaten Sitzungen mit dem LLM ausgearbeitet, wobei die zuvor erstellte Gliederung zur Orientierung dient. Dieser iterative Ansatz gew√§hrleistet Konsistenz und erm√∂glicht eine detaillierte Ausarbeitung.  \n","\n","Durch eine methodische Vorgehensweise verwandelt sich eine anf√§ngliche Idee in ein gut strukturiertes Buch. F√ºr kreative Ergebnisse wird ein Sprachmodell mit einer Temperatur von 0,7 verwendet."]},{"cell_type":"code","execution_count":53,"metadata":{"id":"_Vn1VtpB4xB7","executionInfo":{"status":"ok","timestamp":1764687539425,"user_tz":-60,"elapsed":29038,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Die Zeile installiert das Programm wkhtmltopdf, das HTML in PDF umwandelt.\n","# Alle Ausgaben und Fehler werden dabei unterdr√ºckt.\n","!sudo apt-get install -qq -y wkhtmltopdf > /dev/null 2>&1"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"o0SHZTYMbvZW","executionInfo":{"status":"ok","timestamp":1764687539479,"user_tz":-60,"elapsed":51,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Importe Text-Aufbereitung\n","import markdown\n","import pdfkit\n","import markdown2\n","from weasyprint import HTML, CSS"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"G1dJuKbIYQLv","executionInfo":{"status":"ok","timestamp":1764687583837,"user_tz":-60,"elapsed":26,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Konstanten definieren\n","system_prompt = \"\"\"\n","Du bist ein kreativer Autor und Schriftsteller.\n","\n","Deine Texte sind:\n","- Fesselnd und gut strukturiert\n","- Reich an Details und Beschreibungen\n","- Konsistent im Stil und der Erz√§hlweise\n","- Mit klarer Handlung und Charakterentwicklung\n","- In ansprechendem Deutsch verfasst\n","\n","Bei deinem Schreiben:\n","- Entwickelst du √ºberzeugende Charaktere\n","- Erschaffst atmosph√§rische Szenen\n","- Beh√§ltst den roten Faden bei\n","- Verwendest kreative Sprachbilder\n","- Achtest auf Spannung und Dynamik\n","\n","Halte dich streng an die Anweisungen im user-prompt.\n","\"\"\""]},{"cell_type":"code","execution_count":56,"metadata":{"id":"KabM_QhQolc_","executionInfo":{"status":"ok","timestamp":1764687592169,"user_tz":-60,"elapsed":8,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# EINFACHES ChatPromptTemplate ohne Historie\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    (\"human\", \"{user_input}\")\n","])"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"aSLjURBzYTbN","executionInfo":{"status":"ok","timestamp":1764687614604,"user_tz":-60,"elapsed":2,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Chat-Komponenten initialisieren\n","model_provider=\"openai\"\n","model_name = \"gpt-4\"\n","temperature = 1\n","llm = init_chat_model(model_name, model_provider=model_provider, temperature=temperature)"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"tZf2ItfkfP2C","executionInfo":{"status":"ok","timestamp":1764687617822,"user_tz":-60,"elapsed":3,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Output Parser\n","parser = StrOutputParser()"]},{"cell_type":"code","execution_count":59,"metadata":{"id":"nwSidzgAfVsS","executionInfo":{"status":"ok","timestamp":1764687621044,"user_tz":-60,"elapsed":46,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Chain-Erstellung\n","chain = prompt | llm | parser"]},{"cell_type":"markdown","metadata":{"id":"k5D_R3w8B1yj"},"source":["Einfache Hilfsfunktionen werden implementiert, um das LLM mit einer benutzerdefinierten Systemaufforderung zu befragen. Diese Aufforderung informiert das Modell dar√ºber, dass es beim Schreiben eines Buches assistiert."]},{"cell_type":"markdown","metadata":{"id":"FOWb0XT7RlYg"},"source":["\n","<p><font color='black' size=\"5\">\n","Titel, Synopse, Inhaltsverzeichnis\n","</font></p>\n","\n","F√ºr dieses Buch kann der Benutzer das Thema √ºber die Variable `SUBJECT` festlegen. Anschlie√üend wird das LLM angewiesen, einen zuf√§lligen Titel basierend auf diesem Thema zu generieren. Dabei wird darauf geachtet, dass die Eingabeaufforderung pr√§zise formuliert ist, um zu vermeiden, dass das LLM zus√§tzliche Einleitungen wie ‚ÄûHier ist ein zuf√§lliger Titel‚Äú hinzuf√ºgt."]},{"cell_type":"markdown","metadata":{"id":"FdqpXU2lwwg2"},"source":["<p><font color='black' size=\"5\">\n","Titel\n","</font></p>"]},{"cell_type":"code","execution_count":60,"metadata":{"id":"99gaAOUQl6yf","executionInfo":{"status":"ok","timestamp":1764687675176,"user_tz":-60,"elapsed":19,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["num_chapters = 6                        # Anzahl der Kapitel\n","subject = \"Fantasy Thriller\"            # Genre/Themenbereich\n","user_input = f\"\"\"\n","Erstelle einen zuf√§lligen Titel f√ºr ein Buch zum Thema {subject}. Gebe nur den Titel als Antwort zur√ºck.\n","\"\"\""]},{"cell_type":"code","execution_count":61,"metadata":{"id":"YYclUgtFwGka","colab":{"base_uri":"https://localhost:8080/","height":99},"executionInfo":{"status":"ok","timestamp":1764687678366,"user_tz":-60,"elapsed":2046,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"e18a43ad-3ada-4c82-99d9-9854bf32a550"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## ‚ú® Titel:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"\"Schattenherz: Im Bann der dunklen Magie\""},"metadata":{}}],"source":["parameter = {}\n","parameter['system_prompt'] = system_prompt\n","parameter['user_input'] = user_input\n","response_titel = chain.invoke(parameter)\n","\n","mprint(\"## ‚ú® Titel:\")\n","mprint(\"---\")\n","mprint(response_titel)"]},{"cell_type":"markdown","metadata":{"id":"yqW_R2qaw8dG"},"source":["<p><font color='black' size=\"5\">\n","Synopsis\n","</font></p>"]},{"cell_type":"markdown","metadata":{"id":"fXyOF_oYtMlR"},"source":["Nachdem der Titel festgelegt wurde, kann nun eine zuf√§llige Zusammenfassung des Buches generiert werden. Diese gibt eine erste inhaltliche Orientierung und dient als Grundlage f√ºr die weitere Strukturierung."]},{"cell_type":"code","execution_count":62,"metadata":{"id":"0T1RtoW4ucFP","executionInfo":{"status":"ok","timestamp":1764687716621,"user_tz":-60,"elapsed":22,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["user_input = f\"\"\"\n","Gebe mir eine Inhaltsangabe f√ºr ein Buch \"{response_titel}\" zum Thema \"{subject}\"\n","Gebe nur die Inhaltsangabe zur√ºck, keinen zus√§tzlichen Text.\n","\"\"\""]},{"cell_type":"code","execution_count":63,"metadata":{"id":"S34GrfxewNM5","colab":{"base_uri":"https://localhost:8080/","height":326},"executionInfo":{"status":"ok","timestamp":1764687736280,"user_tz":-60,"elapsed":17838,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"5663cd50-bae8-4db5-aca1-2aaecbe85765"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## ‚ú® Synopsis:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"In \"\"Schattenherz: Im Bann der dunklen Magie\"\" zieht die junge Buchrestauratorin Evangeline nach dem mysteri√∂sen Tod ihrer Eltern in ein abgelegenes Dorf in Transsylvanien, um ein neues Leben zu beginnen und der tragischen Vergangenheit zu entfliehen. Bald jedoch bemerkt sie seltsame Ereignisse in ihrem neuen Zuhause: Fl√ºstern in der Nacht, Schatten, die sich bewegen, und ein unheimlicher Nachbar, der sie viel zu interessiert beobachtet.\n\nEines Tages entdeckt Evangeline in der Bibliothek ein altes, verstaubtes Buch, das fesselt und ihr nicht mehr aus dem Kopf geht. Kaum beginnt sie, die br√ºchigen Seiten dieses Buches voller geheimer Zauber und Vorhersagen zu ber√ºhren, weckt sie unbeabsichtigt eine dunkle Macht aus dem Schlaf. Eine grausame Hexe, Sophia, die vor Jahrhunderten in der Dunkelheit versiegelt wurde, erhebt sich nun erneut, um ihre Rache an der Welt zu nehmen. Unwissentlich in einem Netz aus Magie und Intrigen gefangen, wird Evangeline zur Schl√ºsselfigur in einem verzweifelten Kampf gegen die Zeit und dunkle M√§chte.\n\nW√§hrend sie alte Legenden entschl√ºsselt und Sophia‚Äôs hinterh√§ltigen Plan entlarvt, erlernt Evangeline mit Hilfe des geheimnisvollen Nachbarn die Kunst der wei√üen Magie, um der drohenden Dunkelheit entgegenzutreten. Verstrickt in emotionale Konflikte, Bedrohungen und den immer n√§her r√ºckenden Bann der dunklen Magie, k√§mpft Evangeline nicht nur um ihr eigenes √úberleben, sondern auch um die Rettung ihrer neuen Heimat und m√∂glicherweise der gesamten Welt.\n\n\"Schattenherz: Im Bann der dunklen Magie\" entf√ºhrt den Leser in eine Welt voller Spannung und Mystik, in der die Grenzen zwischen Gut und B√∂se zunehmend verschwimmen. Es ist eine fesselnde Geschichte von Mut und Liebe, Vergangenheit und Gegenwart, Menschlichkeit und √ºbernat√ºrlicher Bedrohung, die bis zur letzten Seite in Atem h√§lt."},"metadata":{}}],"source":["parameter = {}\n","parameter['system_prompt'] = system_prompt\n","parameter['user_input'] = user_input\n","response_synopsis = chain.invoke(parameter)\n","\n","mprint(\"## ‚ú® Synopsis:\")\n","mprint(\"---\")\n","mprint(response_synopsis)"]},{"cell_type":"markdown","metadata":{"id":"8OcaatblxBHu"},"source":["<p><font color='black' size=\"5\">\n","Inhaltsverzeichnis\n","</font></p>"]},{"cell_type":"markdown","metadata":{"id":"U6ju-lJEttjw"},"source":["Nun wird das Inhaltsverzeichnis erstellt, wobei alle zuvor generierten Informationen ber√ºcksichtigt werden. Ein spezifisches Format wird vorgegeben, um eine klare Struktur zu gew√§hrleisten. Obwohl die Kapitelnummern leicht ableitbar w√§ren, werden sie explizit angefordert, da das LLM sie ohnehin bereitstellen m√∂chte. Der Ansatz erleichtert eine konsistente Struktur und erm√∂glicht es, die Nummerierung sp√§ter gezielt zu entfernen, falls erforderlich."]},{"cell_type":"code","execution_count":64,"metadata":{"id":"1kOgwrhjvPKH","executionInfo":{"status":"ok","timestamp":1764687785539,"user_tz":-60,"elapsed":40,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["user_input = f\"\"\"\n","Gebe mir ein Inhaltsverzeichnis f√ºr ein Buch mit dem Titel \"{response_titel}\"\n","f√ºr ein Buch zum Thema {subject}. Die Buchzusammenfassung lautet \"{response_synopsis}\".\n","Gebe das Inhaltsverzeichnis als Liste von Kapitel√ºberschriften zur√ºck.\n","Erstelle nicht mehr als {num_chapters} Kapitel.\n","Trenne Kapitelnummer und Kapitel√ºberschrift mit einem Pipe-Zeichen '|'.\n","Gebe nur die Kapitelnamen zur√ºck, keinen zus√§tzlichen Text.\n","\"\"\""]},{"cell_type":"code","execution_count":65,"metadata":{"id":"qEx8z_D-wf22","colab":{"base_uri":"https://localhost:8080/","height":180},"executionInfo":{"status":"ok","timestamp":1764687792788,"user_tz":-60,"elapsed":4343,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"2f4d0858-c0ea-4b79-fbba-8d652bb6049e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## ‚ú® Inhaltsverzeichnis:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["1| \"Ein neues Leben in alten Mauern\"\n","2| \"Die dunkle Bibliothek und das verstaubte Grimoire\"\n","3| \"Erwachen der schlafenden Hexe\"\n","4| \"Sophia‚Äôs blutige Pfad der Rache\"\n","5| \"Im Labyrinth der wei√üen Magie\"\n","6| \"Das letzte Gefecht: Dunkelheit gegen Licht\"\n"]}],"source":["parameter = {}\n","parameter['system_prompt'] = system_prompt\n","parameter['user_input'] = user_input\n","response_toc = chain.invoke(parameter)\n","\n","mprint(\"## ‚ú® Inhaltsverzeichnis:\")\n","mprint(\"---\")\n","print(response_toc)"]},{"cell_type":"markdown","metadata":{"id":"vMXLYGi_RxNp"},"source":["\n","<p><font color='black' size=\"5\">\n","Erstellen der Kapitel des Buches\n","</font></p>\n","\n","\n","Nun wird eine Funktion erstellt, die den Text eines Kapitels generiert. Damit das LLM √ºber ausreichend Kontext verf√ºgt, werden die Zusammenfassung, das Inhaltsverzeichnis und die entsprechende Kapitelnummer √ºbergeben."]},{"cell_type":"code","execution_count":66,"metadata":{"id":"O4OQUIubzHS7","executionInfo":{"status":"ok","timestamp":1764687817792,"user_tz":-60,"elapsed":75,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Aufbereitung Inhaltsverzeichnis, Titel & initialisieren String f√ºr Buchinhalt\n","titel_liste = [zeile for zeile in response_toc.strip().split('\\n')]\n","response_titel = response_titel.replace('\"', '')\n","book_content = f\"# {response_titel} \\n ---\\n\\n\""]},{"cell_type":"code","execution_count":67,"metadata":{"id":"S_78_a7cxV9t","executionInfo":{"status":"ok","timestamp":1764687841788,"user_tz":-60,"elapsed":96,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# NEUES ChatPromptTemplate f√ºr die iterative Erstellung der Kapitel\n","book_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", system_prompt),\n","    (\"user\", \"\"\"\n","Schreibe Kapitel mit dem Titel {chapter_title} f√ºr ein Buch\n","mit dem Titel {title} zum Thema {subject}.\n","Die Buchzusammenfassung lautet: {synopsis}\n","Das Inhaltsverzeichnis lautet: {toc}\n","\n","Gebe nur den Kapiteltext zur√ºck, keine Kapitel√ºberschrift, keinen Kapiteltitel,\n","keine Kapitelnummer, keinen zus√§tzlichen Text.\n","\"\"\")\n","])"]},{"cell_type":"code","execution_count":68,"metadata":{"id":"untQaaWMytM-","colab":{"base_uri":"https://localhost:8080/","height":253},"executionInfo":{"status":"ok","timestamp":1764688063615,"user_tz":-60,"elapsed":205825,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"33b5f256-0d13-49e8-e833-881120e0a03b"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### Kapitel erstellen"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"‚úì 1| \"Ein neues Leben in alten Mauern\""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"‚úì 2| \"Die dunkle Bibliothek und das verstaubte Grimoire\""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"‚úì 3| \"Erwachen der schlafenden Hexe\""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"‚úì 4| \"Sophia‚Äôs blutige Pfad der Rache\""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"‚úì 5| \"Im Labyrinth der wei√üen Magie\""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"‚úì 6| \"Das letzte Gefecht: Dunkelheit gegen Licht\""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"\n---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### ‚ú® Buch 'Schattenherz: Im Bann der dunklen Magie' erfolgreich erstellt!"},"metadata":{}}],"source":["# Iteration √ºber alle Kapitel mit automatischer Nummerierung\n","mprint(\"### Kapitel erstellen\")\n","mprint(\"---\")\n","\n","for chapter_titel in titel_liste:\n","\n","    # Formatierung des Prompts mit aktuellen Werten anstelle f-String oben\n","    user_input = book_prompt.format_messages(\n","        chapter_title=chapter_titel,\n","        title=response_titel,\n","        subject=subject,\n","        synopsis=response_synopsis,\n","        toc=response_toc\n","    )\n","\n","    # Generierung des Kapitelinhalts\n","    parameter = {}\n","    parameter['system_prompt'] = system_prompt\n","    parameter['user_input'] = user_input\n","    response_content = chain.invoke(parameter)\n","\n","    # Hinzuf√ºgen des Kapitels zum Buchinhalt\n","    book_content += f\"\\n## {chapter_titel}\\n{response_content}\\n\"\n","    mprint(f\"‚úì {chapter_titel}\")\n","\n","mprint(\"\\n---\")\n","mprint(f\"### ‚ú® Buch '{response_titel}' erfolgreich erstellt!\")"]},{"cell_type":"markdown","metadata":{"id":"PX15vegM6cX-"},"source":["<p><font color='black' size=\"5\">\n","PDF-Datei erstellen\n","</font></p>"]},{"cell_type":"code","execution_count":69,"metadata":{"id":"EHKoEj0K7Ev7","executionInfo":{"status":"ok","timestamp":1764688063672,"user_tz":-60,"elapsed":11,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# FontTools-Logging auf WARNING setzen (unterdr√ºckt DEBUG und INFO)\n","import logging\n","logging.getLogger('fontTools').setLevel(logging.WARNING)\n","logging.getLogger('fontTools.subset').setLevel(logging.WARNING)\n","logging.getLogger('fontTools.ttLib').setLevel(logging.WARNING)"]},{"cell_type":"code","execution_count":70,"metadata":{"id":"ZehM5Fjc65y_","executionInfo":{"status":"ok","timestamp":1764688064021,"user_tz":-60,"elapsed":342,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Markdown ‚Üí HTML ‚Üí PDF\n","html = f\"\"\"\n","<!DOCTYPE html>\n","<html>\n","<head>\n","    <meta charset=\"utf-8\">\n","    <style>\n","        @page {{\n","            size: A4;\n","            margin: 25mm 20mm 30mm 20mm;\n","\n","            @bottom-center {{\n","                content: \"Seite \" counter(page) \" von \" counter(pages);\n","                font-size: 10pt;\n","                font-family: DejaVu Sans, sans-serif;\n","            }}\n","        }}\n","        body {{\n","            font-family: \"DejaVu Sans\", sans-serif;\n","            font-size: 12pt;\n","        }}\n","    </style>\n","</head>\n","<body>\n","{markdown2.markdown(book_content)}\n","</body>\n","</html>\n","\"\"\"\n","\n","# PDF erzeugen\n","HTML(string=html).write_pdf(\"my_book.pdf\")"]},{"cell_type":"markdown","metadata":{"id":"NTsMT8sYYh6C"},"source":["Das generierte Buch steht nun zum Download bereit."]},{"cell_type":"code","execution_count":71,"metadata":{"id":"pqcpXCsABZZo","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1764688064049,"user_tz":-60,"elapsed":21,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"b977a6f7-43b7-480a-f3e2-c883142e420f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_24c1b817-c93e-435f-bdd0-f9cc1b3bf0c5\", \"my_book.pdf\", 32909)"]},"metadata":{}}],"source":["from google.colab import files\n","files.download(\"my_book.pdf\")"]},{"cell_type":"markdown","metadata":{"id":"ikTJEPGqN9IH"},"source":["# A | Aufgabe\n","---"]},{"cell_type":"markdown","metadata":{"id":"eGjopsS35P_r"},"source":["Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen."]},{"cell_type":"markdown","metadata":{"id":"lzlpv_R4OHPR"},"source":["\n","<p><font color='black' size=\"5\">\n","Sentiment-Analyse von Produktbewertungen mit LLM Zero-Shot-Klassifizierung\n","</font></p>\n","\n","**Aufgabenstellung**\n","Entwickeln Sie ein Python-Programm, das mithilfe eines Large Language Models (LLM) Produktbewertungen in die Kategorien \"Positiv\", \"Neutral\" oder \"Negativ\" einordnet und zus√§tzlich die betroffenen Produktaspekte (Qualit√§t, Preis, Lieferung, Service) identifiziert.\n","\n","**Lernziele**\n","Nach Abschluss dieser √úbung k√∂nnen Sie:\n","- Zero-Shot-Klassifizierung f√ºr mehrere Kategorien implementieren\n","- Aspekt-basierte Sentiment-Analyse durchf√ºhren\n","- Komplexe Prompts f√ºr Mehrfachklassifizierung erstellen\n","- Strukturierte Ausgaben aus LLM-Antworten generieren\n","\n","**Aufgabendetails**\n","\n","```python\n","# Beispieldaten\n","reviews = [\n","    \"Die Qualit√§t des Produkts ist hervorragend, allerdings finde ich den Preis zu hoch.\",\n","    \"Schnelle Lieferung, guter Service, faire Preise - besser geht es nicht!\",\n","    \"Nach zwei Wochen ging das Ger√§t kaputt. Der Kundenservice war bei der Reklamation leider keine Hilfe.\",\n","    \"Durchschnittliche Qualit√§t, erf√ºllt seinen Zweck. Lieferung dauerte etwas l√§nger als angegeben.\"\n","]\n","```"]},{"cell_type":"markdown","metadata":{"id":"5gffAlWTOwXS"},"source":["**Beispiel-Prompt**\n","```\n","Analysiere die folgende Produktbewertung hinsichtlich:\n","1. Gesamtsentiment (Positiv/Neutral/Negativ)\n","2. Erw√§hnte Produktaspekte (Qualit√§t/Preis/Lieferung/Service)\n","3. Sentiment pro Aspekt\n","\n","Bewertung: {review}\n","\n","Antworte im Format:\n","Gesamtsentiment: [Kategorie]\n","Identifizierte Aspekte:\n","- [Aspekt]: [Sentiment]\n","Begr√ºndung: [Analyse]\n","```"]}],"metadata":{"colab":{"collapsed_sections":["dXyUNJDNOh5U","pC9A-LaYhsta","7atawInd9okS","aQqiPx-V3Kio","J_6EHVIcBe0Q","TgfPV_aXBuv9","HODQfLqHBnUI","x1Wb0diiB1yO","ikTJEPGqN9IH"],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
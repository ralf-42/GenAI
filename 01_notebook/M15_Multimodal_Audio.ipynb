{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "oYvUY6gMBKO1",
    "B27m4yz8h8cu",
    "uE5RXwpTjPcw",
    "44jdmbAHipr3",
    "wRvUmkuGh4Qm",
    "N7ofsiPv12qi"
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)"
   ],
   "metadata": {
    "id": "Ih2CTVBnArVZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font size=\"5\" color='grey'> <b>\n",
    "Multimodal - Audio\n",
    "</b></font> </br></p>\n",
    "\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "6jJZ7wbdArVc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul\nfrom genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages\nsetup_api_keys(['OPENAI_API_KEY', 'HF_TOKEN'], create_globals=False)\nprint()\ncheck_environment()\nprint()\nget_ipinfo()\n# Bei Bedarf: Trennen zwischen Installationsname () und Importname (f√ºr Python) beide Angaben in Klammern\n# install_packages([('markitdown[all]', 'markitdown'), 'langchain_chroma', ]"
   ],
   "metadata": {
    "id": "5a_GNgFlHQ1-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title üõ†Ô∏è Walkaround Gradio/Colab - 2 x starten  { display-mode: \"form\" }\n",
    "#\n",
    "# Walkaround Gradio/Colab nach der Ausf√ºhrung funktioniert auch Gradio Mikro üòäüòä\n",
    "# Code erstellt keine Audio-Aufnahme\n",
    "#\n",
    "from IPython.display import Javascript, display, HTML\n",
    "from google.colab import output\n",
    "from base64 import b64decode\n",
    "import io\n",
    "from IPython.display import Audio\n",
    "from pydub import AudioSegment\n",
    "import time\n",
    "\n",
    "# Verbessertes JavaScript mit besserer Fehlerbehandlung\n",
    "RECORD = \"\"\"\n",
    "const sleep = time => new Promise(resolve => setTimeout(resolve, time));\n",
    "\n",
    "const b2text = blob => new Promise(resolve => {\n",
    "    const reader = new FileReader();\n",
    "    reader.onloadend = e => resolve(e.srcElement.result);\n",
    "    reader.readAsDataURL(blob);\n",
    "});\n",
    "\n",
    "var record = async function(time) {\n",
    "    try {\n",
    "        // Explizite Anfrage f√ºr Medienzugriff mit Fehlermeldung\n",
    "        const stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
    "            .catch(err => {\n",
    "                console.error(\"Mikrofonzugriff verweigert:\", err.message);\n",
    "                return null;\n",
    "            });\n",
    "\n",
    "        if (!stream) {\n",
    "            return \"ERROR: Mikrofonzugriff nicht m√∂glich\";\n",
    "        }\n",
    "\n",
    "        // Recorder erstellen und konfigurieren\n",
    "        const recorder = new MediaRecorder(stream);\n",
    "        const chunks = [];\n",
    "\n",
    "        recorder.ondataavailable = e => chunks.push(e.data);\n",
    "\n",
    "        // Promise f√ºr das Ende der Aufnahme\n",
    "        const recordingPromise = new Promise(resolve => {\n",
    "            recorder.onstop = async () => {\n",
    "                const blob = new Blob(chunks);\n",
    "                const text = await b2text(blob);\n",
    "\n",
    "                // Alle Tracks beenden um Ressourcen freizugeben\n",
    "                stream.getTracks().forEach(track => track.stop());\n",
    "\n",
    "                resolve(text);\n",
    "            };\n",
    "        });\n",
    "\n",
    "        // Aufnahme starten\n",
    "        recorder.start();\n",
    "        console.log(\"Aufnahme gestartet...\");\n",
    "\n",
    "        // Warten und dann stoppen\n",
    "        await sleep(time);\n",
    "        recorder.stop();\n",
    "        console.log(\"Aufnahme beendet.\");\n",
    "\n",
    "        // Auf das Ergebnis warten\n",
    "        return await recordingPromise;\n",
    "    } catch (err) {\n",
    "        console.error(\"Fehler bei der Aufnahme:\", err.message);\n",
    "        return \"ERROR: \" + err.message;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def record(seconds=3):\n",
    "    # Benutzer informieren\n",
    "    print(f\"Bereite Aufnahme vor ({seconds} Sekunden)...\")\n",
    "    print(\"Bitte erteilen Sie die Mikrofonzugriffsberechtigung, wenn der Browser danach fragt.\")\n",
    "\n",
    "    # JavaScript-Code ausf√ºhren\n",
    "    display(Javascript(RECORD))\n",
    "\n",
    "    # Kurze Pause, um sicherzustellen, dass JavaScript geladen wurde\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Explizite Benutzer-Anweisung\n",
    "    display(HTML(\"\"\"\n",
    "    <div style=\"background-color: #f8f9fa; padding: 10px; border-radius: 5px; margin: 10px 0;\">\n",
    "        <p><b>Hinweis:</b> Wenn ein Dialog zur Mikrofonberechtigung erscheint, w√§hlen Sie bitte \"Zulassen\".</p>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "\n",
    "    # Aufnahme starten mit Timeout-Handhabung\n",
    "    try:\n",
    "        print(f\"Aufnahme l√§uft f√ºr {seconds} Sekunden. Bitte sprechen Sie jetzt...\")\n",
    "\n",
    "        # Timeout-Wert etwas h√∂her als die eigentliche Aufnahmezeit setzen\n",
    "        result = output.eval_js(f'record({seconds * 1000})', timeout_sec=(seconds + 5))\n",
    "\n",
    "        # Pr√ºfen auf Fehler\n",
    "        if isinstance(result, str) and result.startswith(\"ERROR\"):\n",
    "            print(f\"Fehler aufgetreten: {result}\")\n",
    "            return None\n",
    "\n",
    "        # Audio dekodieren\n",
    "        binary = b64decode(result.split(',')[1])\n",
    "\n",
    "        # In AudioSegment konvertieren\n",
    "        audio = AudioSegment.from_file(io.BytesIO(binary), format=\"webm\")\n",
    "        print(\"Aufnahme erfolgreich abgeschlossen!\")\n",
    "\n",
    "        return audio\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der Aufnahme: {str(e)}\")\n",
    "        print(\"Tipp: Stellen Sie sicher, dass Sie die Mikrofonberechtigung erteilt haben.\")\n",
    "        return None\n",
    "\n",
    "# Aufnahme starten\n",
    "# print(\"Dr√ºcken Sie Enter, um die Aufnahme zu starten...\")\n",
    "# input()\n",
    "audio = record(1)\n",
    "\n",
    "# Wenn die Aufnahme erfolgreich war, abspielen\n",
    "# if audio is not None:\n",
    "#     display(Audio(data=audio.export(format=\"wav\").read(), rate=audio.frame_rate))\n",
    "#     print(\"Sie k√∂nnen die Audio-Datei oben abspielen.\")"
   ],
   "metadata": {
    "id": "6k9LOybVyBzl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 | Einf√ºhrung\n",
    "---"
   ],
   "metadata": {
    "id": "oYvUY6gMBKO1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "K√ºnstliche Intelligenz er√∂ffnet vielf√§ltige M√∂glichkeiten zur Analyse, Erzeugung und Weiterverarbeitung von Audiodaten. Die folgende √úbersicht gliedert zentrale Aufgabenbereiche der KI-gest√ºtzten Audiotechnologie in thematische Kategorien. Sie reicht von der Klassifikation und Sprachanalyse √ºber die synthetische Sprach- und Musikgenerierung bis hin zu Sicherheits- und Strukturierungsma√ünahmen ‚Äì und bietet so einen praxisnahen Einblick in die aktuellen Einsatzfelder intelligenter Audiotechnik."
   ],
   "metadata": {
    "id": "CuX0rVeCG8yH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "| **Kategorie**                        | **Aufgabe**                               | **Beschreibung**                                                                              |\n",
    "| ------------------------------------ | ----------------------------------------- | --------------------------------------------------------------------------------------------- |\n",
    "| üß† **Analyse & Klassifikation**      | **Audio-Klassifikation**                  | Zuordnung von Audioinhalten zu Kategorien (z.‚ÄØB. Musik, Sprache, Ger√§usch).                   |\n",
    "|                                      | Sprechererkennung                         | Identifikation oder Verifikation einer Person anhand ihrer Stimme.                            |\n",
    "|                                      | **Emotionserkennung**                         | Analyse der Stimme zur Einsch√§tzung von Emotionen (z.‚ÄØB. w√ºtend, fr√∂hlich).                   |\n",
    "|                                      | **Spracherkennung**                           | Erkennung der gesprochenen Sprache (z.‚ÄØB. Deutsch, Englisch, Franz√∂sisch).                    |\n",
    "|                                      | Ereigniserkennung                         | Identifikation spezifischer Ger√§usche oder akustischer Ereignisse (z.‚ÄØB. Sirene, T√ºrknallen). |\n",
    "|                                      | Musikgenre-Klassifikation                 | Automatische Erkennung des Musikstils (z.‚ÄØB. Jazz, Rock, Klassik).                            |\n",
    "| üìù **Generierung & Transformation**  | **Text-to-Speech (TTS)**                  | Umwandlung von Text in nat√ºrliche Sprache (synthetische Sprachausgabe).                       |\n",
    "|                                      | Voice Cloning                             | Imitation einer bestimmten Stimme f√ºr synthetische Sprachgenerierung.                         |\n",
    "|                                      | Sprachstiltransfer                        | Ver√§nderung von Tonlage, Akzent oder Sprechstil (z.‚ÄØB. neutral ‚Üî emotional).                  |\n",
    "|                                      | Musikgenerierung                          | Automatisches Erzeugen von Musikst√ºcken oder Melodien durch KI.                               |\n",
    "|                                      | Audio-Super-Resolution                    | Verbesserung der Klangqualit√§t (z.‚ÄØB. Hochskalierung der Abtastrate).                         |\n",
    "|                                      | Audio-Restauration                        | Entfernung von Rauschen, Klicks oder Verzerrungen aus alten Aufnahmen.                        |\n",
    "| üß© **Erg√§nzung & Vervollst√§ndigung** | L√ºckenf√ºllung in Audio                    | Erg√§nzung fehlender Audiosegmente (z.‚ÄØB. durch KI erzeugte √úberg√§nge).                        |\n",
    "|                                      | Audio-Inpainting                          | Rekonstruktion besch√§digter oder unvollst√§ndiger Audiost√ºcke.                                 |\n",
    "|                                      | Sprachfortsetzung                     | Fortsetzung einer Rede oder Geschichte auf Basis eines gesprochenen Anfangs.                  |\n",
    "| üìö **Informationsgewinnung**         | **Spracherkennung (ASR)**                 | Umwandlung gesprochener Sprache in geschriebenen Text.                                        |\n",
    "|                                      | Schl√ºsselworterkennung (Keyword Spotting) | Detektion bestimmter W√∂rter oder Befehle im Audio (z.‚ÄØB. ‚ÄûHey Siri‚Äú).                         |\n",
    "|                                      | **Audio-Transkription**                      | Erstellung eines schriftlichen Transkripts (z.‚ÄØB. f√ºr Podcasts, Meetings).                    |\n",
    "|                                      | Audio-Indexierung & -Suche                | Durchsuchbarkeit gro√üer Audiodatenbanken durch akustische Merkmale.                           |\n",
    "| üõ°Ô∏è **Sicherheit & Strukturierung**  | Deepfake-Stimmerkennung                   | Erkennung synthetisch erzeugter oder gef√§lschter Stimmen.                                     |\n",
    "|                                      | Anonymisierung                            | Ver√§nderung der Stimme zur Wahrung der Privatsph√§re (z.‚ÄØB. Verzerrung).                       |\n",
    "|                                      | Audio-Wasserzeichen                       | Einbettung oder Erkennung unsichtbarer Marker zum Schutz vor Missbrauch.                      |\n",
    "|                                      | Formatkonvertierung                       | Umwandlung von Audioformaten (z.‚ÄØB. WAV ‚Üî MP3).                                               |\n"
   ],
   "metadata": {
    "id": "gIWL1cY6kR9S"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 | Speech-to-Text (STT)\n",
    "---"
   ],
   "metadata": {
    "id": "B27m4yz8h8cu"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Die Umwandlung gesprochener Sprache in Text, bekannt als Audio-Transkription, erm√∂glicht die automatische Verschriftlichung von Gespr√§chen, Interviews oder Vortr√§gen. Dies erleichtert die Dokumentation, Archivierung und Weiterverarbeitung von Audioinhalten. Moderne Spracherkennungstechnologien nutzen KI-Modelle, um Sprache pr√§zise zu erfassen und in lesbaren Text umzuwandeln.  \n",
    "\n",
    "Transkriptionen sind in vielen Bereichen von Bedeutung ‚Äì von der journalistischen Berichterstattung √ºber die Erstellung von Untertiteln bis hin zur Analyse von Kundenfeedback. Durch den Einsatz automatisierter Systeme kann dieser Prozess erheblich beschleunigt werden, wodurch gro√üe Mengen an Audio effizient verarbeitet werden k√∂nnen."
   ],
   "metadata": {
    "id": "sV3WQC5OyF5i"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import openai\n",
    "from google.colab import files\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Nutzer w√§hlt eine Datei aus und l√§dt sie hoch\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Den ersten Dateinamen extrahieren\n",
    "audio_file_path = list(uploaded.keys())[0]\n",
    "\n",
    "# Audiodatei √∂ffnen und Whisper nutzen\n",
    "with open(audio_file_path, \"rb\") as audio_file:\n",
    "    response = openai.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        file=audio_file,\n",
    "    )\n",
    "\n",
    "# Markdown-Formatierte Ausgabe\n",
    "markdown_output = f\"\"\"\n",
    "# üéôÔ∏è Transkript der Audiodatei\n",
    "\n",
    "**üìÇ Datei:** `{audio_file_path}`\n",
    "**ü§ñ Modell:** `Whisper-1`\n",
    "\n",
    "---\n",
    "\n",
    "{response.text}\n",
    "\"\"\"\n",
    "\n",
    "# Markdown in Google Colab anzeigen\n",
    "display(Markdown(markdown_output))"
   ],
   "metadata": {
    "id": "FcVxaTwgf8ze"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Transkript mit Zeitstempel\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "mD359pIyovAc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "with open(audio_file_path, \"rb\") as audio_file:\n",
    "    response = openai.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        file=audio_file,\n",
    "        response_format=\"verbose_json\"\n",
    "    )\n",
    "\n",
    "# Zugriff auf die Segment-Zeitstempel (S√§tze)\n",
    "display(Markdown(\"## üìÑ Transkript mit Zeitstempel\"))\n",
    "display(Markdown(f\"---\"))\n",
    "\n",
    "for segment in response.segments:\n",
    "    display(Markdown(f\"{segment.start:.2f}s - {segment.end:.2f}s: {segment.text}\"))"
   ],
   "metadata": {
    "id": "2dYp7bj8mGdE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 | Text-to-Speech (TTS)\n",
    "---"
   ],
   "metadata": {
    "id": "uE5RXwpTjPcw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fokus ist die Anwendung von Text-to-Speech (TTS) Large Language Models (LLMs) mit den fortschrittlichen L√∂sungen von OpenAI. Im Mittelpunkt steht das Modell TTS-1, ein leistungsf√§higes und vielseitiges System zur Umwandlung von geschriebenem Text in nat√ºrliche Sprache. TTS-1 wurde f√ºr den Echtzeiteinsatz optimiert und eignet sich daher besonders f√ºr Anwendungen, die eine geringe Latenz erfordern. Das Modell repr√§sentiert einen bedeutenden Fortschritt in der Sprachsynthese, da es mithilfe von Deep-Learning-Techniken qualitativ hochwertige und realistisch klingende Sprachausgaben erzeugt. Im Rahmen dieses Moduls werden die Funktionalit√§ten von TTS-1 untersucht, seine praktischen Einsatzm√∂glichkeiten analysiert und sein Einfluss auf verschiedene Branchen betrachtet ‚Äì von barrierefreien Kommunikationsl√∂sungen bis hin zu interaktiven Sprachantwortsystemen."
   ],
   "metadata": {
    "id": "bzlL3ITv0a-b"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Stimmen\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "Ave6gDyM1hkw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dieser Codeausschnitt demonstriert die Nutzung der Text-to-Speech-API von OpenAI zur Erzeugung gesprochener Sprache aus Text. Zun√§chst werden die ben√∂tigten Bibliotheken importiert: **openai** f√ºr die API-Kommunikation, **IPython.display** zur Audiowiedergabe in Jupyter-Notebooks und **base64** f√ºr die Kodierung. Die Variable `TEXT` enth√§lt den zu konvertierenden Text. Anschlie√üend wird die Funktion `openai.audio.speech.create()` mit drei Parametern aufgerufen: dem Modell (`\"tts-1\"`), der gew√§hlten Stimme (`\"alloy\"`) und dem Eingabetext.  \n",
    "\n",
    "OpenAI stellt mehrere Stimmen mit unterschiedlichen Charakteristiken zur Verf√ºgung:  \n",
    "\n",
    "* **Alloy** ‚Äì neutral  \n",
    "* **Echo** ‚Äì jugendlich  \n",
    "* **Fable** ‚Äì m√§nnlich  \n",
    "* **Onyx** ‚Äì tiefe m√§nnliche Stimme  \n",
    "* **Nova** ‚Äì weiblich  \n",
    "* **Shimmer** ‚Äì warme weibliche Stimme  \n",
    "\n",
    "Jede dieser Stimmen hat eigene klangliche Eigenschaften, sodass je nach Anwendung die passende ausgew√§hlt werden kann. Zudem bietet OpenAI mit `\"tts-1-hd\"` eine hochaufl√∂sende Variante f√ºr eine verbesserte Audioqualit√§t, die jedoch mit einer h√∂heren Latenz verbunden sein kann. Die Funktion liefert ein Antwortobjekt zur√ºck, aus dem der generierte Audioinhalt extrahiert und in der Variable `audio_data` zur weiteren Verarbeitung oder Wiedergabe gespeichert wird."
   ],
   "metadata": {
    "id": "jot1zKM71Mvn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import openai\n",
    "from IPython.display import display, Audio\n",
    "\n",
    "# üìù Text\n",
    "summary_text = \"\"\"\n",
    "In dem Gedicht reitet ein Vater mit seinem Sohn durch die Nacht, w√§hrend der Junge von dem Erlk√∂nig tr√§umt, der ihn mit verlockenden Angeboten anlockt. Der Vater versucht, seinen Sohn zu beruhigen und erkl√§rt, dass die Erscheinungen nur Nebel sind. Doch der Erlk√∂nig wird bedrohlich und fordert den Jungen, was schlie√ülich zu einem tragischen Ende f√ºhrt, als der Vater mit seinem tot geglaubten Kind nach Hause zur√ºckkehrt.\n",
    "\"\"\"\n",
    "\n",
    "# üé§ Verf√ºgbare OpenAI-Stimmen\n",
    "voices = [\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"]\n",
    "\n",
    "# üìÇ Audiodateien f√ºr jede Stimme generieren und speichern\n",
    "audio_files = []\n",
    "for voice in voices:\n",
    "    tts_response = openai.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=voice,\n",
    "        input=summary_text\n",
    "    )\n",
    "\n",
    "    # üìÇ Speichern der Audiodatei\n",
    "    audio_filename = f\"summary_{voice}.mp3\"\n",
    "    with open(audio_filename, \"wb\") as audio_file:\n",
    "        audio_file.write(tts_response.content)\n",
    "\n",
    "    audio_files.append(audio_filename)"
   ],
   "metadata": {
    "id": "zb8mIeNIjS4I"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Ausgabe\n",
    "display(Markdown(\"## üîä ... Speech\"))\n",
    "display(Markdown(\"---\"))\n",
    "\n",
    "# üîä Ausgabe der Audiodateien\n",
    "for file in audio_files:\n",
    "    print(f\"üéß {file} erstellt und abspielbar!\")\n",
    "    display(Audio(file, autoplay=False))\n",
    "    print()"
   ],
   "metadata": {
    "id": "1YAPt0Ywyrug"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4 | Audio-Summary\n",
    "---"
   ],
   "metadata": {
    "id": "44jdmbAHipr3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Die automatische Audio-Zusammenfassung erm√∂glicht es, lange Sprachaufnahmen auf ihre wesentlichen Inhalte zu reduzieren. Mithilfe von KI-gest√ºtzten Algorithmen werden zentrale Aussagen erkannt und in kompakter Form wiedergegeben. Dies erleichtert die schnelle Erfassung wichtiger Informationen, ohne das gesamte Audio anh√∂ren zu m√ºssen.  \n",
    "\n",
    "Diese Technik wird h√§ufig f√ºr Podcasts, Meetings oder Interviews eingesetzt, um relevante Inhalte effizient zusammenzufassen. Durch den Einsatz moderner Spracherkennung und nat√ºrlicher Sprachverarbeitung (NLP) k√∂nnen pr√§zise und verst√§ndliche Zusammenfassungen erstellt werden."
   ],
   "metadata": {
    "id": "NvSnvX0fz1Nu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import openai\n",
    "from google.colab import files\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# üìÇ Nutzer l√§dt eine Audiodatei hoch\n",
    "uploaded = files.upload()\n",
    "audio_file_path = list(uploaded.keys())[0]\n",
    "\n",
    "# üéôÔ∏è Audiodatei transkribieren mit Whisper\n",
    "with open(audio_file_path, \"rb\") as audio_file:\n",
    "    response = openai.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        file=audio_file\n",
    "    )\n",
    "\n",
    "# üìù Transkribierter Text\n",
    "transcribed_text = response.text\n",
    "\n",
    "# üß† GPT-4o-mini: Zusammenfassung\n",
    "gpt_prompt = f\"\"\"\n",
    "Erstelle eine kurze Zusammenfassung des folgenden Textes. Die Zusammenfassung sollte maximal 3-4 S√§tze umfassen und die Hauptpunkte klar darstellen.\n",
    "\n",
    "### Transkript:\n",
    "{transcribed_text}\n",
    "\"\"\"\n",
    "\n",
    "gpt_response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"system\", \"content\": \"Du bist ein Experte f√ºr Textzusammenfassungen.\"},\n",
    "              {\"role\": \"user\", \"content\": gpt_prompt}],\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "# Antwort extrahieren\n",
    "summary_result = gpt_response.choices[0].message.content\n",
    "\n",
    "# Markdown-Formatierte Ausgabe\n",
    "markdown_output = f\"\"\"\n",
    "## üéôÔ∏è Transkript der Audiodatei\n",
    "\n",
    "**üìÇ Datei:** `{audio_file_path}`\n",
    "**ü§ñ Modell:** `Whisper-1`\n",
    "\n",
    "---\n",
    "\n",
    "{transcribed_text}\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úçÔ∏è Zusammenfassung:\n",
    "\n",
    "{summary_result}\n",
    "\"\"\"\n",
    "\n",
    "# üìå Markdown in Google Colab anzeigen\n",
    "display(Markdown(markdown_output))"
   ],
   "metadata": {
    "id": "4JTY4ikLis7Y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# 5 | Audio-/Sprachanalyse\n",
    "---"
   ],
   "metadata": {
    "id": "wRvUmkuGh4Qm"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Durch die Analyse von Inhalten, Tonlage, Betonung und Sprachmelodie kann die emotionale Stimmung in einer Aufnahme erkannt werden. Ein Fokus in diesem Modul ist die Stimmungsanalyse aus Texten, auch **Sentiment Analysis** genannt, sie ist eine Methode zur automatischen Erkennung der emotionalen Haltung in geschriebenen Inhalten. Mithilfe nat√ºrlicher Sprachverarbeitung (NLP) werden Texte auf positive, neutrale oder negative Stimmungen untersucht. Diese Technik wird h√§ufig im Kundenfeedback, in sozialen Medien oder in Produktbewertungen eingesetzt, um Meinungen systematisch auszuwerten.  \n",
    "\n",
    "Moderne KI-Modelle analysieren nicht nur einzelne W√∂rter, sondern ber√ºcksichtigen auch den Kontext, um Stimmungen pr√§ziser zu erfassen. Unternehmen nutzen diese Technologie, um Trends zu erkennen, Stimmungen zu messen oder die Kundenzufriedenheit zu verbessern."
   ],
   "metadata": {
    "id": "GZ70N-_-zIQy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import openai\n",
    "from google.colab import files\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# üìÇ Nutzer l√§dt eine Audiodatei hoch\n",
    "uploaded = files.upload()\n",
    "audio_file_path = list(uploaded.keys())[0]\n",
    "\n",
    "# üéôÔ∏è Audiodatei transkribieren mit Whisper\n",
    "with open(audio_file_path, \"rb\") as audio_file:\n",
    "    response = openai.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        file=audio_file\n",
    "    )\n",
    "\n",
    "# üìù Transkribierter Text\n",
    "transcribed_text = response.text\n",
    "\n",
    "# üß† Schema f√ºr eine Audio/Sprachanalyse\n",
    "prompt = f\"\"\"\n",
    "\n",
    "Hauptpunkt: Inhaltsanalyse:\n",
    "    Themen und Argumentationsstruktur: Was wird gesagt? Wie ist das Gesagte aufgebaut?\n",
    "    Sprachstil: Alltagssprache vs. Fachsprache, formell vs. informell\n",
    "    Rhetorische Mittel: Metaphern, Wiederholungen, Fragen, Ironie, Pausen\n",
    "    Satzstruktur & Wortwahl: Einfach oder komplex? Viele F√ºllw√∂rter?\n",
    "    Erz√§hlperspektive: Ich-Form, Du-Ansprache, distanziert?\n",
    "    Tonfall und Haltung: Neutral, ironisch, kritisch, emotional?\n",
    "\n",
    "Hauptpunkt: Parasprachliche Merkmale (Stimme, Ton, Sprechweise):\n",
    "    Sprechtempo: Schnell, langsam, variierend\n",
    "    Intonation: Monoton oder lebendig? Betonung gezielt eingesetzt?\n",
    "    Lautst√§rke: Konstant, wechselnd, passend zur Situation?\n",
    "    Stimmqualit√§t: Klar, nasal, heiser, angespannt?\n",
    "    Pausen: Nat√ºrlich oder unnat√ºrlich? Strategisch gesetzt?\n",
    "    Versprecher oder Selbstkorrekturen: H√§ufig? Sympathisch oder irritierend?\n",
    "\n",
    "Hauptpunkt: Wirkung auf die Zuh√∂rer:innen\n",
    "    Verst√§ndlichkeit: Wird der Inhalt klar transportiert?\n",
    "    Authentizit√§t: Wirkt die Person glaubw√ºrdig und echt?\n",
    "    Emotionale Ansprache: Ber√ºhrt der Inhalt? Motiviert? Regt zum Denken an?\n",
    "    Zielgruppenorientierung: Passt Sprache und Ton zum Publikum?\n",
    "\n",
    "Hauptpunkt Stimmungsanalyse:\n",
    "    Stimmung: Positiv üòä, Neutral üòê, Negativ üòû\n",
    "    Begr√ºnde Deine Einsch√§tzung\n",
    "\n",
    "Hauptpunkt: Technische Aspekte\n",
    "    Audioqualit√§t: Rauschfrei, Hall, Hintergrundger√§usche?\n",
    "    Mikrofonierung: Richtiger Abstand, Pop-Ger√§usche, Aussteuerung?\n",
    "    Schnitt & Nachbearbeitung: Erkennbar geschnitten? √úberg√§nge sauber?\n",
    "\n",
    "Hauptpunkt: Weitere Aspekte:\n",
    "    Gibt es ethische Aspekte die angesprochen werden m√ºssten.\n",
    "    Gibt es versteckte Botschaften?\n",
    "    Welche Punkte k√∂nnten noch interessant sein, an die ich nicht gedacht habe?\n",
    "\n",
    "Vermeide Formulierungen wie k√∂nnte oder sollte. Nimm konkret Stellung.\n",
    "\n",
    "Stelle bei der Ausgabe der Ergebniss nur bei den Hauptpunkte zur besseren Unterscheidung ein \"üîπ\" voran.\n",
    "\n",
    "### Text:\n",
    "{transcribed_text}\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "EHv1PSywfSzo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Audio-/Sprachanalyse aufrufen\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"system\", \"content\": \"Du bist ein Experte f√ºr Stimmungsanalysen.\"},\n",
    "              {\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "# Antwort extrahieren\n",
    "result = response.choices[0].message.content"
   ],
   "metadata": {
    "id": "XM_cG5H_J2Qo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Ausgabe\n",
    "# üìú Transkript-Header erstellen\n",
    "transcript_header = f\"\"\"\n",
    "## üéôÔ∏è Transkript der Audiodatei\n",
    "\n",
    "**üìÇ Datei:** `{audio_file_path}`\n",
    "**ü§ñ Modell:** `Whisper-1`\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "# Header anzeigen\n",
    "display(Markdown(transcript_header))\n",
    "\n",
    "# üìù Transkriptionstext anzeigen\n",
    "display(Markdown(transcribed_text))\n",
    "\n",
    "# üìä Trennlinie hinzuf√ºgen\n",
    "display(Markdown(\"---\"))\n",
    "\n",
    "# üßê Sprachanalyse anzeigen\n",
    "header = \"## üßê Audio-/Sprachanalyse:\"\n",
    "display(Markdown(header))\n",
    "display(Markdown(result))"
   ],
   "metadata": {
    "id": "ERkLDcijwoNw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 | Audio-Pipeline\n",
    "---"
   ],
   "metadata": {
    "id": "N7ofsiPv12qi"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zQRIrJrLxj3"
   },
   "source": [
    "Eine Audio-Pipleine kombiniert verschiedene Verarbeitungsschritte, um gesprochene Inhalte effizient zu analysieren, zu verarbeiten und bereitzustellen. Durch die Verkn√ºpfung mehrerer Technologien entstehen leistungsstarke Anwendungen, die Sprache in mehreren Stufen umwandeln und auswerten k√∂nnen.\n",
    "\n",
    "`Audio ‚Üí STT ‚Üí Chat ‚Üí TTS  `\n",
    "\n",
    "**Hinweis:**\n",
    "\n",
    "Die hier vorgestellte Technik wird genutzt, um Audioaufnahmen in Colab zu erstellen und wurde mit ChatGPT und [Link](https://gist.github.com/korakot/c21c3476c024ad6d56d5f48b0bca92be) erstellt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Audio aufzeichnen\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "1hpBx1-mu7Qt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dieser Code erstellt eine einfache Audioaufnahme-Anwendung mit Gradio. Hier ist was er macht:\n",
    "\n",
    "1. Der Code importiert ben√∂tigte Bibliotheken:\n",
    "   - `gradio` f√ºr die Erstellung der Benutzeroberfl√§che\n",
    "   - `scipy.io.wavfile` zum Speichern von Audiodateien\n",
    "   - `numpy` f√ºr die Verarbeitung numerischer Daten\n",
    "   - `files` aus Google Colab zum Herunterladen von Dateien\n",
    "\n",
    "2. Die Funktion `aufnehmen_und_speichern()` verarbeitet Audiodaten:\n",
    "   - Sie pr√ºft, ob Audio aufgenommen wurde\n",
    "   - Bei vorhandenen Audiodaten extrahiert sie die Abtastrate und die Audiodaten\n",
    "   - Speichert die Aufnahme als \"audio.wav\"\n",
    "   - L√∂st den Download der Datei aus (in Google Colab)\n",
    "   - Gibt die Audiodaten zur√ºck, damit sie in der Oberfl√§che abgespielt werden k√∂nnen\n",
    "\n",
    "3. Eine Gradio-Oberfl√§che wird erstellt mit:\n",
    "   - Einem Audio-Input (Mikrofon) f√ºr die Aufnahme\n",
    "   - Einem Audio-Output zum Abspielen der aufgenommenen Datei\n",
    "   - Einem Titel und einer Beschreibung\n",
    "\n",
    "4. Schlie√ülich wird die Anwendung mit `demo.launch()` gestartet\n",
    "\n",
    "Zusammengefasst ist dies eine kompakte Webanwendung, die Audio aufnimmt, es speichert, automatisch zum Download anbietet und gleichzeitig die M√∂glichkeit bietet, die Aufnahme anzuh√∂ren."
   ],
   "metadata": {
    "id": "OpFZmA_K4Id8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Import\n",
    "import gradio as gr\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "from google.colab import files\n",
    "from IPython.display import display, HTML, Audio, Markdown\n",
    "from openai import OpenAI\n",
    "from pydub import AudioSegment"
   ],
   "metadata": {
    "id": "Ob7qNi_Twgy7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Funktion Aufnehmen\n",
    "def aufnehmen_und_speichern(audio):\n",
    "    if audio is not None:\n",
    "        # audio ist ein Tupel (sample_rate, data)\n",
    "        sample_rate, data = audio\n",
    "\n",
    "        # Speichern der aufgenommenen Datei\n",
    "        wav.write(\"audio_in.wav\", sample_rate, data)\n",
    "\n",
    "        # Download ausl√∂sen\n",
    "        files.download(\"audio_in.wav\")\n",
    "\n",
    "        # Wichtig: Die aufgenommene Audiodatei korrekt zur√ºckgeben\n",
    "        return (sample_rate, data)\n",
    "    else:\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": true,
    "id": "zc8Yg0gDZn1S"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "demo = gr.Interface(\n",
    "    fn=aufnehmen_und_speichern,\n",
    "    inputs=gr.Audio(sources=\"microphone\", type=\"numpy\"),\n",
    "    outputs=gr.Audio(type=\"numpy\"),  # Setze den gleichen Typ f√ºr Output\n",
    "    title=\"üéôÔ∏è Audio Aufnahme\",\n",
    "    description=\"Nimm ein Audio auf und und speichert es.\",\n",
    "    flagging_mode=\"never\"\n",
    ")"
   ],
   "metadata": {
    "id": "UYTyKb8xw1R9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "demo.launch()"
   ],
   "metadata": {
    "id": "1Gn9bMIqmh-W"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "STT\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "8PpQ-h_4u2kn"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jurTAwUCKRM-"
   },
   "source": [
    "Dieser Code verwendet OpenAI's Whisper-Modell, um eine automatische Transkription einer Audiodatei durchzuf√ºhren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qrf898AGONJ6"
   },
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "def transcribe_audio(filename):\n",
    "    with open(filename, \"rb\") as audio_file:\n",
    "        transcription = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file\n",
    "        )\n",
    "    return transcription.text\n",
    "\n",
    "# Transkribieren Sie die aufgezeichneten Audiodaten\n",
    "transcription = transcribe_audio(\"/content/audio_in.wav\")\n",
    "\n",
    "display(Markdown(\"## üìÑ Transkribiert:\"))\n",
    "display(Markdown(f\"---\"))\n",
    "display(Markdown(f\"{transcription}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Chat\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "KP-QD1--RhNE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def query_llm(prompt):\n",
    "  response = client.chat.completions.create(\n",
    "      model=\"gpt-4o-mini\",  # Use the specified model\n",
    "      messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "      temperature=0.2  # Adjust temperature as needed\n",
    "  )\n",
    "  return response.choices[0].message.content\n",
    "\n",
    "#\n",
    "user_prompt = transcription\n",
    "llm_response = query_llm(user_prompt)\n",
    "\n",
    "display(Markdown(\"## ü§ñ Chat\"))\n",
    "display(Markdown(f\"---\"))\n",
    "display(Markdown(f\"**Prompt:** {user_prompt}\"))\n",
    "display(Markdown(f\"**LLM Antwort:** {llm_response}\"))"
   ],
   "metadata": {
    "id": "lUeuQi8fRmr6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "TTS\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "LUF4xhCi6os4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Der Code nimmt einen Text als Eingabe, wandelt ihn mithilfe eines KI-gest√ºtzten Text-to-Speech-Modells (`tts-1`) in gesprochene Sprache um und gibt die generierte Audiodatei aus."
   ],
   "metadata": {
    "id": "8Oj6aF_y7gIQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_text(text, voice=\"onyx\"):\n",
    "    response = client.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=voice,\n",
    "        input=text\n",
    "    )\n",
    "    audio_data = response.content\n",
    "    return audio_data  # Audiodaten direkt zur√ºckgeben\n",
    "\n",
    "def speak_text(text):\n",
    "    display(Markdown(\"## üé§ ... Audio wird jetzt erstellt ...\"))\n",
    "    audio_data = generate_text(text)\n",
    "    with open(\"audio_out.mp3\", \"wb\") as audio_file:\n",
    "        audio_file.write(audio_data)\n",
    "    print()\n",
    "    display(Markdown(\"## üîä ... Speech\"))\n",
    "    display(Markdown(\"---\"))\n",
    "    print()\n",
    "    display(Audio(audio_data))\n",
    "\n",
    "display(Markdown(\"## üìÑ Text to ...\"))\n",
    "display(Markdown(f\"---\"))\n",
    "display(Markdown(f\"{llm_response}\"))\n",
    "display(Markdown(f\"---\"))\n",
    "speak_text(llm_response)"
   ],
   "metadata": {
    "id": "BZp9rMrG6q5f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Audio-Podcast\n",
    "</font></p>\n"
   ],
   "metadata": {
    "id": "OgCqGs1dn-4O"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ein Podcast ist eine Serie von Audio- oder Videoepisoden, die √ºber das Internet bereitgestellt und meist abonniert werden k√∂nnen. Podcasts sind besonders beliebt, weil sie flexibel konsumiert werden k√∂nnen ‚Äì ob unterwegs, beim Sport oder zu Hause. Die Themenvielfalt reicht von Nachrichten, Bildung und Wissenschaft bis hin zu Unterhaltung, Interviews und pers√∂nlichen Geschichten. Um einen Podcast zu h√∂ren, ben√∂tigt man lediglich eine Podcast-App oder eine entsprechende Webseite."
   ],
   "metadata": {
    "id": "-G65hnEipY2o"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Podcast zum Kurns GenAI:**\n",
    "\n",
    "[DeepDive GenAI](https://open.spotify.com/show/4nxwhKtK2tvNeDoHS91iY9?si=qLmrGhxmStu6NpZD6_DLZg&nd=1&dlsi=b1bfd4c659384aad)"
   ],
   "metadata": {
    "id": "pZsFfkdkpaxR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**WebApps:**\n",
    "\n",
    "[NotebookLM](https://notebooklm.google.com/)     \n",
    "[jellypod](https://jellypod.ai/)"
   ],
   "metadata": {
    "id": "t8SOWpBdoojF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Einleitung generieren ..."
   ],
   "metadata": {
    "id": "w1gQS0aPthaj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Setze einen zuf√§lligen Seed basierend auf der aktuellen Zeit\n",
    "random.seed(time.time() * 1000000)\n",
    "\n",
    "# Zuf√§lliges Vorwort\n",
    "vorwort = [\n",
    "\"\"\"\n",
    "Sch√∂n, dass du dabei bist! In diesem Podcast sprechen wir √ºber Themen, die uns wirklich besch√§ftigen. Heute nehmen wir ein Thema unter die Lupe, das Dich auch schon mal interessiert hat.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Hallo und herzlich willkommen! Heute geht‚Äôs um ein Thema, das aktueller nicht sein k√∂nnte ‚Äì und das dich direkt betrifft.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Willkommen beim Podcast √ºber die gro√üen und kleinen Fragen unseres Alltags. Heute geht‚Äôs um ein Thema, das f√ºr Gespr√§chsstoff sorgt ‚Äì bleib dran!\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "index = random.randint(0, len(vorwort) - 1)\n",
    "intro = vorwort[index]"
   ],
   "metadata": {
    "id": "f14Wr6wb-U-N"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "display(Markdown(\"## üé§ ... Prolog wird erstellt ...\"))\n",
    "audio_data = generate_text(intro, \"nova\")\n",
    "\n",
    "with open(\"audio_prolog.mp3\", \"wb\") as audio_file:\n",
    "    audio_file.write(audio_data)"
   ],
   "metadata": {
    "id": "__-9oRiCtojm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Frage-Audio mit neuer Stimme generieren ..."
   ],
   "metadata": {
    "id": "PTZAIR4KrdHj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "display(Markdown(\"## üé§ ... Frage wird mit neuer Stimme erstellt ...\"))\n",
    "audio_data = generate_text(transcription, \"nova\")\n",
    "with open(\"audio_in.mp3\", \"wb\") as audio_file:\n",
    "    audio_file.write(audio_data)"
   ],
   "metadata": {
    "id": "ueYp6LkWq4LF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nachwort generieren ..."
   ],
   "metadata": {
    "id": "9WONrVEUz-2C"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Setze einen zuf√§lligen Seed basierend auf der aktuellen Zeit\n",
    "random.seed(time.time() * 1000000)\n",
    "\n",
    "# Zuf√§lliges Nachwort\n",
    "nachwort = [\n",
    "\"\"\"\n",
    "Puh, ganz sch√∂n viel Stoff heute! Nimm dir Zeit, das alles wirken zu lassen ‚Äì und wenn‚Äôs dir gefallen hat, abonnier gern den Podcast. Bis zum n√§chsten Mal!\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Krass! Das waren viele spannende Einblicke ‚Äì sch√∂n, dass du dabei warst! Lass das Geh√∂rte in Ruhe auf dich wirken, und wir freuen uns, wenn du beim n√§chsten Mal wieder reinh√∂rst. Bis bald!\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "Wow! Danke, dass du heute dabei warst. Wenn du Anregungen oder Fragen hast, schreib uns gern. Wir freuen uns schon auf die n√§chste Folge mit dir ‚Äì bis bald!\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "index = random.randint(0, len(nachwort) - 1)\n",
    "outtro = nachwort[index]"
   ],
   "metadata": {
    "id": "dgG1UNyk9Gp0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "display(Markdown(\"## üé§ ... Nachwort wird erstellt ...\"))\n",
    "audio_data = generate_text(outtro, \"nova\")\n",
    "\n",
    "with open(\"audio_epilog.mp3\", \"wb\") as audio_file:\n",
    "    audio_file.write(audio_data)"
   ],
   "metadata": {
    "id": "VR_aJDARz-2D"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Audios werden kombiniert"
   ],
   "metadata": {
    "id": "DgB2jtjjv8RH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Audiodateien laden\n",
    "audio1 = AudioSegment.from_file(\"audio_prolog.mp3\")\n",
    "audio2 = AudioSegment.from_file(\"audio_in.mp3\")\n",
    "audio3 = AudioSegment.from_file(\"audio_out.mp3\")\n",
    "audio4 = AudioSegment.from_file(\"audio_epilog.mp3\")\n",
    "display(Markdown(\"## üé§ ... Audios werden kombiniert ...\"))\n",
    "\n",
    "# Sekunden Stille erstellen (Dauer in Millisekunden)\n",
    "pause = AudioSegment.silent(duration=1000)  # 1000 ms = 1 Sekunden\n",
    "\n",
    "# Audiodateien verketten\n",
    "audio_combi = audio1 + audio2 + pause + audio3 + pause + audio4\n",
    "\n",
    "# Ergebnis speichern\n",
    "audio_combi.export(\"audio_combi.mp3\", format=\"mp3\")"
   ],
   "metadata": {
    "id": "jFjRidu-oj1T"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Audio ausgeben"
   ],
   "metadata": {
    "id": "dCy6WIX55MrB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Ausgabe\n",
    "print()\n",
    "display(Markdown(\"## üîä ... Speech\"))\n",
    "display(Markdown(\"---\"))\n",
    "print()\n",
    "\n",
    "# Anzeigen der gespeicherten Datei statt des AudioSegment-Objekts\n",
    "display(Audio(\"audio_combi.mp3\", autoplay=False))"
   ],
   "metadata": {
    "id": "zGWM-Azy5Jub"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# A | Aufgabe\n",
    "---"
   ],
   "metadata": {
    "id": "qQOpYCtX1N16"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen.\n"
   ],
   "metadata": {
    "id": "pwmSuPsGiQp6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Mehrsprachiger Audio√ºbersetzer\n",
    "</font></p>\n"
   ],
   "metadata": {
    "id": "1UViO3SJiSXy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "**Aufgabenstellung:**     \n",
    "Entwickln Sie einen Audio√ºbersetzer, der:\n",
    "\n",
    "+ Eine deutsche Audiodatei transkribiert\n",
    "+ Den transkribierten Text in eine andere Sprache deiner Wahl √ºbersetzt (z.B. Englisch, Spanisch, Franz√∂sisch)\n",
    "+ Die √úbersetzung in eine Audiodatei mit passender Stimme umwandelt\n",
    "+ Eine Benutzeroberfl√§che bietet, die den Originaltext, die √úbersetzung und beide Audiodateien anzeigt\n",
    "\n",
    "**Hinweis:**\n",
    "\n",
    "Verwende Whisper f√ºr die Transkription, aber die √úbersetzung muss mit einem separaten Prompt an das LLM erfolgen. W√§hle eine Stimme, die zur Zielsprache passt. Die L√∂sung soll keine direkte Kopie der Beispiele sein, sondern eine eigene Integration der Komponenten.\n",
    "L√∂sen Sie die Aufgaben Schritt f√ºr Schritt, dass zumindest Teill√∂sungen vorliegen.\n"
   ],
   "metadata": {
    "id": "5SypeaJ72dnw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Audio-Inhaltsklassifikator\n",
    "</font></p>\n"
   ],
   "metadata": {
    "id": "Q_XZHI4C3oYz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Aufgabenstellung:**   \n",
    "\n",
    "Erstellen Sie ein einfaches Programm, das eine Audiodatei transkribiert und anschlie√üend den Inhalt klassifiziert.\n",
    "\n",
    "Das Programm soll:\n",
    "\n",
    "+ Eine Audiodatei mit Whisper transkribieren\n",
    "+ Den transkribierten Text mit einem LLM analysieren, um festzustellen, ob es sich um:\n",
    "\n",
    "    + ein Interview\n",
    "    + einen Monolog/Vortrag\n",
    "    + ein Gespr√§ch zwischen mehreren Personen\n",
    "    + eine andere Kategorie handelt\n",
    "\n",
    "Zus√§tzlich die Hauptthemen oder Schl√ºsselw√∂rter aus dem Inhalt identifizieren\n",
    "Die Ergebnisse √ºbersichtlich ausgeben"
   ],
   "metadata": {
    "id": "cFZQXaLI3tQ3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Hinweis:**\n",
    "\n",
    "L√∂sen Sie die Aufgaben Schritt f√ºr Schritt, dass zumindest Teill√∂sungen vorliegen."
   ],
   "metadata": {
    "id": "d1KVgqiF4MFR"
   }
  }
 ]
}
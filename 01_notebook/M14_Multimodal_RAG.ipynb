{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<p><font size=\"6\" color='grey'> <b>\n",
    "\n",
    "Generative KI. Verstehen. Anwenden. Gestalten.\n",
    "</b></font> </br></p>"
   ],
   "metadata": {
    "id": "Ih2CTVBnArVZ"
   },
   "id": "Ih2CTVBnArVZ"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font size=\"5\" color='grey'> <b>\n",
    "Multimodales RAG\n",
    "</b></font> </br></p>\n",
    "\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "6jJZ7wbdArVc"
   },
   "id": "6jJZ7wbdArVc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9c8df5ef",
    "collapsed": true
   },
   "outputs": [],
   "source": "#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul\nfrom genai_lib.utilities import (\n    check_environment,\n    get_ipinfo,\n    setup_api_keys,\n    mprint,\n    install_packages,\n    mermaid,\n    get_model_profile,\n    extract_thinking,\n    load_prompt\n)\nsetup_api_keys(['OPENAI_API_KEY', 'HF_TOKEN'], create_globals=False)\nprint()\ncheck_environment()\nprint()\nget_ipinfo()",
   "id": "9c8df5ef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4d2e699d"
   },
   "outputs": [],
   "source": [
    "#@title üõ†Ô∏è Installationen { display-mode: \"form\" }\n",
    "install_packages([\n",
    "    ('markitdown[all]', 'markitdown'),\n",
    "])"
   ],
   "id": "4d2e699d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "da4cf78d"
   },
   "outputs": [],
   "source": [
    "#@title üìÇ Dokumente, Bilder { display-mode: \"form\" }\n",
    "!rm -rf files\n",
    "!mkdir files\n",
    "\n",
    "# --- Texte\n",
    "!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02_daten/01_text/biografien_1.txt -o files/biografien_1.txt\n",
    "!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02_daten/01_text/biografien_2.md -o files/biografien_2.md\n",
    "!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02_daten/01_text/biografien_3.pdf -o files/biografien_3.pdf\n",
    "!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02_daten/01_text/biografien_4.docx -o files/biografien_4.docx\n",
    "!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02_daten/01_text/roboter.txt -o files/roboter.txt\n",
    "\n",
    "# --- Bilder\n",
    "!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02_daten/02_bild/retro_robot.jpg -o files/retro_robot.jpg\n",
    "!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02_daten/02_bild/hedra_cyborg.png -o files/hedra_cyborg.png\n",
    "!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02_daten/02_bild/apfel.png -o files/apfel.png\n",
    "!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02_daten/02_bild/zwei_roboter.png -o zwei_roboter.png"
   ],
   "id": "da4cf78d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "‚è∏Ô∏è 5-Minuten-Check:\n",
    "</font></p>\n",
    "\n",
    "**Ziel:** Pr√ºfen, ob du das vorherige Kapitel verstanden hast ‚Äì nicht, ob es gerade l√§uft.\n",
    "\n",
    "**Aufgabe** (5 Minuten, ohne Vorlage):\n",
    "\n",
    "Rekonstruiere die zentrale Idee oder Code-Struktur des letzten Abschnitts selbstst√§ndig\n",
    "(kein Copy & Paste, kein Nachschlagen).\n",
    "\n",
    "W√§hle eine der folgenden Optionen:\n",
    "\n",
    "+ Erkl√§re in 1‚Äì2 S√§tzen, was hier konzeptionell passiert.\n",
    "\n",
    "+ Ver√§ndere eine Kleinigkeit (z. B. Prompt, Parameter, Reihenfolge) und beschreibe die Auswirkung.\n",
    "\n",
    "+ Markiere eine Stelle, die du nicht sicher erkl√§ren kannst, und formuliere eine konkrete Frage dazu.\n",
    "\n",
    "**Hinweis:**\n",
    "Nicht alles muss ‚Äûfertig‚Äú oder ‚Äûkorrekt‚Äú sein. Entscheidend ist, wo dein Verst√§ndnis gerade endet"
   ],
   "metadata": {
    "id": "tlLLkZ_AdEa4"
   },
   "id": "tlLLkZ_AdEa4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 | RAG-Prozess\n",
    "---"
   ],
   "metadata": {
    "id": "B41ZbXwBQ2eW"
   },
   "id": "B41ZbXwBQ2eW"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In dem entwickelten **multimodalen Retrieval-Augmented-Generation (RAG)**-System werden sowohl Text- als auch Bilddaten verarbeitet.\n",
    "F√ºr die **Bildverarbeitung** kommt ein zweistufiger Ansatz zum Einsatz:\n",
    "\n",
    "1. **Image-Embedding:**\n",
    "   Das Bild wird in einen hochdimensionalen Vektorraum eingebettet, um visuelle Merkmale wie Formen, Farben und Strukturen numerisch zu repr√§sentieren.\n",
    "\n",
    "2. **Textbeschreibung und Text-Embedding:**\n",
    "   Zus√§tzlich wird mit *gpt-4o-mini* eine sprachliche Beschreibung des Bildes erzeugt. Diese Beschreibung wird anschlie√üend in ein Text-Embedding √ºberf√ºhrt, um semantische Informationen textbasiert nutzbar zu machen.\n",
    "\n",
    "---\n",
    "\n",
    "**Vorteile des Ansatzes:**\n",
    "\n",
    "* **Erweiterte semantische Repr√§sentation:**\n",
    "  Durch die Kombination von visuellen und sprachlichen Embeddings werden sowohl konkrete als auch konzeptuelle Eigenschaften eines Bildes abgebildet.\n",
    "\n",
    "* **Verbesserte Retrieval-Qualit√§t:**\n",
    "  Textbasierte Suchanfragen k√∂nnen nicht nur √ºber visuelle √Ñhnlichkeiten, sondern auch √ºber die semantisch beschriebene Bedeutung der Bilder beantwortet werden.\n",
    "\n",
    "* **H√∂here Interpretierbarkeit:**\n",
    "  Die generierte Bildbeschreibung erm√∂glicht eine transparente Nachvollziehbarkeit der zugrunde liegenden Repr√§sentationen und unterst√ºtzt bei der Evaluierung der Ergebnisse.\n",
    "\n",
    "* **Gute Erweiterbarkeit:**\n",
    "  Das Verfahren ist modular aufgebaut und l√§sst sich leicht um weitere Modalit√§ten wie Audio oder Video erg√§nzen.\n"
   ],
   "metadata": {
    "id": "w8JVLgSaEljT"
   },
   "id": "w8JVLgSaEljT"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "RAG-Prozess f√ºr Texte\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "RQ6sRV_FCboz"
   },
   "id": "RQ6sRV_FCboz"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/rag_process.png\" width=\"600\" alt=\"Avatar\">"
   ],
   "metadata": {
    "id": "bC_Exz8iWjIN"
   },
   "id": "bC_Exz8iWjIN"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "RAG-Prozess f√ºr Bilder\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "a2aUO4bhCgtv"
   },
   "id": "a2aUO4bhCgtv"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/rag_process_03.png\" width=\"600\" alt=\"Avatar\">"
   ],
   "metadata": {
    "id": "Uubxl-tr_1I_"
   },
   "id": "Uubxl-tr_1I_"
  },
  {
   "cell_type": "markdown",
   "id": "ce95c6f6",
   "metadata": {
    "id": "ce95c6f6"
   },
   "source": [
    "# 2 | Modul `multimodal_rag`\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Python-Modul f√ºr ein **funktionales multimodales RAG-System**, das Text- und Bilddokumente in einer einheitlichen Vektordatenbank verwaltet und durchsucht.\n",
    "\n",
    "**Modalit√§tsrichtungen**\n",
    "\n",
    "| Eingabe (Query) | Ausgabe (Antwort) | Beispiel / Beschreibung | Status |\n",
    "|-----------------|-------------------|-------------------------|--------|\n",
    "| **Text ‚Üí Text** | Textbasierte Frage f√ºhrt zu Textantwort | Klassisches RAG-System (z.B. Chatbot, Q&A) | ‚úÖ |\n",
    "| **Text ‚Üí Bild** | Textanfrage findet relevante Bilder | \"Zeige mir Roboter-Bilder\" | ‚úÖ |\n",
    "| **Bild ‚Üí Text** | Bildanalyse oder Captioning | \"Was ist auf diesem Foto zu sehen?\" | ‚úÖ |\n",
    "| **Bild ‚Üí Bild** | Bildretrieval oder visuelle Transformation | \"Finde √§hnliche Bilder\" | ‚úÖ |\n",
    "| **Bild ‚Üí Text/Bild** | Erweiterte multimodale Suche mit Bild | \"Alle Infos zu diesem Bild\" (CLIP + Semantik + Cross-Modal) | ‚úÖ |\n",
    "| **Text + Bild ‚Üí Text** | Kombination zur Textgenerierung | \"Welche Informationen enth√§lt dieses Diagramm?\" | ‚ùå |\n",
    "| **Text + Bild ‚Üí Bild** | Bedingte Bildgenerierung | \"Mach aus diesem Bild eine Winterversion\" | ‚ùå |\n",
    "\n",
    "**Hauptvorteile**\n",
    "\n",
    "1. **Funktionale Architektur**: Klare Trennung von Konfiguration, Komponenten und Funktionen\n",
    "2. **Einheitliche Datenbank**: ChromaDB mit separaten Collections f√ºr Text und Bilder\n",
    "3. **Hybride Suche**: Text-Embeddings (OpenAI) + Bild-Embeddings (CLIP)\n",
    "4. **Flexible Konfiguration**: Alle Parameter √ºber RAGConfig anpassbar\n",
    "5. **Bildbeschreibung**: Zu jedem Bild wird zus√§tzlich eine Bildbeschreibung erstellt."
   ],
   "metadata": {
    "id": "goAFmj5RYrAK"
   },
   "id": "goAFmj5RYrAK"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Aufbau\n",
    "</font></p>\n",
    "\n",
    "**RAGConfig**\n",
    "- Zentrale Konfigurationsklasse\n",
    "- Anpassbare Parameter (chunk_size, models, thresholds)\n",
    "\n",
    "**RAGComponents**\n",
    "- Container f√ºr alle System-Komponenten\n",
    "- Text-Embeddings, CLIP-Model, LLMs, Collections\n",
    "\n",
    "**Hauptfunktionen**\n",
    "- Datensammlung\n",
    "    - `init_rag_system()`: System-Initialisierung\n",
    "    - `process_directory()`: Bulk-Import von Dateien\n",
    "    - `add_text_document()`: Einzelnes Dokument hinzuf√ºgen\n",
    "    - `add_image_with_description()`: Bild mit Auto-Beschreibung\n",
    "- Abruf & Erweiterung\n",
    "    - `search_texts()`: Text-Suche inkl. Bildbeschreibungen\n",
    "    - `search_images()`: CLIP-basierte Bildsuche\n",
    "    - `search_similar_images()`: Bild‚ÜíBild √Ñhnlichkeitssuche\n",
    "    - `search_text_by_image()`: Bild‚ÜíText Suche\n",
    "    - `multimodal_search()`: Erweiterte multimodale Suche (Text-Query)\n",
    "    - `multimodal_search_by_image()`: Erweiterte multimodale Suche (Bild-Query)"
   ],
   "metadata": {
    "id": "ce_ZmxwJMiNd"
   },
   "id": "ce_ZmxwJMiNd"
  },
  {
   "cell_type": "code",
   "source": [
    "#@markdown   <p><font size=\"4\" color='green'> üßú‚Äç‚ôÄÔ∏è Prozess-Diagramm</font> </br></p>\n",
    "\n",
    "\n",
    "diagram = \"\"\"\n",
    "flowchart TB\n",
    "    User((\"Benutzer\"))\n",
    "\n",
    "    subgraph Init[\"Systeminitialisierung\"]\n",
    "        direction TB\n",
    "        RAGConfig[\"RAGConfig<br/>Konfiguration\"]\n",
    "        init_rag[\"init_rag_system\"]\n",
    "        RAGComponents[\"RAGComponents<br/>Text-Embeddings, CLIP,<br/>LLMs, Collections\"]\n",
    "\n",
    "        RAGConfig --> init_rag\n",
    "        init_rag --> RAGComponents\n",
    "    end\n",
    "\n",
    "    subgraph Generate[\"Datensammlung\"]\n",
    "        direction TB\n",
    "        process_dir[\"process_directory\"]\n",
    "        add_text[\"add_text_document\"]\n",
    "        add_image[\"add_image_with_description\"]\n",
    "        generate_desc[\"generate_image_description<br/>Vision LLM\"]\n",
    "\n",
    "        process_dir --> add_text\n",
    "        process_dir --> add_image\n",
    "        add_image --> generate_desc\n",
    "    end\n",
    "\n",
    "    subgraph Search[\"Suchfunktionen\"]\n",
    "        direction TB\n",
    "        multimodal[\"multimodal_search<br/>Kombinierte Suche\"]\n",
    "        search_text_by_img[\"search_text_by_image<br/>Bild ‚Üí Text\"]\n",
    "        search_images[\"search_images<br/>Text ‚Üí Bild\"]\n",
    "        search_texts[\"search_texts<br/>Text ‚Üí Text\"]\n",
    "        search_similar[\"search_similar_images<br/>Bild ‚Üí Bild\"]\n",
    "\n",
    "        multimodal --> search_images\n",
    "        multimodal --> search_texts\n",
    "        multimodal --> search_similar\n",
    "        multimodal --> search_text_by_img\n",
    "    end\n",
    "\n",
    "    subgraph DB[\"Datenbanken & Modelle\"]\n",
    "        direction LR\n",
    "        text_coll[(\"Text Collection<br/>ChromaDB\")]\n",
    "        image_coll[(\"Image Collection<br/>ChromaDB\")]\n",
    "        clip[\"CLIP Model<br/>ViT-B-32\"]\n",
    "        llm[\"LLM<br/>gpt-4o-mini\"]\n",
    "\n",
    "        text_coll ~~~ image_coll ~~~ clip ~~~ llm\n",
    "    end\n",
    "\n",
    "    %% Benutzer-Verbindungen\n",
    "    User -->|\"#1 Init\"| Init\n",
    "    User -->|\"#2 Generate\"| Generate\n",
    "    User -->|\"#3 Search\"| Search\n",
    "\n",
    "    %% Init zu DB\n",
    "    RAGComponents -.->|\"initialisiert\"| DB\n",
    "\n",
    "    %% Generate zu DB\n",
    "    Generate -.->|\"speichert\"| DB\n",
    "\n",
    "    %% Search zu DB\n",
    "    Search -.->|\"abfragt\"| DB\n",
    "\n",
    "    %% Styling\n",
    "    style Init fill:#ffebee,stroke:#c62828,stroke-width:2px\n",
    "    style Generate fill:#fff3e0,stroke:#e65100,stroke-width:2px\n",
    "    style Search fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\n",
    "    style DB fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px\n",
    "    style User fill:#cddc39,stroke:#827717,stroke-width:2px\n",
    "\"\"\"\n",
    "mermaid(diagram, 1000, 700)\n"
   ],
   "metadata": {
    "cellView": "form",
    "id": "WYLKH2v2AoKi"
   },
   "id": "WYLKH2v2AoKi",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Import des Moduls\n",
    "from genai_lib.multimodal_rag import (\n",
    "    # Init\n",
    "    init_rag_system,\n",
    "    get_system_status,\n",
    "\n",
    "    # Generate\n",
    "    process_directory,\n",
    "    add_text_document,\n",
    "    add_image_with_description,\n",
    "\n",
    "    # Search\n",
    "    multimodal_search,\n",
    "    multimodal_search_by_image,\n",
    "    search_texts,\n",
    "    search_images,\n",
    "    search_text_by_image,\n",
    "    search_similar_images,\n",
    ")"
   ],
   "metadata": {
    "id": "JkDO4IE9T7fl"
   },
   "id": "JkDO4IE9T7fl",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 | RAG initialisieren\n",
    "---"
   ],
   "metadata": {
    "id": "Szr8GWgQHRaY"
   },
   "id": "Szr8GWgQHRaY"
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialisierung\n",
    "rag = init_rag_system()"
   ],
   "metadata": {
    "id": "pLf6ZW9b7N2r"
   },
   "id": "pLf6ZW9b7N2r",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Konfiguration abfragen\n",
    "print(rag.config)\n",
    "\n",
    "# Konfiguration √§ndern (Beispiel: chunk_size √§ndern)\n",
    "# rag.config.chunk_size = 300"
   ],
   "metadata": {
    "id": "jHAP9o6hXJHs"
   },
   "id": "jHAP9o6hXJHs",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4 | Datensammlung\n",
    "---"
   ],
   "metadata": {
    "id": "BMa1SMEvU5GO"
   },
   "id": "BMa1SMEvU5GO"
  },
  {
   "cell_type": "code",
   "source": [
    "# Dokumente/Bilder laden & verarbeiten\n",
    "process_directory(rag, './files', auto_describe_images=True)"
   ],
   "metadata": {
    "id": "ADyLHzEPKBqE"
   },
   "id": "ADyLHzEPKBqE",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5 | Suche mit Text\n",
    "---"
   ],
   "metadata": {
    "id": "P3qn5ylkQM43"
   },
   "id": "P3qn5ylkQM43"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Text ‚Üí Text\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "x8cIHLcuZjGx"
   },
   "id": "x8cIHLcuZjGx"
  },
  {
   "cell_type": "code",
   "source": [
    "result = search_texts(rag, \"Was weisst Du √ºber Cyborgs?\")\n",
    "mprint(result)"
   ],
   "metadata": {
    "id": "_4QC-Rp3ZyYL"
   },
   "id": "_4QC-Rp3ZyYL",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Text ‚Üí Bild\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "KTjhySRZZqvQ"
   },
   "id": "KTjhySRZZqvQ"
  },
  {
   "cell_type": "code",
   "source": [
    "result = search_images(rag, \"Was weisst Du √ºber Cyborgs?\")\n",
    "mprint(result)"
   ],
   "metadata": {
    "id": "EC4mMFYXZqa8"
   },
   "id": "EC4mMFYXZqa8",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Text ‚Üí Text/Bild\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "bO9ANZ84Z0VL"
   },
   "id": "bO9ANZ84Z0VL"
  },
  {
   "cell_type": "code",
   "source": [
    "result = multimodal_search(rag, \"Was weisst Du √ºber Cyborgs?\")\n",
    "mprint(result)"
   ],
   "metadata": {
    "id": "wNUlfGbzJ_mx"
   },
   "id": "wNUlfGbzJ_mx",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 | Suche mit Bild\n",
    "---"
   ],
   "metadata": {
    "id": "MKp-dUs0d5lS"
   },
   "id": "MKp-dUs0d5lS"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Bild ‚Üí Bild\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "luEyYK_3aDZa"
   },
   "id": "luEyYK_3aDZa"
  },
  {
   "cell_type": "code",
   "source": [
    "result = search_similar_images(rag, \"./zwei_roboter.png\", k=5)\n",
    "mprint(\"## üñºÔ∏è Suche Bild ‚Üí Bild\")\n",
    "mprint(\"---\")\n",
    "for img in result:\n",
    "    mprint(f\"{img['filename']}: √Ñhnlichkeit: {img['similarity']}\")"
   ],
   "metadata": {
    "id": "dMaVQXrwd96e"
   },
   "id": "dMaVQXrwd96e",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Bild ‚Üí Text\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "QTalbIXZaJQY"
   },
   "id": "QTalbIXZaJQY"
  },
  {
   "cell_type": "code",
   "source": [
    "result = search_text_by_image(rag, \"./zwei_roboter.png\", k=5)\n",
    "mprint(result)"
   ],
   "metadata": {
    "id": "5GBaaj6QUIq_"
   },
   "id": "5GBaaj6QUIq_",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ey94tp3oj2o",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Bild ‚Üí Text/Bild\n",
    "</font></p>"
   ],
   "metadata": {
    "id": "ey94tp3oj2o"
   }
  },
  {
   "cell_type": "code",
   "id": "epstql0erd",
   "source": [
    "result = multimodal_search_by_image(rag, \"./zwei_roboter.png\", k_similar_images=3, k_text=3, k_related_images=2)\n",
    "mprint(result)"
   ],
   "metadata": {
    "id": "epstql0erd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "v1pnxcg3exq",
   "source": [
    "# 7 | Aufgabe\n",
    "---"
   ],
   "metadata": {
    "id": "v1pnxcg3exq"
   }
  },
  {
   "cell_type": "markdown",
   "id": "vfr2jthhvw8",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Teste das Multimodale RAG-System mit eigenen Queries\n",
    "</font></p>\n",
    "\n",
    "**Ziel**: Nutze das fertige multimodale RAG-System, um verschiedene Such-Operationen durchzuf√ºhren.\n",
    "\n"
   ],
   "metadata": {
    "id": "vfr2jthhvw8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Schritt 1: Einfache Queries testen**\n",
    "\n",
    "F√ºhre die folgenden Queries mit dem RAG-System aus und beobachte, welche Ergebnisse zur√ºckgeliefert werden:\n",
    "\n",
    "```python\n",
    "# Query 1: Text ‚Üí Text (Klassisches RAG)\n",
    "result = multimodal_search(rag, \"Wer war Alan Turing?\")\n",
    "mprint(result)\n",
    "\n",
    "# Query 2: Text ‚Üí Bild (Textanfrage findet Bilder)\n",
    "result = multimodal_search(rag, \"Zeige mir Bilder von Robotern\")\n",
    "mprint(result)\n",
    "\n",
    "# Query 3: Bild ‚Üí Bild (Finde √§hnliche Bilder)\n",
    "similar_images = search_similar_images(rag, \"./files/apfel.jpg\", k=3)\n",
    "mprint(\"## üñºÔ∏è √Ñhnliche Bilder:\")\n",
    "for img in similar_images:\n",
    "    mprint(f\"  ‚Ä¢ {img['filename']}: √Ñhnlichkeit {img['similarity']:.2f}\")\n",
    "\n",
    "# Query 4: Bild ‚Üí Text (Finde Textinformationen zu einem Bild)\n",
    "text_result = search_text_by_image(rag, \"./files/hedra_cyborg.png\", k=3)\n",
    "mprint(text_result)\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "id": "a9_HhGimXnlO"
   },
   "id": "a9_HhGimXnlO"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Schritt 2: Eigene Queries erstellen**\n",
    "\n",
    "Entwickle mindestens **3 eigene Queries**, die verschiedene Modalit√§ten nutzen:\n",
    "\n",
    "**Beispiele f√ºr kreative Queries:**\n",
    "\n",
    "- **Text‚ÜíText**: \"Was ist der Unterschied zwischen Robotern und Cyborgs?\"\n",
    "- **Text‚ÜíBild**: \"Finde alle futuristischen Bilder\"\n",
    "- **Bild‚ÜíBild**: Suche √§hnliche Bilder zu `a_retro-futuristic_robot_dall_e.jpg`\n",
    "- **Bild‚ÜíText**: Finde Textinformationen, die zum Apfel-Bild passen\n",
    "\n"
   ],
   "metadata": {
    "id": "jGDaXV7rXqSO"
   },
   "id": "jGDaXV7rXqSO"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Schritt 3: Bonus-Challenge (Optional)**\n",
    "\n",
    "Experimentiere mit den Suchparametern:\n",
    "\n",
    "```python\n",
    "# √Ñndere die Anzahl der Ergebnisse (k)\n",
    "result = multimodal_search(rag, \"Roboter\", k=5)\n",
    "\n",
    "# Vergleiche verschiedene Bilder f√ºr die Bild‚ÜíBild Suche\n",
    "similar_1 = search_similar_images(rag, \"./files/apfel.jpg\", k=3)\n",
    "similar_2 = search_similar_images(rag, \"./files/hedra_cyborg.png\", k=3)\n",
    "\n",
    "# Finde heraus, welche Bilder am √§hnlichsten zueinander sind\n",
    "```"
   ],
   "metadata": {
    "id": "v_WzXtmKXsGa"
   },
   "id": "v_WzXtmKXsGa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# A | √Ñhnlichkeitsmessung\n",
    "---"
   ],
   "metadata": {
    "id": "knkBxX2Two-i"
   },
   "id": "knkBxX2Two-i"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "1. Text-√Ñhnlichkeit (semantisch)\n",
    "</font></p>\n",
    "\n",
    "  Embedding-Modell: OpenAI text-embedding-3-small\n",
    "\n",
    "  Messmethode:\n",
    "  - ChromaDB nutzt L2-Distanz (Euklidische Distanz) f√ºr Text-Embeddings\n",
    "  - Werte: 0 = identisch, 2 = maximal entfernt\n",
    "  - Konvertierung Distanz zu √Ñhnlichkeit: similarity = max(0, 1 - (score / 2))\n",
    "  - Threshold: text_threshold: 1.2 (Zeile 51), Mindest-√Ñhnlichkeit: 0.3\n",
    "\n",
    "  Verwendung in:\n",
    "  - search_texts() - Text-Dokumente und Bildbeschreibungen durchsuchen\n",
    "  - multimodal_search() - Kombinierte Suche\n",
    "\n",
    "  ---"
   ],
   "metadata": {
    "id": "U0zYvcUGwtel"
   },
   "id": "U0zYvcUGwtel"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "2. Bild-√Ñhnlichkeit (visuell)\n",
    "</font></p>\n",
    "\n",
    "  Embedding-Modell: CLIP clip-ViT-B-32\n",
    "\n",
    "  Messmethode:\n",
    "  - ChromaDB nutzt Cosine-Distanz f√ºr Bild-Embeddings (Zeile 131: \"hnsw:space\": \"cosine\")\n",
    "  - Werte: 0 = identisch, 2 = maximal entfernt\n",
    "  - Konvertierung zu √Ñhnlichkeit:\n",
    "  similarity = round(max(0, 1 - distance), 3)\n",
    "  - Threshold: image_threshold: 0.8\n",
    "\n",
    "  Verwendung in:\n",
    "  - search_images() - Text ‚Üí Bild Suche √ºber CLIP   \n",
    "  - search_similar_images() - Bild ‚Üí Bild Suche  \n",
    "\n",
    "  ---\n",
    "  "
   ],
   "metadata": {
    "id": "JHroT_lGw4YI"
   },
   "id": "JHroT_lGw4YI"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "3. Cross-Modal-Retrieval (Text ‚Üî Bild)\n",
    "</font></p>\n",
    "\n",
    "\n",
    "  Methode: Indirekte √Ñhnlichkeit √ºber Bildbeschreibungen\n",
    "\n",
    "  Ablauf:\n",
    "  1. Text ‚Üí Bild: Text-Suche findet Bildbeschreibungen ‚Üí verkn√ºpfte Bilder werden abgerufen  \n",
    "  2. Bild ‚Üí Text: Bild-Suche findet √§hnliche Bilder ‚Üí deren Beschreibungen werden f√ºr semantische Textsuche verwendet\n",
    "\n",
    "  Verkn√ºpfung: Beide Collections sind √ºber text_doc_id ‚Üî image_doc_id referenziert\n",
    "\n",
    "  ---\n",
    "  "
   ],
   "metadata": {
    "id": "n-wEhLp4w9Vs"
   },
   "id": "n-wEhLp4w9Vs"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Zusammenfassung der Metriken:\n",
    "</font></p>\n",
    "\n",
    "  | Modalit√§t | Embedding-Modell              | Distanzmetrik  | Threshold | √Ñhnlichkeitsbereich |\n",
    "  |-----------|-------------------------------|----------------|-----------|---------------------|\n",
    "  | Text      | OpenAI text-embedding-3-small | L2-Distanz     | 1.2       | 0.3 - 1.0           |\n",
    "  | Bild      | CLIP ViT-B-32                 | Cosine-Distanz | 0.8       | 0.0 - 1.0           |\n",
    "\n",
    "  Die Konvertierung 1 - (distance / 2) normalisiert beide Distanzma√üe auf einen √Ñhnlichkeitswert von 0 bis 1, wobei 1 = maximale √Ñhnlichkeit bedeutet."
   ],
   "metadata": {
    "id": "hYXY40XQxHDo"
   },
   "id": "hYXY40XQxHDo"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "B41ZbXwBQ2eW",
    "ce95c6f6",
    "Szr8GWgQHRaY",
    "BMa1SMEvU5GO",
    "P3qn5ylkQM43",
    "MKp-dUs0d5lS",
    "v1pnxcg3exq",
    "knkBxX2Two-i"
   ],
   "toc_visible": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
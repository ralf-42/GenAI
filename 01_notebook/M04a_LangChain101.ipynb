{"cells":[{"cell_type":"markdown","id":"37rMyCQz2JNw","metadata":{"id":"37rMyCQz2JNw"},"source":["<p><font size=\"6\" color='grey'> <b>\n","\n","Generative KI. Verstehen. Anwenden. Gestalten.\n","</b></font> </br></p>"]},{"cell_type":"markdown","id":"R5CfUEMJdvFQ","metadata":{"id":"R5CfUEMJdvFQ"},"source":["<p><font size=\"5\" color='grey'> <b>\n","LangChain 101\n","</b></font> </br></p>\n","\n","---"]},{"cell_type":"code","source":["#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n","!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul\n","from genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages, load_chat_prompt_template, mermaid\n","setup_api_keys(['OPENAI_API_KEY'], create_globals=False)\n","print()\n","check_environment()\n","print()\n","get_ipinfo()"],"metadata":{"id":"PnT8m8TTUhF5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764695633224,"user_tz":-60,"elapsed":3146,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"03a2946a-a616-4125-e3d6-41d85b46d920"},"id":"PnT8m8TTUhF5","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úì OPENAI_API_KEY erfolgreich gesetzt\n","\n","Python Version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n","\n","Installierte LangChain- und LangGraph-Bibliotheken:\n","langchain                                1.1.0\n","langchain-chroma                         1.0.0\n","langchain-classic                        1.0.0\n","langchain-community                      0.4.1\n","langchain-core                           1.1.0\n","langchain-ollama                         1.0.0\n","langchain-openai                         1.1.0\n","langchain-text-splitters                 1.0.0\n","langgraph                                1.0.3\n","langgraph-checkpoint                     3.0.1\n","langgraph-prebuilt                       1.0.5\n","langgraph-sdk                            0.2.10\n","\n","IP-Adresse: 35.224.134.208\n","Hostname: 208.134.224.35.bc.googleusercontent.com\n","Stadt: Council Bluffs\n","Region: Iowa\n","Land: US\n","Koordinaten: 41.2619,-95.8608\n","Provider: AS396982 Google LLC\n","Postleitzahl: 51502\n","Zeitzone: America/Chicago\n"]}]},{"cell_type":"markdown","id":"91ee89a4-477b-41ce-a511-aadd4c2fd547","metadata":{"id":"91ee89a4-477b-41ce-a511-aadd4c2fd547"},"source":["# 1 | Was ist LangChain?\n","---"]},{"cell_type":"markdown","id":"b5004f8c","metadata":{"id":"b5004f8c"},"source":["LangChain ist ein Framework zur **Entwicklung von Anwendungen mit gro√üen Sprachmodellen** (LLMs). Es vereinfacht die Integration von LLMs in eigene Anwendungen durch:\n","\n","- Modulare Komponenten f√ºr verschiedene Aufgaben\n","- Vorgefertigte Ketten (Chains) f√ºr komplexe Workflows\n","- Einfache Integration externer Datenquellen\n","- Werkzeuge f√ºr das Speichern von Konversationskontexten\n","\n"]},{"cell_type":"markdown","id":"tl73bT-kKkia","metadata":{"id":"tl73bT-kKkia"},"source":["[Einf√ºhrung](https://python.langchain.com/docs/introduction/)   \n","[Konzepte](https://python.langchain.com/docs/concepts/)   \n","[API-References](https://python.langchain.com/api_reference/index.html)   \n","[Integrations](https://python.langchain.com/docs/integrations/providers/)\n","\n","---\n","\n","[Tutorials](https://python.langchain.com/docs/tutorials/)   \n","[How-to-Guides](https://python.langchain.com/docs/how_to/)   "]},{"cell_type":"markdown","id":"fGV8FwGmIwmR","metadata":{"id":"fGV8FwGmIwmR"},"source":["# 2 | √úberblick Konzepte\n","---\n","\n","\n"]},{"cell_type":"markdown","source":["Die Darstellung veranschaulicht das Grundprinzip von LangChain: die Verkettung verschiedener Konzepte zu einem durchg√§ngigen Verarbeitungsprozess, der Eingaben in strukturierte Ausgaben umwandelt."],"metadata":{"id":"nZ2ZMEd3ulKZ"},"id":"nZ2ZMEd3ulKZ"},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/langchain_prozess_01.png)"],"metadata":{"id":"KdBKNv3gzJAq"},"id":"KdBKNv3gzJAq"},{"cell_type":"markdown","source":["Konzepte von LangChain sind grundlegende Bausteine und Prinzipien. Hier sind einige der wichtigsten Konzepte:\n","\n","+ Nachrichten (**Messages**): Kommunikationseinheiten in Chat-Modellen f√ºr Ein- und Ausgabe.\n","+ Prompt-Vorlagen (**Prompt template**s): Komponenten zur Erstellung strukturierter Prompts f√ºr LLMs.\n","+ Chat-Modelle (**Chat models**): LLMs, die √ºber eine Chat-API verf√ºgbar sind und Sequenzen von Nachrichten verarbeiten.\n","+ Chains (**Chains**): Verkn√ºpfungen mehrerer LLMs oder anderer Komponenten f√ºr komplexere Anwendungen.\n","+ Strukturierte Ausgabe (**Structured output**): Technik, um Chat-Modelle in strukturierten Formaten antworten zu lassen.\n","+ Chat-Verlauf (**Chat history**): Eine Sequenz von Nachrichten, die eine Konversation darstellt.\n","+ Retrieval Augmented Generation (**RAG**): Technik zur Verbesserung von Sprachmodellen durch Kombination mit externen Wissensbasen.\n","+ Retriever (**Retriever**): Komponenten, die relevante Dokumente aus einer Wissensbasis abrufen.\n","+ Agenten (**Agents**): Nutzen Sprachmodelle, um Aktionssequenzen auszuw√§hlen und mit externen Ressourcen zu interagieren.\n","+ Tools (**Tools**): Funktionen mit definierten Schemata f√ºr Name, Beschreibung und Argumente.\n"],"metadata":{"id":"cCuow5DOEsID"},"id":"cCuow5DOEsID"},{"cell_type":"markdown","id":"EouwC_wdZb-c","metadata":{"id":"EouwC_wdZb-c"},"source":["# 3 | Nachrichten-Typen\n","---"]},{"cell_type":"markdown","id":"yS2mM2J1aXlY","metadata":{"id":"yS2mM2J1aXlY"},"source":["Beim Einsatz von Large Language Models (LLMs) wie GPT gibt es drei Hauptarten von Nachrichten (Messages), die die Interaktion mit dem Modell steuern. Diese Messages definieren, wie das Modell reagiert, welche Eingaben es erh√§lt und welche Ausgaben es generiert.\n","\n"]},{"cell_type":"markdown","id":"sGqRXptbbQ2a","metadata":{"id":"sGqRXptbbQ2a"},"source":["**System Message:**   \n","Die `System Message` dient dazu, das **Verhalten des LLMs** zu steuern. Sie ist eine nicht sichtbare Nachricht, die dem Modell Anweisungen gibt, wie es sich verhalten soll. Dabei kann sie die Pers√∂nlichkeit, den Tonfall und die Einschr√§nkungen des Modells beeinflussen. Die Systemnachricht funktioniert wie eine Art \"Rollenspiel-Anweisung\", um das Modell in eine gew√ºnschte Rolle zu versetzen.\n","\n","*Merkmale:*\n","- Definiert, wie das Modell reagieren soll\n","- Legt Pers√∂nlichkeit, Verhalten und Einschr√§nkungen fest\n","- Wird zu Beginn einer Sitzung gesetzt und bleibt bestehen\n","- Nicht sichtbar f√ºr den Benutzer\n","\n","*Beispiel einer System Message:*\n","```json\n","{\n","  \"role\": \"system\",\n","  \"content\": \"Du bist ein KI-Assistent, der pr√§zise und informative Antworten in einem professionellen Ton liefert.\"\n","}\n","```\n","Dieses Beispiel weist das Modell an, in einem professionellen Tonfall kurze und informative Antworten zu geben.\n","\n","---\n","\n"]},{"cell_type":"markdown","id":"upsIzpcqbbP7","metadata":{"id":"upsIzpcqbbP7"},"source":["**User Message**   \n","Die `User Message` ist die eigentliche **Eingabe des Benutzer**s. Sie kann eine Frage, eine Aufforderung, kontextrelevanten Inhalt oder ein Befehl sein. Das Modell nutzt diese Nachricht als Ausgangspunkt f√ºr die Generierung einer Antwort.\n","\n","*Merkmale:*\n","- Direkte Eingabe des Benutzers\n","- Kann eine Frage, Aufforderung, kontextrelevanten Inhalt oder einen Befehl enthalten\n","- Basis f√ºr die Antwort des Modells\n","\n","*Beispiel einer User Message:*\n","```json\n","{\n","  \"role\": \"user\",\n","  \"content\": \"Was sind die wichtigsten Unterschiede zwischen KI und maschinellem Lernen?\"\n","}\n","```\n","Hier fragt der Benutzer nach den Unterschieden zwischen KI und maschinellem Lernen.\n","\n","---\n"]},{"cell_type":"markdown","id":"CozrrtYXbhZb","metadata":{"id":"CozrrtYXbhZb"},"source":["**AI Message**   \n","Die `AI Message` ist die **Antwort, die das Modell generiert**. Sie basiert auf der User Message und den Anweisungen aus der System Message. Die AI Message kann verschiedene Eigenschaften haben, ist aber haupts√§chlich auf den Inhalt fokussiert.\n","\n","*Merkmale:*\n","- Antwort des Modells auf die Benutzeranfrage\n","- Kann verschiedene Eigenschaften haben (z. B. L√§nge, Stil)\n","- Haupts√§chlich inhaltlich relevant\n","\n","*Beispiel einer AI Message:*\n","```json\n","{\n","  \"role\": \"assistant\",\n","  \"content\": \"K√ºnstliche Intelligenz (KI) ist ein weites Feld, das verschiedene Teilgebiete umfasst, darunter auch das maschinelle Lernen (ML). ML konzentriert sich auf die Erstellung von Modellen, die aus Daten lernen und Vorhersagen treffen.\"\n","}\n","```\n","Hier gibt das Modell eine inhaltliche Antwort auf die Frage des Benutzers.\n","\n","\n"]},{"cell_type":"markdown","source":["**System-Message vs. User-Message**\n","\n","\n","\n","*System-Message*\n","\n","**Funktion:** Persistente Verhaltensregeln √ºber verschiedene Aufgaben hinweg\n","\n","**Enth√§lt:**\n","- Identit√§t & Rolle des Modells\n","- Grunds√§tzliche Methodik & Denkansatz\n","- Durchg√§ngige Qualit√§tsstandards\n","- Tool-/Methodenkompetenz\n","- Generische Few-Shot-Examples\n","\n","**Metapher:** Das Betriebssystem ‚Äì selten ver√§ndert, verhaltensbestimmend\n","\n","\n"],"metadata":{"id":"a8QAtdYdg0Im"},"id":"a8QAtdYdg0Im"},{"cell_type":"markdown","source":["\n","*User-Message*\n","\n","**Funktion:** Konkrete Aufgabe und spezifischer Kontext f√ºr diese Interaktion\n","\n","**Enth√§lt:**\n","- Situativer Kontext & Hintergrund\n","- Klare Aufgabenstellung\n","- Erwartete Format/Umfang/Detailgrad\n","- Spezifische Daten & Randbedingungen\n","- Aufgabenspezifische Verhaltensanpassungen\n","\n","**Metapher:** Die Arbeitsanweisung ‚Äì variiert bei jeder Anfrage\n","\n"],"metadata":{"id":"cy765AHZhaWV"},"id":"cy765AHZhaWV"},{"cell_type":"markdown","id":"PiHnhKHVbofP","metadata":{"id":"PiHnhKHVbofP"},"source":["**Kurz gefasst**   \n","\n","+  **System Message** legt die Regeln und das Verhalten des Modells fest.\n","+ **User Message** stellt eine Anfrage oder einen Befehl.\n","+ **AI Message** gibt die generierte Antwort basierend auf den vorherigen Nachrichten.\n","\n","\n"]},{"cell_type":"markdown","source":["# 4 | Ein einfaches Modell\n","---\n"],"metadata":{"id":"BMPwDNTCCOfx"},"id":"BMPwDNTCCOfx"},{"cell_type":"code","execution_count":6,"metadata":{"id":"e1926d03","executionInfo":{"status":"ok","timestamp":1764695633354,"user_tz":-60,"elapsed":126,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Importe\n","from langchain.chat_models import init_chat_model\n","from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n","from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n","from langchain_core.prompts.few_shot import FewShotPromptTemplate\n","from langchain_core.output_parsers.string import StrOutputParser"],"id":"e1926d03"},{"cell_type":"code","source":["# Parameter\n","model_provider=\"openai\"\n","model_name = \"gpt-4o-mini\"\n","temperature = 0.0"],"metadata":{"id":"RG-y_bG8HOOi","executionInfo":{"status":"ok","timestamp":1764695633368,"user_tz":-60,"elapsed":13,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":7,"outputs":[],"id":"RG-y_bG8HOOi"},{"cell_type":"code","source":["# Modell definieren\n","llm = init_chat_model(model_name, model_provider=model_provider, temperature=temperature)"],"metadata":{"id":"yd3MebxgGYgy","executionInfo":{"status":"ok","timestamp":1764695636964,"user_tz":-60,"elapsed":3553,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":8,"outputs":[],"id":"yd3MebxgGYgy"},{"cell_type":"code","source":["#@markdown   <p><font size=\"4\" color='green'>  üßú‚Äç‚ôÄÔ∏è Mermaid</font> </br></p>\n","\n","diagram = \"\"\"\n","graph TD\n","    A[Start]\n","    A --> B{Model Parameter}\n","    B --> C[Provider OpenAI]\n","    B --> D[Model gpt-4o-mini]\n","    B --> E[Temperature 0]\n","    C --> F[Initialize Chat Model]\n","    D --> F\n","    E --> F\n","    F --> G[LLM configured]\n","    G --> H[End]\n","\"\"\"\n","mermaid(diagram, width=800, height=600)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":622},"collapsed":true,"cellView":"form","id":"Py9vykNMBxY9","executionInfo":{"status":"ok","timestamp":1764695641905,"user_tz":-60,"elapsed":4940,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"a42ea429-d5b9-4f3a-e197-dabc7c0685de"},"id":"Py9vykNMBxY9","execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.SVG object>"],"image/svg+xml":"<svg xmlns=\"http://www.w3.org/2000/svg\" aria-roledescription=\"flowchart-v2\" role=\"graphics-document document\" viewBox=\"0 0 653.34375 713.3125\" class=\"flowchart\" id=\"container\" width=\"800\" height=\"600\"><style>#container{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:16px;fill:#333;}@keyframes edge-animation-frame{from{stroke-dashoffset:0;}}@keyframes dash{to{stroke-dashoffset:0;}}#container .edge-animation-slow{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 50s linear infinite;stroke-linecap:round;}#container .edge-animation-fast{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 20s linear infinite;stroke-linecap:round;}#container .error-icon{fill:#552222;}#container .error-text{fill:#552222;stroke:#552222;}#container .edge-thickness-normal{stroke-width:1px;}#container .edge-thickness-thick{stroke-width:3.5px;}#container .edge-pattern-solid{stroke-dasharray:0;}#container .edge-thickness-invisible{stroke-width:0;fill:none;}#container .edge-pattern-dashed{stroke-dasharray:3;}#container .edge-pattern-dotted{stroke-dasharray:2;}#container .marker{fill:#333333;stroke:#333333;}#container .marker.cross{stroke:#333333;}#container svg{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:16px;}#container p{margin:0;}#container .label{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;color:#333;}#container .cluster-label text{fill:#333;}#container .cluster-label span{color:#333;}#container .cluster-label span p{background-color:transparent;}#container .label text,#container span{fill:#333;color:#333;}#container .node rect,#container .node circle,#container .node ellipse,#container .node polygon,#container .node path{fill:#ECECFF;stroke:#9370DB;stroke-width:1px;}#container .rough-node .label text,#container .node .label text,#container .image-shape .label,#container .icon-shape .label{text-anchor:middle;}#container .node .katex path{fill:#000;stroke:#000;stroke-width:1px;}#container .rough-node .label,#container .node .label,#container .image-shape .label,#container .icon-shape .label{text-align:center;}#container .node.clickable{cursor:pointer;}#container .root .anchor path{fill:#333333!important;stroke-width:0;stroke:#333333;}#container .arrowheadPath{fill:#333333;}#container .edgePath .path{stroke:#333333;stroke-width:2.0px;}#container .flowchart-link{stroke:#333333;fill:none;}#container .edgeLabel{background-color:rgba(232,232,232, 0.8);text-align:center;}#container .edgeLabel p{background-color:rgba(232,232,232, 0.8);}#container .edgeLabel rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#container .labelBkg{background-color:rgba(232, 232, 232, 0.5);}#container .cluster rect{fill:#ffffde;stroke:#aaaa33;stroke-width:1px;}#container .cluster text{fill:#333;}#container .cluster span{color:#333;}#container div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:12px;background:hsl(80, 100%, 96.2745098039%);border:1px solid #aaaa33;border-radius:2px;pointer-events:none;z-index:100;}#container .flowchartTitleText{text-anchor:middle;font-size:18px;fill:#333;}#container rect.text{fill:none;stroke-width:0;}#container .icon-shape,#container .image-shape{background-color:rgba(232,232,232, 0.8);text-align:center;}#container .icon-shape p,#container .image-shape p{background-color:rgba(232,232,232, 0.8);padding:2px;}#container .icon-shape rect,#container .image-shape rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#container :root{--mermaid-font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;}</style><g><marker orient=\"auto\" markerHeight=\"8\" markerWidth=\"8\" markerUnits=\"userSpaceOnUse\" refY=\"5\" refX=\"5\" viewBox=\"0 0 10 10\" class=\"marker flowchart-v2\" id=\"container_flowchart-v2-pointEnd\"><path style=\"stroke-width: 1; stroke-dasharray: 1, 0;\" class=\"arrowMarkerPath\" d=\"M 0 0 L 10 5 L 0 10 z\"/></marker><marker orient=\"auto\" markerHeight=\"8\" markerWidth=\"8\" markerUnits=\"userSpaceOnUse\" refY=\"5\" refX=\"4.5\" viewBox=\"0 0 10 10\" class=\"marker flowchart-v2\" id=\"container_flowchart-v2-pointStart\"><path style=\"stroke-width: 1; stroke-dasharray: 1, 0;\" class=\"arrowMarkerPath\" d=\"M 0 5 L 10 10 L 10 0 z\"/></marker><marker orient=\"auto\" markerHeight=\"11\" markerWidth=\"11\" markerUnits=\"userSpaceOnUse\" refY=\"5\" refX=\"11\" viewBox=\"0 0 10 10\" class=\"marker flowchart-v2\" id=\"container_flowchart-v2-circleEnd\"><circle style=\"stroke-width: 1; stroke-dasharray: 1, 0;\" class=\"arrowMarkerPath\" r=\"5\" cy=\"5\" cx=\"5\"/></marker><marker orient=\"auto\" markerHeight=\"11\" markerWidth=\"11\" markerUnits=\"userSpaceOnUse\" refY=\"5\" refX=\"-1\" viewBox=\"0 0 10 10\" class=\"marker flowchart-v2\" id=\"container_flowchart-v2-circleStart\"><circle style=\"stroke-width: 1; stroke-dasharray: 1, 0;\" class=\"arrowMarkerPath\" r=\"5\" cy=\"5\" cx=\"5\"/></marker><marker orient=\"auto\" markerHeight=\"11\" markerWidth=\"11\" markerUnits=\"userSpaceOnUse\" refY=\"5.2\" refX=\"12\" viewBox=\"0 0 11 11\" class=\"marker cross flowchart-v2\" id=\"container_flowchart-v2-crossEnd\"><path style=\"stroke-width: 2; stroke-dasharray: 1, 0;\" class=\"arrowMarkerPath\" d=\"M 1,1 l 9,9 M 10,1 l -9,9\"/></marker><marker orient=\"auto\" markerHeight=\"11\" markerWidth=\"11\" markerUnits=\"userSpaceOnUse\" refY=\"5.2\" refX=\"-1\" viewBox=\"0 0 11 11\" class=\"marker cross flowchart-v2\" id=\"container_flowchart-v2-crossStart\"><path style=\"stroke-width: 2; stroke-dasharray: 1, 0;\" class=\"arrowMarkerPath\" d=\"M 1,1 l 9,9 M 10,1 l -9,9\"/></marker><g class=\"root\"><g class=\"clusters\"/><g class=\"edgePaths\"><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_A_B_0\" d=\"M332.953,62L332.953,66.167C332.953,70.333,332.953,78.667,333.023,86.417C333.094,94.167,333.234,101.334,333.304,104.917L333.375,108.501\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_B_C_0\" d=\"M273.602,229.962L244.136,244.02C214.67,258.079,155.737,286.196,126.271,303.754C96.805,321.313,96.805,328.313,96.805,331.813L96.805,335.313\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_B_D_0\" d=\"M333.453,289.813L333.37,293.896C333.286,297.979,333.12,306.146,333.036,313.729C332.953,321.313,332.953,328.313,332.953,331.813L332.953,335.313\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_B_E_0\" d=\"M392.777,230.489L421.118,244.459C449.458,258.43,506.139,286.371,534.48,303.842C562.82,321.313,562.82,328.313,562.82,331.813L562.82,335.313\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_C_F_0\" d=\"M96.805,393.313L96.805,397.479C96.805,401.646,96.805,409.979,118.165,418.849C139.526,427.72,182.247,437.127,203.608,441.831L224.969,446.534\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_D_F_0\" d=\"M332.953,393.313L332.953,397.479C332.953,401.646,332.953,409.979,332.953,417.646C332.953,425.313,332.953,432.313,332.953,435.813L332.953,439.313\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_E_F_0\" d=\"M562.82,393.313L562.82,397.479C562.82,401.646,562.82,409.979,542.506,418.741C522.191,427.504,481.562,436.695,461.247,441.29L440.933,445.886\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_F_G_0\" d=\"M332.953,497.313L332.953,501.479C332.953,505.646,332.953,513.979,332.953,521.646C332.953,529.313,332.953,536.313,332.953,539.813L332.953,543.313\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_G_H_0\" d=\"M332.953,601.313L332.953,605.479C332.953,609.646,332.953,617.979,332.953,625.646C332.953,633.313,332.953,640.313,332.953,643.813L332.953,647.313\"/></g><g class=\"edgeLabels\"><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g></g><g class=\"nodes\"><g transform=\"translate(332.953125, 35)\" id=\"flowchart-A-0\" class=\"node default\"><rect height=\"54\" width=\"95.046875\" y=\"-27\" x=\"-47.5234375\" style=\"\" class=\"basic label-container\"/><g transform=\"translate(-17.5234375, -12)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"24\" width=\"35.046875\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Start</p></span></div></foreignObject></g></g><g transform=\"translate(332.953125, 200.65625)\" id=\"flowchart-B-2\" class=\"node default\"><polygon transform=\"translate(-88.65625,88.65625)\" class=\"label-container\" points=\"88.65625,0 177.3125,-88.65625 88.65625,-177.3125 0,-88.65625\"/><g transform=\"translate(-61.65625, -12)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"24\" width=\"123.3125\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Model Parameter</p></span></div></foreignObject></g></g><g transform=\"translate(96.8046875, 366.3125)\" id=\"flowchart-C-4\" class=\"node default\"><rect height=\"54\" width=\"177.609375\" y=\"-27\" x=\"-88.8046875\" style=\"\" class=\"basic label-container\"/><g transform=\"translate(-58.8046875, -12)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"24\" width=\"117.609375\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Provider OpenAI</p></span></div></foreignObject></g></g><g transform=\"translate(332.953125, 366.3125)\" id=\"flowchart-D-6\" class=\"node default\"><rect height=\"54\" width=\"194.6875\" y=\"-27\" x=\"-97.34375\" style=\"\" class=\"basic label-container\"/><g transform=\"translate(-67.34375, -12)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"24\" width=\"134.6875\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Model gpt-4o-mini</p></span></div></foreignObject></g></g><g transform=\"translate(562.8203125, 366.3125)\" id=\"flowchart-E-8\" class=\"node default\"><rect height=\"54\" width=\"165.046875\" y=\"-27\" x=\"-82.5234375\" style=\"\" class=\"basic label-container\"/><g transform=\"translate(-52.5234375, -12)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"24\" width=\"105.046875\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Temperature 0</p></span></div></foreignObject></g></g><g transform=\"translate(332.953125, 470.3125)\" id=\"flowchart-F-10\" class=\"node default\"><rect height=\"54\" width=\"208.15625\" y=\"-27\" x=\"-104.078125\" style=\"\" class=\"basic label-container\"/><g transform=\"translate(-74.078125, -12)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"24\" width=\"148.15625\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Initialize Chat Model</p></span></div></foreignObject></g></g><g transform=\"translate(332.953125, 574.3125)\" id=\"flowchart-G-16\" class=\"node default\"><rect height=\"54\" width=\"169.171875\" y=\"-27\" x=\"-84.5859375\" style=\"\" class=\"basic label-container\"/><g transform=\"translate(-54.5859375, -12)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"24\" width=\"109.171875\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>LLM configured</p></span></div></foreignObject></g></g><g transform=\"translate(332.953125, 678.3125)\" id=\"flowchart-H-18\" class=\"node default\"><rect height=\"54\" width=\"87.359375\" y=\"-27\" x=\"-43.6796875\" style=\"\" class=\"basic label-container\"/><g transform=\"translate(-13.6796875, -12)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"24\" width=\"27.359375\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>End</p></span></div></foreignObject></g></g></g></g></g></svg>"},"metadata":{}}]},{"cell_type":"markdown","id":"-JwJD0rLIyT8","metadata":{"id":"-JwJD0rLIyT8"},"source":["# 5 | Prompts\n","---"]},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/langchain_prozess_02.png)"],"metadata":{"id":"j9stNgKo93Eg"},"id":"j9stNgKo93Eg"},{"cell_type":"markdown","source":["\n","\n","Prompts sind die Eingaben bzw. Auftr√§ge an das LLM. LangChain bietet verschiedene Template-Systeme. Ein **Template** ist eine **Vorlage** ‚Äì genauer gesagt ein Text mit **Platzhaltern**, die sp√§ter durch echte Werte ersetzt werden.\n","\n","\n","Ein **PromptTemplate** ist eine Vorlage f√ºr einen Eingabetext, der an ein Sprachmodell geschickt wird.\n","Er enth√§lt Platzhalter `{placeholder}`, die sp√§ter mit Nutzereingaben oder Kontext bef√ºllt werden.\n","\n","**Templates machen die KI-Prompts:**\n","\n","+ Wiederverwendbar (z.‚ÄØB. f√ºr viele Themen oder Fragen)\n","\n","+ Strukturiert (z.‚ÄØB. mit Rollen und Anweisungen)\n","\n","+ Dynamisch erweiterbar (mit Memory, Tools, etc.)"],"metadata":{"id":"QMVO4fzPFZ88"},"id":"QMVO4fzPFZ88"},{"cell_type":"markdown","id":"HqTUeMW20084","metadata":{"id":"HqTUeMW20084"},"source":["## 5.1 | Simple Prompt\n"]},{"cell_type":"markdown","source":["<p><font color='darkblue' size=\"4\">\n","<b>‚ùóHinweis:</b>\n","</font></p>\n","Ohne Memory-Management hat das Modell ein kurzes Ged√§chnis.\n"],"metadata":{"id":"5JF1_AvYNYY_"},"id":"5JF1_AvYNYY_"},{"cell_type":"markdown","source":["**Prompt wird als Zeichenkette √ºbergeben:**"],"metadata":{"id":"1azC75R5JpWC"},"id":"1azC75R5JpWC"},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(\"Was ist Generative KI?\")"],"metadata":{"id":"jPizca8FJV5b","executionInfo":{"status":"ok","timestamp":1764695652888,"user_tz":-60,"elapsed":10952,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"jPizca8FJV5b","execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Antwort\n","type(response)"],"metadata":{"id":"EHjmjLgW3ENA","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1764695652914,"user_tz":-60,"elapsed":14,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"a04faf69-646d-4296-a848-ca566270004f"},"id":"EHjmjLgW3ENA","execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["langchain_core.messages.ai.AIMessage"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.messages.ai.AIMessage</b><br/>def __init__(content: str | list[str | dict] | None=None, content_blocks: list[types.ContentBlock] | None=None, **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/langchain_core/messages/ai.py</a>Message from an AI.\n","\n","An `AIMessage` is returned from a chat model as a response to a prompt.\n","\n","This message represents the output of the model and consists of both\n","the raw output as returned by the model and standardized fields\n","(e.g., tool calls, usage metadata) added by the LangChain framework.</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 153);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["response"],"metadata":{"id":"4ZYngACo4RWN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764695652972,"user_tz":-60,"elapsed":53,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"b7ee828d-4d14-4f2d-9626-d32cea037c98"},"id":"4ZYngACo4RWN","execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='Generative KI, oder generative k√ºnstliche Intelligenz, bezieht sich auf eine Klasse von Algorithmen und Modellen, die in der Lage sind, neue Inhalte zu erzeugen. Diese Inhalte k√∂nnen in verschiedenen Formen vorliegen, darunter Texte, Bilder, Musik, Videos und mehr. Generative KI nutzt oft Techniken des maschinellen Lernens, insbesondere neuronale Netzwerke, um Muster in bestehenden Daten zu erkennen und darauf basierend neue, √§hnliche Daten zu generieren.\\n\\nEin bekanntes Beispiel f√ºr generative KI sind Sprachmodelle wie GPT (Generative Pre-trained Transformer), die in der Lage sind, menschen√§hnliche Texte zu schreiben, Fragen zu beantworten oder Geschichten zu erz√§hlen. In der Bildgenerierung gibt es Modelle wie DALL-E oder Midjourney, die aus Textbeschreibungen Bilder erstellen k√∂nnen.\\n\\nGenerative KI hat viele Anwendungen, darunter:\\n\\n1. **Kreative Inhalte**: Erstellung von Kunst, Musik, Literatur und Design.\\n2. **Datenaugmentation**: Generierung zus√§tzlicher Trainingsdaten f√ºr maschinelles Lernen.\\n3. **Simulationen**: Erzeugung realistischer Szenarien f√ºr Training und Forschung.\\n4. **Personalisierung**: Anpassung von Inhalten an individuelle Nutzerpr√§ferenzen.\\n\\nDie Technologie wirft jedoch auch ethische und gesellschaftliche Fragen auf, insbesondere in Bezug auf Urheberrecht, F√§lschungen und die potenzielle Verbreitung von Fehlinformationen.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 297, 'prompt_tokens': 13, 'total_tokens': 310, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_50906f2aac', 'id': 'chatcmpl-CiOEtpExPzOFBSEaobVYzKHhmNbDl', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--05063156-fb65-4807-b1ea-b28aa6ba9794-0', usage_metadata={'input_tokens': 13, 'output_tokens': 297, 'total_tokens': 310, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["for r in response:\n","    print(r)"],"metadata":{"id":"jveXqUjbpo0s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764695653009,"user_tz":-60,"elapsed":32,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"ec5953b9-f580-4625-a644-6107e12adb56"},"id":"jveXqUjbpo0s","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["('content', 'Generative KI, oder generative k√ºnstliche Intelligenz, bezieht sich auf eine Klasse von Algorithmen und Modellen, die in der Lage sind, neue Inhalte zu erzeugen. Diese Inhalte k√∂nnen in verschiedenen Formen vorliegen, darunter Texte, Bilder, Musik, Videos und mehr. Generative KI nutzt oft Techniken des maschinellen Lernens, insbesondere neuronale Netzwerke, um Muster in bestehenden Daten zu erkennen und darauf basierend neue, √§hnliche Daten zu generieren.\\n\\nEin bekanntes Beispiel f√ºr generative KI sind Sprachmodelle wie GPT (Generative Pre-trained Transformer), die in der Lage sind, menschen√§hnliche Texte zu schreiben, Fragen zu beantworten oder Geschichten zu erz√§hlen. In der Bildgenerierung gibt es Modelle wie DALL-E oder Midjourney, die aus Textbeschreibungen Bilder erstellen k√∂nnen.\\n\\nGenerative KI hat viele Anwendungen, darunter:\\n\\n1. **Kreative Inhalte**: Erstellung von Kunst, Musik, Literatur und Design.\\n2. **Datenaugmentation**: Generierung zus√§tzlicher Trainingsdaten f√ºr maschinelles Lernen.\\n3. **Simulationen**: Erzeugung realistischer Szenarien f√ºr Training und Forschung.\\n4. **Personalisierung**: Anpassung von Inhalten an individuelle Nutzerpr√§ferenzen.\\n\\nDie Technologie wirft jedoch auch ethische und gesellschaftliche Fragen auf, insbesondere in Bezug auf Urheberrecht, F√§lschungen und die potenzielle Verbreitung von Fehlinformationen.')\n","('additional_kwargs', {'refusal': None})\n","('response_metadata', {'token_usage': {'completion_tokens': 297, 'prompt_tokens': 13, 'total_tokens': 310, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_50906f2aac', 'id': 'chatcmpl-CiOEtpExPzOFBSEaobVYzKHhmNbDl', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None})\n","('type', 'ai')\n","('name', None)\n","('id', 'lc_run--05063156-fb65-4807-b1ea-b28aa6ba9794-0')\n","('tool_calls', [])\n","('invalid_tool_calls', [])\n","('usage_metadata', {'input_tokens': 13, 'output_tokens': 297, 'total_tokens': 310, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n"]}]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"cOJGeiLGJb-g","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1764695653031,"user_tz":-60,"elapsed":17,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"4a062b72-a4ee-48f8-8e91-631a28212417"},"id":"cOJGeiLGJb-g","execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Generative KI, oder generative k√ºnstliche Intelligenz, bezieht sich auf eine Klasse von Algorithmen und Modellen, die in der Lage sind, neue Inhalte zu erzeugen. Diese Inhalte k√∂nnen in verschiedenen Formen vorliegen, darunter Texte, Bilder, Musik, Videos und mehr. Generative KI nutzt oft Techniken des maschinellen Lernens, insbesondere neuronale Netzwerke, um Muster in bestehenden Daten zu erkennen und darauf basierend neue, √§hnliche Daten zu generieren.\n\nEin bekanntes Beispiel f√ºr generative KI sind Sprachmodelle wie GPT (Generative Pre-trained Transformer), die in der Lage sind, menschen√§hnliche Texte zu schreiben, Fragen zu beantworten oder Geschichten zu erz√§hlen. In der Bildgenerierung gibt es Modelle wie DALL-E oder Midjourney, die aus Textbeschreibungen Bilder erstellen k√∂nnen.\n\nGenerative KI hat viele Anwendungen, darunter:\n\n1. **Kreative Inhalte**: Erstellung von Kunst, Musik, Literatur und Design.\n2. **Datenaugmentation**: Generierung zus√§tzlicher Trainingsdaten f√ºr maschinelles Lernen.\n3. **Simulationen**: Erzeugung realistischer Szenarien f√ºr Training und Forschung.\n4. **Personalisierung**: Anpassung von Inhalten an individuelle Nutzerpr√§ferenzen.\n\nDie Technologie wirft jedoch auch ethische und gesellschaftliche Fragen auf, insbesondere in Bezug auf Urheberrecht, F√§lschungen und die potenzielle Verbreitung von Fehlinformationen."},"metadata":{}}]},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(\"Was war meine letzte Frage?\")"],"metadata":{"id":"9fayTzIrNN2b","executionInfo":{"status":"ok","timestamp":1764695654443,"user_tz":-60,"elapsed":1397,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"9fayTzIrNN2b","execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"IahQJ0mFNUUV","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1764695654485,"user_tz":-60,"elapsed":28,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"3ce2f14c-2144-4b68-dda3-52022f80295e"},"id":"IahQJ0mFNUUV","execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Es tut mir leid, aber ich kann keine vorherigen Interaktionen oder Fragen speichern oder abrufen. Wie kann ich Ihnen heute helfen?"},"metadata":{}}]},{"cell_type":"markdown","source":["**Prompt wird als formatierte String-Variable √ºbergeben:**"],"metadata":{"id":"aKRhzB0oJnUG"},"id":"aKRhzB0oJnUG"},{"cell_type":"code","source":["# f-String f√ºr simple prompt mit template\n","thema = \"Machine Learning\"\n","prompt = f\"Erkl√§re {thema} in einfachen Worten.\""],"metadata":{"id":"8s5cQY2oHUvW","executionInfo":{"status":"ok","timestamp":1764695654508,"user_tz":-60,"elapsed":16,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"8s5cQY2oHUvW","execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(prompt)"],"metadata":{"id":"plYGfaoLJ8DI","executionInfo":{"status":"ok","timestamp":1764695659404,"user_tz":-60,"elapsed":4885,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":18,"outputs":[],"id":"plYGfaoLJ8DI"},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"SLX0zaONJ8DJ","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1764695659436,"user_tz":-60,"elapsed":23,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"c9640691-0b7b-4cd1-e232-4bdf3c288379"},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine Learning, oder maschinelles Lernen, ist ein Teilbereich der K√ºnstlichen Intelligenz, der es Computern erm√∂glicht, aus Daten zu lernen und Muster zu erkennen, ohne dass sie daf√ºr explizit programmiert werden m√ºssen. \n\nStell dir vor, du hast viele Bilder von Katzen und Hunden. Anstatt einem Computer genau zu sagen, wie eine Katze oder ein Hund aussieht, zeigst du ihm einfach viele Beispiele. Der Computer analysiert diese Bilder und lernt, die Unterschiede zwischen Katzen und Hunden zu erkennen. \n\nWenn du ihm dann ein neues Bild zeigst, kann er basierend auf dem, was er gelernt hat, vorhersagen, ob es sich um eine Katze oder einen Hund handelt. \n\nZusammengefasst: Machine Learning ist wie das Lernen eines Menschen ‚Äì je mehr Informationen und Erfahrungen er hat, desto besser kann er Entscheidungen treffen oder Vorhersagen machen."},"metadata":{}}],"id":"SLX0zaONJ8DJ"},{"cell_type":"markdown","source":["**Prompt wird als template √ºbergeben:**"],"metadata":{"id":"n-VVZEHDKSIw"},"id":"n-VVZEHDKSIw"},{"cell_type":"code","source":["# Einfaches Template mit Platzhaltern\n","simple_prompt_template = PromptTemplate.from_template(\n","    \"Erkl√§re {thema} in einfachen Worten.\"\n",")"],"metadata":{"id":"koh0-_3r_HxG","executionInfo":{"status":"ok","timestamp":1764695659520,"user_tz":-60,"elapsed":75,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"koh0-_3r_HxG","execution_count":20,"outputs":[]},{"cell_type":"code","source":["# Prompt-Erstellung\n","thema = \"Generative KI\"\n","prompt = simple_prompt_template.format(thema=thema)\n","prompt"],"metadata":{"id":"nFrCjL6j_xo1","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1764695659542,"user_tz":-60,"elapsed":62,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"c5658f81-2108-4da0-b367-270d142078e0"},"id":"nFrCjL6j_xo1","execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Erkl√§re Generative KI in einfachen Worten.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(prompt)"],"metadata":{"id":"-of5FbM02qfZ","executionInfo":{"status":"ok","timestamp":1764695663414,"user_tz":-60,"elapsed":3904,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"-of5FbM02qfZ","execution_count":22,"outputs":[]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"lmEE-3NDAE15","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1764695663458,"user_tz":-60,"elapsed":29,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"4b34c067-cd97-41ec-95ca-802bff0331e8"},"id":"lmEE-3NDAE15","execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Generative KI ist eine Art von k√ºnstlicher Intelligenz, die in der Lage ist, neue Inhalte zu erstellen. Das k√∂nnen Texte, Bilder, Musik oder sogar Videos sein. Sie funktioniert, indem sie aus gro√üen Mengen von Daten lernt, wie bestimmte Inhalte aussehen oder klingen. \n\nStell dir vor, du hast viele Bilder von Katzen und Hunden. Eine generative KI kann dann lernen, wie diese Tiere aussehen, und schlie√ülich ein ganz neues Bild von einem Tier erstellen, das es noch nie zuvor gesehen hat. \n\nIm Grunde genommen ist generative KI wie ein kreativer Computer, der neue Ideen und Inhalte basierend auf dem, was er gelernt hat, entwickeln kann."},"metadata":{}}]},{"cell_type":"markdown","id":"8akKmCCc06nm","metadata":{"id":"8akKmCCc06nm"},"source":["## 5.2 | ChatPromptTemplate üìå"]},{"cell_type":"markdown","source":["Das **ChatPromptTemplate** ist eine spezielle Prompt-Vorlage f√ºr Chatmodelle (z.‚ÄØB. GPT-3.5, GPT-4), die **mehrere Rollen und Nachrichten** unterst√ºtzt ‚Äì also genau das, was Chat-Modelle eigentlich brauchen.\n","\n","Es geh√∂rt zur LangChain-Bibliothek und baut auf dem Prinzip auf:\n","\n","\n","**Wer sagt was? ‚Üí system, user, assistant, etc.**"],"metadata":{"id":"SVoHwxtrMM_j"},"id":"SVoHwxtrMM_j"},{"cell_type":"markdown","source":["In LangChain unterst√ºtzte Rollen:\n","\n","+ \"system\" - f√ºr Systemnachrichten\n","+ \"assistant\" oder \"ai\" - f√ºr Assistenten-/KI-Antworten\n","+ \"user\" oder \"human\" - f√ºr Benutzernachrichten"],"metadata":{"id":"Po7W2FKz-VEd"},"id":"Po7W2FKz-VEd"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Variante 1: Liste & Tupel üìå\n","</font></p>"],"metadata":{"id":"yw4qr0mC5A6m"},"id":"yw4qr0mC5A6m"},{"cell_type":"code","execution_count":24,"id":"iW9URvON0IXr","metadata":{"id":"iW9URvON0IXr","executionInfo":{"status":"ok","timestamp":1764695663493,"user_tz":-60,"elapsed":24,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Template als Liste & Tupel\n","chat_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    (\"human\", \"Erkl√§re mir {thema}\"),\n","])"]},{"cell_type":"code","source":["# Variablen zuweisen\n","system_prompt = \"Du bist ein hilfreicher und humorvoller Assistent.\"\n","thema = \"Machine Learning\""],"metadata":{"id":"XrEDZ3Vz174e","executionInfo":{"status":"ok","timestamp":1764695663561,"user_tz":-60,"elapsed":50,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"XrEDZ3Vz174e","execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Template & Variablen kombinieren\n","prompt = chat_template.format_messages(system_prompt=system_prompt, thema=thema)"],"metadata":{"id":"x3pxKD_R2BvX","executionInfo":{"status":"ok","timestamp":1764695663578,"user_tz":-60,"elapsed":7,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"x3pxKD_R2BvX","execution_count":26,"outputs":[]},{"cell_type":"code","source":["# Sprachmodell aufrufe\n","response = llm.invoke(prompt)"],"metadata":{"id":"CLU3o3dG4OGM","executionInfo":{"status":"ok","timestamp":1764695675471,"user_tz":-60,"elapsed":11886,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"CLU3o3dG4OGM","execution_count":27,"outputs":[]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"YnfES3Q24n0M","colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"status":"ok","timestamp":1764695675510,"user_tz":-60,"elapsed":27,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"f50ce456-211f-43fe-c0e0-599833bac9b9"},"id":"YnfES3Q24n0M","execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Klar, ich erkl√§re dir Machine Learning! Stell dir vor, du hast einen Hund, der nicht wei√ü, wie man ‚ÄûSitz‚Äú macht. Du zeigst ihm immer wieder, was er tun soll, und wenn er es richtig macht, gibst du ihm ein Leckerli. Nach einer Weile lernt der Hund, dass ‚ÄûSitz‚Äú bedeutet, seinen Hintern auf den Boden zu setzen, um das Leckerli zu bekommen. \n\nSo √§hnlich funktioniert Machine Learning! Es ist ein Teilbereich der K√ºnstlichen Intelligenz, bei dem Computer aus Daten lernen, ohne dass sie explizit programmiert werden. Hier sind die Grundprinzipien:\n\n1. **Daten**: Du brauchst eine Menge von Beispielen (Daten), um dem Computer zu zeigen, was er lernen soll. Das k√∂nnen Bilder, Texte oder Zahlen sein.\n\n2. **Modelle**: Das ist wie das Gehirn des Computers. Es verarbeitet die Daten und versucht, Muster zu erkennen. Es gibt verschiedene Arten von Modellen, je nachdem, was du erreichen m√∂chtest.\n\n3. **Training**: Hier kommt der spannende Teil! Du f√ºtterst das Modell mit deinen Daten und l√§sst es lernen. Es passt seine internen Parameter an, um die besten Vorhersagen zu treffen.\n\n4. **Testen**: Nachdem das Modell trainiert wurde, testest du es mit neuen Daten, um zu sehen, wie gut es gelernt hat. Wenn es gut abschneidet, kannst du es verwenden!\n\n5. **Verbesserung**: Wenn das Modell nicht gut abschneidet, kannst du es weiter trainieren, mehr Daten hinzuf√ºgen oder das Modell selbst anpassen.\n\nDas Ziel von Machine Learning ist es, Vorhersagen zu treffen oder Entscheidungen zu treffen, basierend auf den gelernten Mustern. Und das Beste daran? Es kann in vielen Bereichen eingesetzt werden, von der Spracherkennung √ºber Bildverarbeitung bis hin zu Empfehlungen f√ºr deine n√§chste Lieblingsserie!\n\nAlso, wenn du das n√§chste Mal einen Hund siehst, der ‚ÄûSitz‚Äú macht, denk daran: Er hat gerade ein bisschen ‚ÄûMachine Learning‚Äú betrieben! üê∂‚ú®"},"metadata":{}}]},{"cell_type":"markdown","source":["**... oder ...**"],"metadata":{"id":"6FrHMy1PYH0S"},"id":"6FrHMy1PYH0S"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Variante 2: Liste & Dictionary\n","</font></p>"],"metadata":{"id":"lkaCyUqq6OD0"},"id":"lkaCyUqq6OD0"},{"cell_type":"code","source":["# Template als Liste & Dictionary\n","chat_template = ChatPromptTemplate.from_messages([\n","    {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher und humorvoller Assistent.\"},\n","    {\"role\": \"human\", \"content\": \"Erkl√§re mir {thema}\"},\n","])"],"metadata":{"id":"kxEP8r3VDWwI","executionInfo":{"status":"ok","timestamp":1764695675531,"user_tz":-60,"elapsed":25,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"kxEP8r3VDWwI","execution_count":29,"outputs":[]},{"cell_type":"code","source":["# Variablen zuweisen\n","system_prompt = \"Du bist ein hilfreicher und humorvoller Assistent.\"\n","thema = \"Machine Learning\""],"metadata":{"id":"naATTZyS4zEB","executionInfo":{"status":"ok","timestamp":1764695675560,"user_tz":-60,"elapsed":13,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":30,"outputs":[],"id":"naATTZyS4zEB"},{"cell_type":"code","source":["# Template & Variablen kombinieren\n","prompt = chat_template.format_messages(system_prompt=system_prompt, thema=thema)"],"metadata":{"id":"O235HSRl4zEC","executionInfo":{"status":"ok","timestamp":1764695675573,"user_tz":-60,"elapsed":7,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":31,"outputs":[],"id":"O235HSRl4zEC"},{"cell_type":"code","source":["# Sprachmodell aufrufe\n","response = llm.invoke(prompt)"],"metadata":{"id":"Ow6KjCLG4zEC","executionInfo":{"status":"ok","timestamp":1764695690989,"user_tz":-60,"elapsed":15407,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":32,"outputs":[],"id":"Ow6KjCLG4zEC"},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"6PtZzYUSC7-F","colab":{"base_uri":"https://localhost:8080/","height":499},"executionInfo":{"status":"ok","timestamp":1764695691024,"user_tz":-60,"elapsed":25,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"e20636ab-5b7d-4623-82a7-f9b325653a47"},"id":"6PtZzYUSC7-F","execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Klar, ich erkl√§re dir gerne Machine Learning! \n\nMachine Learning (ML) ist ein Teilbereich der K√ºnstlichen Intelligenz (KI), der es Computern erm√∂glicht, aus Daten zu lernen und Muster zu erkennen, ohne dass sie explizit programmiert werden m√ºssen. Man k√∂nnte sagen, es ist wie das Lernen eines neuen Hobbys ‚Äì je mehr du √ºbst, desto besser wirst du darin!\n\nHier sind die grundlegenden Konzepte:\n\n1. **Daten**: Machine Learning ben√∂tigt Daten, um zu lernen. Das k√∂nnen Bilder, Texte, Zahlen oder sogar T√∂ne sein. Je mehr qualitativ hochwertige Daten du hast, desto besser kann das Modell lernen.\n\n2. **Modelle**: Ein Modell ist eine mathematische Struktur, die die Beziehung zwischen den Eingabedaten und den Ausgaben beschreibt. Man kann sich das wie ein Rezept vorstellen, das dir sagt, wie du aus den Zutaten (Daten) ein Gericht (Vorhersage) zubereitest.\n\n3. **Training**: W√§hrend des Trainingsprozesses wird das Modell mit Daten gef√ºttert. Es versucht, Muster zu erkennen und seine Vorhersagen zu verbessern. Das ist wie beim √úben eines Instruments ‚Äì am Anfang klingt es vielleicht schief, aber mit der Zeit wird es besser!\n\n4. **√úberwachung**: Es gibt verschiedene Arten von Machine Learning, darunter:\n   - **√úberwachtes Lernen**: Hier hat man ein ‚ÄûLehrer‚Äú, der dem Modell sagt, was richtig und was falsch ist. Zum Beispiel, wenn du ein Modell trainierst, um Katzen von Hunden zu unterscheiden, zeigst du ihm viele Bilder von beiden und sagst ihm, welches Bild was ist.\n   - **Un√ºberwachtes Lernen**: Hier gibt es keinen Lehrer. Das Modell versucht, selbst Muster in den Daten zu finden. Es ist wie ein Kind, das im Spielzeugzimmer nach neuen Spielzeugen sucht, ohne zu wissen, was es finden wird.\n   - **Best√§rkendes Lernen**: Hier lernt das Modell durch Belohnungen und Bestrafungen, √§hnlich wie ein Hund, der f√ºr das Sitzen ein Leckerli bekommt.\n\n5. **Anwendungen**: Machine Learning wird in vielen Bereichen eingesetzt, von der Spracherkennung √ºber Bildverarbeitung bis hin zu Empfehlungen in Online-Shops. Wenn dir Netflix vorschl√§gt, was du als N√§chstes schauen solltest, ist das ein Beispiel f√ºr Machine Learning in Aktion!\n\nZusammengefasst: Machine Learning ist wie ein intelligenter Koch, der aus vielen Rezepten lernt, um das perfekte Gericht zuzubereiten ‚Äì und das alles, ohne dass du ihm jedes Mal sagen musst, was er tun soll! Wenn du noch mehr Fragen hast oder tiefer in ein bestimmtes Thema eintauchen m√∂chtest, lass es mich wissen!"},"metadata":{}}]},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Vergleich: f-string vs. PromptTemplate vs. ChatPromptTemplate\n","</font></p>"],"metadata":{"id":"2uSD48umMXI7"},"id":"2uSD48umMXI7"},{"cell_type":"markdown","source":["\n","\n","| Vorteil                      | `f-string` (Python)           | `PromptTemplate`              | `ChatPromptTemplate`            |\n","| ---------------------------- | ----------------------------- | ----------------------------- | ------------------------------- |\n","| üöª Rollenstruktur            | ‚ùå Keine Unterst√ºtzung         | ‚ùå Nur einfacher Text          | ‚úÖ Klar: `system`, `user`, `ai`  |\n","| üìö Konversationsaufbau       | ‚ùå Nicht geeignet              | ‚ùå Nicht geeignet              | ‚úÖ Ideal f√ºr Multi-Turn-Dialoge <br>    (mehreren Gespr√§chsrunden)  |\n","| üîÑ Kombinierbar mit `Memory` | ‚ùå Nein                        | ‚ùå Nein                        | ‚úÖ Ja, via `MessagesPlaceholder` |\n","| üîå F√ºr Chatmodelle wie GPT   | ‚ö†Ô∏è Manuell als String         | ‚ö†Ô∏è Nur als Flie√ütext          | ‚úÖ Nativ & strukturiert          |\n","| üîç Klarheit & Wartbarkeit    | ‚ö†Ô∏è Inline, schwer erweiterbar | ‚ö†Ô∏è Nur Inline-Text            | ‚úÖ Saubere Message-Trennung      |\n","| üß™ Prompt-Testbarkeit        | ‚ùå Keine Validierung           | ‚ö†Ô∏è Eingeschr√§nkt               | ‚úÖ Besser strukturiert           |\n","| ‚ö° Performance                | ‚úÖ Schnell & direkt            | ‚úÖ Schnell                     | ‚úÖ Schnell                       |\n","| üîß Validierung von Variablen | ‚ùå Keine                       | ‚úÖ Automatisch                 | ‚úÖ Automatisch                   |\n","| üß© Wiederverwendbarkeit      | ‚ùå Eingeschr√§nkt               | ‚úÖ Als Template-Objekt         | ‚úÖ Als Template-Objekt           |\n","| üìù Partial Variables         | ‚ùå Nein                        | ‚úÖ Ja                          | ‚úÖ Ja                            |\n","| üîÄ Chains & Pipelines        | ‚ùå Nicht integrierbar          | ‚úÖ LangChain-kompatibel        | ‚úÖ LangChain-kompatibel          |\n","| üéØ Output Parser             | ‚ùå Manuell                     | ‚úÖ Integrierbar                | ‚úÖ Integrierbar                  |"],"metadata":{"id":"UvoCBqi0qc4B"},"id":"UvoCBqi0qc4B"},{"cell_type":"markdown","source":["\n","\n","**Anwendungsempfehlungen**\n","\n","| Methode               | Wann verwenden?                                                                 |\n","| --------------------- | ------------------------------------------------------------------------------- |\n","| **f-string**          | Einfache, einmalige Prompts ¬∑ Prototyping & schnelle Tests ¬∑ Keine LangChain-Integration |\n","| **PromptTemplate**    | Single-Turn Prompts mit Variablen ¬∑ Completion-Modelle (GPT-3, etc.) ¬∑ Keine Konversation |\n","| **ChatPromptTemplate** | Multi-Turn Dialoge ¬∑ Chat-Modelle (GPT-4, Claude, etc.) ¬∑ Komplexe Konversationen mit Memory |\n"],"metadata":{"id":"IrQUS1yvpsFQ"},"id":"IrQUS1yvpsFQ"},{"cell_type":"markdown","metadata":{"id":"Ps344Ejw0-ih"},"source":["## 5.3 | Few-Shot Prompt ‚ú≥Ô∏è\n"],"id":"Ps344Ejw0-ih"},{"cell_type":"markdown","source":["Few-Shot-Prompting ist eine Technik, bei der man einem KI-Modell einige Beispiele (sogenannte \"Shots\") vorlegt, bevor es eine neue Aufgabe l√∂sen soll. Das Modell kann aus diesen Beispielen das Muster erkennen und auf neue, √§hnliche F√§lle anwenden."],"metadata":{"id":"LKY3IaJSDLvt"},"id":"LKY3IaJSDLvt"},{"cell_type":"markdown","source":["`FewShotPromptTemplate` ist eine Prompt-Vorlage, die automatisch mehrere Beispiele einf√ºgt, bevor das eigentliche Nutzer-Input kommt."],"metadata":{"id":"NnBh2dz8M2fi"},"id":"NnBh2dz8M2fi"},{"cell_type":"code","execution_count":34,"metadata":{"id":"cEopgOiS0KU_","executionInfo":{"status":"ok","timestamp":1764695691069,"user_tz":-60,"elapsed":37,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["beispiele = [\n","    {\"frage\": \"Was ist Python?\", \"antwort\": \"Eine interpretierende Programmiersprache.\"},\n","    {\"frage\": \"Was ist Java?\", \"antwort\": \"Eine objektorientierte Sprache.\"}\n","]"],"id":"cEopgOiS0KU_"},{"cell_type":"code","source":["beispiel_prompt_template = PromptTemplate.from_template(\n","    \"Frage: {frage}\\nAntwort: {antwort}\"\n",")"],"metadata":{"id":"fHEYrEPcDD9S","executionInfo":{"status":"ok","timestamp":1764695691089,"user_tz":-60,"elapsed":31,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":35,"outputs":[],"id":"fHEYrEPcDD9S"},{"cell_type":"code","source":["system_prompt = \"Du bist ein Assistent, der Fragen zu Programmiersprachen beantwortet. Halte deine Antworten kurz und pr√§zise:\""],"metadata":{"id":"eAk2iqPrjynu","executionInfo":{"status":"ok","timestamp":1764695691100,"user_tz":-60,"elapsed":27,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"eAk2iqPrjynu","execution_count":36,"outputs":[]},{"cell_type":"code","source":["# Few-Shot-Prompt\n","few_shot_prompt_template = FewShotPromptTemplate(\n","    examples=beispiele,\n","    example_prompt=beispiel_prompt_template,\n","    prefix=system_prompt,\n","    suffix=\"Frage: {frage}\\nAntwort:\",\n","    input_variables=[\"frage\"]\n",")\n","\n","prompt = few_shot_prompt_template.format(frage=\"Was ist Go?\")"],"metadata":{"id":"BpIbLhIhDcUp","executionInfo":{"status":"ok","timestamp":1764695691103,"user_tz":-60,"elapsed":24,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":37,"outputs":[],"id":"BpIbLhIhDcUp"},{"cell_type":"markdown","source":["**Struktur des finalen Prompts:**\n","\n","```\n","[prefix]\n","[formatiertes Beispiel 1]\n","[formatiertes Beispiel 2]\n","...\n","[suffix mit aktueller Frage]\n","```\n","\n"],"metadata":{"id":"tpDH0jAgknKS"},"id":"tpDH0jAgknKS"},{"cell_type":"code","source":["response = llm.invoke(prompt)"],"metadata":{"id":"KZPoIO6gDo23","executionInfo":{"status":"ok","timestamp":1764695692863,"user_tz":-60,"elapsed":1769,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":38,"outputs":[],"id":"KZPoIO6gDo23"},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"Yr0YleeiD_x7","colab":{"base_uri":"https://localhost:8080/","height":99},"executionInfo":{"status":"ok","timestamp":1764695692903,"user_tz":-60,"elapsed":35,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"442d814c-96a9-4625-aa9e-80ac46737ed2"},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Eine kompilierte Sprache, die f√ºr Effizienz und Parallelit√§t entwickelt wurde."},"metadata":{}}],"id":"Yr0YleeiD_x7"},{"cell_type":"markdown","source":["**Vorteile eines `FewShotPromptTemplate`**"],"metadata":{"id":"XFbved9KM_WJ"},"id":"XFbved9KM_WJ"},{"cell_type":"markdown","source":["| Vorteil                      | Beschreibung                                                      |\n","| ---------------------------- | ----------------------------------------------------------------- |\n","| üß† Lerneffekt f√ºr das Modell | Das Modell erkennt aus Beispielen, wie es antworten soll          |\n","| üß∞ Flexibilit√§t              | Beispiele k√∂nnen dynamisch gesetzt oder generiert werden          |\n","| üß± Struktur & Konsistenz     | Einheitlicher Aufbau aller Beispiele √ºber `example_prompt`        |\n","| ü™Ñ Kein Training n√∂tig       | Kein Fine-Tuning ‚Äì nur durch Prompting                            |\n","| üõ† Kombinierbar              | Kann mit Templates, Tools, Chains, OutputParser kombiniert werden |\n"],"metadata":{"id":"EWjuAd2_M-E7"},"id":"EWjuAd2_M-E7"},{"cell_type":"markdown","id":"pt37u3cnbgc","source":["## 5.4 | Eigenes Prompt-Template üìå"],"metadata":{"id":"pt37u3cnbgc"}},{"cell_type":"markdown","id":"ik5lbyv7vc","source":["Die `load_chat_prompt_template()` Funktion aus der `genai_lib.utilities` erm√∂glicht das Laden von Prompt-Templates direkt von GitHub oder aus lokalen Dateien.\n","\n","**Vorteile:**\n","- Zentrale Verwaltung von Prompts im `05_prompt/` Verzeichnis\n","- Wiederverwendbarkeit √ºber mehrere Notebooks hinweg\n","- Versionskontrolle durch Git\n","- Einfaches Teilen von Best-Practice-Prompts\n","\n","**Hinweis:** In Google Colab muss die GitHub-URL verwendet werden, da lokale Pfade nicht verf√ºgbar sind."],"metadata":{"id":"ik5lbyv7vc"}},{"cell_type":"markdown","id":"rjewn4pkze","source":["**Template-Struktur (`05_prompt/text_zusammenfassung.py`):**\n","\n","```python\n","# Prompt Template als Python-Datei\n","messages = [\n","    (\n","        \"system\",\n","        \"Du bist ein Experte f√ºr die Erstellung pr√§gnanter Textzusammenfassungen...\"\n","    ),\n","    (\n","        \"human\",\n","        \"Bitte fasse den folgenden Text zusammen:\\n\\n{text}\\n\\n...\"\n","    )\n","]\n","```\n","\n","**Wichtig:** Templates m√ºssen als **Python-Dateien (.py)** erstellt werden und eine Variable `messages` definieren, die eine Liste von Tupeln mit `(role, content)` enth√§lt."],"metadata":{"id":"rjewn4pkze"}},{"cell_type":"code","id":"fl5appj1wrf","source":["from genai_lib.utilities import load_chat_prompt_template\n","\n","# Template von GitHub laden\n","url = \"https://github.com/ralf-42/GenAI/blob/main/05_prompt/text_zusammenfassung.py\"\n","chat_template = load_chat_prompt_template(url)\n","\n","# Template mit Beispieltext testen\n","beispiel_text = \"\"\"\n","LangChain ist ein Framework zur Entwicklung von Anwendungen mit gro√üen Sprachmodellen (LLMs).\n","Es bietet modulare Komponenten f√ºr verschiedene Aufgaben, vorgefertigte Ketten f√ºr komplexe\n","Workflows und eine einfache Integration externer Datenquellen. Mit LangChain k√∂nnen Entwickler\n","Chat-Modelle, Embeddings, Vektordatenbanken und Agents effizient kombinieren, um\n","leistungsstarke KI-Anwendungen zu erstellen.\n","\"\"\"\n","\n","# Prompt mit Variablen f√ºllen\n","prompt = chat_template.format_messages(text=beispiel_text)\n","\n","# Aufruf\n","response = llm.invoke(prompt)"],"metadata":{"id":"fl5appj1wrf","colab":{"base_uri":"https://localhost:8080/","height":436},"executionInfo":{"status":"error","timestamp":1764696119966,"user_tz":-60,"elapsed":830,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"08f445c8-b3ef-471f-fd28-ba9d80a4d57b"},"execution_count":69,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid decimal literal (tmpged9znzp.py, line 120)","traceback":["Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n","  File \u001b[1;32m\"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \u001b[1;32m\"/tmp/ipython-input-3409781549.py\"\u001b[0m, line \u001b[1;32m5\u001b[0m, in \u001b[1;35m<cell line: 0>\u001b[0m\n    chat_template = load_chat_prompt_template(url)\n","  File \u001b[1;32m\"/usr/local/lib/python3.12/dist-packages/genai_lib/utilities.py\"\u001b[0m, line \u001b[1;32m368\u001b[0m, in \u001b[1;35mload_chat_prompt_template\u001b[0m\n    spec.loader.exec_module(module)\n","  File \u001b[1;32m\"<frozen importlib._bootstrap_external>\"\u001b[0m, line \u001b[1;32m995\u001b[0m, in \u001b[1;35mexec_module\u001b[0m\n","  File \u001b[1;32m\"<frozen importlib._bootstrap_external>\"\u001b[0m, line \u001b[1;32m1133\u001b[0m, in \u001b[1;35mget_code\u001b[0m\n","  File \u001b[1;32m\"<frozen importlib._bootstrap_external>\"\u001b[0m, line \u001b[1;32m1063\u001b[0m, in \u001b[1;35msource_to_code\u001b[0m\n","\u001b[0;36m  File \u001b[0;32m\"<frozen importlib._bootstrap>\"\u001b[0;36m, line \u001b[0;32m488\u001b[0;36m, in \u001b[0;35m_call_with_frames_removed\u001b[0;36m\u001b[0m\n","\u001b[0;36m  File \u001b[0;32m\"/tmp/tmpged9znzp.py\"\u001b[0;36m, line \u001b[0;32m120\u001b[0m\n\u001b[0;31m    <title>GenAI/05_prompt/text_zusammenfassung.py at main ¬∑ ralf-42/GenAI ¬∑ GitHub</title>\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"]}]},{"cell_type":"code","id":"zqaw2rdob6b","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"zqaw2rdob6b","colab":{"base_uri":"https://localhost:8080/","height":134},"executionInfo":{"status":"ok","timestamp":1764695894655,"user_tz":-60,"elapsed":53,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"2124cdb9-1c29-44c1-e1fe-4e9dc5f19562"},"execution_count":42,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"LangChain ist ein Framework zur Entwicklung von Anwendungen mit gro√üen Sprachmodellen (LLMs), das modulare Komponenten f√ºr verschiedene Aufgaben und vorgefertigte Ketten f√ºr komplexe Workflows bietet. Es erm√∂glicht eine einfache Integration externer Datenquellen und die effiziente Kombination von Chat-Modellen, Embeddings, Vektordatenbanken und Agents. Dadurch k√∂nnen Entwickler leistungsstarke KI-Anwendungen erstellen."},"metadata":{}}]},{"cell_type":"markdown","id":"8c859b9f","metadata":{"id":"8c859b9f"},"source":["# 6 | Weitere Modelle üìå\n","---"]},{"cell_type":"markdown","source":["LangChain ist mit zahlreichen LLMs ‚Äì sowohl eines als auch verschiedener Anbieter ‚Äì kompatibel und bietet vielseitige Konfigurationsoptionen f√ºr individuelle Anwendungsf√§lle."],"metadata":{"id":"5y6J-chfEOPH"},"id":"5y6J-chfEOPH"},{"cell_type":"markdown","source":["**Beispiel `gpt-4o` und `gpt-5-mini`**"],"metadata":{"id":"Oc0UPT5eTyTR"},"id":"Oc0UPT5eTyTR"},{"cell_type":"markdown","source":["+ gpt-4o:    \n","gpt-4o ist ein multimodales KI-Modell von OpenAI, das Text- und Bildeingaben verarbeiten kann und sich durch hohe Geschwindigkeit sowie eine nat√ºrliche, menschen√§hnliche Interaktion auszeichnet ‚Äì ideal f√ºr allt√§gliche Konversationen, kreative Aufgaben und allgemeine Chatanwendungen\n","+ gpt-5-mini:     \n","gpt-5-mini ist die mittlere Modellvariante der GPT-5-Familie, die ein ausgewogenes Verh√§ltnis zwischen Leistungsf√§higkeit und Kosten bietet und f√ºr allt√§gliche Aufgaben, allgemeine Automatisierung sowie kosteng√ºnstige Anwendungen im gro√üen Ma√üstab optimiert wurde"],"metadata":{"id":"f8yl7OApTvcQ"},"id":"f8yl7OApTvcQ"},{"cell_type":"code","source":["# Modelle konfigurieren\n","\n","# Chat-Modell\n","chat_model = init_chat_model(\"gpt-4o\", model_provider=model_provider)\n","\n","# Reasoning-Modell\n","resoning_model = init_chat_model(\"gpt-5-mini\", model_provider=model_provider)"],"metadata":{"id":"aDGqhlxoENvs","executionInfo":{"status":"ok","timestamp":1764695902914,"user_tz":-60,"elapsed":84,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"aDGqhlxoENvs","execution_count":43,"outputs":[]},{"cell_type":"code","source":["# Gemeinsamer simple prompt\n","prompt = \"Was ist der Unterschied zwischen Prompting und Retrieval-Augmented-Generation?\""],"metadata":{"id":"PywoNdLrPCfO","executionInfo":{"status":"ok","timestamp":1764695902922,"user_tz":-60,"elapsed":4,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"PywoNdLrPCfO","execution_count":44,"outputs":[]},{"cell_type":"code","source":["# Aufruf\n","response = chat_model.invoke(prompt)"],"metadata":{"id":"iriw2JZPOazT","executionInfo":{"status":"ok","timestamp":1764695910431,"user_tz":-60,"elapsed":7506,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"iriw2JZPOazT","execution_count":45,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Chat-Model:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"9OlEy8jQPck1","colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"status":"ok","timestamp":1764695910490,"user_tz":-60,"elapsed":14,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"07adefc6-049d-4337-d356-171d17f886ac"},"id":"9OlEy8jQPck1","execution_count":46,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Chat-Model:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Prompting und Retrieval-Augmented-Generation (RAG) sind beides Methoden, die in der Interaktion mit KI-Modellen Anwendung finden, insbesondere im Bereich der nat√ºrlichen Sprachverarbeitung. Hier sind die Hauptunterschiede zwischen ihnen:\n\n1. **Prompting:**\n   - Prompting bezieht sich auf die Technik, bei der Nutzer spezifische Eingaben oder \"Prompts\" an ein Sprachmodell geben, um eine gew√ºnschte Antwort oder ein bestimmtes Verhalten zu erzielen.\n   - Die Qualit√§t und Art der Antwort h√§ngt stark von der Formulierung des Prompts ab. Ein gut definierter Prompt kann dem Modell helfen, pr√§zisere oder relevantere Antworten zu generieren.\n   - Prompting wird oft ohne zus√§tzlichen Zugriff auf externe Datenquellen durchgef√ºhrt. Das Sprachmodell verwendet sein bestehendes Training, um Antworten zu generieren.\n\n2. **Retrieval-Augmented-Generation (RAG):**\n   - RAG kombiniert die F√§higkeiten eines Retrieval-Systems und eines generativen Modells. Es nutzt externe Wissensdatenbanken oder Dokumentensammlungen, um Informationen abzurufen, die dann verwendet werden, um eine fundierte Antwort zu generieren.\n   - Der Prozess besteht in der Regel aus zwei Hauptschritten: Zuerst werden relevante Informationen aus einer externen Datenbank abgerufen (Retrieval), und dann wird das generative Modell verwendet, um diese Informationen in eine koh√§rente Antwort einzubetten (Generation).\n   - Dadurch kann RAG aktuelle und spezifische Informationen einbinden, die √ºber das hinausgehen, was nur im Sprachmodell gespeichert ist.\n\nInsgesamt bietet RAG eine M√∂glichkeit, die Reichweite und Aktualit√§t eines Sprachmodells zu erweitern, indem es mit echtem, aktuellem Wissen aus externen Quellen kombiniert wird, w√§hrend Prompting sich auf die maximal verf√ºgbare Nutzung des bereits im Modell trainierten Wissens konzentriert."},"metadata":{}}]},{"cell_type":"code","source":["# Aufruf\n","response = resoning_model.invoke(prompt)"],"metadata":{"id":"_Znkwm2SPOrX","executionInfo":{"status":"ok","timestamp":1764695925679,"user_tz":-60,"elapsed":15185,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"_Znkwm2SPOrX","execution_count":47,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Reasoning-Model:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"N0hcQkSfQyVK","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1764695925719,"user_tz":-60,"elapsed":5,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"348a90a2-1724-441f-96f2-ed03bdc466c7"},"id":"N0hcQkSfQyVK","execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Reasoning-Model:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Kurzfassung\n- Prompting: Du gibst dem LLM direkt eine Eingabe (Prompt) und das Modell erzeugt die Antwort nur aus seinem internen Wissen und seinen Gewichten (‚Äûclosed‚Äëbook‚Äú).\n- Retrieval‚ÄëAugmented Generation (RAG): Vor der Generierung wird externes Wissen abgerufen (z. B. Dokumente aus einer Datenbank oder Vektor‚ÄëSuche). Die abgerufenen Texte werden dem LLM zusammen mit dem Prompt gegeben, damit die Antwort auf diesen externen Quellen basiert (‚Äûopen‚Äëbook‚Äú).\n\nWas passiert technisch?\n- Prompting\n  - Ein Schritt: Prompt ‚Üí LLM ‚Üí Antwort.\n  - Das Modell greift nur auf das gelernte Gewichtsspeicher zur√ºck.\n- RAG\n  - Zwei Schritte: 1) Retriever sucht relevante Dokumente (BM25 oder dichte Vektor‚ÄëSuche mit Embeddings). 2) Generator (LLM) erh√§lt Prompt + abgerufene Dokumentausschnitte und erzeugt die Antwort.\n  - Optional: Re‚Äëranking, Zusammenfassung der Treffer, Quellenverweise.\n\nWesentliche Unterschiede (√úbersicht)\n- Datenquelle\n  - Prompting: nur interne Modellkenntnis (Training bis zu einem Zeitpunkt).\n  - RAG: externe, ggf. aktuelle/wenig bekannte Dokumente.\n- Faktentreue & Nachvollziehbarkeit\n  - Prompting: erh√∂htes Halluzinationsrisiko bei spezifischen/aktuellen Fakten.\n  - RAG: bessere Grounding, erm√∂glicht Quellenangaben (aber immer noch m√∂gliches Halluzinieren).\n- Skalierbarkeit & Wartung\n  - Prompting: einfacher, kein extra Index/DB n√∂tig.\n  - RAG: aufw√§ndiger (Embedding‚ÄëErzeugung, VektorDB, Daten‚ÄëPipelines).\n- Latenz & Kosten\n  - Prompting: schnellere Pipeline, meist g√ºnstiger.\n  - RAG: zus√§tzlicher Retrieval‚ÄëSchritt ‚Üí h√∂here Latenz und Infrastrukturkosten.\n- Anwendungsf√§lle\n  - Prompting: kreative Texte, allgemeines Wissen, Chat/Conversational Flow.\n  - RAG: kundenspezifische FAQ, Unternehmenswissen, rechtliche/medizinische Antworten, aktuelle Informationen.\n\nBeispiel (vereinfacht)\n- Prompting:\n  - Prompt: ‚ÄûFasse die Kernpunkte des Klimawandels zusammen.‚Äú\n  - LLM antwortet aus seinem gelernten Wissen.\n- RAG:\n  - Retriever: Suche relevante aktuelle Artikel/Interne Doku ‚Üí Top‚Äë5 Treffer.\n  - Prompt an LLM: ‚ÄûNutze die folgenden Texte (1‚Äì5) und beantworte: ‚Ä¶‚Äú ‚Üí LLM erzeugt Antwort mit Bezug auf diese Quellen.\n\nGute Praktiken beim Einsatz von RAG\n- Dokumente sinnvoll chunk‚Äëen (L√§nge beachten).\n- Embeddings regelm√§√üig aktualisieren, wenn Daten sich √§ndern.\n- Top‚Äëk und Re‚ÄëRanking abstimmen (Precision vs. Recall).\n- Kontextbegrenzung (Tokenlimit) beachten ‚Äî ggf. Zusammenfassungen der Treffer verwenden.\n- Quellenangabe und Verifizierungs‚ÄëSchicht einbauen (Fact‚Äëchecking).\n- Monitoring auf Halluzinationen und Retrieval‚ÄëQualit√§t.\n\nFazit\n- Prompting reicht f√ºr generelle, kreative und nicht‚Äëhochverbindliche Aufgaben.\n- RAG solltest du einsetzen, wenn Antworten auf aktuelle, unternehmensspezifische oder nachpr√ºfbare Informationen zugreifen m√ºssen ‚Äî es erh√∂ht Grounding und Nachvollziehbarkeit, verlangt aber mehr Infrastruktur und Sorgfalt."},"metadata":{}}]},{"cell_type":"markdown","source":["**üìä Vergleich: Chat-Modell vs. Reasoning-Modell**"],"metadata":{"id":"YJljcFw4Rz6W"},"id":"YJljcFw4Rz6W"},{"cell_type":"markdown","source":["| **Aspekt**                      | **Chat-Modell**                                                           | **Reasoning-Modell**                                                                                |\n","| ------------------------------- | ------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------- |\n","| **Stil**                        | Erkl√§rend, klassisch strukturiert mit Flie√ütext                           | Punktuell, technisch und systematisch                                                               |\n","| **Formatierung**                | Flie√ütext mit Abs√§tzen und Beispielen                                     | Aufz√§hlungen (‚Ä¢) und strukturierte Unterpunkte                                                      |\n","| **Definition Prompting**        | Kurze Definition als Text, danach Beispiel                                | Technisch klar: ‚Äûparametrisches Wissen‚Äú, Rolle der Prompt-Formulierung betont                       |\n","| **Definition RAG**              | Flie√ütext-Erkl√§rung mit Ablaufbeschreibung und Beispiel                   | Detaillierte Schritte: Abruf, Kontextintegration, Vorteile bzgl. Aktualit√§t und Pr√§zision erl√§utert |\n","| **Beispiele**                   | Ja ‚Äì einfache, praxisnahe Beispiele (z.‚ÄØB. Klimawandel, Quantencomputing) | Nein ‚Äì keine konkreten Beispiele, aber kontextbezogene Beschreibung                                 |\n","| **Vergleich Prompting vs. RAG** | Abstrakte Gegen√ºberstellung in einem abschlie√üenden Absatz                | Direkter Vergleich im letzten Abschnitt mit Betonung der Wissensquellen                             |\n","| **Technische Tiefe**            | Mittel ‚Äì f√ºr Einsteiger\\:innen gut verst√§ndlich                           | Hoch ‚Äì Begriffe wie ‚Äûparametrisches Wissen‚Äú, Fokus auf Architektur                                  |\n","| **Zielgruppe**                  | Allgemeines Publikum, Einstieg in das Thema                               | Fortgeschrittene Nutzer\\:innen, Technik-affine Leserschaft                                          |\n","| **St√§rken des Modells**         | Klare Erkl√§rung, gute didaktische Struktur                                | Pr√§zise Begriffsabgrenzung, hohe Informationsdichte                                                 |\n","\n"],"metadata":{"id":"OzPIlXnqR2Fq"},"id":"OzPIlXnqR2Fq"},{"cell_type":"markdown","source":["**üìä Vergleich: Chat-Modelle**"],"metadata":{"id":"GO40ayL6plxO"},"id":"GO40ayL6plxO"},{"cell_type":"markdown","source":["| Model              | Creator    | Open?  | API Name(s) (falls verf√ºgbar)                            | Context Window             |\n","| ------------------ | ---------- | ------ | -------------------------------------------------------- | -------------------------- |\n","| **GPT-5**          | OpenAI     | Closed | `gpt-5`, `gpt-5-mini`, `gpt-5-nano`, `gpt-5-chat-latest` | 400K Tokens (~296 Seiten)  |\n","| **GPT-4.1**        | OpenAI     | Closed | `gpt-4.1`, `gpt-4.1-mini`, `gpt-4.1-nano`                | 1M Tokens (~741 Seiten)    |\n","| **GPT-4o**         | OpenAI     | Closed | `gpt-4o`, `gpt-4o-mini`                                  | 128K Tokens (~95 Seiten)   |\n","| **GPT-4 Turbo**    | OpenAI     | Closed | `gpt-4-turbo`, `gpt-4-turbo-2024-04-09`                  | 128K Tokens (~95 Seiten)   |\n","| **gpt-oss-120b**   | OpenAI     | Open   | ‚Äî                                                        | 131K Tokens (~97 Seiten)   |\n","| **gpt-oss-20b**    | OpenAI     | Open   | ‚Äî                                                        | 131K Tokens (~97 Seiten)   |\n","| **Claude 4**       | Anthropic  | Closed | `claude-sonnet-4-20250514`, `claude-opus-4-20250514`     | 200K Tokens (~148 Seiten)  |\n","| **Gemini 2.5 Pro** | Google     | Closed | `gemini-2.5-pro`, `gemini-2.5-pro-exp-03-25`             | 1M Tokens (~741 Seiten)    |\n","| **Llama 3.1 405B** | Meta       | Open   | ‚Äî                                                        | 512K Tokens (~379 Seiten)  |\n","| **DeepSeek-V3**    | DeepSeek   | Open   | ‚Äî                                                        | 128K Tokens (~95 Seiten)   |\n","| **Mistral 7B**     | Mistral.AI | Open   | ‚Äî                                                        | 32K Tokens (~24 Seiten)    |\n","\n","<br>\n","\n","*Umrechnung Token/Seiten:  ~1.350 Tokens ‚âà 1 Seite deutschsprachiger Text*"],"metadata":{"id":"RpcsYzLApH7v"},"id":"RpcsYzLApH7v"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","\n","Ein anderer Modell-Provider: `groq`\n","</font></p>"],"metadata":{"id":"8y5xNtM1oMs3"},"id":"8y5xNtM1oMs3"},{"cell_type":"code","source":["install_packages([('langchain-groq', 'langchain_groq')])"],"metadata":{"id":"ec6V1NJRnjPv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764695926270,"user_tz":-60,"elapsed":550,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"04ecb7bc-c751-475f-cd48-d76a01fc139a"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["üîÑ Installiere langchain-groq...\n","‚úÖ langchain-groq erfolgreich installiert und importiert\n"]}],"id":"ec6V1NJRnjPv"},{"cell_type":"code","source":["setup_api_keys(['GROQ_API_KEY'])"],"metadata":{"id":"wXDH1ysxfo_K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764695926895,"user_tz":-60,"elapsed":622,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"5bd92630-b7be-45e2-fd88-07f0def6bf0a"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úì GROQ_API_KEY erfolgreich gesetzt\n"]}],"id":"wXDH1ysxfo_K"},{"cell_type":"code","source":["model_name = \"qwen/qwen3-32b\"\n","temperature = 0\n","llm_groq = init_chat_model(model_name, model_provider=\"groq\", temperature=temperature)"],"metadata":{"id":"cVSRS2Kaf1M7","executionInfo":{"status":"ok","timestamp":1764695927328,"user_tz":-60,"elapsed":429,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":51,"outputs":[],"id":"cVSRS2Kaf1M7"},{"cell_type":"code","source":["response_groq = llm_groq.invoke(\"Wer war Albert Einstein?\")"],"metadata":{"id":"oRl7X3y-nm92","executionInfo":{"status":"ok","timestamp":1764695945967,"user_tz":-60,"elapsed":18635,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":52,"outputs":[],"id":"oRl7X3y-nm92"},{"cell_type":"code","source":["mprint(response_groq.content)"],"metadata":{"id":"fhwKoyS3n0ho","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1764695946019,"user_tz":-60,"elapsed":47,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"6d0149de-2254-4cc8-fbbc-598e72e2f987"},"execution_count":53,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"<think>\nOkay, the user is asking \"Wer war Albert Einstein?\" which means \"Who was Albert Einstein?\" in German. I need to provide a comprehensive answer in German. Let me start by recalling the key points about Einstein.\n\nFirst, his full name is Albert Einstein. He was a physicist, born in 1879 in Germany and died in 1955 in the USA. He's most famous for the theory of relativity, both special and general. The equation E=mc¬≤ is a big part of that. He also contributed to quantum theory, like the photoelectric effect, which earned him the Nobel Prize in Physics in 1921.\n\nI should mention his early life, maybe his education. He studied in Zurich, graduated from ETH Zurich. He worked at the patent office in Bern, which is where he developed some of his theories. His work on Brownian motion helped confirm the existence of atoms.\n\nHe fled Nazi Germany in 1933, moved to the US, became a citizen there. He was a pacifist and later an advocate for civil rights. His political views and activism are also important to note.\n\nI need to structure the answer logically: start with his basic info, then his major contributions, his education and career, his political stance, and legacy. Make sure to highlight his most famous works and the impact they had. Also, mention the Nobel Prize and the significance of E=mc¬≤. Avoid too much technical jargon since the user might be looking for a general overview.\n\nCheck if there are any common misconceptions to address, like the myth that he failed math as a child. Also, clarify that he was a theoretical physicist, not an experimental one. Maybe mention his later years and how he worked on a unified field theory without success.\n\nEnsure the answer is in clear, concise German, using proper terminology. Double-check dates and facts for accuracy. Make sure to cover all the key aspects without going into excessive detail. The user probably wants a balanced summary that covers who he was, what he did, and his significance in history and science.\n</think>\n\nAlbert Einstein (1879‚Äì1955) war ein deutsch-amerikanischer Physiker, der als einer der einflussreichsten Wissenschaftler des 20. Jahrhunderts gilt. Seine bahnbrechenden Arbeiten revolutionierten unser Verst√§ndnis von Raum, Zeit, Energie und Schwerkraft.\n\n### **Lebenswichtige Fakten**\n- **Geboren**: 14. M√§rz 1879 in Ulm, Deutsches Kaiserreich.  \n- **Gestorben**: 18. April 1955 in Princeton, New Jersey, USA.  \n- **Staatsangeh√∂rigkeit**: Deutscher (bis 1933), sp√§ter Schweizer und US-amerikanischer B√ºrger.  \n\n---\n\n### **Wissenschaftliche Leistungen**\n1. **Relativit√§tstheorie**  \n   - **Spezielle Relativit√§tstheorie (1905)**:  \n     - Einf√ºhrung der ber√ºhmten Gleichung **E = mc¬≤**, die die √Ñquivalenz von Masse und Energie beschreibt.  \n     - Raum und Zeit sind relativ und h√§ngen vom Beobachter ab (z. B. Zeitdilatation, L√§ngenkontraktion).  \n   - **Allgemeine Relativit√§tstheorie (1915)**:  \n     - Die Schwerkraft wird als Kr√ºmmung der Raumzeit durch Masse und Energie erkl√§rt.  \n     - Vorhersage von Ph√§nomenen wie Lichtablenkung durch massive Objekte oder Gravitationswellen.  \n\n2. **Quantentheorie**  \n   - Erkl√§rung des **Photoeffekts** (1905): Licht besteht aus diskreten Energiepaketen (Photonen), was die Quantenhypothese von Max Planck st√ºtzte.  \n   - Dies brachte ihm 1921 den **Nobelpreis f√ºr Physik** ein.  \n\n3. **Andere Beitr√§ge**  \n   - **Brown‚Äôsche Bewegung**: Theoretische Erkl√§rung der zuf√§lligen Bewegung von Teilchen in Fl√ºssigkeiten, die die Existenz von Atomen best√§tigte.  \n   - **Statistische Mechanik**: Arbeiten zur Entropie und Quantenstatistik.  \n\n---\n\n### **Leben und Karriere**\n- **Ausbildung**: Studium an der Eidgen√∂ssischen Technischen Hochschule (ETH) Z√ºrich (1900‚Äì1905).  \n- **Patentamt Bern**: 1902‚Äì1909 arbeitete er als Technischer Assistent am Schweizerischen Patentamt, wo er seine bahnbrechenden Arbeiten schrieb.  \n- **Akademische Karriere**: Professor in Z√ºrich, Prag, Berlin und Princeton.  \n- **Flucht vor dem Nationalsozialismus**: 1933 emigrierte er in die USA und lebte fortan in Princeton.  \n\n---\n\n### **Politische und gesellschaftliche Engagement**\n- **Pazifist**: Vor 1914 ein Gegner des Krieges, sp√§ter kritisch gegen√ºber dem Zweiten Weltkrieg.  \n- **Atomwaffen**: 1939 schrieb er an Roosevelt, um die Entwicklung der Atombombe zu warnen. Nach Hiroshima/Kurume wurde er ein starker Gegner ihrer Verwendung.  \n- **Zivilrechte**: Aktivist f√ºr Rassengleichheit und freie Meinungs√§u√üerung.  \n\n---\n\n### **Erbe**\n- Seine Theorien pr√§gten die moderne Physik und Technologie (z. B. GPS nutzt relativistische Korrekturen).  \n- Sein Bild ist ein Symbol f√ºr Genialit√§t und wissenschaftliche Kreativit√§t.  \n- **Posthume Anerkennung**: 2005 wurde das **Internationale Jahr der Physik** zu seinen 100. Geburtstag ausgerufen.  \n\nEinstein bleibt eine ikonische Figur, deren Gedanken die Wissenschaft und Philosophie bis heute beeinflussen."},"metadata":{}}],"id":"fhwKoyS3n0ho"},{"cell_type":"markdown","id":"c314ca71","metadata":{"id":"c314ca71"},"source":["# 7 | Chain üìå\n","---"]},{"cell_type":"markdown","source":["Chains verbinden mehrere Komponenten/Bausteine zu einer Verarbeitungskette. LangChain Expression Language (LCEL) ist das **neue Programmiermodell in LangChain v0.3+**, das die Entwicklung von LLM-Anwendungen vereinfacht.\n"],"metadata":{"id":"cEEMxhSBHrXA"},"id":"cEEMxhSBHrXA"},{"cell_type":"markdown","source":["\n","**Hier die Kernpunkte:**\n","\n","1. Was ist LCEL?\n","```python\n","# LCEL nutzt den Pipe-Operator (|) f√ºr klare Verkettungen\n","chain = prompt | model\n","```\n","- Eine deklarative Sprache zum Verketten von LangChain-Komponenten\n","- Erm√∂glicht linearen Datenfluss zwischen Komponenten\n","- Basiert auf dem Pipe-Operator (|) f√ºr intuitive Verbindungen\n","\n","2. Warum LCEL nutzen?\n","- Bessere Lesbarkeit des Codes\n","- Einfachere Wartung und Debugging\n","- Verbesserte Performance durch optimierte Ausf√ºhrung\n","- Bessere Typsicherheit und Fehlererkennung\n","- Unterst√ºtzt modernes Streaming und Async-Operationen\n","\n","3. Praktisches Beispiel:"],"metadata":{"id":"ltTQItwXH6w8"},"id":"ltTQItwXH6w8"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Simple Chain üìå\n","</font></p>\n"],"metadata":{"id":"BndZGbyWKs1C"},"id":"BndZGbyWKs1C"},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","# 1. Prompt-Template\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"human\", \"Erkl√§re mir {user_input}\")\n","])\n","\n","# 2. Modell\n","model_name = \"gpt-4o-mini\"\n","temperature = 0\n","llm = init_chat_model(model_name, model_provider=\"openai\", temperature=temperature)\n","\n","#  3. Einfache LCEL-Kette\n","chain = prompt | llm\n","\n","# 4. Ausf√ºhrung\n","response = chain.invoke({\"user_input\": \"LangChain Expression Language\"})"],"metadata":{"id":"PsWtjWROIW9d","executionInfo":{"status":"ok","timestamp":1764695958608,"user_tz":-60,"elapsed":12572,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"PsWtjWROIW9d","execution_count":54,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"VWYAG9aWJpZd","colab":{"base_uri":"https://localhost:8080/","height":522},"executionInfo":{"status":"ok","timestamp":1764695958632,"user_tz":-60,"elapsed":14,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"872b1dd8-5ab6-4892-8a9d-6a806e5d5445"},"id":"VWYAG9aWJpZd","execution_count":55,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"LangChain Expression Language (LCEL) ist eine spezielle Sprache, die in der LangChain-Bibliothek verwendet wird, um komplexe Datenverarbeitungs- und Abfrageoperationen zu vereinfachen. LangChain selbst ist ein Framework, das Entwicklern hilft, Anwendungen mit Sprachmodellen zu erstellen, indem es verschiedene Komponenten wie Datenquellen, Agenten und Chain-Logik integriert.\n\nHier sind einige der Hauptmerkmale und Konzepte von LCEL:\n\n1. **Ausdr√ºcke**: LCEL erm√∂glicht es, Ausdr√ºcke zu definieren, die auf Daten angewendet werden k√∂nnen. Diese Ausdr√ºcke k√∂nnen einfache Berechnungen, Datenfilterungen oder komplexe Transformationen umfassen.\n\n2. **Variablen**: In LCEL k√∂nnen Variablen verwendet werden, um Werte zu speichern und in Ausdr√ºcken zu referenzieren. Das macht es einfacher, mit dynamischen Daten zu arbeiten.\n\n3. **Funktionen**: LCEL unterst√ºtzt die Definition und Verwendung von Funktionen, die wiederverwendbare Logik kapseln. Das ist besonders n√ºtzlich, wenn man h√§ufige Operationen auf Daten durchf√ºhren m√∂chte.\n\n4. **Integration mit Sprachmodellen**: LCEL ist so konzipiert, dass es nahtlos mit Sprachmodellen interagiert. Das bedeutet, dass man die Ausdruckssprache nutzen kann, um die Eingaben und Ausgaben von Sprachmodellen zu steuern und zu verarbeiten.\n\n5. **Flexibilit√§t**: Die Sprache ist flexibel und anpassbar, sodass Entwickler sie an ihre spezifischen Anforderungen anpassen k√∂nnen.\n\nEin einfaches Beispiel k√∂nnte so aussehen:\n\n```lcel\nlet x = 10\nlet y = 20\nlet sum = x + y\n```\n\nIn diesem Beispiel werden zwei Variablen `x` und `y` definiert, und ihre Summe wird in der Variablen `sum` gespeichert.\n\nInsgesamt ist LCEL ein leistungsf√§higes Werkzeug, um die Interaktion mit Daten und Sprachmodellen in LangChain zu optimieren. Wenn du mehr √ºber spezifische Funktionen oder Anwendungsf√§lle wissen m√∂chtest, lass es mich wissen! Und keine Sorge, ich verspreche, nicht zu viele technische Begriffe zu verwenden ‚Äì ich bin hier, um zu helfen, nicht um dich zu verwirren! üòÑ"},"metadata":{}}]},{"cell_type":"markdown","source":["\n","LCEL ist der empfohlene Weg f√ºr alle neuen LangChain-Projekte, da es die Entwicklung vereinfacht und zukunftssicher macht."],"metadata":{"id":"49Wu_XreIZY2"},"id":"49Wu_XreIZY2"},{"cell_type":"markdown","source":["# 8 | Output-Handling\n","---"],"metadata":{"id":"LVqVL9CWRXU7"},"id":"LVqVL9CWRXU7"},{"cell_type":"markdown","source":["# 8.1 | OutputParser üìå"],"metadata":{"id":"89XLSUAvCE8P"},"id":"89XLSUAvCE8P"},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/langchain_prozess_03.png)"],"metadata":{"id":"erhTv-_o97q9"},"id":"erhTv-_o97q9"},{"cell_type":"markdown","source":["Ein OutputParser ist ein Konzept in LangChain, das hilft, die Antworten eines Modells richtig weiterzuverarbeiten. Es nimmt die **rohe Ausgabe** (zum Beispiel einen langen Text) und formt sie in ein **bestimmtes Format**, das sp√§ter in der Anwendung leichter benutzt werden kann."],"metadata":{"id":"EVBcCwJERfaj"},"id":"EVBcCwJERfaj"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Simple Chain mit Parser üìå\n","</font></p>"],"metadata":{"id":"oK4lZB3EGKFK"},"id":"oK4lZB3EGKFK"},{"cell_type":"code","source":["# 1. Prompt-Template\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"human\", \"Erkl√§re mir {user_input}\")\n","])\n","\n","# 2. Modell\n","model_name = \"gpt-4o-mini\"\n","temperature = 0\n","llm = init_chat_model(model_name, model_provider=\"openai\", temperature=temperature)\n","\n","# 3. Parser\n","parser = StrOutputParser()\n","\n","# 4. Einfache LCEL-Kette\n","chain = prompt | llm | parser\n","\n","# 5. Ausf√ºhrung\n","response = chain.invoke({\"user_input\": \"LangChain Expression Language\"})"],"metadata":{"id":"PP_A_bGc7ovZ","executionInfo":{"status":"ok","timestamp":1764695967407,"user_tz":-60,"elapsed":8768,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":56,"outputs":[],"id":"PP_A_bGc7ovZ"},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response)"],"metadata":{"id":"qMcn5ctu7ova","colab":{"base_uri":"https://localhost:8080/","height":418},"executionInfo":{"status":"ok","timestamp":1764695967444,"user_tz":-60,"elapsed":22,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"9048c683-4dc0-4f45-8245-897ebe90b9f9"},"execution_count":57,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"LangChain Expression Language (LCEL) ist eine spezielle Sprache, die in der LangChain-Plattform verwendet wird, um komplexe Datenverarbeitungs- und Abfrageoperationen zu erm√∂glichen. LangChain selbst ist ein Framework, das Entwicklern hilft, Anwendungen mit Sprachmodellen zu erstellen, indem es verschiedene Komponenten wie Datenquellen, Agenten und Werkzeuge integriert.\n\nHier sind einige grundlegende Aspekte von LCEL:\n\n1. **Zweck**: LCEL wird verwendet, um Abfragen und Transformationen auf Daten durchzuf√ºhren, die von Sprachmodellen verarbeitet werden. Es erm√∂glicht eine pr√§zise und flexible Interaktion mit den Daten.\n\n2. **Syntax**: Die Syntax von LCEL ist so gestaltet, dass sie intuitiv und leicht verst√§ndlich ist. Sie √§hnelt oft SQL oder anderen Abfragesprachen, was es Entwicklern erleichtert, sich einzuarbeiten.\n\n3. **Funktionen**: LCEL unterst√ºtzt eine Vielzahl von Funktionen, die es erm√∂glichen, Daten zu filtern, zu aggregieren und zu transformieren. Dies kann von einfachen mathematischen Operationen bis hin zu komplexen logischen Ausdr√ºcken reichen.\n\n4. **Integration**: LCEL ist nahtlos in die LangChain-Architektur integriert, was bedeutet, dass es einfach ist, es in Anwendungen zu verwenden, die auf Sprachmodellen basieren.\n\n5. **Anwendungsf√§lle**: Typische Anwendungsf√§lle f√ºr LCEL sind Datenanalysen, das Erstellen von Berichten, das Filtern von Informationen und das Generieren von Antworten auf komplexe Fragen.\n\nWenn du mehr √ºber spezifische Funktionen oder Beispiele wissen m√∂chtest, lass es mich wissen! Und keine Sorge, ich verspreche, nicht in eine endlose Schleife von Abfragen zu geraten! üòÑ"},"metadata":{}}],"id":"qMcn5ctu7ova"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Simple Chain mit Parser & Format im System-Prompt\n","</font></p>"],"metadata":{"id":"pNKOzTlQ8Kpl"},"id":"pNKOzTlQ8Kpl"},{"cell_type":"code","source":["# 1. Einfacher Prompt mit explizite Formatvorgabe\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent. Formatiere Deine Antwort im json-Format mit den Infos zu Name und Alter.\"),\n","    (\"human\", \"{user_input}\")\n","])\n","\n","# 2. Modell\n","model_name = \"gpt-4o-mini\"\n","temperature\t= 0\n","llm = init_chat_model(model_name, model_provider=\"openai\", temperature=temperature)\n","\n","# 3. Parser\n","parser = StrOutputParser()\n","\n","# 4. Chain\n","chain = prompt | llm | parser\n","\n","# 5. Ausf√ºhrung\n","response = chain.invoke({\"user_input\": \"Bitte gibt den Namen einer fiktiven Person und das Alter\"})"],"metadata":{"id":"QKtYSBLhVlts","executionInfo":{"status":"ok","timestamp":1764695968223,"user_tz":-60,"elapsed":771,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":58,"outputs":[],"id":"QKtYSBLhVlts"},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response)"],"metadata":{"id":"BGceDe7Kw6ot","colab":{"base_uri":"https://localhost:8080/","height":145},"executionInfo":{"status":"ok","timestamp":1764695968241,"user_tz":-60,"elapsed":30,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"ae18a739-03f3-4e58-e401-4a6311f1c773"},"id":"BGceDe7Kw6ot","execution_count":59,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"```json\n{\n  \"Name\": \"Maximilian M√ºller\",\n  \"Alter\": 28\n}\n```"},"metadata":{}}]},{"cell_type":"markdown","id":"5efri5ponin","source":["# 8.2 | with_structured_output() ‚ú≥Ô∏è\n"],"metadata":{"id":"5efri5ponin"}},{"cell_type":"markdown","id":"9hnfp8g8bkj","source":["Im vorherigen Beispielen wurden Formatierung √ºber **System-Prompts** und/oder √ºber einen **OututParser** gesteuert. Das funktioniert, hat aber Nachteile:\n","\n","- ‚ùå Keine automatische Validierung (Typen k√∂nnen falsch sein)\n","- ‚ùå String-Parsing fehleranf√§llig\n","- ‚ùå Keine Typsicherheit im Code\n","- ‚ùå Prompt-Engineering f√ºr Format n√∂tig\n","\n","**Moderne L√∂sung: `with_structured_output()`**\n","\n","LangChain 1.0+ bietet eine elegante L√∂sung: Mit `with_structured_output()` kann man die gew√ºnschte Ausgabestruktur **direkt als Pydantic-Modell** definieren. Das Modell sorgt automatisch f√ºr:\n","\n","- ‚úÖ Validierte, typsichere Ausgaben\n","- ‚úÖ Automatische Format-Konvertierung\n","- ‚úÖ Klare Struktur ohne Prompt-Engineering\n","- ‚úÖ IDE-Unterst√ºtzung (Autocomplete, Type-Hints)"],"metadata":{"id":"9hnfp8g8bkj"}},{"cell_type":"markdown","id":"p8qe3tuhedi","source":["<p><font color='black' size=\"5\">\n","Einfaches Beispiel mit with_structured_output()\n","</font></p>"],"metadata":{"id":"p8qe3tuhedi"}},{"cell_type":"code","id":"a3xf7g08nw","source":["from pydantic import BaseModel, Field\n","\n","# 1. Pydantic-Modell definiert die gew√ºnschte Struktur\n","class PersonInfo(BaseModel):\n","    \"\"\"Informationen √ºber eine Person\"\"\"\n","    name: str = Field(description=\"Vollst√§ndiger Name der Person\")\n","    alter: int = Field(description=\"Alter der Person in Jahren\")\n","    beruf: str = Field(description=\"Aktueller Beruf der Person\")\n","\n","# 2. Modell mit strukturierter Ausgabe\n","llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", temperature=0)\n","structured_llm = llm.with_structured_output(PersonInfo)\n","\n","# 3. Einfache Anfrage - automatische Formatierung & Validierung\n","response = structured_llm.invoke(\n","    \"Erstelle eine fiktive Person: Name, Alter und Beruf\"\n",")\n","\n","# 4. Typsichere Ausgabe - response ist ein PersonInfo-Objekt\n","print(f\"Typ: {type(response)}\")\n","print(f\"Name: {response.name}\")\n","print(f\"Alter: {response.alter}\")\n","print(f\"Beruf: {response.beruf}\")"],"metadata":{"id":"a3xf7g08nw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764695968993,"user_tz":-60,"elapsed":749,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"1b6da5d0-03df-4364-ab15-5b32b6b97cd7"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["Typ: <class '__main__.PersonInfo'>\n","Name: Lena M√ºller\n","Alter: 28\n","Beruf: Grafikdesignerin\n"]}]},{"cell_type":"markdown","id":"ybps752g89","source":["**Vergleich: Prompt-basierte Formatierung vs. StrOutputParser vs. with_structured_output()**\n","\n","| Aspekt | Prompt-basiert | StrOutputParser | with_structured_output() |\n","|--------|----------------|----------------|---------------------------|\n","| **Definition** | Via System-Prompt: ‚ÄûFormatiere im JSON-Format...‚Äú | Expliziter Parser, der LLM-Ausgabe als String extrahiert oder leicht nachbearbeitet | Automatische Strukturierung per Pydantic-Modell (`class PersonInfo(BaseModel)`) |\n","| **Validierung** | ‚ùå Keine ‚Äì Modell kann beliebig antworten | ‚ö†Ô∏è Nur minimale (z. B. String-Cleaning) | ‚úÖ Strikte Schema-Validierung durch Pydantic |\n","| **Typsicherheit** | ‚ùå Nur Strings | ‚ùå Nur Strings | ‚úÖ Typsichere Python-Objekte |\n","| **Fehlerbehandlung** | ‚ö†Ô∏è Manuelles Parsing und Regex n√∂tig | ‚ö†Ô∏è Begrenzte Kontrolle (nur einfache Parsing-Fehler) | ‚úÖ Automatische Validierung, Exception bei Schemafehlern |\n","| **Code-Qualit√§t** | ‚ö†Ô∏è Fragil, stark Prompt-abh√§ngig | ‚úÖ Etwas klarer, aber keine echte Struktur | ‚úÖ Robust, wartbar, klar definierte Typen |\n","| **IDE-Support** | ‚ùå Keine Autovervollst√§ndigung | ‚ùå Keine | ‚úÖ Vollst√§ndig durch Type-Hints |\n","| **Anwendungsfall** | Einfache oder explorative Ausgaben | Wenn nur reine Textausgabe n√∂tig ist | F√ºr strukturierte, verl√§ssliche Daten (JSON, Objekte) |\n","| **Lernkurve** | ‚úÖ Sehr einfach | ‚úÖ Einfach | ‚ö†Ô∏è Erfordert Grundkenntnisse in Pydantic und LangChain-Schemas |\n","| **Beispiel** | ‚ÄûNenne Name und Alter als JSON.‚Äú | `StrOutputParser().parse(output)` | `llm.with_structured_output(PersonInfo)` |\n","\n"],"metadata":{"id":"ybps752g89"}},{"cell_type":"markdown","source":["\n","**Wann welchen Ansatz?**\n","\n","- **Prompt-basiert**: F√ºr einfache Texte, Zusammenfassungen, kreative Inhalte  \n","- **StrOutputParser**: Wenn nur reine Textausgabe oder einfache Formatkorrektur n√∂tig ist  \n","- **with_structured_output()**: F√ºr strukturierte Daten, APIs, Datenbanken, komplexe Workflows"],"metadata":{"id":"uncqM3AxDjh9"},"id":"uncqM3AxDjh9"},{"cell_type":"markdown","source":["# 9 | Runnables (opt.)\n","---\n"],"metadata":{"id":"KRqrnpMx3Hwf"},"id":"KRqrnpMx3Hwf"},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/langchain_prozess_01.png)"],"metadata":{"id":"LFOITVoEzpkm"},"id":"LFOITVoEzpkm"},{"cell_type":"markdown","source":["Ein **Runnable** ist ein elementarer Baustein in LangChain. Er nimmt eine Eingabe und liefert eine Ausgabe - wie eine Funktion mit klaren Regeln. Man kann sich Konzepte wie Prompts, LLMs oder Parser als Runnables vorstellen. Runnable kann man als **Oberbegriff** f√ºr Prompts, LLMs, Parser und √§hnliche Komponenten verwenden.\n","\n","\n","\n","In der Grafik sieht man die **drei wichtigsten Runnables**: Prompt-Template, LLM und Parser, die zusammen eine Kette bilden.\n","\n","In LangChain sind die wichtigsten Komponenten als Runnables verf√ºgbar:\n","\n","+ Prompts/Templates (der erste Baustein in der Kette)\n","+ LLMs (das Herzst√ºck in der Mitte)\n","+ Output-Parser (verwandelt die LLM-Antwort in ein nutzbares Format)\n","+ Komplette Chains (die ganze Verarbeitungskette)\n","+ Tools (f√ºr spezielle Aufgaben)\n","+ Retriever (holen zus√§tzliche Informationen)\n","\n","\n","Vorteile von Runnables:\n","\n","+ Gleiche Bedienung: Alle Runnables (Prompt, LLM, Parser) funktionieren nach demselben Prinzip\n","+ Einfaches Verbinden: Man kann sie unkompliziert zu einer Kette zusammenf√ºgen - genau wie im Bild gezeigt\n","+ Anpassungsf√§hig: Sie funktionieren sowohl einzeln als auch in der Gruppe\n","Schrittweise Ausgabe: Ergebnisse k√∂nnen st√ºckweise weitergegeben werden"],"metadata":{"id":"DnUDR_983M4l"},"id":"DnUDR_983M4l"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Benutzerdefinierte Runnables erstellen\n","</font></p>"],"metadata":{"id":"WWJ7U1iJ6b8Q"},"id":"WWJ7U1iJ6b8Q"},{"cell_type":"markdown","source":["Man kann auch eigene Runnables erstellen, indem man eine Klasse definiert, die das Runnable-Interface implementiert oder eine Funktion mit `RunnableLambda` umsetzt:\n"],"metadata":{"id":"PAvbE_TR6g87"},"id":"PAvbE_TR6g87"},{"cell_type":"code","source":["from langchain_core.runnables import RunnableLambda\n","from langchain_core.output_parsers.string import StrOutputParser\n","\n","# 1. Mit RunnableLambda eine einfache Transformation erstellen\n","def text_verdoppeln(eingabe):\n","    return f\"{eingabe} <br> <br>  {eingabe}\"\n","\n","verdoppler = RunnableLambda(text_verdoppeln)\n","\n","# Verwendung des eigenen Runnables\n","verdoppler.invoke(\"Dies ist ein Text.\")"],"metadata":{"id":"_0wEf_hL6peM","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1764695969025,"user_tz":-60,"elapsed":23,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"b704604a-4092-43a2-ffc4-85166a01a012"},"id":"_0wEf_hL6peM","execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Dies ist ein Text. <br> <br>  Dies ist ein Text.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":61}]},{"cell_type":"code","source":["# Einfacher Prompt ohne explizite Formatvorgabe\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent. Antworte kurz.\"),\n","    (\"human\", \"Eingabe: {user_input}\")\n","])\n","\n","# In eine Kette einbauen - Korrektur der Pipe-Operatoren\n","chain_mit_verdoppler = prompt | llm | parser | verdoppler\n","\n","# Test der Kette\n","response = chain_mit_verdoppler.invoke({\"user_input\": \"Wer war Einstein?\"})"],"metadata":{"id":"NvZdOf1961xb","executionInfo":{"status":"ok","timestamp":1764695970303,"user_tz":-60,"elapsed":1270,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"NvZdOf1961xb","execution_count":62,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response)"],"metadata":{"id":"_elA0imx2HZA","colab":{"base_uri":"https://localhost:8080/","height":169},"executionInfo":{"status":"ok","timestamp":1764695970362,"user_tz":-60,"elapsed":29,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"f8ebda8b-77ba-40f1-f18c-ca7e445ace17"},"id":"_elA0imx2HZA","execution_count":63,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Albert Einstein war ein theoretischer Physiker, der f√ºr seine Relativit√§tstheorie bekannt ist. Er gilt als einer der bedeutendsten Wissenschaftler des 20. Jahrhunderts. <br> <br>  Albert Einstein war ein theoretischer Physiker, der f√ºr seine Relativit√§tstheorie bekannt ist. Er gilt als einer der bedeutendsten Wissenschaftler des 20. Jahrhunderts."},"metadata":{}}]},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Runnable-Methoden\n","</font></p>\n","\n","Jedes Runnable unterst√ºtzt standardm√§√üig folgende Methoden:\n","\n","1. **invoke()**: F√ºr einzelne, synchrone Anfragen\n","2. **batch()**: Verarbeitet mehrere Eingaben parallel\n","3. **stream()**: Gibt Teilergebnisse zur√ºck, sobald sie verf√ºgbar sind\n","4. **ainvoke()**: Asynchrone Version von invoke\n","5. **abatch()**: Asynchrone Version von batch\n","6. **astream()**: Asynchrone Version von stream\n","\n"],"metadata":{"id":"CjFvHqWA3S2V"},"id":"CjFvHqWA3S2V"},{"cell_type":"markdown","source":["Hier ein einfaches Beispiel mit den verschiedenen Aufrufmethoden:\n"],"metadata":{"id":"a9zHZAMP3bCL"},"id":"a9zHZAMP3bCL"},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers.string import StrOutputParser\n","\n","# Einfaches Template erstellen\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"human\", \"{user_input}\")\n","])\n","\n","# Runnable-Kette erstellen\n","chain = prompt | llm | parser"],"metadata":{"id":"kQTKHK2D4DBb","executionInfo":{"status":"ok","timestamp":1764695970382,"user_tz":-60,"elapsed":10,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"kQTKHK2D4DBb","execution_count":64,"outputs":[]},{"cell_type":"code","source":["# 1. invoke - Einzelne Anfrage\n","response = chain.invoke({\"user_input\": \"Was ist ein Runnable in LangChain?\"})\n","\n","mprint(\"## üì£ Invoke response:\")\n","mprint(\"---\")\n","mprint(response)"],"metadata":{"id":"TVWtHSYw5Jla","colab":{"base_uri":"https://localhost:8080/","height":377},"executionInfo":{"status":"ok","timestamp":1764695976831,"user_tz":-60,"elapsed":6437,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"7204a39a-ea14-4747-cff6-d7eb0edc4536"},"id":"TVWtHSYw5Jla","execution_count":65,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Invoke response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"In LangChain bezieht sich der Begriff \"Runnable\" auf ein Konzept, das es erm√∂glicht, verschiedene Komponenten oder Funktionen in einer Kette von Verarbeitungsschritten zu organisieren und auszuf√ºhren. Ein Runnable ist im Grunde genommen ein Objekt oder eine Funktion, die eine bestimmte Aufgabe ausf√ºhren kann, und es kann in verschiedenen Kontexten verwendet werden, um die Modularit√§t und Wiederverwendbarkeit von Code zu f√∂rdern.\n\nHier sind einige wichtige Punkte zu Runnables in LangChain:\n\n1. **Modularit√§t**: Runnables erm√∂glichen es Entwicklern, verschiedene Teile ihrer Anwendung in separate, wiederverwendbare Einheiten zu unterteilen. Das macht den Code √ºbersichtlicher und einfacher zu warten.\n\n2. **Kombinierbarkeit**: Runnables k√∂nnen miteinander kombiniert werden, um komplexere Abl√§ufe zu erstellen. Man kann beispielsweise mehrere Runnables in einer Pipeline anordnen, um Daten zu verarbeiten oder Entscheidungen zu treffen.\n\n3. **Flexibilit√§t**: Da Runnables in verschiedenen Kontexten eingesetzt werden k√∂nnen, bieten sie eine hohe Flexibilit√§t. Man kann sie leicht anpassen oder erweitern, um neue Anforderungen zu erf√ºllen.\n\n4. **Einfache Ausf√ºhrung**: Runnables sind so konzipiert, dass sie einfach ausgef√ºhrt werden k√∂nnen, oft mit einer klaren Schnittstelle, die es erm√∂glicht, Eingaben zu √ºbergeben und Ausgaben zu erhalten.\n\nWenn du also in LangChain mit Runnables arbeitest, kannst du dir das wie eine Art \"Schweizer Taschenmesser\" f√ºr deine Programmieraufgaben vorstellen ‚Äì vielseitig, praktisch und immer bereit, dir bei der L√∂sung deiner Probleme zu helfen!"},"metadata":{}}]},{"cell_type":"code","source":["# 2. batch - Mehrere Anfragen parallel\n","response = chain.batch([\n","    {\"user_input\": \"Erkl√§re den Begriff LCEL kurz.\"},\n","    {\"user_input\": \"Was sind die Hauptvorteile von Runnables?\"}\n","])\n","mprint(\"##  üì£ Batch response::\")\n","mprint(\"---\")\n","for i, ergebnis in enumerate(response, 1):\n","    mprint(f\"Anfrage {i}: {response[:50]}...\\n\")"],"metadata":{"id":"UAet4KSq5K2D","colab":{"base_uri":"https://localhost:8080/","height":609},"executionInfo":{"status":"ok","timestamp":1764695987989,"user_tz":-60,"elapsed":11149,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"77511ab8-d94f-458f-b4c9-aa7664fbdc4d"},"id":"UAet4KSq5K2D","execution_count":66,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"##  üì£ Batch response::"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Anfrage 1: ['LCEL steht f√ºr \"Low Carbon Energy Lab\" und bezieht sich auf Einrichtungen oder Initiativen, die sich mit der Forschung und Entwicklung von Technologien zur Reduzierung von Kohlenstoffemissionen besch√§ftigen. Diese Labore konzentrieren sich oft auf erneuerbare Energien, Energieeffizienz und innovative L√∂sungen zur Bek√§mpfung des Klimawandels. Wenn du mehr Details oder spezifische Informationen ben√∂tigst, lass es mich wissen!', 'Runnables in Java sind eine praktische M√∂glichkeit, um Aufgaben in einem separaten Thread auszuf√ºhren. Hier sind einige der Hauptvorteile von Runnables:\\n\\n1. **Thread-Sicherheit**: Runnables erm√∂glichen es, Aufgaben in einem separaten Thread auszuf√ºhren, was die Hauptanwendung nicht blockiert. Das ist besonders n√ºtzlich f√ºr langwierige Aufgaben, die die Benutzeroberfl√§che nicht einfrieren lassen sollen.\\n\\n2. **Einfachheit**: Die Implementierung eines Runnables ist einfach. Man muss lediglich die `run()`-Methode √ºberschreiben und kann dann das Runnable an einen Thread √ºbergeben.\\n\\n3. **Flexibilit√§t**: Runnables k√∂nnen leicht an verschiedene Threads √ºbergeben werden, was die Wiederverwendbarkeit des Codes erh√∂ht. Man kann dasselbe Runnable in verschiedenen Kontexten verwenden.\\n\\n4. **Kombinierbarkeit**: Runnables k√∂nnen einfach kombiniert werden, um komplexere Aufgaben zu erstellen. Man kann mehrere Runnables in einem ExecutorService ausf√ºhren, um parallele Verarbeitung zu erm√∂glichen.\\n\\n5. **Keine R√ºckgabewerte**: Runnables sind ideal f√ºr Aufgaben, die keine R√ºckgabewerte ben√∂tigen. Wenn du einfach nur eine Aufgabe ausf√ºhren m√∂chtest, ohne ein Ergebnis zur√ºckzugeben, ist ein Runnable die richtige Wahl.\\n\\n6. **Leichtgewichtig**: Runnables sind leichtgewichtig im Vergleich zu anderen Ans√§tzen wie `Callable`, da sie keine R√ºckgabewerte oder Ausnahmen behandeln m√ºssen.\\n\\n7. **Integration mit Executor-Framework**: Runnables lassen sich gut mit dem Executor-Framework von Java kombinieren, was die Verwaltung von Threads und die Ausf√ºhrung von Aufgaben vereinfacht.\\n\\nUnd nicht zu vergessen: Wenn du ein Runnable erstellst, hast du die M√∂glichkeit, deine Kreativit√§t auszuleben ‚Äì vielleicht m√∂chtest du ja einen Thread erstellen, der die besten Witze erz√§hlt, w√§hrend er auf die n√§chste Aufgabe wartet! üòÑ']...\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Anfrage 2: ['LCEL steht f√ºr \"Low Carbon Energy Lab\" und bezieht sich auf Einrichtungen oder Initiativen, die sich mit der Forschung und Entwicklung von Technologien zur Reduzierung von Kohlenstoffemissionen besch√§ftigen. Diese Labore konzentrieren sich oft auf erneuerbare Energien, Energieeffizienz und innovative L√∂sungen zur Bek√§mpfung des Klimawandels. Wenn du mehr Details oder spezifische Informationen ben√∂tigst, lass es mich wissen!', 'Runnables in Java sind eine praktische M√∂glichkeit, um Aufgaben in einem separaten Thread auszuf√ºhren. Hier sind einige der Hauptvorteile von Runnables:\\n\\n1. **Thread-Sicherheit**: Runnables erm√∂glichen es, Aufgaben in einem separaten Thread auszuf√ºhren, was die Hauptanwendung nicht blockiert. Das ist besonders n√ºtzlich f√ºr langwierige Aufgaben, die die Benutzeroberfl√§che nicht einfrieren lassen sollen.\\n\\n2. **Einfachheit**: Die Implementierung eines Runnables ist einfach. Man muss lediglich die `run()`-Methode √ºberschreiben und kann dann das Runnable an einen Thread √ºbergeben.\\n\\n3. **Flexibilit√§t**: Runnables k√∂nnen leicht an verschiedene Threads √ºbergeben werden, was die Wiederverwendbarkeit des Codes erh√∂ht. Man kann dasselbe Runnable in verschiedenen Kontexten verwenden.\\n\\n4. **Kombinierbarkeit**: Runnables k√∂nnen einfach kombiniert werden, um komplexere Aufgaben zu erstellen. Man kann mehrere Runnables in einem ExecutorService ausf√ºhren, um parallele Verarbeitung zu erm√∂glichen.\\n\\n5. **Keine R√ºckgabewerte**: Runnables sind ideal f√ºr Aufgaben, die keine R√ºckgabewerte ben√∂tigen. Wenn du einfach nur eine Aufgabe ausf√ºhren m√∂chtest, ohne ein Ergebnis zur√ºckzugeben, ist ein Runnable die richtige Wahl.\\n\\n6. **Leichtgewichtig**: Runnables sind leichtgewichtig im Vergleich zu anderen Ans√§tzen wie `Callable`, da sie keine R√ºckgabewerte oder Ausnahmen behandeln m√ºssen.\\n\\n7. **Integration mit Executor-Framework**: Runnables lassen sich gut mit dem Executor-Framework von Java kombinieren, was die Verwaltung von Threads und die Ausf√ºhrung von Aufgaben vereinfacht.\\n\\nUnd nicht zu vergessen: Wenn du ein Runnable erstellst, hast du die M√∂glichkeit, deine Kreativit√§t auszuleben ‚Äì vielleicht m√∂chtest du ja einen Thread erstellen, der die besten Witze erz√§hlt, w√§hrend er auf die n√§chste Aufgabe wartet! üòÑ']...\n"},"metadata":{}}]},{"cell_type":"code","source":["# 3. stream - Schrittweise Ausgabe\n","mprint(\"##  üì£ Stream response:\")\n","mprint(\"---\")\n","for chunk in chain.stream({\"user_input\": \"Erkl√§re die Abk√ºrzung LCEL kurz.\"}):\n","    print(chunk, end=\" ‚û° \")  # Zeichen trennt die via stream gelieferten Ergebnisse/Ausgaben\n","print(\"\\n\")"],"metadata":{"id":"VJn4e_IX5L-b","colab":{"base_uri":"https://localhost:8080/","height":131},"executionInfo":{"status":"ok","timestamp":1764695989777,"user_tz":-60,"elapsed":1800,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"47e06e13-6b5a-469e-ca6d-b4928893183c"},"id":"VJn4e_IX5L-b","execution_count":67,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"##  üì£ Stream response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" ‚û° Die ‚û°  Ab ‚û° k ‚û° √ºr ‚û° zung ‚û°  LC ‚û° EL ‚û°  steht ‚û°  f√ºr ‚û°  \" ‚û° Low ‚û°  Carbon ‚û°  Energy ‚û°  Lab ‚û° \". ‚û°  Es ‚û°  handelt ‚û°  sich ‚û°  dabei ‚û°  um ‚û°  Einrichtungen ‚û°  oder ‚û°  Forschungs ‚û° zent ‚û° ren ‚û° , ‚û°  die ‚û°  sich ‚û°  mit ‚û°  der ‚û°  Entwicklung ‚û°  und ‚û°  F√∂rderung ‚û°  von ‚û°  Technologien ‚û°  und ‚û°  Strateg ‚û° ien ‚û°  zur ‚û°  Red ‚û° uz ‚û° ierung ‚û°  von ‚û°  Koh ‚û° len ‚û° stoff ‚û° em ‚û° ission ‚û° en ‚û°  im ‚û°  Ener ‚û° gies ‚û° ektor ‚û°  besch√§ftigen ‚û° . ‚û°  Wenn ‚û°  du ‚û°  mehr ‚û°  Details ‚û°  oder ‚û°  spezif ‚û° ische ‚û°  Informationen ‚û°  zu ‚û°  einem ‚û°  bestimmten ‚û°  LC ‚û° EL ‚û°  ben√∂t ‚û° igst ‚û° , ‚û°  lass ‚û°  es ‚û°  mich ‚û°  wissen ‚û° ! ‚û°  ‚û°  ‚û°  ‚û° \n","\n"]}]},{"cell_type":"markdown","id":"F_5gn03b46rC","metadata":{"id":"F_5gn03b46rC"},"source":["# A | Aufgabe\n","---"]},{"cell_type":"markdown","id":"36LyGMmo5jb3","metadata":{"id":"36LyGMmo5jb3"},"source":["Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen."]},{"cell_type":"markdown","id":"Je-szbGU7IJ-","metadata":{"id":"Je-szbGU7IJ-"},"source":["\n","<p><font color='black' size=\"5\">\n","Erstellen einer LangChain-Kette zur Textanalyse\n","</font></p>"]},{"cell_type":"markdown","id":"kevePvA-7PzW","metadata":{"id":"kevePvA-7PzW"},"source":["Entwickeln Sie eine Verarbeitungskette, die einen Text analysiert und verschiedene Informationen dar√ºber extrahiert.\n","\n","**Schritte:**\n","1. Erstelle ein ChatPromptTemplate, das ein LLM anweist, einen Text zu analysieren\n","2. Die Analyse soll folgende Aspekte umfassen:\n","   - Hauptthema des Textes\n","   - Tonalit√§t (formal, informell, etc.)\n","   - Schl√ºsselw√∂rter (5-10)\n","   - Kurze Zusammenfassung (max. 3 S√§tze)\n","3. Formatiere die Ausgabe strukturiert mit Markdown-√úberschriften\n","4. Teste die Kette mit mindestens zwei verschiedenen Texten\n","\n","\n"]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["91ee89a4-477b-41ce-a511-aadd4c2fd547","fGV8FwGmIwmR","EouwC_wdZb-c","HqTUeMW20084","c314ca71","LVqVL9CWRXU7","89XLSUAvCE8P","5efri5ponin","KRqrnpMx3Hwf","F_5gn03b46rC"],"toc_visible":true},"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":5}
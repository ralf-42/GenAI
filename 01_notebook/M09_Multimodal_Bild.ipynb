{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["PnODVpOdGWOS","h4lbK7_ljUbb","NS2tCPL_uKLY","Ii6TLuoCGsR-","b_qDQ5vxeXV7","jNHqE7q5eaqR","0-eSumHCmlFZ","D8I9UIkZ7yRP","NkrKt6cPGWOT"],"gpuType":"T4","machine_shape":"hm","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["<p><font size=\"6\" color='grey'> <b>\n","\n","Generative KI. Verstehen. Anwenden. Gestalten.\n","</b></font> </br></p>"],"metadata":{"id":"Ih2CTVBnArVZ"}},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b>\n","Multimodal - Bild\n","</b></font> </br></p>\n","\n","\n","---"],"metadata":{"id":"6jJZ7wbdArVc"}},{"cell_type":"code","source":["#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n","!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul\n","from genai_lib.utilities import (\n","    check_environment,\n","    get_ipinfo,\n","    setup_api_keys,\n","    mprint,\n","    install_packages,\n","    mermaid,\n","    get_model_profile,\n","    extract_thinking,\n","    load_chat_prompt_template\n",")\n","setup_api_keys(['OPENAI_API_KEY', 'HF_TOKEN'], create_globals=False)\n","print()\n","check_environment()\n","print()\n","get_ipinfo()"],"metadata":{"id":"Hf10l_FTn3aM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title üõ†Ô∏è Installationen { display-mode: \"form\" }\n","install_packages(['ultralytics', ])"],"metadata":{"id":"rOH-32M005pP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title üìÇ Dokumente, Bilder { display-mode: \"form\" }\n","!rm -rf files\n","!mkdir files\n","\n","# --- Bilder\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02_daten/02_bild/apfel.png -o files/apfel.png\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02_daten/02_bild/peoples.png -o files/peoples.png\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02_daten/02_bild/hedra_cyborg.png -o files/hedra_cyborg.png\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02_daten/02_bild/hedra_cyborg_mask_image.png -o files/hedra_cyborg_mask_image.png"],"metadata":{"id":"RpPqzbbJxxsF"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"helper_funcs"},"source":["#@title üõ†Ô∏è Hilfsfunktionen { display-mode: \"form\" }\n","import base64\n","import requests\n","from io import BytesIO\n","from PIL import Image as PILImage\n","from IPython.display import display, Image, Markdown\n","from openai import OpenAI\n","\n","client = OpenAI() # Zentraler Client\n","\n","def encode_image(image_path):\n","    \"\"\"Kodiert ein Bild in Base64.\"\"\"\n","    with open(image_path, \"rb\") as image_file:\n","        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n","\n","def show_img(img, width=512):\n","    \"\"\"Zeigt ein PIL-Bild skaliert an.\"\"\"\n","    ratio = width / img.width\n","    new_size = (width, int(img.height * ratio))\n","    display(img.resize(new_size))\n","\n","def analyze_image(image_path, prompt, model=\"gpt-4o-mini\", max_tokens=300):\n","    \"\"\"Analysiert ein Bild mit einem Vision-Modell.\"\"\"\n","    base64_image = encode_image(image_path)\n","    messages = [{\n","        \"role\": \"user\",\n","        \"content\": [\n","            {\"type\": \"text\", \"text\": prompt},\n","            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n","        ]\n","    }]\n","    response = client.chat.completions.create(\n","        model=model,\n","        messages=messages,\n","        max_tokens=max_tokens,\n","    )\n","    return response.choices[0].message.content"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","‚è∏Ô∏è 5-Minuten-Check:\n","</font></p>\n","\n","**Ziel:** Pr√ºfen, ob du das vorherige Kapitel verstanden hast ‚Äì nicht, ob es gerade l√§uft.\n","\n","**Aufgabe** (5 Minuten, ohne Vorlage):\n","\n","Rekonstruiere die zentrale Idee oder Code-Struktur des letzten Abschnitts selbstst√§ndig\n","(kein Copy & Paste, kein Nachschlagen).\n","\n","W√§hle eine der folgenden Optionen:\n","\n","+ Erkl√§re in 1‚Äì2 S√§tzen, was hier konzeptionell passiert.\n","\n","+ Ver√§ndere eine Kleinigkeit (z. B. Prompt, Parameter, Reihenfolge) und beschreibe die Auswirkung.\n","\n","+ Markiere eine Stelle, die du nicht sicher erkl√§ren kannst, und formuliere eine konkrete Frage dazu.\n","\n","**Hinweis:**\n","Nicht alles muss ‚Äûfertig‚Äú oder ‚Äûkorrekt‚Äú sein. Entscheidend ist, wo dein Verst√§ndnis gerade endet"],"metadata":{"id":"fYcfw5Kua6vt"}},{"cell_type":"markdown","metadata":{"id":"PnODVpOdGWOS"},"source":["\n","\n","# 1 | Intro\n","---"]},{"cell_type":"markdown","source":["Dieses Modul besch√§ftigt sich mit der Analyse von Bildern und ihrer Rolle als Basis f√ºr multimodale Systeme. Solche Modelle stellen einen bedeutenden Fortschritt in der KI dar, da sie geschriebene Sprache in visuelle Darstellungen umwandeln. Bekannte Beispiele wie DALL¬∑E, Midjourney und Stable Diffusion nutzen Deep-Learning-Techniken, um detaillierte Bilder auf Grundlage von Textbeschreibungen zu erzeugen. Dabei erfassen sie die Zusammenh√§nge zwischen sprachlichen und visuellen Elementen, wodurch sie in der Lage sind, Szenen, Objekte und Konzepte entsprechend der gegebenen Beschreibung zu generieren.  \n","\n","Multimodale Modelle gehen √ºber diese Funktionalit√§t hinaus, indem sie mehrere Datenquellen kombinieren ‚Äì in der Regel Text,  Bilder, Audio oder Video. Ihr Ziel ist es, verschiedene Informationsarten simultan zu verarbeiten und in Beziehung zu setzen. So k√∂nnen sie beispielsweise Bildbeschreibungen aus Text erstellen, Fragen zu visuellen Inhalten beantworten oder sogar Videos auf Basis von Texteingaben generieren. Durch die Verbindung der F√§higkeiten von Text- und Bildmodellen erweitern multimodale Systeme das Anwendungsspektrum der KI und erm√∂glichen eine flexiblere, kontextbezogene Interaktion.  \n","\n","W√§hrend **Text-zu-Bild-Modelle** prim√§r darauf ausgerichtet sind, sprachliche Eingaben in visuelle Inhalte umzusetzen, erm√∂glichen **multimodale Modelle** eine komplexere Verkn√ºpfung verschiedener Medientypen. In diesem Modul wird untersucht, wie diese Modelle funktionieren und wie ihre Architektur die Entwicklung weiterf√ºhrender multimodaler Systeme beeinflusst. Zudem werden praxisnahe Anwendungsf√§lle betrachtet sowie m√∂gliche zuk√ºnftige Entwicklungen aufgezeigt, die sich aus der Kombination unterschiedlicher Datenmodalit√§ten ergeben."],"metadata":{"id":"eazq6nuElGNS"}},{"cell_type":"markdown","source":["OpenAI bietet mit DALL¬∑E und GPT Image 1 zwei fortschrittliche KI-Modelle zur Bildgenerierung an. Beide Modelle wandeln Texteingaben in Bilder um, unterscheiden sich jedoch in ihren Funktionen und Einsatzm√∂glichkeiten.\n","\n"],"metadata":{"id":"ZUFRWWopmOpQ"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","DALL¬∑E\n","</font></p>\n","\n","- **Funktion:** Generiert Bilder aus Textbeschreibungen.\n","- **Stilvielfalt:** Von realistisch bis abstrakt, viele Motive und Stile m√∂glich.\n","- **API-Zugriff:** √úber die OpenAI-API leicht in Webanwendungen, Apps und Plattformen integrierbar.\n","- **Anwendungsbereiche:** Design, Marketing, Unterhaltung, Bildung.\n","- **Vorteil:** Schnelle und kreative Visualisierung von Ideen.\n"],"metadata":{"id":"su68WRiQHUMQ"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","GPT-Image-1\n","</font></p>\n","\n","- **Funktion:** Modernes, multimodales Bildmodell ‚Äì verarbeitet sowohl Text- als auch Bildeingaben.\n","- **Bildbearbeitung:** Unterst√ºtzt gezielte Bildbearbeitung (z.‚ÄØB. Inpainting, Kombination mehrerer Bilder).\n","- **Flexible Steuerung:** Nutzer k√∂nnen Parameter wie Bildgr√∂√üe, Qualit√§t und Hintergrundtyp anpassen.\n","- **Integration:** √úber API in verschiedene Plattformen (z.‚ÄØB. Figma, Adobe) einbindbar.\n","- **Sicherheit:** Moderation und Kennzeichnung der generierten Bilder durch C2PA-Metadaten.\n","- **Anwendungsbereiche:** Besonders geeignet f√ºr professionelle Kreativ- und Content-Workflows.\n","\n"],"metadata":{"id":"_NcaYKPcHboa"}},{"cell_type":"markdown","source":["**Einsatzspektrum GPT-Image-1/DALL¬∑E:**"],"metadata":{"id":"-bOo3Vubg1xS"}},{"cell_type":"markdown","source":["\n","\n","| Nr. | Aufgabe | Kurzbeschreibung | Beispiel | GPT-Image-1 | DALL¬∑E 3 |\n","|----|-----------|------------------|-----------|--------------|-----------|\n","| 1 | **Text-zu-Bild** | Textbeschreibung ‚Üí Bild; steuerbar nach Stil, Licht, Komposition. | *Fuchs mit Kamera im impressionistischen Stil* | ‚úÖ Pr√§zise & realistisch | ‚úÖ Kreativ & illustrativ |\n","| 2 | **Inpainting** | Objekte entfernen oder ersetzen per Maske. | *Person entfernen, Hintergrund auff√ºllen* | ‚úÖ Voll √ºber API | ‚ö†Ô∏è Nur in Chat-UI |\n","| 3 | **Outpainting** | Bild √ºber R√§nder erweitern. | *Panorama verbreitern* | ‚úÖ Pr√§zise steuerbar | ‚ö†Ô∏è Nur Chat-UI |\n","| 4 | **Bild-Variation** | Gleiche Szene, anderer Stil. | *Nachtaufnahme im Cyberpunk-Stil* | ‚úÖ Reproduzierbar | ‚ö†Ô∏è Nur √ºber Text-Prompt |\n","| 5 | **Stil-Transfer** | Stil√§nderung √ºber Prompt. | *√ñlgem√§lde- oder Comic-Look* | ‚úÖ Kontrolliert | ‚úÖ √Ñsthetisch, aber variabel |\n","| 6 | **Compositing** | Elemente kombinieren oder reparieren. | *Zwei Personen in ein Bild setzen* | ‚úÖ √úber Masken m√∂glich | ‚ö†Ô∏è Kein Multi-Input |\n","\n"],"metadata":{"id":"VZhlkuqhiVAy"}},{"cell_type":"markdown","source":["<p><font color='red' size=\"4\">\n","<b>Hinweis</b>\n","</font></p>\n","\n","Das Modell GPT-Image-1 kann nur von verifizierten Organisationen genutzt werden, weil OpenAI damit die Nutzung gezielt kontrollieren und Missbrauch verhindern m√∂chte. Als Hintergrund wird angegeben, GPT-Image-1 besonders realistische und detaillierte Bilder erzeugen kann. Damit steigt das Risiko, dass das Modell f√ºr problematische Zwecke wie Deepfakes, Falschinformationen oder unangemessene Inhalte eingesetzt wird."],"metadata":{"id":"7NK1nsJTLv54"}},{"cell_type":"markdown","source":["# 2 | √úbersicht Bildverarbeitung\n","---"],"metadata":{"id":"h4lbK7_ljUbb"}},{"cell_type":"markdown","source":["| **Kategorie**                        | **Aufgabe**                      | **Beschreibung**                                                                      |\n","| ------------------------------------ | -------------------------------- | ------------------------------------------------------------------------------------- |\n","| üß† **Analyse & Klassifikation**      | **Bildklassifikation**           | Zuordnung eines Bildes zu vordefinierten Klassen (z.‚ÄØB. Katze, Hund, Auto).           |\n","|                                      | **Objekterkennung**                  | Erkennung und Lokalisierung mehrerer Objekte in einem Bild mit Bounding Boxes.        |\n","|                                      | Bildsegmentierung                | Pixelgenaue Zuordnung von Bildbereichen zu Klassen (z.‚ÄØB. Stra√üe, Baum, Mensch).      |\n","|                                      | Gesichtserkennung                | Identifikation oder Verifikation von Personen auf Bildern.                            |\n","|                                      | Emotionserkennung                | Analyse der Gesichtsausdr√ºcke zur Einsch√§tzung von Emotionen.                         |\n","|                                      | Anomalieerkennung                | Erkennung ungew√∂hnlicher oder fehlerhafter Bildinhalte (z.‚ÄØB. Produktionsfehler).     |\n","| üìù **Generierung & Transformation**  | **Bildgenerierung**              | Erzeugung synthetischer Bilder, z.‚ÄØB. aus Text (Text-to-Image) oder Rauschen.         |\n","|                                      | Bild-zu-Bild-√úbersetzung         | Umwandlung von Bildern (z.‚ÄØB. Skizze ‚Üí Foto, Tag ‚Üí Nacht).                            |\n","|                                      | Stiltransfer                     | √úbertragung des Stils eines Bildes auf ein anderes (z.‚ÄØB. Van Gogh ‚Üí Foto).           |\n","|                                      | Super-Resolution                 | Hochskalierung von Bildern mit k√ºnstlicher Detailsch√§rfung.                           |\n","|                                      | Bildrestauration                 | Entfernung von Rauschen, Kratzern oder Artefakten in alten oder besch√§digten Bildern. |\n","|                                      | Colorisierung                    | Umf√§rben von Schwarzwei√ü-Bildern mit realistischen Farben.                            |\n","| üß© **Erg√§nzung & Vervollst√§ndigung** | **Inpainting**                       | Auff√ºllen fehlender oder besch√§digter Bildbereiche.                                   |\n","|                                      | **Bildvervollst√§ndigung**        | Vorhersage fehlender Bildbereiche basierend auf Kontext.                              |\n","|                                      | Hintergrundentfernung            | Trennung des Vordergrundobjekts vom Hintergrund.                                      |\n","| üìö **Informationsgewinnung**         | Texterkennung (OCR)              | Erkennung und Extraktion von Text aus Bildern.                                        |\n","|                                      | Schrifterkennung                 | Identifikation von Schriftarten oder Handschriften.                                   |\n","|                                      | **Bildbeschreibung (Captioning)**    | Automatische Erzeugung eines beschreibenden Textes zu einem Bild.                     |\n","|                                      | visuelle Fragebeantwortung (VQA) | Beantwortung von Fragen basierend auf einem Bildinhalt.                               |\n","| üõ°Ô∏è **Sicherheit & Strukturierung**  | Deepfake-Erkennung               | Analyse, ob ein Bild manipuliert oder synthetisch erzeugt wurde.                      |\n","|                                      | Gesichtsverpixelung              | Anonymisierung durch Unkenntlichmachung von Gesichtern.                               |\n","|                                      | Wasserzeichen-Erkennung          | Detektion sichtbarer oder unsichtbarer Wasserzeichen.                                 |\n","|                                      | Formatkonvertierung              | Umwandlung zwischen Bildformaten (z.‚ÄØB. PNG ‚Üî JPG).                                   |\n"],"metadata":{"id":"NBDZGdImjSfc"}},{"cell_type":"markdown","metadata":{"id":"NS2tCPL_uKLY"},"source":["\n","# 3 | Bildgenerierung\n","---"]},{"cell_type":"markdown","source":["Die Bildgenerierung mit K√ºnstlicher Intelligenz (KI) erm√∂glicht es, aus einfachen Texteingaben oder bestehenden Bildern v√∂llig neue, realistische oder kreative Grafiken zu erstellen. Moderne KI-Modelle wie DALL¬∑E 3 und GPT Image von OpenAI setzen dabei auf fortschrittliche Algorithmen, die Texte pr√§zise interpretieren und in beeindruckende visuelle Darstellungen umsetzen. Diese Technologie findet Anwendung in vielen Bereichen ‚Äì von Design und Marketing √ºber Kunst bis hin zur Bildung ‚Äì und er√∂ffnet neue kreative M√∂glichkeiten f√ºr Einsteiger und Profis gleicherma√üen.\n"],"metadata":{"id":"WF14JiXeInfI"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Prompt-Beispiele\n","</font></p>"],"metadata":{"id":"KwpLS-kkUMrU"}},{"cell_type":"markdown","source":["Prompts aus: [Vers√§umte Bilder](https://bilderinstitut.de/versaeumte-bilder-bonn)\n","\n","**Foto Elvira F√∂lzer** standing proud as a female archaeologist, looking directly in the camera, as a 70 year old, on an bexcavation side with antic shards and ruins, photography from 1920, leica style, film gain --s 250 --v 6.1\n","\n","\n","**Foto Maria von Linde**n standing proud with crossed arms, as a 70 year old female scientist, dressed like a man, with slicked back hair, looking like a man, wearing a lab cout, standing in a lecture hall in Bonn in front of students, teaching chemistry, leica style from 1920 --v 6.0 --s 250\n","\n","\n","**Foto Leah Goldberg** as a 30 year old female scientist, sitting proud behind a desk with books, writing, illuminated by a small lamp, leica style from 1933, film gain --s 250 --v 6.1"],"metadata":{"id":"3VyqDwDuh5Sj"}},{"cell_type":"markdown","source":["... oder einfach: Erstelle eine Bild von einem Prosche 911 auf einer K√ºstenstrasse. Das Bild sollte foto-realistisch und schwarz-weiss sein."],"metadata":{"id":"KGhdN4qpikVM"}},{"cell_type":"markdown","source":["**‚ö†Ô∏è Wichtig: Import-Namenskonflikte vermeiden**\n","\n","<details>\n","\n","In diesem Notebook werden zwei Module verwendet, die beide eine `Image`-Funktion exportieren:\n","\n","1. **`PIL.Image`** - Zum √ñffnen, Bearbeiten und Speichern von Bildern\n","2. **`IPython.display.Image`** - Zum Anzeigen von Bildern in Jupyter\n","\n","**Problem ohne Aliasing:**\n","```python\n","from PIL import Image          # Image = PIL.Image\n","from IPython.display import Image  # ‚ùå √úberschreibt PIL.Image!\n","```\n","\n","**‚úÖ L√∂sung: Aliasing verwenden**\n","\n","Wir verwenden in diesem Notebook folgende Konvention:\n","```python\n","from PIL import Image as PILImage              # Bildmanipulation\n","from IPython.display import Image as IPImage   # Anzeige in Jupyter\n","```\n","\n","**Alternative L√∂sungen:**\n","```python\n","# Option 2: Modulimport statt direkter Import\n","import PIL.Image\n","from IPython import display\n","# Nutzung: PIL.Image.open() und display.Image()\n","\n","# Option 3: Nur PIL importieren, display() f√ºr Anzeige\n","from PIL import Image\n","from IPython.display import display\n","# Nutzung: Image.open() und display(image)\n","```"],"metadata":{"id":"bNTa1985VX7s"}},{"cell_type":"code","source":["# 1. Prompt\n","prompt = \"Foto Elvira F√∂lzer standing proud as a female archaeologist, looking directly in the camera, as a 70 year old, on an excavation side with antic shards and ruins, photography from 1920, leica style, film gain --s 250 --v 6.1\"\n","\n","# 2. Model\n","response = client.images.generate(model=\"dall-e-3\", prompt=prompt, size=\"1024x1024\", quality=\"hd\", n=1)\n","img = PILImage.open(BytesIO(requests.get(response.data[0].url).content))\n","\n","# Bild speichern & anzeigen\n","img.save(\"bild_elvira.png\")\n","show_img(img)"],"metadata":{"id":"yJtGp01SIsJU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"Portrait of Elvira F√∂lzer, a 70-year-old female archaeologist standing proudly on an excavation site with ancient shards and ruins, photographed in 1920s Leica style, film grain, high detail.\"\n","\n","response = client.images.generate(model=\"gpt-image-1\", prompt=prompt, size=\"1024x1024\", quality=\"high\", n=1)\n","img_data = base64.b64decode(response.data[0].b64_json)\n","img = PILImage.open(BytesIO(img_data))\n","\n","img.save(\"bild_gpt_image_1.png\")\n","show_img(img)"],"metadata":{"id":"fBwkSjJFyOV2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Erl√§uterung:**\n","\n","`BytesIO` ist ein datei√§hnliches Objekt, das Daten im Speicher statt auf der Festplatte h√§lt. Es verh√§lt sich wie eine Datei, aber alles passiert im Arbeitsspeicher."],"metadata":{"id":"w3G-unbwJp_t"}},{"cell_type":"markdown","metadata":{"id":"Ii6TLuoCGsR-"},"source":["\n","# 4 | Bildklassifizierung\n","---"]},{"cell_type":"markdown","source":["Die Bildklassifizierung ist eine grundlegende Aufgabe in der Computer Vision, bei der einem gesamten Bild eine einzige Kategorie (Klasse) zugewiesen wird.\n","\n","**Wie funktioniert Bildklassifizierung?**\n","+ Eingabe: Ein einzelnes Bild wird als Eingabe verwendet.\n","+ Merkmalsextraktion: Ein neuronales Netzwerk analysiert das Bild und extrahiert relevante Merkmale (z. B. Kanten, Farben, Formen, Muster).\n","+ Klassifizierung: Das Modell ordnet das Bild einer vordefinierten Kategorie zu, z. B. \"Hund\", \"Katze\" oder \"Auto\".\n","+ Ausgabe: Eine einzige Klasse (Label) mit einer Wahrscheinlichkeitsbewertung wird zur√ºckgegeben.\n","\n","\n","Beispiel f√ºr Bildklassifizierung\n","Stelle dir vor, du hast ein Bild von einem Hund. Ein Bildklassifizierungsmodell verarbeitet das Bild und gibt die Kategorie \"Hund\" mit einer bestimmten Wahrscheinlichkeit (z. B. 95 %) zur√ºck.\n","\n","+ Eingabebild: Ein Bild eines Hundes\n","+ Modell-Ausgabe: \"Hund\" (95%)\n","\n","Falls das Bild eine Katze zeigt, gibt das Modell m√∂glicherweise \"Katze\" (90%) als Ergebnis zur√ºck.\n","\n","**Einschr√§nkungen der Bildklassifizierung**     \n","Das Modell kann nur eine Klasse pro Bild erkennen, auch wenn mehrere Objekte im Bild vorhanden sind.\n","Es gibt keine Information √ºber die Position oder Anzahl der Objekte im Bild.\n","Anwendungsf√§lle f√ºr Bildklassifizierung\n","+ Erkennung von medizinischen Anomalien (z. B. Klassifikation von R√∂ntgenbildern)\n","+ Identifikation von Pflanzen oder Tieren anhand von Bildern\n","+ Sentiment-Analyse anhand von Gesichtsmerkmalen\n","\n","**Bekannte Modelle f√ºr Bildklassifizierung**    \n","+ CNNs (Convolutional Neural Networks) wie ResNet, VGG, EfficientNet\n","+ Pretrained Modelle: MobileNet, Inception, AlexNet"],"metadata":{"id":"mq1NAdlem-lu"}},{"cell_type":"markdown","source":["Das Modell **google/vit-base-patch16-224** ist ein Vision Transformer (ViT) Modell, das von Google entwickelt wurde und sich auf Bildklassifizierungsaufgaben spezialisiert hat. Es geh√∂rt zur Familie der Transformer-Modelle, die urspr√ºnglich f√ºr die Verarbeitung nat√ºrlicher Sprache (NLP) konzipiert wurden, aber hier auf visuelle Daten angewendet werden."],"metadata":{"id":"t3p7eCzP38SF"}},{"cell_type":"code","source":["from transformers import pipeline\n","from IPython.display import Image, display\n","\n","classifier = pipeline(\"image-classification\", model=\"google/vit-base-patch16-224\")\n","\n","image_path = \"/content/files/apfel.png\"\n","\n","display(Image(image_path, width=500))\n","\n","# Klassifizierung\n","classifier(image_path)"],"metadata":{"id":"IceolLZw3vvt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","\n","Man kann allerdings auch gpt-4o-mini mit einem entsprechenden Prompt hierzu einsetzen."],"metadata":{"id":"6BU8Ui1M3uhR"}},{"cell_type":"code","source":["image_path = \"/content/files/apfel.png\"\n","prompt = \"Was ist auf diesem Bild abgebildet? Nennen nur das wesentliche Objekt.\"\n","\n","result = analyze_image(image_path, prompt)\n","print(result)\n","\n","# Bild anzeigen\n","display(Image(image_path, width=500))"],"metadata":{"id":"ZLsOlRByQk4Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Hinweis zu Base64-Kodierung:**\n"],"metadata":{"id":"B5XSxRoO3IoU"}},{"cell_type":"markdown","source":["Wann braucht man eine Base64-Kodierung?\n","\n","‚úÖ Ja - wenn bin√§re Daten (z. B. Bilder) in einem textbasierten Format √ºbertragen werden m√ºssen\n","    ‚Üí z. B. JSON, Prompt, REST-Payload ohne Multipart\n","\n","‚ùå Nein - wenn\n","\n","+ eine √∂ffentlich erreichbare URL existiert\n","\n","+ ein bin√§rer Upload (Multipart/Form-Data) unterst√ºtzt wird\n"],"metadata":{"id":"BlUTLwDb3gTB"}},{"cell_type":"markdown","source":["\n","\n","Fazit:\n","\n","* üìÅ **Lokale Datei** ‚Üí **Base64 + `data:`-URL**\n","* üåê **Online-URL** ‚Üí **keine Base64 n√∂tig**\n","\n"],"metadata":{"id":"wjMfa7cT3rQh"}},{"cell_type":"markdown","source":["# 5 | Inpainting\n","---"],"metadata":{"id":"b_qDQ5vxeXV7"}},{"cell_type":"markdown","source":["Inpainting ist eine Technik im Bereich der k√ºnstlichen Intelligenz und Bildverarbeitung, bei der bestimmte Teile eines Bildes automatisch erg√§nzt oder repariert werden. Die Technik wird eingesetzt, um:\n","\n","+ Besch√§digte Bereiche in Bildern zu rekonstruieren\n","+ Unerw√ºnschte Objekte aus Bildern zu entfernen\n","+ Fehlende Teile in Bildern zu erg√§nzen\n","\n","**Inpainting mit Maske:**   \n","Bei Inpainting mit Masken geht es darum, genau zu definieren, welche Bereiche eines Bildes rekonstruiert werden sollen. Die Maske ist dabei ein Schl√ºsselelement, das dem Algorithmus mitteilt, wo er arbeiten soll. Eine Maske ist ein Bin√§rbild (Schwarz-Wei√ü-Bild), das die gleichen Dimensionen wie das Originalbild hat:\n","\n","+ Wei√üe Bereiche (255): Zeigen an, welche Teile des Bildes rekonstruiert/gef√ºllt werden sollen\n","+ Schwarze Bereiche (0): Zeigen an, welche Teile des Originalbildes erhalten bleiben sollen"],"metadata":{"id":"vf_OjFoae1CJ"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Modell/API-√úbersicht\n","</font></p>"],"metadata":{"id":"a8zZGTRviJ7I"}},{"cell_type":"markdown","source":["| Modell / Anbieter         | API-Zugang m√∂glich? | Hinweise |\n","| :------------------------| :------------------ | :------- |\n","| **Picsart Outpainting**   | Ja                 | Outpainting-Service per API, gezieltes Erweitern oder Ersetzen von Bildbereichen. Auch Inpainting-Funktion verf√ºgbar. |\n","| **IOPaint**               | Nein (Web-App & Open Source) | Open-Source-Tool f√ºr Inpainting/Outpainting, lokal oder selbst gehostet, keine √∂ffentliche API dokumentiert. Unterst√ºtzt verschiedene KI-Modelle, WebUI verf√ºgbar. |\n","| **Recraft**               | Nein (Web-App)     | KI-Inpainting und Outpainting mit hoher Pr√§zision, Fokus auf kreatives Editing, keine √∂ffentliche API dokumentiert. |\n","| **Flux Tools (Black Forest Lab)** | Nein (ComfyUI-Workflow) | Inpainting & Outpainting im ComfyUI-Workflow, leistungsstarke ‚ÄûFill‚Äú-Modelle, keine eigenst√§ndige API, Integration √ºber ComfyUI m√∂glich. |\n","| **Amazon Bedrock (SDXL, Titan Image Generator)** | Ja | Text-gef√ºhrtes Inpainting & Outpainting per API, unterst√ºtzt Stable Diffusion XL und Titan Image Generator, flexible Integration, keine Infrastrukturverwaltung n√∂tig. |\n","| **Google Vertex AI (Imagen 2)** | Ja | Inpainting (Objekte einf√ºgen/entfernen) per API, Outpainting-Funktion f√ºr Bildbereichserweiterung, Integration in Vertex AI Generative AI Studio. |\n","| **OpenAI DALL¬∑E**         | Ja                 | Inpainting (‚ÄûEdit‚Äú-Funktion) und Outpainting per API und Web-App, nat√ºrliche Sprachsteuerung, nahtlose Bildbearbeitung und Erweiterung. |\n"],"metadata":{"id":"I0R8YWm4iFAY"}},{"cell_type":"code","source":["prompt = \"Ersetze den Kopf durch den Kopf eines alten Mannes. Behalte Licht, Schatten und Stil des Originals.\"\n","image_path = \"/content/files/hedra_cyborg.png\"\n","mask_path = \"/content/files/hedra_cyborg_mask_image.png\"\n","\n","with open(image_path, \"rb\") as img_f, open(mask_path, \"rb\") as mask_f:\n","    response = client.images.edit(\n","        model=\"gpt-image-1\",\n","        image=img_f,\n","        mask=mask_f,\n","        prompt=prompt,\n","        size=\"1024x1024\",\n","        output_format=\"png\"\n","    )\n","\n","img_data = base64.b64decode(response.data[0].b64_json)\n","img = PILImage.open(BytesIO(img_data))\n","show_img(img)"],"metadata":{"id":"uR8Xd1pFgM6g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6 | Outpainting\n","---"],"metadata":{"id":"jNHqE7q5eaqR"}},{"cell_type":"markdown","source":["Outpainting ist eine Technik der Bildbearbeitung und KI-gest√ºtzten Bildgenerierung, bei der ein vorhandenes Bild √ºber seine urspr√ºnglichen Grenzen hinaus erweitert wird. Dabei werden neue Bildinhalte an den R√§ndern erg√§nzt, die stilistisch und thematisch zum Original passen. Outpainting wird h√§ufig verwendet, um Bilder zu vergr√∂√üern, Hintergr√ºnde zu erweitern oder kreative Kompositionen zu schaffen. Moderne Methoden basieren oft auf neuronalen Netzwerken wie Stable Diffusion, die mithilfe von Textbeschreibungen (Prompts) realistische und koh√§rente Bildbereiche hinzuf√ºgen k√∂nnen. So entsteht ein nahtlos erweitertes Bild, das √ºber das urspr√ºngliche Motiv hinausgeht."],"metadata":{"id":"gbnzT6MWx03f"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Modell/API-√úbersicht\n","</font></p>"],"metadata":{"id":"EtpCtz98iaYo"}},{"cell_type":"code","source":["from PIL import Image as PILImage, ImageDraw  # Aliasing: Vermeidet Konflikt\n","from io import BytesIO\n","import base64\n","from IPython.display import display\n","from IPython.display import Image as IPImage\n","\n","# client = OpenAI()\n","\n","\n","# 1) Ausgangsbild laden\n","orig = PILImage.open(\"/content/files/hedra_cyborg.png\").convert(\"RGBA\")\n","\n","# Zielgr√∂√üe f√ºr Outpainting (Querformat, √§hnlich 16:9)\n","api_size = (1536, 1024)\n","\n","# 2) Neues Canvas im Zielverh√§ltnis erzeugen\n","canvas = PILImage.new(\"RGBA\", api_size, (0, 0, 0, 0))\n","\n","# Original zentriert einf√ºgen\n","x_offset = (api_size[0] - orig.width) // 2\n","y_offset = (api_size[1] - orig.height) // 2\n","canvas.paste(orig, (x_offset, y_offset))\n","canvas.save(\"canvas.png\")\n","\n","# 3) Maske erzeugen (wei√ü = behalten, transparent = erg√§nzen)\n","mask = PILImage.new(\"RGBA\", api_size, (0, 0, 0, 0))\n","draw = ImageDraw.Draw(mask)\n","draw.rectangle(\n","    [x_offset, y_offset, x_offset + orig.width, y_offset + orig.height],\n","    fill=(255, 255, 255, 255)\n",")\n","mask.save(\"mask.png\")\n","\n","# 4) API-Aufruf\n","prompt = (\n","    \"Erweitere das Bild nach au√üen. \"\n","    \"F√ºge harmonisch passende Umgebung, Licht und Texturen hinzu, \"\n","    \"sodass der Stil und die Atmosph√§re des Originals erhalten bleiben.\"\n","    \"Behalte Licht, Schatten und Stil des Originals.\"\n",")\n","\n","with open(\"canvas.png\", \"rb\") as img, open(\"mask.png\", \"rb\") as m:\n","    response = client.images.edit(\n","        model=\"gpt-image-1\",\n","        image=img,\n","        mask=m,\n","        prompt=prompt,\n","        size=\"1536x1024\",  # gpt-image-1 hat feste Formate\n","        quality=\"high\",\n","        output_format=\"png\"\n","    )\n","\n","# Ausgangsbild\n","display(IPImage(\"/content/files/hedra_cyborg.png\", width=512))\n","\n","# 5) Ausgabe speichern & anzeigen\n","img_data = base64.b64decode(response.data[0].b64_json)\n","img = PILImage.open(BytesIO(img_data))\n","\n","filename = \"outpaint_1536x1024.png\"\n","img.save(filename)\n","print(f\"Gespeichert: {filename}\")\n","\n","# Vorschau (z. B. in Jupyter)\n","img_resized = img.resize((768, 512))\n","display(img_resized)"],"metadata":{"id":"ficIqIRxft7L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# 7 | Objekterkennung\n","---"],"metadata":{"id":"0-eSumHCmlFZ"}},{"cell_type":"markdown","source":["Die Objekterkennung geht einen Schritt weiter als die Bildklassifizierung. Hier wird nicht nur bestimmt, welche Objekte in einem Bild vorhanden sind, sondern auch wo sie sich befinden.\n","\n","**Wie funktioniert Objekterkennung?**   \n","+ Eingabe: Ein einzelnes Bild wird als Eingabe verwendet.\n","+ Merkmalsextraktion & Vorschl√§ge f√ºr Regionen: Das Modell analysiert das Bild und identifiziert potenzielle Bereiche, in denen sich Objekte befinden k√∂nnten.\n","+ Klassifizierung & Lokalisierung: Jedes erkannte Objekt wird einer bestimmten Klasse zugeordnet, und die Position wird durch eine Bounding Box (ein rechteckiger Bereich um das Objekt) beschrieben.\n","+ Ausgabe: Eine Liste mit allen erkannten Objekten, ihrer Klasse und ihrer Position im Bild wird zur√ºckgegeben.\n","\n","\n","**Beispiel f√ºr Objekterkennung**    \n","Stelle dir vor, du hast ein Bild mit einem Hund, einer Katze und einem Auto. Ein Objekterkennungsmodell kann alle drei Objekte gleichzeitig identifizieren und ihre Positionen angeben.\n","\n","+ Eingabebild: Ein Bild mit einem Hund, einer Katze und einem Auto\n","+ Modell-Ausgabe:\n",">Hund (95%) - Position: (x1, y1, x2, y2)   \n","Katze (90%) - Position: (x3, y3, x4, y4)   \n","Auto (99%) - Position: (x5, y5, x6, y6)   \n","\n","Hierbei sind (x1, y1, x2, y2) die Koordinaten der Bounding Box f√ºr den Hund, (x3, y3, x4, y4) f√ºr die Katze usw.\n","\n","**Einschr√§nkungen der Objekterkennung**   \n","Aufwendigere Berechnungen im Vergleich zur einfachen Bildklassifizierung.\n","Schwieriger zu trainieren, da genaue Positionen f√ºr Trainingsdaten erforderlich sind.\n","Anwendungsf√§lle f√ºr Objekterkennung\n","+ Autonomes Fahren (Erkennung von Fu√üg√§ngern, Verkehrsschildern)\n","+ √úberwachungssysteme (Erkennen von Eindringlingen oder gef√§hrlichen Objekten)\n","+ Analyse von Bildern im Einzelhandel (z. B. automatische Produktz√§hlung in Regalen)\n","\n","**Bekannte Modelle f√ºr Objekterkennung**   \n","+ YOLO (You Only Look Once) ‚Äì sehr schnell und effizient\n","+ Faster R-CNN ‚Äì pr√§zise, aber langsamer\n","+ SSD (Single Shot MultiBox Detector) ‚Äì guter Kompromiss zwischen Geschwindigkeit und Genauigkeit"],"metadata":{"id":"DLi09JRQ7VJE"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","YOLO\n","</font></p>"],"metadata":{"id":"l_m63Q1DyvMZ"}},{"cell_type":"markdown","source":["**YOLO (You Only Look Once)** ist ein leistungsf√§higer Algorithmus f√ºr die Objekterkennung in Bildern und Videos mithilfe k√ºnstlicher Intelligenz (KI). Es handelt sich um ein Modell, das auf Convolutional Neural Networks (CNN) basiert und sich durch seine Geschwindigkeit und Genauigkeit auszeichnet.\n","\n","**Funktionsweise**\n","\n","YOLO teilt ein Bild in ein Gitter auf und verarbeitet es in einem einzigen Durchlauf. Jede Gitterzelle ist f√ºr die Erkennung von Objekten in ihrem Bereich verantwortlich. Der Algorithmus sagt gleichzeitig Begrenzungsrahmen (Bounding Boxes) und Klassenwahrscheinlichkeiten f√ºr Objekte voraus.\n","\n","**Vorteile**\n","\n","1. **Geschwindigkeit**: YOLO kann Bilder in Echtzeit mit bis zu 45 Bildern pro Sekunde verarbeiten, was es ideal f√ºr Anwendungen wie Video√ºberwachung oder autonomes Fahren macht.\n","\n","2. **Genauigkeit**: Trotz seiner Geschwindigkeit erreicht YOLO eine hohe Erkennungsgenauigkeit und weist nur wenige Hintergrundfehler auf.\n","\n","3. **Verallgemeinerungsf√§higkeit**: Neuere Versionen von YOLO bieten eine verbesserte Leistung bei der Erkennung von Objekten in neuen Umgebungen.\n","\n","**Entwicklung**\n","\n","Seit seiner Einf√ºhrung im Jahr 2015 hat YOLO mehrere Iterationen durchlaufen, wobei jede Version Verbesserungen und Optimierungen mit sich brachte. Die neueste Version ist YOLO v7, die weitere Fortschritte in Bezug auf Geschwindigkeit und Genauigkeit bietet.\n","\n","YOLO hat die Objekterkennung revolutioniert und findet Anwendung in verschiedenen Bereichen wie autonomen Fahrzeugen, √úberwachungssystemen, Robotik und generativer KI."],"metadata":{"id":"8QV8uf3My0X5"}},{"cell_type":"markdown","source":["\n","\n","[Ultralytics | YOLO](https://www.ultralytics.com/de)"],"metadata":{"id":"x5eiwATzikyM"}},{"cell_type":"markdown","source":["Der Code verwendet die Ultralytics YOLOv8 Bibliothek, um Objekterkennung auf einem hochgeladenen Bild durchzuf√ºhren. Er l√§dt ein vortrainiertes YOLOv8m-Modell, erm√∂glicht das Hochladen eines Bildes √ºber Google Colab, f√ºhrt die Objekterkennung auf diesem Bild durch (mit einer Konfidenzschwelle von 0.2), speichert das Bild mit den Erkennungsergebnissen und die entsprechenden Textdateien und zeigt schlie√ülich das bearbeitete Bild in der Colab-Ausgabe an."],"metadata":{"id":"BWsp-C2v4iiJ"}},{"cell_type":"code","source":["# Import\n","import ultralytics\n","from ultralytics import YOLO\n","from google.colab import files\n","from IPython.display import display, Markdown\n","from IPython.display import Image as IPImage\n","\n","# YOLO-Modell laden\n","model = YOLO(\"yolov8m.pt\")"],"metadata":{"id":"P9ZQmS_OmzQB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os # Import the os module for path manipulation\n","\n","# Use the specified image file\n","image_path = \"/content/files/peoples.png\"\n","\n","# YOLO-Vorhersage durchf√ºhren\n","results = model.predict(source=image_path, save=True, conf=0.2, save_txt=True)\n","\n","# Pfad zum gespeicherten Bild mit den Erkennungen\n","result_image_dir = results[0].save_dir\n","result_image_name = os.path.splitext(os.path.basename(image_path))[0] + '.jpg' # .png zu .jpg √§ndern\n","result_image_path = os.path.join(result_image_dir, result_image_name)\n","\n","# List files in the save directory to verify the filename\n","print(f\"Files in save directory ({result_image_dir}):\")\n","for filename in os.listdir(result_image_dir):\n","    print(filename)\n","\n","display(IPImage(result_image_path, width=750))"],"metadata":{"id":"9XNzrZMPN-a6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 8 | Bildbeschreibung"],"metadata":{"id":"D8I9UIkZ7yRP"}},{"cell_type":"markdown","source":["Die Aufgabe Bildbeschreibung (Image2Text) beschreibt die automatische Generierung von Text aus Bildern mithilfe von K√ºnstlicher Intelligenz (KI). Dabei werden neuronale Netze, insbesondere tiefgehende Modelle in Kombination mit Transformern, genutzt, um visuelle Inhalte zu analysieren und zu beschreiben.\n","\n","**Anwendungsf√§lle**    \n","+ Bildbeschreibung (Image Captioning): Generierung von Textbeschreibungen zu Bildern (z. B. f√ºr barrierefreie Webseiten oder Archivierung).\n","+ Optische Zeichenerkennung (OCR): Extraktion von Text aus Bildern oder gescannten Dokumenten.\n","+ Visuelle Frage-Antwort-Systeme (Visual Question Answering, VQA): Beantwortung von Fragen zu Bildinhalten.\n","+ Content-Moderation: Automatische Identifikation und Beschreibung von problematischen oder sensiblen Bildinhalten."],"metadata":{"id":"DSKhCOVn78PW"}},{"cell_type":"markdown","source":["Der Code l√§dt ein Bild in Google Colab hoch, kodiert es in Base64 und sendet es zusammen mit einer Frage (\"Was ist auf dem Bild?\") an das OpenAI-Modell \"gpt-4o-mini\". Die Antwort des Modells (eine detaillierte Beschreibung im Markdown-Format) wird angezeigt, ebenso wie das hochgeladene Bild selbst (mit einer maximalen Breite von 500 Pixeln).\n"],"metadata":{"id":"fpf8fY2SUbCZ"}},{"cell_type":"code","source":["import base64\n","from openai import OpenAI\n","from google.colab import files\n","from IPython.display import Image, display, Markdown\n","\n","client = OpenAI()"],"metadata":{"id":"qz2KVUuV_JAf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_path = \"/content/files/peoples.png\"\n","prompt = \"Was ist auf dem Bild? Erstelle eine ausf√ºhrliche Beschreibung im Markdown-Format. Pr√ºfe die Bildinhalte sorgf√§ltig!\"\n","\n","result = analyze_image(image_path, prompt, max_tokens=600)\n","\n","display(Image(image_path, width=750))\n","display(Markdown(result))"],"metadata":{"id":"x8E407Y0RA0y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NkrKt6cPGWOT"},"source":["\n","# A | Aufgabe\n","---"]},{"cell_type":"markdown","source":["Bereit f√ºr eine neue Herausforderung? In diesen Aufgaben werden Sie die leistungsstarken Vision-Modelle von **MediaPipe** kennenlernen. Mit diesen Modellen k√∂nnen sie u.a. Objekterkennung, Handgesten und Gesichtsausdr√ºcke in Echtzeit analysieren. Entdecken Sie die Potenziale der GenAI. Die Aufgabestellungen unten bieten weitere Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen.\n","\n","[MediaPipe](https://mediapipe-studio.webapps.google.com/home)\n","\n","[MediaPipe L√∂sungsleitfaden](https://ai.google.dev/edge/mediapipe/solutions/guide?hl=de)"],"metadata":{"id":"U54N7byl5_jS"}},{"cell_type":"markdown","metadata":{"id":"pC9A-LaYhsta"},"source":["<p><font color='black' size=\"5\">\n","Coverbild LLM-Buch\n","</font></p>"]},{"cell_type":"markdown","source":["In einem fr√ºheren Modul wurde ein LLM-basierten Buchgenerator erstellt, der die Erstellung eines Buches von Anfang bis Ende automatisiert. Dieser Prozess umfasste die Erstellung einer Zusammenfassung, die Gliederung des Inhaltsverzeichnisses und das iterative Schreiben der Kapitel in ein strukturiertes Markdown-Dokument. Jetzt wird diese Arbeit erweitert: Erstellen Sie mit DALL¬∑E ein Coverbild f√ºr das Buch, das den textbasierten Inhalt erg√§nzt.\n","\n"],"metadata":{"id":"G1KmhbS8nw05"}}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["i0xOygLv6LaK","oYvUY6gMBKO1","MQFjKL7uDgfA","6ls-Rt2LwCfT","wR5J9U9bg5Br","hzCTsn14DyU0","3TjZs_TRht1n"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<p><font size=\"6\" color='grey'> <b>\n","\n","Generative KI. Verstehen. Anwenden. Gestalten.\n","</b></font> </br></p>"],"metadata":{"id":"Ih2CTVBnArVZ"}},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b>\n","OutputParser\n","</b></font> </br></p>\n","\n","\n","---"],"metadata":{"id":"6jJZ7wbdArVc"}},{"cell_type":"code","source":["#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n","!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul\n","from genai_lib.utilities import (\n","    check_environment,\n","    get_ipinfo,\n","    setup_api_keys,\n","    mprint,\n","    install_packages,\n","    mermaid,\n","    get_model_profile,\n","    extract_thinking,\n","    load_chat_prompt_template\n",")\n","setup_api_keys(['OPENAI_API_KEY'], create_globals=False)\n","print()\n","check_environment()\n","print()\n","get_ipinfo()"],"metadata":{"id":"F27EoMohZg_X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","‚è∏Ô∏è 5-Minuten-Check:\n","</font></p>\n","\n","**Ziel:** Pr√ºfen, ob du das vorherige Kapitel verstanden hast ‚Äì nicht, ob es gerade l√§uft.\n","\n","**Aufgabe** (5 Minuten, ohne Vorlage):\n","\n","Rekonstruiere die zentrale Idee oder Code-Struktur des letzten Abschnitts selbstst√§ndig\n","(kein Copy & Paste, kein Nachschlagen).\n","\n","W√§hle eine der folgenden Optionen:\n","\n","+ Erkl√§re in 1‚Äì2 S√§tzen, was hier konzeptionell passiert.\n","\n","+ Ver√§ndere eine Kleinigkeit (z. B. Prompt, Parameter, Reihenfolge) und beschreibe die Auswirkung.\n","\n","+ Markiere eine Stelle, die du nicht sicher erkl√§ren kannst, und formuliere eine konkrete Frage dazu.\n","\n","**Hinweis:**\n","Nicht alles muss ‚Äûfertig‚Äú oder ‚Äûkorrekt‚Äú sein. Entscheidend ist, wo dein Verst√§ndnis gerade endet"],"metadata":{"id":"SP3uxBkdasRI"}},{"cell_type":"markdown","source":["# 1 | Intro\n","---"],"metadata":{"id":"i0xOygLv6LaK"}},{"cell_type":"markdown","metadata":{"id":"pC9A-LaYhsta"},"source":["LangChain stellt eine Vielzahl von Ausgabeparsern bereit, die speziell darauf ausgelegt sind, Informationen aus den Ergebnissen gro√üer Sprachmodelle (LLMs) effizient zu extrahieren und zu strukturieren. Diese Parser sind essenzielle Bestandteile der LangChain-Architektur und h√§ufig zentrale Elemente sogenannter LangChain-Ketten. Solche Ketten bestehen aus konfigurierbaren Abfolgen von Operationen, die Modellausgaben verarbeiten und f√ºr weiterf√ºhrende Anwendungen nutzbar machen.\n","\n","\n","**Warum sind OutputParser so wichtig?**\n","+ LLMs geben standardm√§√üig unstrukturierte Texte zur√ºck.\n","+ OutputParser sind n√∂tig, um das LLM-Output strukturiert weiterzuverarbeiten.\n","+ Besonders bei komplexen Anwendungen (z. B. Ketten mit mehreren Modellen, Agenten oder RAG-Systemen) m√ºssen die Antworten klar definiert sein.\n","\n","**`with_structured_output()` als State-of-the-Art (LangChain 1.0+):**\n","+ Die Methode **`with_structured_output()`** ist die moderne L√∂sung f√ºr strukturierte Ausgaben seit LangChain 1.0+.\n","+ Nutzt die native **OpenAI Structured Output API**, die Schema-Konformit√§t auf Modell-Ebene garantiert.\n","+ **Garantierte Zuverl√§ssigkeit** durch automatisches Retry bei Schema-Verletzungen.\n","+ **Einfacher Code** ohne manuelle Format-Instruktionen im Prompt.\n","+ **Production-Ready** - Teil der stabilen LangChain 1.0+ API."]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Parser-Kategorien und ihr Status:\n","</font></p>"],"metadata":{"id":"Ny8JpOMURS7c"}},{"cell_type":"markdown","source":["| Parser-Typ | Beispiele | Status | Verwendung |\n","|-----------|-----------|--------|------------|\n","| **Einfache Parser** | `StrOutputParser`, `JsonOutputParser` | ‚úÖ **Essentiell** | Standard in LCEL-Chains |\n","| **Strukturierte Parser** | `PydanticOutputParser`, `XMLOutputParser` | ‚ö†Ô∏è **Gr√∂√ütenteils ersetzt** | Durch `with_structured_output()` |\n","| **Spezial-Parser** | `OutputFixingParser`, `RetryParser` | üîß **Nische** | Fehlerbehandlung, Legacy |"],"metadata":{"id":"f2Mmh0ciRNPA"}},{"cell_type":"markdown","source":["Die Methode with_structured_output() wird durch mehrere Anbieter unterst√ºtzt ..."],"metadata":{"id":"PDlEU_XUVRnN"}},{"cell_type":"markdown","source":["\n","\n","| Anbieter  | Modelle mit Support                                         |\n","|-----------|------------------------------------------------------------|\n","| OpenAI    | GPT-4, GPT-4o, GPT-3.5 mit [translate:Function Calling/JSON]       |\n","| Mistral   | Mistral Small, weitere Mistral Modelle                      |\n","| Google    | VertexAI LLMs                                              |\n","| Anthropic | Claude Modelle mit [translate:Tool-/Funktionsschnittstellen]       |\n"],"metadata":{"id":"ae8nrgvMVMw9"}},{"cell_type":"markdown","source":["# 2 | OutputParser (einfach)\n","---"],"metadata":{"id":"oYvUY6gMBKO1"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Basismodell\n","</font></p>"],"metadata":{"id":"UsPBjrlDoo04"}},{"cell_type":"code","source":["# Importe LangChain 1.0+\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain.chat_models import init_chat_model\n","from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n","from pydantic import BaseModel, Field"],"metadata":{"id":"_m84Eigb4LDn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Modell (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0.0)"],"metadata":{"id":"78SUuguIrrSh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='blue' size=\"4\">\n","Ohne Nachbearbeitung mit einem Parser\n","</font></p>"],"metadata":{"id":"jMvv2MQNrggb"}},{"cell_type":"code","source":["input = \"Was ist Machine Learning?\"\n","response = llm.invoke(input)"],"metadata":{"id":"MWmROWbwrrPr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for r in response:\n","    print(r)"],"metadata":{"id":"ezuKTok2rrMb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='blue' size=\"4\">\n","Mit Nachbearbeitung mit einem Parser\n","</font></p>"],"metadata":{"id":"CovCEerkv8Eb"}},{"cell_type":"code","source":["parser = StrOutputParser()\n","chain = llm | parser\n","response = chain.invoke(input)"],"metadata":{"id":"9t7QNRv3rrJi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MQFjKL7uDgfA"},"source":["# 3 | Datenformat Parser\n","---"]},{"cell_type":"markdown","source":["LangChain bietet eine breite Palette an Parsern, die unterschiedliche Datenformate verarbeiten k√∂nnen, was seine Einsatzm√∂glichkeiten erheblich erweitert. Es unterst√ºtzt unter anderem die nahtlose Integration von Pandas-Datenrahmen, kommagetrennten Listen, JSON-Strukturen sowie Datums- und Zeitobjekten. Diese Flexibilit√§t erm√∂glicht eine effiziente Anpassung an verschiedene Arten von Dateneingaben und macht LangChain zu einem leistungsstarken Werkzeug f√ºr die Analyse und Verarbeitung von Daten. Im Folgenden werden einige dieser Parser genauer betrachtet, ihre praktischen Anwendungen demonstriert und aufgezeigt, wie sie zur Optimierung von Prozessen und zur Gewinnung wertvoller Erkenntnisse beitragen k√∂nnen."],"metadata":{"id":"Q1oRFrLCx8eW"}},{"cell_type":"markdown","source":["**Beispiel:** Die Ausgabe des LLM kann in einem JSON-Format strukturiert werden."],"metadata":{"id":"S69wM83z-1Hh"}},{"cell_type":"code","source":["#\n","# Variante: Prompt\n","#\n","prompt = (\n","    \"Bitte gib mir eine fiktive Person mit Name und Alter im JSON-Format zur√ºck, z.B. \"\n","    '{\"name\": \"Max\", \"age\": 30}'\n",")\n","\n","response = llm.invoke(prompt)\n","\n","mprint(\"### ü§ñ KI:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"KIYp-bKe5t3e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#\n","# Parser: JsonOutputParser\n","#\n","parser = JsonOutputParser()\n","\n","# Prompt\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"You are a creative Assistant.\"),\n","    (\"human\", \"Give name and age of a person in JSON format.\"),\n","])\n","\n","# Kette\n","chain = prompt | llm | parser\n","\n","# Ausf√ºhren\n","response = chain.invoke({})\n","\n","mprint(\"### ü§ñ KI:\")\n","mprint(\"---\")\n","print(response)"],"metadata":{"id":"0dJs83IJ5t6X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4 | Structured Output (mittel)"],"metadata":{"id":"6ls-Rt2LwCfT"}},{"cell_type":"markdown","source":["Der **Structured Output** Ansatz in LangChain mit der Methode `with_structured_output()` ist die **state-of-the-art L√∂sung** f√ºr strukturierte LLM-Ausgaben in **LangChain**. Diese Methode nutzt die native Structured Output API von OpenAI, die garantiert, dass die Ausgabe dem definierten Schema entspricht.\n","\n","**Vorteile von `with_structured_output()`:**\n","- **Garantierte Schema-Konformit√§t**: OpenAI API erzwingt die Struktur auf Modell-Ebene\n","- **Kein weiterer Parser n√∂tig**: Direkter R√ºckgabewert als Pydantic-Objekt\n","- **H√∂here Zuverl√§ssigkeit**: Automatisches Retry bei Schema-Verletzungen\n","- **Bessere Effizienz**: Keine zus√§tzlichen Format-Instruktionen im Prompt\n","- **Typ-Sicherheit**: Vollst√§ndige IDE-Unterst√ºtzung und Type-Hints\n","- **Production-Ready**: Teil der stabilen LangChain 1.0+ API\n","- **Best Practice**: Empfohlener Ansatz f√ºr alle strukturierten Outputs\n","\n","Der erste Schritt besteht in der Definition eines **Pydantic BaseModel**, das die Struktur der erwarteten Ausgabe definiert. Anschlie√üend wird das LLM mit `with_structured_output()` konfiguriert."],"metadata":{"id":"n2igGVtTXE3I"}},{"cell_type":"code","source":["#@title üßú‚Äç‚ôÄÔ∏è Prozess-Diagramm { display-mode: \"form\" }\n","\n","diagram = \"\"\"\n","\n","flowchart TD\n","    A[\"LLM.with_structured_output(PydanticModel)\"] --> B[\"Konfiguriertes LLM\"]\n","    B --> C[\"Prompt senden\"]\n","    C --> D[\"LLM generiert strukturierten Output\"]\n","    D --> E[\"Parsing + Validierung\"]\n","    E --> F[\"Pydantic-Objekt\"]\n","    F --> G[\"Weiterverarbeitung\"]\n","\n","\"\"\"\n","mermaid(diagram, width=500, height=600)"],"metadata":{"id":"vIwMJ4X7fpz2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pydantic ist eine Python-Bibliothek, die automatisch Daten validiert und konvertiert, indem sie Typ-Hinweise nutzt und Datenstrukturen deklarativ mit Python-Klassen zu definieren."],"metadata":{"id":"UYWUmADCUabb"}},{"cell_type":"code","source":["# Pydantic-Modell definieren\n","class Response(BaseModel):\n","    user_input: str = Field(description=\"Frage des Benutzers\")\n","    response: str = Field(description=\"Antwort auf die Frage des Benutzers\")\n","    source: str = Field(description=\"Verwendete Quelle (Website) f√ºr die Antwort\")\n","\n","print(Response.model_json_schema())"],"metadata":{"id":"Hc9NtLEdNnr_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LLM mit strukturiertem Output konfigurieren. Kein Parser mehr n√∂tig!\n","structured_llm = llm.with_structured_output(Response)"],"metadata":{"id":"WBsMARRKmYze"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# System-Prompt\n","system_prompt = \"\"\"\n","Beantworte die folgende Frage so pr√§zise wie m√∂glich.\n","Gib auch die Quelle deiner Information an.\n","\"\"\"\n","\n","# Chat-Prompt-Template mit externen Variablen\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    (\"human\", \"{user_input}\")\n","])\n","\n","user_input = \"Was ist Machine Learning?\""],"metadata":{"id":"iCRaQa0LqORq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Chain erstellen - kein Parser mehr n√∂tig!\n","chain = prompt | structured_llm"],"metadata":{"id":"RIM9uWjhYjfd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Funktion zur Aufbereitung der Druck-Ausgabe\n","def markdown_out(response):\n","    mprint(f\"### üßë Mensch\")\n","    mprint(f\"{response.user_input}\")\n","    mprint(f\"### ü§ñ KI:\")\n","    mprint(f\"{response.response}\")\n","    mprint(f\"### Quelle:\\n {response.source}\")"],"metadata":{"id":"uaWn2wPJGmus"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aufruf Chain mit Dictionary (ChatPromptTemplate)\n","parameter = {}\n","parameter[\"system_prompt\"] = system_prompt\n","parameter[\"user_input\"] = user_input\n","response = chain.invoke(parameter)\n","\n","markdown_out(response)"],"metadata":{"id":"sVOMD53Jq5lT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y09-epHS5McO"},"source":["<p><font color='black' size=\"5\">\n","Datum/Uhrzeit\n","</font></p>"]},{"cell_type":"markdown","source":["Durch die Verwendung von Pydantics `datetime` Typ wird automatisch eine Validierung durchgef√ºhrt und das Datum in ein standardisiertes Python `datetime`-Objekt umgewandelt.\n","\n","Dies ist besonders hilfreich f√ºr Anwendungen wie Terminplanung, Datenanalyse oder andere Bereiche, in denen eine pr√§zise Verarbeitung zeitlicher Informationen erforderlich ist. Der moderne Ansatz mit `with_structured_output()` ist zuverl√§ssiger als der veraltete `DatetimeOutputParser` und nutzt die garantierte Schema-Konformit√§t der OpenAI API."],"metadata":{"id":"-zDlyRDUM4qS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRuzzhXF5Oa1"},"outputs":[],"source":["# Importe f√ºr Datum/Uhrzeit\n","from datetime import datetime"]},{"cell_type":"code","source":["# Pydantic-Modell f√ºr Datum/Uhrzeit (LangChain 1.0+ Ansatz)\n","class DateResponse(BaseModel):\n","    date: datetime = Field(description=\"Das extrahierte Datum als datetime-Objekt\")\n","    explanation: str = Field(description=\"Kurze Erkl√§rung zum Datum\")\n","\n","# Prompt-Template\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Beantworte die Frage und gib das Datum zur√ºck. Wenn du das Datum nicht kennst, verwende 1111-01-01.\"),\n","    (\"human\", \"{user_input}\")\n","])"],"metadata":{"id":"TBQjsTUvgSCb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kg5ZkOetFiPx"},"source":["Wir erstellen die Chain mit `with_structured_output()` - kein Parser mehr n√∂tig!"]},{"cell_type":"code","source":["# LLM mit strukturiertem Output\n","structured_llm = llm.with_structured_output(DateResponse)"],"metadata":{"id":"OFkQUWZCrRYb"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2w5gYty_5pKd"},"outputs":[],"source":["# Chain ohne Parser!\n","chain = prompt | structured_llm"]},{"cell_type":"markdown","metadata":{"id":"dD-gNvBHFooj"},"source":["Wir werden nach zwei Daten fragen, einem realen und einem fiktiven."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ymu48Ggg518q"},"outputs":[],"source":["parameter = {}\n","parameter[\"user_input\"] = \"Wann wurde die Programmiersprache Python eingef√ºhrt?\"\n","response = chain.invoke(parameter)\n","\n","# Ergebnis: 20.02.1991 (oder 1991-02-20 als datetime)\n","print(f\"Datum: {response.date}\")\n","print(f\"Erkl√§rung: {response.explanation}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nOPWYpxr6As1"},"outputs":[],"source":["parameter = {}\n","parameter[\"user_input\"] = \"An welchem Tag startet der Gro√üe Krieg im Videospiel Fallout?\"\n","response = chain.invoke(parameter)\n","\n","# Ergebnis: 23.10.2077 (oder 2077-10-23 als datetime)\n","print(f\"Datum: {response.date}\")\n","print(f\"Erkl√§rung: {response.explanation}\")"]},{"cell_type":"markdown","metadata":{"id":"wR5J9U9bg5Br"},"source":["# 5 | Structured Output (komplex)\n","---"]},{"cell_type":"markdown","source":["Nun wird ein Beispiel mit einer gr√∂√üeren Anzahl an Werten getestet. Das folgende Programm nimmt Texte in beliebigen Sprachen entgegen und √ºbersetzt sie automatisch ins Deutsche, Englische, Spanische und Chinesische. Dank `with_structured_output()` ist die Struktur garantiert - ohne zus√§tzliche Format-Instruktionen."],"metadata":{"id":"nN1AWb1m9OK4"}},{"cell_type":"markdown","source":["**Schritt 1:** Definiere ein **Pydantic-Modell** f√ºr die erwartete Ausgabe. Mit `with_structured_output()` wird die Struktur garantiert."],"metadata":{"id":"lpn8vHKwcEui"}},{"cell_type":"code","source":["# Pydantic-Modell definieren\n","class TranslationResponse(BaseModel):\n","    detected: str = Field(description=\"Die Sprache der Nutzer-Eingabe\")\n","    german: str = Field(description=\"German translation\")\n","    english: str = Field(description=\"English translation\")\n","    spanish: str = Field(description=\"Spanish translation\")\n","    chinese: str = Field(description=\"Chinese translation\")"],"metadata":{"id":"keSxWb0LuBVD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Schritt 2:** Erstelle einen **einfachen Prompt** - keine Format-Instruktionen mehr n√∂tig!"],"metadata":{"id":"zyAeqALjcbFt"}},{"cell_type":"code","source":["# ChatPromptTemplate\n","system_prompt = \"\"\"\n","√úbersetze in die angegebenen Sprachen.\n","\"\"\"\n","\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    (\"human\", \"{user_input}\")\n","])"],"metadata":{"id":"IwdqbixCuGi3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Schritt 3:** Chain ohne Parser - das strukturierte LLM gibt direkt ein typsicheres Pydantic-Objekt zur√ºck!"],"metadata":{"id":"8emVa-S1cuPI"}},{"cell_type":"code","source":["# LLM mit strukturiertem Output konfigurieren (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0.0)\n","structured_llm = llm.with_structured_output(TranslationResponse)"],"metadata":{"id":"VrhMbHb7uDDK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Chain - kein Parser mehr n√∂tig!\n","chain = prompt | structured_llm"],"metadata":{"id":"L5IVtdrQuSJJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sa6_dAk2ugre"},"source":["Zun√§chst wird ein deutscher Satz getestet, um zu beobachten, wie die √úbersetzung in die drei Zielsprachen erfolgt."]},{"cell_type":"code","source":["# Beispiel 1: Deutscher Satz\n","parameter = {}\n","parameter[\"system_prompt\"] = system_prompt\n","parameter[\"user_input\"] = \"Wann wurde Python eingef√ºhrt?\"\n","\n","response = chain.invoke(parameter)\n","\n","# Ausgabe √ºber Pydantic-Attribute\n","print(f\"{'detected':10s}:  {response.detected}\")\n","print(f\"{'german':10s}:  {response.german}\")\n","print(f\"{'english':10s}:  {response.english}\")\n","print(f\"{'spanish':10s}:  {response.spanish}\")\n","print(f\"{'chinese':10s}:  {response.chinese}\")"],"metadata":{"id":"ZQPcRed6uVLG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dann wird ein englischer Satz getestet, um zu beobachten, wie die √úbersetzung in die drei Zielsprachen erfolgt."],"metadata":{"id":"3lYL9wUcemex"}},{"cell_type":"code","source":["# Beispiel 2: Englischer Satz\n","parameter = {}\n","parameter[\"system_prompt\"] = system_prompt\n","parameter[\"user_input\"] = \"Who rides so late through night and wind?\"\n","\n","response = chain.invoke(parameter)\n","\n","# Ausgabe √ºber Pydantic-Attribute\n","print(f\"{'detected':10s}:  {response.detected}\")\n","print(f\"{'german':10s}:  {response.german}\")\n","print(f\"{'english':10s}:  {response.english}\")\n","print(f\"{'spanish':10s}:  {response.spanish}\")\n","print(f\"{'chinese':10s}:  {response.chinese}\")"],"metadata":{"id":"IDovWLGauWU9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hzCTsn14DyU0"},"source":["# 6 | Eigene Parser\n","---"]},{"cell_type":"markdown","source":["In bestimmten Szenarien kann es sinnvoll sein, einen benutzerdefinierten Parser zu erstellen, um die Modellausgabe eindeutig zu formatieren.  \n","\n","Daf√ºr bietet sich die Verwendung von **RunnableLambda** oder **RunnableGenerator** in LCEL an, was f√ºr viele F√§lle ein guten Ansatz ist.  "],"metadata":{"id":"DEXj9woZzzy0"}},{"cell_type":"markdown","metadata":{"id":"isGwPhF4GUM5"},"source":["<p><font color='black' size=\"5\">\n","Anwendungsfall\n","</font></p>"]},{"cell_type":"markdown","source":["\n","\n","Large Language Models (LLMs) wie GPT-4 k√∂nnen Text generieren, der Code und erkl√§rende Beschreibungen nahtlos vermischt. Dies kann zwar f√ºr Lern- und Dokumentationszwecke unglaublich n√ºtzlich sein, kann aber eine Herausforderung darstellen, wenn aus solchen Ausgaben mit gemischtem Inhalt nur der Code extrahiert und ausgef√ºhrt werden muss. Um dies zu beheben, implementieren wir eine einfache Funktion, die nicht-Python-Codezeilen aus einer gegebenen Textzeichenfolge entfernt.\n","\n","Bei diesem Ansatz werden regul√§re Ausdr√ºcke verwendet, um Zeilen zu identifizieren und beizubehalten, die der typischen Python-Syntax entsprechen, w√§hrend Zeilen verworfen werden, die beschreibender Text zu sein scheinen. Aufgrund der inh√§renten Komplexit√§t und Variabilit√§t sowohl von Python-Code als auch von nat√ºrlicher Sprache kann diese Methode jedoch nie perfekt sein. Sie basiert auf heuristischen Mustern, die Code manchmal f√§lschlicherweise als Text klassifizieren oder umgekehrt.\n","\n","Im n√§chsten Abschnitt werden wir untersuchen, wie ein anderes LLM beim Entfernen von Nicht-Python-Code helfen kann und m√∂glicherweise eine ausgefeiltere und genauere L√∂sung bietet. Das folgende Beispiel enth√§lt eine Mischung aus LLM-Kommentaren und generiertem Code.\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"sPq64ydYgJ42"}},{"cell_type":"code","source":["from langchain_core.runnables import RunnableLambda\n","\n","# Benutzer-Funktion zur Trennung von Erl√§uterung und Code\n","def parse_ai_message(ai_message):\n","    \"\"\"Trennt die Erl√§uterung und den Code aus einer AIMessage und gibt sie separat zur√ºck.\"\"\"\n","    text = ai_message.content  # Extrahiere den reinen Textinhalt der AIMessage\n","\n","    if \"```\" in text:\n","        # Trennen der Erl√§uterung und des Codes\n","        parts = text.split(\"```\")\n","        explanation = parts[0].strip()\n","        code = parts[1].strip() if len(parts) > 1 else \"\"\n","    else:\n","        # Falls kein Codeblock vorhanden ist, geben wir nur die Erl√§uterung zur√ºck\n","        explanation = text.strip()\n","        code = \"\"\n","\n","    return explanation, code  # R√ºckgabe von zwei separaten Werten"],"metadata":{"id":"6wjQlnF7qeso"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# RunnableLambda f√ºr das Parsing erstellen\n","parser = RunnableLambda(parse_ai_message)"],"metadata":{"id":"mkG-_kCOqFzV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Eingabe an die Pipeline senden und Ergebnis ausgeben\n","user_input = \"\"\"\n","    Erkl√§re mir, wie man eine einfache Funktion in Python erstellt und gib ein Beispiel zur Berechnung von Primzahlen.\n","    Verwende bei Formeln das Format $ Formel $\n","\"\"\""],"metadata":{"id":"k4PH2BAnqMB_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LLM und Parser verketten\n","chain = llm | parser"],"metadata":{"id":"HMnKM2X3qJ5h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["explanation, code = chain.invoke(user_input)"],"metadata":{"id":"NnhhC7JZqOHU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mprint(\"### ü§ñ Erl√§uterung\")\n","mprint(explanation)\n","mprint(\"### ü§ñ Code\")\n","print(code)"],"metadata":{"id":"AmMQsyLDqRE4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3TjZs_TRht1n"},"source":["# A | Aufgabe\n","---"]},{"cell_type":"markdown","source":["Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen."],"metadata":{"id":"smOJ1YD15z_Q"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Trennung Erl√§uterung/Code\n","</font></p>"],"metadata":{"id":"ZXw7agzteGHR"}},{"cell_type":"markdown","source":["Anstelle des eigenen Parsers wird `with_structured_output` verwendet."],"metadata":{"id":"Lp-xPCoOeN2u"}},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","JSON-Parser mit LangChain\n","</font></p>"],"metadata":{"id":"L5IWdaBS0UZD"}},{"cell_type":"markdown","source":["\n","**Ziel:** Verst√§ndnis f√ºr den Einsatz von `JsonOutputParser` in LangChain.\n","\n","**Hinweis:** Der `JsonOutputParser` ist f√ºr einfache JSON-Ausgaben n√ºtzlich, aber f√ºr strukturierte Daten ist `with_structured_output()` die bessere Wahl!\n","\n","**Aufgabe:**  \n","1. Nutze den `JsonOutputParser` von LangChain, um eine KI-Antwort in JSON zu formatieren.  \n","2. Lasse ein Language Model (z. B. OpenAI GPT) eine Liste von drei zuf√§lligen St√§dten in Deutschland generieren.  \n","3. Verwende den Parser, um die Ausgabe in ein JSON-Format umzuwandeln.\n","\n","**Erwartete Ausgabe (Beispiel):**\n","```json\n","{\n","  \"cities\": [\"Berlin\", \"Hamburg\", \"M√ºnchen\"]\n","}\n","```\n"],"metadata":{"id":"JTdhJeN8u99p"}},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Extraktion von Schl√ºsselwerten\n","</font></p>"],"metadata":{"id":"UCpw6-OHvHJJ"}},{"cell_type":"markdown","source":["\n","**Ziel:** Nutzung von `with_structured_output()`, um strukturierte Daten aus nat√ºrlicher Sprache zu extrahieren.\n","\n","**Aufgabe:**  \n","1. Definiere eine `Pydantic`-Datenklasse mit den Feldern: `name` (str), `alter` (int), `stadt` (str).  \n","2. Verwende `with_structured_output()` mit einem Language Model.  \n","3. Extrahiere aus einer gegebenen Beschreibung eine Person.\n","\n","**Beispiel-Eingabe:**\n","> \"Max Mustermann ist 35 Jahre alt und lebt in Berlin.\"\n","\n","**Beispiel-Code:**\n","```python\n","class Person(BaseModel):\n","    name: str = Field(description=\"Name der Person\")\n","    alter: int = Field(description=\"Alter in Jahren\")\n","    stadt: str = Field(description=\"Wohnort\")\n","\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0.0)\n","structured_llm = llm.with_structured_output(Person)\n","```\n","\n","**Erwartete Ausgabe (Pydantic-Modell):**\n","```python\n","Person(name=\"Max Mustermann\", alter=35, stadt=\"Berlin\")\n","```\n","\n","Als JSON:\n","```json\n","{\n","  \"name\": \"Max Mustermann\",\n","  \"alter\": 35,\n","  \"stadt\": \"Berlin\"\n","}\n","```"],"metadata":{"id":"geRHkOMCvKum"}},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Parser f√ºr Listenformate\n","</font></p>"],"metadata":{"id":"8-AzxHafvSE_"}},{"cell_type":"markdown","source":["\n","**Ziel:** Implementierung eines eigenen Parsers zur Umwandlung von KI-Ausgaben in Listen.\n","\n","**Aufgabe:**  \n","1. Erstelle eine eigene Parser-Klasse, die eine durch Kommas getrennte Liste in ein Listenformat umwandelt.  \n","2. Verwende diesen Parser, um eine Liste von f√ºnf beliebten B√ºchern aus einer Language-Model-Antwort zu extrahieren.\n","\n","**Beispiel-Eingabe:**\n","> \"Die Verwandlung, Faust, Der Prozess, Die Blechtrommel, Der Vorleser\"\n","\n","**Erwartete Ausgabe:**\n","```python\n","[\"Die Verwandlung\", \"Faust\", \"Der Prozess\", \"Die Blechtrommel\", \"Der Vorleser\"]\n","```"],"metadata":{"id":"ytCh-C2uvUFw"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Kombination Parser & PromptTemplate\n","</font></p>"],"metadata":{"id":"I3LRv5YAviSF"}},{"cell_type":"markdown","source":["\n","**Ziel:** Verwendung von `with_structured_output()` in Kombination mit `PromptTemplate`.\n","\n","**Aufgabe:**  \n","1. Erstelle ein `PromptTemplate`, das eine strukturierte Antwort √ºber ein Land ausgibt (Name, Hauptstadt, Einwohnerzahl).  \n","2. Verwende `with_structured_output()` mit einem Pydantic-Modell.\n","\n","**Beispiel-Pydantic-Modell:**\n","```python\n","class CountryInfo(BaseModel):\n","    name: str = Field(description=\"Name des Landes\")\n","    hauptstadt: str = Field(description=\"Hauptstadt des Landes\")\n","    einwohner: int = Field(description=\"Einwohnerzahl\")\n","\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0.0)\n","structured_llm = llm.with_structured_output(CountryInfo)\n","```\n","\n","**Beispiel-Prompt:**\n","> \"Gib mir Informationen zu Frankreich.\"\n","\n","**Erwartete Ausgabe:**\n","```python\n","CountryInfo(name=\"Frankreich\", hauptstadt=\"Paris\", einwohner=67000000)\n","```\n","\n","Als JSON:\n","```json\n","{\n","  \"name\": \"Frankreich\",\n","  \"hauptstadt\": \"Paris\",\n","  \"einwohner\": 67000000\n","}\n","```\n","\n","**Vorteil:** Keine manuelle Format-Instruktionen n√∂tig - die API garantiert die Struktur!\n"],"metadata":{"id":"bZqMjEWavk_i"}}]}
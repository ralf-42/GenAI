{"cells":[{"cell_type":"markdown","id":"37rMyCQz2JNw","metadata":{"id":"37rMyCQz2JNw"},"source":["<p><font size=\"6\" color='grey'> <b>\n","\n","Generative KI. Verstehen. Anwenden. Gestalten.\n","</b></font> </br></p>"]},{"cell_type":"markdown","id":"R5CfUEMJdvFQ","metadata":{"id":"R5CfUEMJdvFQ"},"source":["<p><font size=\"5\" color='grey'> <b>\n","LangChain 101\n","</b></font> </br></p>\n","\n","---"]},{"cell_type":"code","source":["#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n","!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul\n","from genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages, load_chat_prompt_template, mermaid\n","setup_api_keys(['OPENAI_API_KEY'], create_globals=False)\n","print()\n","check_environment()\n","print()\n","get_ipinfo()"],"metadata":{"id":"PnT8m8TTUhF5","collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764779306054,"user_tz":-60,"elapsed":60174,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"f4071d57-33e1-4e53-bcc1-e14843b772e1"},"id":"PnT8m8TTUhF5","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úì OPENAI_API_KEY erfolgreich gesetzt\n","\n","Python Version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n","\n","Installierte LangChain- und LangGraph-Bibliotheken:\n","langchain                                1.1.0\n","langchain-chroma                         1.0.0\n","langchain-classic                        1.0.0\n","langchain-community                      0.4.1\n","langchain-core                           1.1.0\n","langchain-ollama                         1.0.0\n","langchain-openai                         1.1.0\n","langchain-text-splitters                 1.0.0\n","langgraph                                1.0.3\n","langgraph-checkpoint                     3.0.1\n","langgraph-prebuilt                       1.0.5\n","langgraph-sdk                            0.2.10\n","\n","IP-Adresse: 34.16.194.63\n","Hostname: 63.194.16.34.bc.googleusercontent.com\n","Stadt: Las Vegas\n","Region: Nevada\n","Land: US\n","Koordinaten: 36.1750,-115.1372\n","Provider: AS396982 Google LLC\n","Postleitzahl: 89111\n","Zeitzone: America/Los_Angeles\n"]}]},{"cell_type":"markdown","id":"91ee89a4-477b-41ce-a511-aadd4c2fd547","metadata":{"id":"91ee89a4-477b-41ce-a511-aadd4c2fd547"},"source":["# 1 | Was ist LangChain?\n","---"]},{"cell_type":"markdown","id":"b5004f8c","metadata":{"id":"b5004f8c"},"source":["LangChain ist ein Framework zur **Entwicklung von Anwendungen mit gro√üen Sprachmodellen** (LLMs). Es vereinfacht die Integration von LLMs in eigene Anwendungen durch:\n","\n","- Modulare Komponenten f√ºr verschiedene Aufgaben\n","- Vorgefertigte Ketten (Chains) f√ºr komplexe Workflows\n","- Einfache Integration externer Datenquellen\n","- Werkzeuge f√ºr das Speichern von Konversationskontexten\n","\n"]},{"cell_type":"markdown","id":"tl73bT-kKkia","metadata":{"id":"tl73bT-kKkia"},"source":["[Einf√ºhrung](https://python.langchain.com/docs/introduction/)   \n","[Konzepte](https://python.langchain.com/docs/concepts/)   \n","[API-References](https://python.langchain.com/api_reference/index.html)   \n","[Integrations](https://python.langchain.com/docs/integrations/providers/)\n","\n","---\n","\n","[Tutorials](https://python.langchain.com/docs/tutorials/)   \n","[How-to-Guides](https://python.langchain.com/docs/how_to/)   "]},{"cell_type":"markdown","id":"fGV8FwGmIwmR","metadata":{"id":"fGV8FwGmIwmR"},"source":["# 2 | √úberblick Konzepte\n","---\n","\n","\n"]},{"cell_type":"markdown","source":["Die Darstellung veranschaulicht das Grundprinzip von LangChain: die Verkettung verschiedener Konzepte zu einem durchg√§ngigen Verarbeitungsprozess, der Eingaben in strukturierte Ausgaben umwandelt."],"metadata":{"id":"nZ2ZMEd3ulKZ"},"id":"nZ2ZMEd3ulKZ"},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/langchain_prozess_01.png)"],"metadata":{"id":"KdBKNv3gzJAq"},"id":"KdBKNv3gzJAq"},{"cell_type":"markdown","source":["Konzepte von LangChain sind grundlegende Bausteine und Prinzipien. Hier sind einige der wichtigsten Konzepte:\n","\n","+ Nachrichten (**Messages**): Kommunikationseinheiten in Chat-Modellen f√ºr Ein- und Ausgabe.\n","+ Prompt-Vorlagen (**Prompt template**s): Komponenten zur Erstellung strukturierter Prompts f√ºr LLMs.\n","+ Chat-Modelle (**Chat models**): LLMs, die √ºber eine Chat-API verf√ºgbar sind und Sequenzen von Nachrichten verarbeiten.\n","+ Chains (**Chains**): Verkn√ºpfungen mehrerer LLMs oder anderer Komponenten f√ºr komplexere Anwendungen.\n","+ Strukturierte Ausgabe (**Structured output**): Technik, um Chat-Modelle in strukturierten Formaten antworten zu lassen.\n","+ Chat-Verlauf (**Chat history**): Eine Sequenz von Nachrichten, die eine Konversation darstellt.\n","+ Retrieval Augmented Generation (**RAG**): Technik zur Verbesserung von Sprachmodellen durch Kombination mit externen Wissensbasen.\n","+ Retriever (**Retriever**): Komponenten, die relevante Dokumente aus einer Wissensbasis abrufen.\n","+ Agenten (**Agents**): Nutzen Sprachmodelle, um Aktionssequenzen auszuw√§hlen und mit externen Ressourcen zu interagieren.\n","+ Tools (**Tools**): Funktionen mit definierten Schemata f√ºr Name, Beschreibung und Argumente.\n"],"metadata":{"id":"cCuow5DOEsID"},"id":"cCuow5DOEsID"},{"cell_type":"markdown","id":"EouwC_wdZb-c","metadata":{"id":"EouwC_wdZb-c"},"source":["# 3 | Nachrichten-Typen\n","---"]},{"cell_type":"markdown","id":"yS2mM2J1aXlY","metadata":{"id":"yS2mM2J1aXlY"},"source":["Beim Einsatz von Large Language Models (LLMs) wie GPT gibt es drei Hauptarten von Nachrichten (Messages), die die Interaktion mit dem Modell steuern. Diese Messages definieren, wie das Modell reagiert, welche Eingaben es erh√§lt und welche Ausgaben es generiert.\n","\n"]},{"cell_type":"markdown","id":"sGqRXptbbQ2a","metadata":{"id":"sGqRXptbbQ2a"},"source":["**System Message:**   \n","Die `System Message` dient dazu, das **Verhalten des LLMs** zu steuern. Sie ist eine nicht sichtbare Nachricht, die dem Modell Anweisungen gibt, wie es sich verhalten soll. Dabei kann sie die Pers√∂nlichkeit, den Tonfall und die Einschr√§nkungen des Modells beeinflussen. Die Systemnachricht funktioniert wie eine Art \"Rollenspiel-Anweisung\", um das Modell in eine gew√ºnschte Rolle zu versetzen.\n","\n","*Merkmale:*\n","- Definiert, wie das Modell reagieren soll\n","- Legt Pers√∂nlichkeit, Verhalten und Einschr√§nkungen fest\n","- Wird zu Beginn einer Sitzung gesetzt und bleibt bestehen\n","- Nicht sichtbar f√ºr den Benutzer\n","\n","*Beispiel einer System Message:*\n","```json\n","{\n","  \"role\": \"system\",\n","  \"content\": \"Du bist ein KI-Assistent, der pr√§zise und informative Antworten in einem professionellen Ton liefert.\"\n","}\n","```\n","Dieses Beispiel weist das Modell an, in einem professionellen Tonfall kurze und informative Antworten zu geben.\n","\n","---\n","\n"]},{"cell_type":"markdown","id":"upsIzpcqbbP7","metadata":{"id":"upsIzpcqbbP7"},"source":["**User Message**   \n","Die `User Message` ist die eigentliche **Eingabe des Benutzer**s. Sie kann eine Frage, eine Aufforderung, kontextrelevanten Inhalt oder ein Befehl sein. Das Modell nutzt diese Nachricht als Ausgangspunkt f√ºr die Generierung einer Antwort.\n","\n","*Merkmale:*\n","- Direkte Eingabe des Benutzers\n","- Kann eine Frage, Aufforderung, kontextrelevanten Inhalt oder einen Befehl enthalten\n","- Basis f√ºr die Antwort des Modells\n","\n","*Beispiel einer User Message:*\n","```json\n","{\n","  \"role\": \"user\",\n","  \"content\": \"Was sind die wichtigsten Unterschiede zwischen KI und maschinellem Lernen?\"\n","}\n","```\n","Hier fragt der Benutzer nach den Unterschieden zwischen KI und maschinellem Lernen.\n","\n","---\n"]},{"cell_type":"markdown","id":"CozrrtYXbhZb","metadata":{"id":"CozrrtYXbhZb"},"source":["**AI Message**   \n","Die `AI Message` ist die **Antwort, die das Modell generiert**. Sie basiert auf der User Message und den Anweisungen aus der System Message. Die AI Message kann verschiedene Eigenschaften haben, ist aber haupts√§chlich auf den Inhalt fokussiert.\n","\n","*Merkmale:*\n","- Antwort des Modells auf die Benutzeranfrage\n","- Kann verschiedene Eigenschaften haben (z. B. L√§nge, Stil)\n","- Haupts√§chlich inhaltlich relevant\n","\n","*Beispiel einer AI Message:*\n","```json\n","{\n","  \"role\": \"assistant\",\n","  \"content\": \"K√ºnstliche Intelligenz (KI) ist ein weites Feld, das verschiedene Teilgebiete umfasst, darunter auch das maschinelle Lernen (ML). ML konzentriert sich auf die Erstellung von Modellen, die aus Daten lernen und Vorhersagen treffen.\"\n","}\n","```\n","Hier gibt das Modell eine inhaltliche Antwort auf die Frage des Benutzers.\n","\n","\n"]},{"cell_type":"markdown","source":["**System-Message vs. User-Message**\n","\n","\n","\n","*System-Message*\n","\n","**Funktion:** Persistente Verhaltensregeln √ºber verschiedene Aufgaben hinweg\n","\n","**Enth√§lt:**\n","- Identit√§t & Rolle des Modells\n","- Grunds√§tzliche Methodik & Denkansatz\n","- Durchg√§ngige Qualit√§tsstandards\n","- Tool-/Methodenkompetenz\n","- Generische Few-Shot-Examples\n","\n","**Metapher:** Das Betriebssystem ‚Äì selten ver√§ndert, verhaltensbestimmend\n","\n","\n"],"metadata":{"id":"a8QAtdYdg0Im"},"id":"a8QAtdYdg0Im"},{"cell_type":"markdown","source":["\n","*User-Message*\n","\n","**Funktion:** Konkrete Aufgabe und spezifischer Kontext f√ºr diese Interaktion\n","\n","**Enth√§lt:**\n","- Situativer Kontext & Hintergrund\n","- Klare Aufgabenstellung\n","- Erwartete Format/Umfang/Detailgrad\n","- Spezifische Daten & Randbedingungen\n","- Aufgabenspezifische Verhaltensanpassungen\n","\n","**Metapher:** Die Arbeitsanweisung ‚Äì variiert bei jeder Anfrage\n","\n"],"metadata":{"id":"cy765AHZhaWV"},"id":"cy765AHZhaWV"},{"cell_type":"markdown","id":"PiHnhKHVbofP","metadata":{"id":"PiHnhKHVbofP"},"source":["**Kurz gefasst**   \n","\n","+  **System Message** legt die Regeln und das Verhalten des Modells fest.\n","+ **User Message** stellt eine Anfrage oder einen Befehl.\n","+ **AI Message** gibt die generierte Antwort basierend auf den vorherigen Nachrichten.\n","\n","\n"]},{"cell_type":"code","id":"uwwloofx7h","source":["#@markdown   <p><font size=\"4\" color='green'>  üßú‚Äç‚ôÄÔ∏è Mermaid - Message-Typen Workflow</font> </br></p>\n","\n","diagram = \"\"\"\n","graph TD\n","    A[\"System Message\"] --> D[\"LLM Verarbeitung\"]\n","    B[\"User Message\"] --> D\n","    D --> C[\"AI Message\"]\n","\n","    A1[\"Definiert Verhalten<br/>Rolle & Pers√∂nlichkeit<br/>Einschr√§nkungen\"] -.-> A\n","    B1[\"Benutzer-Eingabe<br/>Frage oder Befehl<br/>Kontext\"] -.-> B\n","    C1[\"Generierte Antwort<br/>Basierend auf System + User<br/>Strukturiertes Format\"] -.-> C\n","\n","    style A fill:#ffe1e1\n","    style B fill:#e1f5ff\n","    style C fill:#e1ffe1\n","    style D fill:#fff4e1\n","\"\"\"\n","mermaid(diagram, width=700, height=450)"],"metadata":{"id":"uwwloofx7h","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"status":"ok","timestamp":1764779306937,"user_tz":-60,"elapsed":876,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"c91efd8b-6818-432e-8e8c-dff6d859ff4e"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.SVG object>"],"image/svg+xml":"<svg xmlns=\"http://www.w3.org/2000/svg\" aria-roledescription=\"flowchart-v2\" role=\"graphics-document document\" viewBox=\"0 0 655.7734375 502\" class=\"flowchart\" id=\"container\" width=\"700\" height=\"450\"><style>#container{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:16px;fill:#333;}@keyframes edge-animation-frame{from{stroke-dashoffset:0;}}@keyframes dash{to{stroke-dashoffset:0;}}#container .edge-animation-slow{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 50s linear infinite;stroke-linecap:round;}#container .edge-animation-fast{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 20s linear infinite;stroke-linecap:round;}#container .error-icon{fill:#552222;}#container .error-text{fill:#552222;stroke:#552222;}#container .edge-thickness-normal{stroke-width:1px;}#container .edge-thickness-thick{stroke-width:3.5px;}#container .edge-pattern-solid{stroke-dasharray:0;}#container .edge-thickness-invisible{stroke-width:0;fill:none;}#container .edge-pattern-dashed{stroke-dasharray:3;}#container .edge-pattern-dotted{stroke-dasharray:2;}#container .marker{fill:#333333;stroke:#333333;}#container .marker.cross{stroke:#333333;}#container svg{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:16px;}#container p{margin:0;}#container .label{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;color:#333;}#container .cluster-label text{fill:#333;}#container .cluster-label span{color:#333;}#container .cluster-label span p{background-color:transparent;}#container .label text,#container span{fill:#333;color:#333;}#container .node rect,#container .node circle,#container .node ellipse,#container .node polygon,#container .node path{fill:#ECECFF;stroke:#9370DB;stroke-width:1px;}#container .rough-node .label text,#container .node .label text,#container .image-shape .label,#container .icon-shape .label{text-anchor:middle;}#container .node .katex path{fill:#000;stroke:#000;stroke-width:1px;}#container .rough-node .label,#container .node .label,#container .image-shape .label,#container .icon-shape .label{text-align:center;}#container .node.clickable{cursor:pointer;}#container .root .anchor path{fill:#333333!important;stroke-width:0;stroke:#333333;}#container .arrowheadPath{fill:#333333;}#container .edgePath .path{stroke:#333333;stroke-width:2.0px;}#container .flowchart-link{stroke:#333333;fill:none;}#container .edgeLabel{background-color:rgba(232,232,232, 0.8);text-align:center;}#container .edgeLabel p{background-color:rgba(232,232,232, 0.8);}#container .edgeLabel rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#container .labelBkg{background-color:rgba(232, 232, 232, 0.5);}#container .cluster rect{fill:#ffffde;stroke:#aaaa33;stroke-width:1px;}#container .cluster text{fill:#333;}#container .cluster span{color:#333;}#container div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:12px;background:hsl(80, 100%, 96.2745098039%);border:1px solid #aaaa33;border-radius:2px;pointer-events:none;z-index:100;}#container .flowchartTitleText{text-anchor:middle;font-size:18px;fill:#333;}#container rect.text{fill:none;stroke-width:0;}#container .icon-shape,#container .image-shape{background-color:rgba(232,232,232, 0.8);text-align:center;}#container .icon-shape p,#container .image-shape p{background-color:rgba(232,232,232, 0.8);padding:2px;}#container .icon-shape rect,#container .image-shape rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#container :root{--mermaid-font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;}</style><g><marker orient=\"auto\" markerHeight=\"8\" markerWidth=\"8\" markerUnits=\"userSpaceOnUse\" refY=\"5\" refX=\"5\" viewBox=\"0 0 10 10\" class=\"marker flowchart-v2\" id=\"container_flowchart-v2-pointEnd\"><path style=\"stroke-width: 1; stroke-dasharray: 1, 0;\" class=\"arrowMarkerPath\" d=\"M 0 0 L 10 5 L 0 10 z\"/></marker><marker orient=\"auto\" markerHeight=\"8\" markerWidth=\"8\" markerUnits=\"userSpaceOnUse\" refY=\"5\" refX=\"4.5\" viewBox=\"0 0 10 10\" class=\"marker flowchart-v2\" id=\"container_flowchart-v2-pointStart\"><path style=\"stroke-width: 1; stroke-dasharray: 1, 0;\" class=\"arrowMarkerPath\" d=\"M 0 5 L 10 10 L 10 0 z\"/></marker><marker orient=\"auto\" markerHeight=\"11\" markerWidth=\"11\" markerUnits=\"userSpaceOnUse\" refY=\"5\" refX=\"11\" viewBox=\"0 0 10 10\" class=\"marker flowchart-v2\" id=\"container_flowchart-v2-circleEnd\"><circle style=\"stroke-width: 1; stroke-dasharray: 1, 0;\" class=\"arrowMarkerPath\" r=\"5\" cy=\"5\" cx=\"5\"/></marker><marker orient=\"auto\" markerHeight=\"11\" markerWidth=\"11\" markerUnits=\"userSpaceOnUse\" refY=\"5\" refX=\"-1\" viewBox=\"0 0 10 10\" class=\"marker flowchart-v2\" id=\"container_flowchart-v2-circleStart\"><circle style=\"stroke-width: 1; stroke-dasharray: 1, 0;\" class=\"arrowMarkerPath\" r=\"5\" cy=\"5\" cx=\"5\"/></marker><marker orient=\"auto\" markerHeight=\"11\" markerWidth=\"11\" markerUnits=\"userSpaceOnUse\" refY=\"5.2\" refX=\"12\" viewBox=\"0 0 11 11\" class=\"marker cross flowchart-v2\" id=\"container_flowchart-v2-crossEnd\"><path style=\"stroke-width: 2; stroke-dasharray: 1, 0;\" class=\"arrowMarkerPath\" d=\"M 1,1 l 9,9 M 10,1 l -9,9\"/></marker><marker orient=\"auto\" markerHeight=\"11\" markerWidth=\"11\" markerUnits=\"userSpaceOnUse\" refY=\"5.2\" refX=\"-1\" viewBox=\"0 0 11 11\" class=\"marker cross flowchart-v2\" id=\"container_flowchart-v2-crossStart\"><path style=\"stroke-width: 2; stroke-dasharray: 1, 0;\" class=\"arrowMarkerPath\" d=\"M 1,1 l 9,9 M 10,1 l -9,9\"/></marker><g class=\"root\"><g class=\"clusters\"/><g class=\"edgePaths\"><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_A_D_0\" d=\"M117.945,214L117.945,218.167C117.945,222.333,117.945,230.667,132.067,244.62C146.189,258.574,174.432,278.148,188.554,287.935L202.676,297.722\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_B_D_0\" d=\"M371.898,214L371.898,218.167C371.898,222.333,371.898,230.667,357.777,244.62C343.655,258.574,315.412,278.148,301.29,287.935L287.168,297.722\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_D_C_0\" d=\"M244.922,354L244.922,364.167C244.922,374.333,244.922,394.667,255.432,408.84C265.943,423.012,286.964,431.025,297.474,435.031L307.985,439.037\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-dotted edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_A1_A_0\" d=\"M117.945,110L117.945,114.167C117.945,118.333,117.945,126.667,117.945,134.333C117.945,142,117.945,149,117.945,152.5L117.945,156\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-dotted edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_B1_B_0\" d=\"M371.898,110L371.898,114.167C371.898,118.333,371.898,126.667,371.898,134.333C371.898,142,371.898,149,371.898,152.5L371.898,156\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-dotted edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_C1_C_0\" d=\"M517.773,390L517.773,394.167C517.773,398.333,517.773,406.667,507.263,414.84C496.752,423.012,475.731,431.025,465.221,435.031L454.71,439.037\"/></g><g class=\"edgeLabels\"><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g></g><g class=\"nodes\"><g transform=\"translate(117.9453125, 187)\" id=\"flowchart-A-0\" class=\"node default\"><rect height=\"54\" width=\"177.125\" y=\"-27\" x=\"-88.5625\" style=\"fill:#ffe1e1 !important\" class=\"basic label-container\"/><g transform=\"translate(-58.5625, -12)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"24\" width=\"117.125\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>System Message</p></span></div></foreignObject></g></g><g transform=\"translate(244.921875, 327)\" id=\"flowchart-D-1\" class=\"node default\"><rect height=\"54\" width=\"185.703125\" y=\"-27\" x=\"-92.8515625\" style=\"fill:#fff4e1 !important\" class=\"basic label-container\"/><g transform=\"translate(-62.8515625, -12)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"24\" width=\"125.703125\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>LLM Verarbeitung</p></span></div></foreignObject></g></g><g transform=\"translate(371.8984375, 187)\" id=\"flowchart-B-2\" class=\"node default\"><rect height=\"54\" width=\"158.25\" y=\"-27\" x=\"-79.125\" style=\"fill:#e1f5ff !important\" class=\"basic label-container\"/><g transform=\"translate(-49.125, -12)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"24\" width=\"98.25\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>User Message</p></span></div></foreignObject></g></g><g transform=\"translate(381.34765625, 467)\" id=\"flowchart-C-5\" class=\"node default\"><rect height=\"54\" width=\"139.25\" y=\"-27\" x=\"-69.625\" style=\"fill:#e1ffe1 !important\" class=\"basic label-container\"/><g transform=\"translate(-39.625, -12)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"24\" width=\"79.25\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>AI Message</p></span></div></foreignObject></g></g><g transform=\"translate(117.9453125, 59)\" id=\"flowchart-A1-6\" class=\"node default\"><rect height=\"102\" width=\"219.890625\" y=\"-51\" x=\"-109.9453125\" style=\"\" class=\"basic label-container\"/><g transform=\"translate(-79.9453125, -36)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"72\" width=\"159.890625\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Definiert Verhalten<br/>Rolle &amp; Pers√∂nlichkeit<br/>Einschr√§nkungen</p></span></div></foreignObject></g></g><g transform=\"translate(371.8984375, 59)\" id=\"flowchart-B1-8\" class=\"node default\"><rect height=\"102\" width=\"188.015625\" y=\"-51\" x=\"-94.0078125\" style=\"\" class=\"basic label-container\"/><g transform=\"translate(-64.0078125, -36)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"72\" width=\"128.015625\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Benutzer-Eingabe<br/>Frage oder Befehl<br/>Kontext</p></span></div></foreignObject></g></g><g transform=\"translate(517.7734375, 327)\" id=\"flowchart-C1-10\" class=\"node default\"><rect height=\"126\" width=\"260\" y=\"-63\" x=\"-130\" style=\"\" class=\"basic label-container\"/><g transform=\"translate(-100, -48)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"96\" width=\"200\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\"><span class=\"nodeLabel\"><p>Generierte Antwort<br/>Basierend auf System + User<br/>Strukturiertes Format</p></span></div></foreignObject></g></g></g></g></g></svg>"},"metadata":{}}]},{"cell_type":"markdown","source":["# 4 | Ein einfaches Modell\n","---\n"],"metadata":{"id":"BMPwDNTCCOfx"},"id":"BMPwDNTCCOfx"},{"cell_type":"code","execution_count":3,"metadata":{"id":"e1926d03","executionInfo":{"status":"ok","timestamp":1764779306994,"user_tz":-60,"elapsed":56,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Importe\n","from langchain.chat_models import init_chat_model\n","from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n","from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n","from langchain_core.prompts.few_shot import FewShotPromptTemplate\n","from langchain_core.output_parsers.string import StrOutputParser"],"id":"e1926d03"},{"cell_type":"code","source":["# Modell definieren (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0.0)"],"metadata":{"id":"RG-y_bG8HOOi","executionInfo":{"status":"ok","timestamp":1764779312651,"user_tz":-60,"elapsed":5650,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":4,"outputs":[],"id":"RG-y_bG8HOOi"},{"cell_type":"markdown","id":"-JwJD0rLIyT8","metadata":{"id":"-JwJD0rLIyT8"},"source":["# 5 | Prompts\n","---"]},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/langchain_prozess_02.png)"],"metadata":{"id":"j9stNgKo93Eg"},"id":"j9stNgKo93Eg"},{"cell_type":"markdown","source":["\n","\n","Prompts sind die Eingaben bzw. Auftr√§ge an das LLM. LangChain bietet verschiedene Template-Systeme. Ein **Template** ist eine **Vorlage** ‚Äì genauer gesagt ein Text mit **Platzhaltern**, die sp√§ter durch echte Werte ersetzt werden.\n","\n","\n","Ein **PromptTemplate** ist eine Vorlage f√ºr einen Eingabetext, der an ein Sprachmodell geschickt wird.\n","Er enth√§lt Platzhalter `{placeholder}`, die sp√§ter mit Nutzereingaben oder Kontext bef√ºllt werden.\n","\n","**Templates machen die KI-Prompts:**\n","\n","+ Wiederverwendbar (z.‚ÄØB. f√ºr viele Themen oder Fragen)\n","\n","+ Strukturiert (z.‚ÄØB. mit Rollen und Anweisungen)\n","\n","+ Dynamisch erweiterbar (mit Memory, Tools, etc.)"],"metadata":{"id":"QMVO4fzPFZ88"},"id":"QMVO4fzPFZ88"},{"cell_type":"markdown","id":"HqTUeMW20084","metadata":{"id":"HqTUeMW20084"},"source":["## 5.1 | Simple Prompt\n"]},{"cell_type":"markdown","source":["<p><font color='darkblue' size=\"4\">\n","<b>‚ùóHinweis:</b>\n","</font></p>\n","Ohne Memory-Management hat das Modell ein kurzes Ged√§chnis.\n"],"metadata":{"id":"5JF1_AvYNYY_"},"id":"5JF1_AvYNYY_"},{"cell_type":"markdown","source":["**Prompt wird als Zeichenkette √ºbergeben:**"],"metadata":{"id":"1azC75R5JpWC"},"id":"1azC75R5JpWC"},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(\"Was ist Generative KI?\")"],"metadata":{"id":"jPizca8FJV5b","executionInfo":{"status":"ok","timestamp":1764779321128,"user_tz":-60,"elapsed":8459,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"jPizca8FJV5b","execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Antwort\n","type(response)"],"metadata":{"id":"EHjmjLgW3ENA","colab":{"base_uri":"https://localhost:8080/","height":203},"executionInfo":{"status":"ok","timestamp":1764779321180,"user_tz":-60,"elapsed":47,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"8ac0fa30-df9d-400e-f6e7-cf03cd451253"},"id":"EHjmjLgW3ENA","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["langchain_core.messages.ai.AIMessage"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.messages.ai.AIMessage</b><br/>def __init__(content: str | list[str | dict] | None=None, content_blocks: list[types.ContentBlock] | None=None, **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/langchain_core/messages/ai.py</a>Message from an AI.\n","\n","An `AIMessage` is returned from a chat model as a response to a prompt.\n","\n","This message represents the output of the model and consists of both\n","the raw output as returned by the model and standardized fields\n","(e.g., tool calls, usage metadata) added by the LangChain framework.</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 153);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["response"],"metadata":{"id":"4ZYngACo4RWN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764779321220,"user_tz":-60,"elapsed":38,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"ed780b04-0974-4328-c74d-1913428a1320"},"id":"4ZYngACo4RWN","execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='Generative KI, oder generative k√ºnstliche Intelligenz, bezieht sich auf eine Klasse von Algorithmen und Modellen, die in der Lage sind, neue Inhalte zu erzeugen. Diese Inhalte k√∂nnen in verschiedenen Formen vorliegen, darunter Texte, Bilder, Musik, Videos und mehr. Generative KI nutzt oft Techniken des maschinellen Lernens, insbesondere neuronale Netzwerke, um Muster in bestehenden Daten zu erkennen und darauf basierend neue, √§hnliche Daten zu generieren.\\n\\nEin bekanntes Beispiel f√ºr generative KI sind Sprachmodelle wie GPT (Generative Pre-trained Transformer), die in der Lage sind, menschen√§hnliche Texte zu schreiben. Ein weiteres Beispiel sind generative adversarielle Netzwerke (GANs), die verwendet werden, um realistische Bilder zu erzeugen, indem sie zwei neuronale Netzwerke gegeneinander antreten lassen.\\n\\nGenerative KI hat viele Anwendungen, darunter:\\n\\n1. **Kreative Inhalte**: Erstellung von Kunst, Musik, Geschichten und anderen kreativen Arbeiten.\\n2. **Datenaugmentation**: Erzeugung zus√§tzlicher Trainingsdaten f√ºr maschinelles Lernen.\\n3. **Simulationen**: Erzeugung realistischer Szenarien f√ºr Training und Forschung.\\n4. **Personalisierung**: Anpassung von Inhalten an individuelle Nutzerpr√§ferenzen.\\n\\nDie Technologie hat das Potenzial, viele Bereiche zu revolutionieren, bringt jedoch auch Herausforderungen und ethische Fragen mit sich, insbesondere in Bezug auf Urheberrecht, F√§lschungen und die Verbreitung von Fehlinformationen.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 314, 'prompt_tokens': 13, 'total_tokens': 327, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_50906f2aac', 'id': 'chatcmpl-Cik0PIOeM5o7zGS3G4x87D2Q1zQJN', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--93bd761b-dd0f-4856-aab9-e5f5d787528c-0', usage_metadata={'input_tokens': 13, 'output_tokens': 314, 'total_tokens': 327, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["for r in response:\n","    print(r)"],"metadata":{"id":"jveXqUjbpo0s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764779321284,"user_tz":-60,"elapsed":61,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"2aed3b8a-e12b-4969-c8d7-27b8332421a7"},"id":"jveXqUjbpo0s","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["('content', 'Generative KI, oder generative k√ºnstliche Intelligenz, bezieht sich auf eine Klasse von Algorithmen und Modellen, die in der Lage sind, neue Inhalte zu erzeugen. Diese Inhalte k√∂nnen in verschiedenen Formen vorliegen, darunter Texte, Bilder, Musik, Videos und mehr. Generative KI nutzt oft Techniken des maschinellen Lernens, insbesondere neuronale Netzwerke, um Muster in bestehenden Daten zu erkennen und darauf basierend neue, √§hnliche Daten zu generieren.\\n\\nEin bekanntes Beispiel f√ºr generative KI sind Sprachmodelle wie GPT (Generative Pre-trained Transformer), die in der Lage sind, menschen√§hnliche Texte zu schreiben. Ein weiteres Beispiel sind generative adversarielle Netzwerke (GANs), die verwendet werden, um realistische Bilder zu erzeugen, indem sie zwei neuronale Netzwerke gegeneinander antreten lassen.\\n\\nGenerative KI hat viele Anwendungen, darunter:\\n\\n1. **Kreative Inhalte**: Erstellung von Kunst, Musik, Geschichten und anderen kreativen Arbeiten.\\n2. **Datenaugmentation**: Erzeugung zus√§tzlicher Trainingsdaten f√ºr maschinelles Lernen.\\n3. **Simulationen**: Erzeugung realistischer Szenarien f√ºr Training und Forschung.\\n4. **Personalisierung**: Anpassung von Inhalten an individuelle Nutzerpr√§ferenzen.\\n\\nDie Technologie hat das Potenzial, viele Bereiche zu revolutionieren, bringt jedoch auch Herausforderungen und ethische Fragen mit sich, insbesondere in Bezug auf Urheberrecht, F√§lschungen und die Verbreitung von Fehlinformationen.')\n","('additional_kwargs', {'refusal': None})\n","('response_metadata', {'token_usage': {'completion_tokens': 314, 'prompt_tokens': 13, 'total_tokens': 327, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_50906f2aac', 'id': 'chatcmpl-Cik0PIOeM5o7zGS3G4x87D2Q1zQJN', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None})\n","('type', 'ai')\n","('name', None)\n","('id', 'lc_run--93bd761b-dd0f-4856-aab9-e5f5d787528c-0')\n","('tool_calls', [])\n","('invalid_tool_calls', [])\n","('usage_metadata', {'input_tokens': 13, 'output_tokens': 314, 'total_tokens': 327, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n"]}]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"cOJGeiLGJb-g","colab":{"base_uri":"https://localhost:8080/","height":365},"executionInfo":{"status":"ok","timestamp":1764779321312,"user_tz":-60,"elapsed":6,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"0b6596be-e068-4d33-8a70-0e3efe69b864"},"id":"cOJGeiLGJb-g","execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Generative KI, oder generative k√ºnstliche Intelligenz, bezieht sich auf eine Klasse von Algorithmen und Modellen, die in der Lage sind, neue Inhalte zu erzeugen. Diese Inhalte k√∂nnen in verschiedenen Formen vorliegen, darunter Texte, Bilder, Musik, Videos und mehr. Generative KI nutzt oft Techniken des maschinellen Lernens, insbesondere neuronale Netzwerke, um Muster in bestehenden Daten zu erkennen und darauf basierend neue, √§hnliche Daten zu generieren.\n\nEin bekanntes Beispiel f√ºr generative KI sind Sprachmodelle wie GPT (Generative Pre-trained Transformer), die in der Lage sind, menschen√§hnliche Texte zu schreiben. Ein weiteres Beispiel sind generative adversarielle Netzwerke (GANs), die verwendet werden, um realistische Bilder zu erzeugen, indem sie zwei neuronale Netzwerke gegeneinander antreten lassen.\n\nGenerative KI hat viele Anwendungen, darunter:\n\n1. **Kreative Inhalte**: Erstellung von Kunst, Musik, Geschichten und anderen kreativen Arbeiten.\n2. **Datenaugmentation**: Erzeugung zus√§tzlicher Trainingsdaten f√ºr maschinelles Lernen.\n3. **Simulationen**: Erzeugung realistischer Szenarien f√ºr Training und Forschung.\n4. **Personalisierung**: Anpassung von Inhalten an individuelle Nutzerpr√§ferenzen.\n\nDie Technologie hat das Potenzial, viele Bereiche zu revolutionieren, bringt jedoch auch Herausforderungen und ethische Fragen mit sich, insbesondere in Bezug auf Urheberrecht, F√§lschungen und die Verbreitung von Fehlinformationen."},"metadata":{}}]},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(\"Was war meine letzte Frage?\")"],"metadata":{"id":"9fayTzIrNN2b","executionInfo":{"status":"ok","timestamp":1764779322601,"user_tz":-60,"elapsed":1288,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"9fayTzIrNN2b","execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"IahQJ0mFNUUV","colab":{"base_uri":"https://localhost:8080/","height":117},"executionInfo":{"status":"ok","timestamp":1764779322642,"user_tz":-60,"elapsed":16,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"9ce6f82f-78a5-4cd5-e03f-b2a1d95c2559"},"id":"IahQJ0mFNUUV","execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Leider kann ich nicht auf vorherige Interaktionen zugreifen oder sie speichern. Wenn du mir deine letzte Frage erneut stellst, helfe ich dir gerne weiter!"},"metadata":{}}]},{"cell_type":"markdown","source":["**Prompt wird als formatierte String-Variable √ºbergeben:**"],"metadata":{"id":"aKRhzB0oJnUG"},"id":"aKRhzB0oJnUG"},{"cell_type":"code","source":["# f-String f√ºr simple prompt mit template\n","thema = \"Machine Learning\"\n","prompt = f\"Erkl√§re {thema} in einfachen Worten.\""],"metadata":{"id":"8s5cQY2oHUvW","executionInfo":{"status":"ok","timestamp":1764779322687,"user_tz":-60,"elapsed":44,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"8s5cQY2oHUvW","execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(prompt)"],"metadata":{"id":"plYGfaoLJ8DI","executionInfo":{"status":"ok","timestamp":1764779327070,"user_tz":-60,"elapsed":4380,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":13,"outputs":[],"id":"plYGfaoLJ8DI"},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"SLX0zaONJ8DJ","colab":{"base_uri":"https://localhost:8080/","height":216},"executionInfo":{"status":"ok","timestamp":1764779327120,"user_tz":-60,"elapsed":47,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"e6c83b49-03e7-44a8-b022-1b921aced646"},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine Learning, oder maschinelles Lernen, ist ein Teilbereich der K√ºnstlichen Intelligenz, bei dem Computer lernen, Muster und Zusammenh√§nge in Daten zu erkennen, ohne dass sie daf√ºr explizit programmiert werden m√ºssen. \n\nStell dir vor, du zeigst einem Computer viele Bilder von Katzen und Hunden. Anstatt ihm zu sagen, was eine Katze oder ein Hund ist, lernt der Computer selbst, die Unterschiede zu erkennen, indem er die Bilder analysiert. Wenn du ihm sp√§ter ein neues Bild zeigst, kann er dir sagen, ob es eine Katze oder ein Hund ist, basierend auf dem, was er gelernt hat.\n\nIm Grunde genommen geht es beim maschinellen Lernen darum, dass Computer aus Erfahrungen lernen, √§hnlich wie Menschen, und dadurch besser in bestimmten Aufgaben werden."},"metadata":{}}],"id":"SLX0zaONJ8DJ"},{"cell_type":"markdown","source":["**Prompt wird als template √ºbergeben:**"],"metadata":{"id":"n-VVZEHDKSIw"},"id":"n-VVZEHDKSIw"},{"cell_type":"code","source":["# Einfaches Template mit Platzhaltern\n","simple_prompt_template = PromptTemplate.from_template(\n","    \"Erkl√§re {thema} in einfachen Worten.\"\n",")"],"metadata":{"id":"koh0-_3r_HxG","executionInfo":{"status":"ok","timestamp":1764779327130,"user_tz":-60,"elapsed":5,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"koh0-_3r_HxG","execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Prompt-Erstellung\n","thema = \"Generative KI\"\n","prompt = simple_prompt_template.format(thema=thema)\n","prompt"],"metadata":{"id":"nFrCjL6j_xo1","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1764779327154,"user_tz":-60,"elapsed":10,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"c25d4233-0fe8-45da-e86c-605b8b0aa8af"},"id":"nFrCjL6j_xo1","execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Erkl√§re Generative KI in einfachen Worten.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(prompt)"],"metadata":{"id":"-of5FbM02qfZ","executionInfo":{"status":"ok","timestamp":1764779329773,"user_tz":-60,"elapsed":2615,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"-of5FbM02qfZ","execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"lmEE-3NDAE15","colab":{"base_uri":"https://localhost:8080/","height":198},"executionInfo":{"status":"ok","timestamp":1764779329824,"user_tz":-60,"elapsed":45,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"18b25c9f-ff6a-4b51-aa4d-23a1f252adcc"},"id":"lmEE-3NDAE15","execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Generative KI ist eine Art von k√ºnstlicher Intelligenz, die in der Lage ist, neue Inhalte zu erstellen. Das k√∂nnen Texte, Bilder, Musik oder sogar Videos sein. Sie funktioniert, indem sie aus gro√üen Mengen von Daten lernt, wie diese Inhalte normalerweise aussehen oder klingen. \n\nStell dir vor, du hast viele Bilder von Katzen und Hunden. Eine generative KI kann dann lernen, wie diese Tiere aussehen, und schlie√ülich ein ganz neues Bild von einem Tier erstellen, das es noch nie zuvor gegeben hat. \n\nIm Grunde genommen ist generative KI wie ein kreativer Computer, der neue Ideen und Inhalte basierend auf dem, was er gelernt hat, entwickeln kann."},"metadata":{}}]},{"cell_type":"markdown","id":"8akKmCCc06nm","metadata":{"id":"8akKmCCc06nm"},"source":["## 5.2 | ChatPromptTemplate üìå"]},{"cell_type":"markdown","source":["Das **ChatPromptTemplate** ist eine spezielle Prompt-Vorlage f√ºr Chatmodelle (z.‚ÄØB. GPT-3.5, GPT-4), die **mehrere Rollen und Nachrichten** unterst√ºtzt ‚Äì also genau das, was Chat-Modelle eigentlich brauchen.\n","\n","Es geh√∂rt zur LangChain-Bibliothek und baut auf dem Prinzip auf:\n","\n","\n","**Wer sagt was? ‚Üí system, user, assistant, etc.**"],"metadata":{"id":"SVoHwxtrMM_j"},"id":"SVoHwxtrMM_j"},{"cell_type":"markdown","source":["In LangChain unterst√ºtzte Rollen:\n","\n","+ \"system\" - f√ºr Systemnachrichten\n","+ \"assistant\" oder \"ai\" - f√ºr Assistenten-/KI-Antworten\n","+ \"user\" oder \"human\" - f√ºr Benutzernachrichten"],"metadata":{"id":"Po7W2FKz-VEd"},"id":"Po7W2FKz-VEd"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Variante 1: Liste & Tupel üìå\n","</font></p>"],"metadata":{"id":"yw4qr0mC5A6m"},"id":"yw4qr0mC5A6m"},{"cell_type":"code","execution_count":19,"id":"iW9URvON0IXr","metadata":{"id":"iW9URvON0IXr","executionInfo":{"status":"ok","timestamp":1764779329842,"user_tz":-60,"elapsed":16,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Template als Liste & Tupel\n","chat_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    (\"human\", \"Erkl√§re mir {thema}\"),\n","])"]},{"cell_type":"code","source":["# Variablen zuweisen\n","system_prompt = \"Du bist ein hilfreicher und humorvoller Assistent.\"\n","thema = \"Machine Learning\""],"metadata":{"id":"XrEDZ3Vz174e","executionInfo":{"status":"ok","timestamp":1764779329845,"user_tz":-60,"elapsed":4,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"XrEDZ3Vz174e","execution_count":20,"outputs":[]},{"cell_type":"code","source":["# Template & Variablen kombinieren\n","prompt = chat_template.format_messages(system_prompt=system_prompt, thema=thema)"],"metadata":{"id":"x3pxKD_R2BvX","executionInfo":{"status":"ok","timestamp":1764779329865,"user_tz":-60,"elapsed":19,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"x3pxKD_R2BvX","execution_count":21,"outputs":[]},{"cell_type":"code","source":["# Sprachmodell aufrufe\n","response = llm.invoke(prompt)"],"metadata":{"id":"CLU3o3dG4OGM","executionInfo":{"status":"ok","timestamp":1764779343268,"user_tz":-60,"elapsed":13405,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"CLU3o3dG4OGM","execution_count":22,"outputs":[]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"YnfES3Q24n0M","colab":{"base_uri":"https://localhost:8080/","height":482},"executionInfo":{"status":"ok","timestamp":1764779343317,"user_tz":-60,"elapsed":35,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"1e97e166-50fc-4ca5-db2b-75329b2f029f"},"id":"YnfES3Q24n0M","execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Klar, ich erkl√§re dir gerne Machine Learning! \n\nMachine Learning (ML) ist ein Teilbereich der K√ºnstlichen Intelligenz (KI), der es Computern erm√∂glicht, aus Daten zu lernen und Muster zu erkennen, ohne dass sie explizit programmiert werden m√ºssen. Man k√∂nnte sagen, es ist wie das Lernen eines neuen Hobbys ‚Äì je mehr du √ºbst, desto besser wirst du darin!\n\nHier sind die grundlegenden Konzepte:\n\n1. **Daten**: Machine Learning ben√∂tigt Daten, um zu lernen. Das k√∂nnen Bilder, Texte, Zahlen oder sogar T√∂ne sein. Je mehr qualitativ hochwertige Daten du hast, desto besser kann das Modell lernen.\n\n2. **Modelle**: Ein Modell ist eine mathematische Darstellung, die aus den Daten lernt. Es gibt verschiedene Arten von Modellen, je nachdem, was du erreichen m√∂chtest. Zum Beispiel gibt es Entscheidungsb√§ume, neuronale Netze und viele andere.\n\n3. **Training**: W√§hrend des Trainingsprozesses wird das Modell mit den Daten gef√ºttert. Es versucht, Muster zu erkennen und Vorhersagen zu treffen. Wenn es falsch liegt, wird es angepasst, um beim n√§chsten Mal besser abzuschneiden. Das ist wie beim √úben eines Instruments ‚Äì du machst Fehler, lernst daraus und wirst besser!\n\n4. **Testen**: Nach dem Training wird das Modell mit neuen, unbekannten Daten getestet, um zu sehen, wie gut es funktioniert. Hier zeigt sich, ob das Modell wirklich gelernt hat oder ob es nur auswendig gelernt hat (das nennt man √úberanpassung).\n\n5. **Anwendung**: Wenn das Modell gut funktioniert, kann es in der realen Welt eingesetzt werden, zum Beispiel f√ºr Spracherkennung, Bildklassifizierung oder sogar f√ºr die Vorhersage von Aktienkursen.\n\nUnd das Beste daran? Machine Learning kann in vielen Bereichen eingesetzt werden, von der Medizin √ºber die Automobilindustrie bis hin zur Unterhaltung. Es ist wie ein Schweizer Taschenmesser f√ºr Daten ‚Äì vielseitig und immer bereit, dir zu helfen!\n\nWenn du noch spezifischere Fragen hast oder mehr √ºber einen bestimmten Aspekt wissen m√∂chtest, lass es mich wissen!"},"metadata":{}}]},{"cell_type":"markdown","source":["**... oder ...**"],"metadata":{"id":"6FrHMy1PYH0S"},"id":"6FrHMy1PYH0S"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Variante 2: Liste & Dictionary\n","</font></p>"],"metadata":{"id":"lkaCyUqq6OD0"},"id":"lkaCyUqq6OD0"},{"cell_type":"code","source":["# Template als Liste & Dictionary\n","chat_template = ChatPromptTemplate.from_messages([\n","    {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher und humorvoller Assistent.\"},\n","    {\"role\": \"human\", \"content\": \"Erkl√§re mir {thema}\"},\n","])"],"metadata":{"id":"kxEP8r3VDWwI","executionInfo":{"status":"ok","timestamp":1764779343343,"user_tz":-60,"elapsed":23,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"kxEP8r3VDWwI","execution_count":24,"outputs":[]},{"cell_type":"code","source":["# Variablen zuweisen\n","system_prompt = \"Du bist ein hilfreicher und humorvoller Assistent.\"\n","thema = \"Machine Learning\""],"metadata":{"id":"naATTZyS4zEB","executionInfo":{"status":"ok","timestamp":1764779343346,"user_tz":-60,"elapsed":1,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":25,"outputs":[],"id":"naATTZyS4zEB"},{"cell_type":"code","source":["# Template & Variablen kombinieren\n","prompt = chat_template.format_messages(system_prompt=system_prompt, thema=thema)"],"metadata":{"id":"O235HSRl4zEC","executionInfo":{"status":"ok","timestamp":1764779343349,"user_tz":-60,"elapsed":2,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":26,"outputs":[],"id":"O235HSRl4zEC"},{"cell_type":"code","source":["# Sprachmodell aufrufe\n","response = llm.invoke(prompt)"],"metadata":{"id":"Ow6KjCLG4zEC","executionInfo":{"status":"ok","timestamp":1764779353252,"user_tz":-60,"elapsed":9902,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":27,"outputs":[],"id":"Ow6KjCLG4zEC"},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"6PtZzYUSC7-F","colab":{"base_uri":"https://localhost:8080/","height":488},"executionInfo":{"status":"ok","timestamp":1764779353291,"user_tz":-60,"elapsed":5,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"f80a8870-0b9a-4a46-8c9a-491391a29261"},"id":"6PtZzYUSC7-F","execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Klar, ich erkl√§re dir gerne Machine Learning! \n\nStell dir vor, du hast einen Hund, der nicht wei√ü, wie man ‚ÄûSitz‚Äú macht. Du zeigst ihm immer wieder, was er tun soll, und wenn er es richtig macht, gibst du ihm ein Leckerli. Nach einer Weile lernt der Hund, dass ‚ÄûSitz‚Äú bedeutet, sich hinzusetzen, um das Leckerli zu bekommen. \n\nMachine Learning funktioniert √§hnlich, aber anstelle von Hunden arbeiten wir mit Computern. Es ist ein Teilbereich der K√ºnstlichen Intelligenz, bei dem Computer aus Daten lernen, ohne explizit programmiert zu werden. \n\nHier sind die grundlegenden Schritte:\n\n1. **Daten sammeln**: Du brauchst eine Menge Daten, die das Problem beschreiben, das du l√∂sen m√∂chtest. Das k√∂nnen Bilder, Texte, Zahlen oder alles M√∂gliche sein.\n\n2. **Modell w√§hlen**: Du w√§hlst ein Modell aus, das die Beziehung zwischen den Daten und dem gew√ºnschten Ergebnis lernen soll. Das kann ein einfaches lineares Modell oder ein komplexes neuronales Netzwerk sein.\n\n3. **Training**: Das Modell wird mit den gesammelten Daten ‚Äûtrainiert‚Äú. Dabei passt es seine internen Parameter an, um die besten Vorhersagen zu treffen. Hier kommt der Hund ins Spiel: Es ist wie das st√§ndige √úben, bis er ‚ÄûSitz‚Äú versteht.\n\n4. **Testen**: Nach dem Training testest du das Modell mit neuen, unbekannten Daten, um zu sehen, wie gut es gelernt hat. Wenn es gut abschneidet, ist das wie ein Hund, der brav ‚ÄûSitz‚Äú macht, wenn du es ihm sagst!\n\n5. **Anpassen**: Wenn das Modell nicht gut funktioniert, kannst du es anpassen, mehr Daten sammeln oder ein anderes Modell ausprobieren ‚Äì wie beim Hundetraining, wo du vielleicht eine andere Belohnung ausprobieren musst.\n\nMachine Learning wird in vielen Bereichen eingesetzt, von der Spracherkennung √ºber Bildverarbeitung bis hin zu Empfehlungen f√ºr Filme oder Produkte. Es ist ein spannendes Feld, das st√§ndig w√§chst und sich weiterentwickelt!\n\nWenn du noch mehr Fragen hast oder etwas genauer wissen m√∂chtest, sag einfach Bescheid! üê∂üíª"},"metadata":{}}]},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Vergleich: f-string vs. PromptTemplate vs. ChatPromptTemplate\n","</font></p>"],"metadata":{"id":"2uSD48umMXI7"},"id":"2uSD48umMXI7"},{"cell_type":"markdown","source":["\n","\n","| Vorteil                      | `f-string` (Python)           | `PromptTemplate`              | `ChatPromptTemplate`            |\n","| ---------------------------- | ----------------------------- | ----------------------------- | ------------------------------- |\n","| üöª Rollenstruktur            | ‚ùå Keine Unterst√ºtzung         | ‚ùå Nur einfacher Text          | ‚úÖ Klar: `system`, `user`, `ai`  |\n","| üìö Konversationsaufbau       | ‚ùå Nicht geeignet              | ‚ùå Nicht geeignet              | ‚úÖ Ideal f√ºr Multi-Turn-Dialoge <br>    (mehreren Gespr√§chsrunden)  |\n","| üîÑ Kombinierbar mit `Memory` | ‚ùå Nein                        | ‚ùå Nein                        | ‚úÖ Ja, via `MessagesPlaceholder` |\n","| üîå F√ºr Chatmodelle wie GPT   | ‚ö†Ô∏è Manuell als String         | ‚ö†Ô∏è Nur als Flie√ütext          | ‚úÖ Nativ & strukturiert          |\n","| üîç Klarheit & Wartbarkeit    | ‚ö†Ô∏è Inline, schwer erweiterbar | ‚ö†Ô∏è Nur Inline-Text            | ‚úÖ Saubere Message-Trennung      |\n","| üß™ Prompt-Testbarkeit        | ‚ùå Keine Validierung           | ‚ö†Ô∏è Eingeschr√§nkt               | ‚úÖ Besser strukturiert           |\n","| ‚ö° Performance                | ‚úÖ Schnell & direkt            | ‚úÖ Schnell                     | ‚úÖ Schnell                       |\n","| üîß Validierung von Variablen | ‚ùå Keine                       | ‚úÖ Automatisch                 | ‚úÖ Automatisch                   |\n","| üß© Wiederverwendbarkeit      | ‚ùå Eingeschr√§nkt               | ‚úÖ Als Template-Objekt         | ‚úÖ Als Template-Objekt           |\n","| üìù Partial Variables         | ‚ùå Nein                        | ‚úÖ Ja                          | ‚úÖ Ja                            |\n","| üîÄ Chains & Pipelines        | ‚ùå Nicht integrierbar          | ‚úÖ LangChain-kompatibel        | ‚úÖ LangChain-kompatibel          |\n","| üéØ Output Parser             | ‚ùå Manuell                     | ‚úÖ Integrierbar                | ‚úÖ Integrierbar                  |"],"metadata":{"id":"UvoCBqi0qc4B"},"id":"UvoCBqi0qc4B"},{"cell_type":"markdown","source":["\n","\n","**Anwendungsempfehlungen**\n","\n","| Methode               | Wann verwenden?                                                                 |\n","| --------------------- | ------------------------------------------------------------------------------- |\n","| **f-string**          | Einfache, einmalige Prompts ¬∑ Prototyping & schnelle Tests ¬∑ Keine LangChain-Integration |\n","| **PromptTemplate**    | Single-Turn Prompts mit Variablen ¬∑ Completion-Modelle (GPT-3, etc.) ¬∑ Keine Konversation |\n","| **ChatPromptTemplate** | Multi-Turn Dialoge ¬∑ Chat-Modelle (GPT-4, Claude, etc.) ¬∑ Komplexe Konversationen mit Memory |\n"],"metadata":{"id":"IrQUS1yvpsFQ"},"id":"IrQUS1yvpsFQ"},{"cell_type":"markdown","metadata":{"id":"Ps344Ejw0-ih"},"source":["## 5.3 | Few-Shot Prompt ‚ú≥Ô∏è\n"],"id":"Ps344Ejw0-ih"},{"cell_type":"markdown","source":["Few-Shot-Prompting ist eine Technik, bei der man einem KI-Modell einige Beispiele (sogenannte \"Shots\") vorlegt, bevor es eine neue Aufgabe l√∂sen soll. Das Modell kann aus diesen Beispielen das Muster erkennen und auf neue, √§hnliche F√§lle anwenden."],"metadata":{"id":"LKY3IaJSDLvt"},"id":"LKY3IaJSDLvt"},{"cell_type":"markdown","source":["`FewShotPromptTemplate` ist eine Prompt-Vorlage, die automatisch mehrere Beispiele einf√ºgt, bevor das eigentliche Nutzer-Input kommt."],"metadata":{"id":"NnBh2dz8M2fi"},"id":"NnBh2dz8M2fi"},{"cell_type":"code","execution_count":29,"metadata":{"id":"cEopgOiS0KU_","executionInfo":{"status":"ok","timestamp":1764779353316,"user_tz":-60,"elapsed":22,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["beispiele = [\n","    {\"frage\": \"Was ist Python?\", \"antwort\": \"Eine interpretierende Programmiersprache.\"},\n","    {\"frage\": \"Was ist Java?\", \"antwort\": \"Eine objektorientierte Sprache.\"}\n","]"],"id":"cEopgOiS0KU_"},{"cell_type":"code","source":["beispiel_prompt_template = PromptTemplate.from_template(\n","    \"Frage: {frage}\\nAntwort: {antwort}\"\n",")"],"metadata":{"id":"fHEYrEPcDD9S","executionInfo":{"status":"ok","timestamp":1764779353337,"user_tz":-60,"elapsed":19,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":30,"outputs":[],"id":"fHEYrEPcDD9S"},{"cell_type":"code","source":["system_prompt = \"Du bist ein Assistent, der Fragen zu Programmiersprachen beantwortet. Halte deine Antworten kurz und pr√§zise:\""],"metadata":{"id":"eAk2iqPrjynu","executionInfo":{"status":"ok","timestamp":1764779353339,"user_tz":-60,"elapsed":3,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"eAk2iqPrjynu","execution_count":31,"outputs":[]},{"cell_type":"code","source":["# Few-Shot-Prompt\n","few_shot_prompt_template = FewShotPromptTemplate(\n","    examples=beispiele,\n","    example_prompt=beispiel_prompt_template,\n","    prefix=system_prompt,\n","    suffix=\"Frage: {frage}\\nAntwort:\",\n","    input_variables=[\"frage\"]\n",")\n","\n","prompt = few_shot_prompt_template.format(frage=\"Was ist Go?\")"],"metadata":{"id":"BpIbLhIhDcUp","executionInfo":{"status":"ok","timestamp":1764779353342,"user_tz":-60,"elapsed":2,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":32,"outputs":[],"id":"BpIbLhIhDcUp"},{"cell_type":"markdown","source":["**Struktur des finalen Prompts:**\n","\n","```\n","[prefix]\n","[formatiertes Beispiel 1]\n","[formatiertes Beispiel 2]\n","...\n","[suffix mit aktueller Frage]\n","```\n","\n"],"metadata":{"id":"tpDH0jAgknKS"},"id":"tpDH0jAgknKS"},{"cell_type":"code","source":["response = llm.invoke(prompt)"],"metadata":{"id":"KZPoIO6gDo23","executionInfo":{"status":"ok","timestamp":1764779353992,"user_tz":-60,"elapsed":649,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":33,"outputs":[],"id":"KZPoIO6gDo23"},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"Yr0YleeiD_x7","colab":{"base_uri":"https://localhost:8080/","height":99},"executionInfo":{"status":"ok","timestamp":1764779354033,"user_tz":-60,"elapsed":37,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"c09582a0-6ac1-43d3-bb13-fbe59ead379b"},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Eine kompilierte Sprache mit Fokus auf Einfachheit und Effizienz."},"metadata":{}}],"id":"Yr0YleeiD_x7"},{"cell_type":"markdown","source":["**Vorteile eines `FewShotPromptTemplate`**"],"metadata":{"id":"XFbved9KM_WJ"},"id":"XFbved9KM_WJ"},{"cell_type":"markdown","source":["| Vorteil                      | Beschreibung                                                      |\n","| ---------------------------- | ----------------------------------------------------------------- |\n","| üß† Lerneffekt f√ºr das Modell | Das Modell erkennt aus Beispielen, wie es antworten soll          |\n","| üß∞ Flexibilit√§t              | Beispiele k√∂nnen dynamisch gesetzt oder generiert werden          |\n","| üß± Struktur & Konsistenz     | Einheitlicher Aufbau aller Beispiele √ºber `example_prompt`        |\n","| ü™Ñ Kein Training n√∂tig       | Kein Fine-Tuning ‚Äì nur durch Prompting                            |\n","| üõ† Kombinierbar              | Kann mit Templates, Tools, Chains, OutputParser kombiniert werden |\n"],"metadata":{"id":"EWjuAd2_M-E7"},"id":"EWjuAd2_M-E7"},{"cell_type":"markdown","id":"pt37u3cnbgc","source":["## 5.4 | Eigenes Prompt-Template üìå"],"metadata":{"id":"pt37u3cnbgc"}},{"cell_type":"markdown","id":"ik5lbyv7vc","source":["Die `load_chat_prompt_template()` Funktion aus der `genai_lib.utilities` erm√∂glicht das Laden von Prompt-Templates direkt von GitHub oder aus lokalen Dateien.\n","\n","**Vorteile:**\n","- Zentrale Verwaltung von Prompts im `05_prompt/` Verzeichnis\n","- Wiederverwendbarkeit √ºber mehrere Notebooks hinweg\n","- Versionskontrolle durch Git\n","- Einfaches Teilen von Best-Practice-Prompts\n","\n","**Hinweis:** In Google Colab muss die GitHub-URL verwendet werden, da lokale Pfade nicht verf√ºgbar sind."],"metadata":{"id":"ik5lbyv7vc"}},{"cell_type":"markdown","id":"rjewn4pkze","source":["**Template-Struktur (`05_prompt/text_zusammenfassung.py`):**\n","\n","```python\n","# Prompt Template als Python-Datei\n","messages = [\n","    (\n","        \"system\",\n","        \"Du bist ein Experte f√ºr die Erstellung pr√§gnanter Textzusammenfassungen...\"\n","    ),\n","    (\n","        \"human\",\n","        \"Bitte fasse den folgenden Text zusammen:\\n\\n{text}\\n\\n...\"\n","    )\n","]\n","```\n","\n","**Wichtig:** Templates m√ºssen als **Python-Dateien (.py)** erstellt werden und eine Variable `messages` definieren, die eine Liste von Tupeln mit `(role, content)` enth√§lt."],"metadata":{"id":"rjewn4pkze"}},{"cell_type":"code","id":"fl5appj1wrf","source":["from genai_lib.utilities import load_chat_prompt_template\n","\n","# Template von GitHub laden\n","url = \"https://github.com/ralf-42/GenAI/blob/main/05_prompt/text_zusammenfassung.py\"\n","chat_template = load_chat_prompt_template(url)\n","\n","# Template mit Beispieltext testen\n","beispiel_text = \"\"\"\n","LangChain ist ein Framework zur Entwicklung von Anwendungen mit gro√üen Sprachmodellen (LLMs).\n","Es bietet modulare Komponenten f√ºr verschiedene Aufgaben, vorgefertigte Ketten f√ºr komplexe\n","Workflows und eine einfache Integration externer Datenquellen. Mit LangChain k√∂nnen Entwickler\n","Chat-Modelle, Embeddings, Vektordatenbanken und Agents effizient kombinieren, um\n","leistungsstarke KI-Anwendungen zu erstellen.\n","\"\"\"\n","\n","# Prompt mit Variablen f√ºllen\n","prompt = chat_template.format_messages(text=beispiel_text)\n","\n","# Aufruf\n","response = llm.invoke(prompt)"],"metadata":{"id":"fl5appj1wrf","executionInfo":{"status":"ok","timestamp":1764779356326,"user_tz":-60,"elapsed":2288,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","id":"zqaw2rdob6b","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"zqaw2rdob6b","colab":{"base_uri":"https://localhost:8080/","height":134},"executionInfo":{"status":"ok","timestamp":1764779356438,"user_tz":-60,"elapsed":107,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"2f53fbcb-63c8-476a-d3d4-c78ac2e172f8"},"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"LangChain ist ein Framework zur Entwicklung von Anwendungen mit gro√üen Sprachmodellen (LLMs), das modulare Komponenten und vorgefertigte Ketten f√ºr komplexe Workflows bietet. Es erm√∂glicht eine einfache Integration externer Datenquellen und die effiziente Kombination von Chat-Modellen, Embeddings, Vektordatenbanken und Agents. Dadurch k√∂nnen Entwickler leistungsstarke KI-Anwendungen erstellen."},"metadata":{}}]},{"cell_type":"markdown","id":"8c859b9f","metadata":{"id":"8c859b9f"},"source":["# 6 | Weitere Modelle üìå\n","---"]},{"cell_type":"markdown","source":["LangChain ist mit zahlreichen LLMs ‚Äì sowohl eines als auch verschiedener Anbieter ‚Äì kompatibel und bietet vielseitige Konfigurationsoptionen f√ºr individuelle Anwendungsf√§lle."],"metadata":{"id":"5y6J-chfEOPH"},"id":"5y6J-chfEOPH"},{"cell_type":"markdown","source":["**Beispiel `gpt-4o` und `gpt-5-mini`**"],"metadata":{"id":"Oc0UPT5eTyTR"},"id":"Oc0UPT5eTyTR"},{"cell_type":"markdown","source":["+ gpt-4o:    \n","gpt-4o ist ein multimodales KI-Modell von OpenAI, das Text- und Bildeingaben verarbeiten kann und sich durch hohe Geschwindigkeit sowie eine nat√ºrliche, menschen√§hnliche Interaktion auszeichnet ‚Äì ideal f√ºr allt√§gliche Konversationen, kreative Aufgaben und allgemeine Chatanwendungen\n","+ gpt-5-mini:     \n","gpt-5-mini ist die mittlere Modellvariante der GPT-5-Familie, die ein ausgewogenes Verh√§ltnis zwischen Leistungsf√§higkeit und Kosten bietet und f√ºr allt√§gliche Aufgaben, allgemeine Automatisierung sowie kosteng√ºnstige Anwendungen im gro√üen Ma√üstab optimiert wurde"],"metadata":{"id":"f8yl7OApTvcQ"},"id":"f8yl7OApTvcQ"},{"cell_type":"code","source":["# Modelle konfigurieren (Kurznotation: \"provider:model\")\n","chat_model = init_chat_model(\"openai:gpt-4o\")\n","resoning_model = init_chat_model(\"openai:gpt-5-mini\")"],"metadata":{"id":"aDGqhlxoENvs","executionInfo":{"status":"ok","timestamp":1764779356453,"user_tz":-60,"elapsed":11,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"aDGqhlxoENvs","execution_count":37,"outputs":[]},{"cell_type":"code","source":["# Gemeinsamer simple prompt\n","prompt = \"Was ist der Unterschied zwischen Prompting und Retrieval-Augmented-Generation?\""],"metadata":{"id":"PywoNdLrPCfO","executionInfo":{"status":"ok","timestamp":1764779356472,"user_tz":-60,"elapsed":3,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"PywoNdLrPCfO","execution_count":38,"outputs":[]},{"cell_type":"code","source":["# Aufruf\n","response = chat_model.invoke(prompt)"],"metadata":{"id":"iriw2JZPOazT","executionInfo":{"status":"ok","timestamp":1764779366199,"user_tz":-60,"elapsed":9726,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"iriw2JZPOazT","execution_count":39,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Chat-Model:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"9OlEy8jQPck1","colab":{"base_uri":"https://localhost:8080/","height":459},"executionInfo":{"status":"ok","timestamp":1764779366259,"user_tz":-60,"elapsed":30,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"2504d466-91f1-4315-9cea-f223d9ad36bb"},"id":"9OlEy8jQPck1","execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Chat-Model:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Sowohl Prompting als auch Retrieval-Augmented Generation (RAG) sind Techniken, die im Bereich der Verarbeitung nat√ºrlicher Sprache und bei der Nutzung von KI-Modellen wie GPT-3 oder √§hnlichen Modellen angewendet werden. Beide Ans√§tze haben jedoch unterschiedliche Ziele und Funktionsweisen:\n\n**Prompting:**\n- Beim Prompting wird ein Sprachmodell direkt durch Eingabe eines Prompts, also eines Texteingabefragmentes, gefragt, eine Aufgabe auszuf√ºhren. Der Prompt enth√§lt in der Regel Hinweise oder Instruktionen, die dem Modell helfen sollen, die gew√ºnschte Ausgabe zu erzeugen.\n- Die Wirksamkeit der Ausgabe h√§ngt stark von der Qualit√§t und Klarheit des Prompts ab. Ein gut gestalteter Prompt kann zu besseren und relevanteren Antworten f√ºhren.\n- Diese Methode nutzt das bereits in das Modell einprogrammierte Wissen und die F√§higkeiten und ist in Bezug auf externe Informationen begrenzt auf das, was das Modell bis zum Zeitpunkt seines Trainings gelernt hat.\n\n**Retrieval-Augmented Generation (RAG):**\n- RAG kombiniert das Sprachmodell mit einem Informationsabrufsystem, um die Generierung zu verbessern. Bevor das Modell eine Antwort generiert, sucht es in einer Datenbank oder einem Informationsspeicher nach relevanten Informationen.\n- Das Abrufmodul durchsucht externe Quellen oder Datenbanken, um aktuelle oder detaillierte Informationen bereitzustellen, die dann dem Sprachmodell als Kontext f√ºr die Generierung der Antwort zur Verf√ºgung gestellt werden.\n- Diese Methode ist besonders n√ºtzlich, um auf Anfragen zu antworten, bei denen aktuelle oder spezialisierte Informationen ben√∂tigt werden, die nicht im urspr√ºnglichen Trainingssatz des Modells enthalten sind.\n\nZusammengefasst besteht der Hauptunterschied darin, dass Prompting das Modell allein mit einem speziellen Eingabefragment leitet, w√§hrend RAG zus√§tzlich auf externe Informationen zugreift, um die Antwort des Modells zu verbessern und zu bereichern."},"metadata":{}}]},{"cell_type":"code","source":["# Aufruf\n","response = resoning_model.invoke(prompt)"],"metadata":{"id":"_Znkwm2SPOrX","executionInfo":{"status":"ok","timestamp":1764779382972,"user_tz":-60,"elapsed":16717,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"_Znkwm2SPOrX","execution_count":41,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Reasoning-Model:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"N0hcQkSfQyVK","colab":{"base_uri":"https://localhost:8080/","height":977},"executionInfo":{"status":"ok","timestamp":1764779383090,"user_tz":-60,"elapsed":82,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"4efe55b0-8ade-467c-a5b8-d2ef8994dd47"},"id":"N0hcQkSfQyVK","execution_count":42,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Reasoning-Model:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Kurz gesagt:\n\n- Prompting = du gibst dem Modell direkt eine Eingabe (Prompt) und das Modell antwortet allein auf Basis seines internen Wissens und der im Prompt enthaltenen Instruktionen (closed‚Äëbook).\n- Retrieval‚ÄëAugmented‚ÄëGeneration (RAG) = das Modell kombiniert seine Generationsf√§higkeiten mit externem, abfragbarem Wissen: zuerst werden relevante Dokumente/Passagen aus einem Index abgerufen, dann wird mit diesen Kontextinformationen eine Antwort generiert (open‚Äëbook).\n\nWesentliche Unterschiede und Eigenschaften\n\n- Quellen:\n  - Prompting: nur das im Modell gelernte Wissen (bis zu seinem Trainingscutoff).\n  - RAG: nutzt externe, aktuelle oder domain‚Äëspezifische Quellen (Dokumente, Datenbanken, Webseiten).\n- Ablauf:\n  - Prompting: Prompt ‚Üí Modell ‚Üí Antwort.\n  - RAG: Query ‚Üí Retriever (z. B. Vektor‚ÄëSearch, BM25) ‚Üí relevante Texte ‚Üí Generator nimmt Texte als Kontext im Prompt ‚Üí Antwort (oft mit Zitaten).\n- Faktenhaftigkeit:\n  - Prompting neigt eher zu Halluzinationen bei faktischen Fragen au√üerhalb des Trainingswissens.\n  - RAG verbessert Korrektheit und Nachvollziehbarkeit, weil Antworten auf konkrete Quellen gest√ºtzt werden k√∂nnen.\n- Aktualit√§t:\n  - Prompting ist limitiert durch das Trainingsdatum des Modells.\n  - RAG erm√∂glicht Zugriff auf aktuelle Informationen, solange der Index aktuell ist.\n- Komplexit√§t/Implementierung:\n  - Prompting ist einfach: nur Promptformulierung und evtl. few‚Äëshot Beispiele.\n  - RAG braucht Indexierung (Embeddings), Retrieval‚ÄëInfrastruktur, ggf. Dokument‚ÄëPreprocessing, und Prompt‚ÄëPipelines ‚Äî h√∂herer Implementierungsaufwand.\n- Kosten und Latenz:\n  - Prompting: meist schneller und g√ºnstiger pro Anfrage.\n  - RAG: zus√§tzlicher Kosten- und Latenz‚ÄëOverhead f√ºr Embeddings, Suche und l√§ngere Prompts.\n- Transparenz und Nachvollziehbarkeit:\n  - Prompting: schwerer zu belegen, woher ein Fakt stammt.\n  - RAG: leichter mit Quellenangaben, bessere Auditierbarkeit.\n\nTypische Einsatzf√§lle\n- Prompting: kreative Texte, √úbersetzungen, allgemeine Erkl√§rungen, schnelle Prototypen, wenn keine spezielle oder aktuelle Wissensbasis n√∂tig ist.\n- RAG: Frage‚ÄëAntworting √ºber Firmen‚ÄëDocs, Support‚ÄëBots mit Zugriff auf Knowledge‚ÄëBase, Forschungssuche, alles, wo korrekte, belegbare oder aktuelle Fakten wichtig sind.\n\nKurzbeispiel\n- Prompting: ‚ÄûErkl√§re Quantenverschr√§nkung f√ºr Laien.‚Äú\n- RAG: System sucht in Unternehmens‚ÄëHandb√ºchern nach relevanten Abschnitten und antwortet: ‚ÄûLaut Dokument X, Abschnitt Y, bedeutet ‚Ä¶‚Äú und f√ºgt ggf. Zitat/Link hinzu.\n\nHinweis\nOft kombiniert man beides: retrieved Texte werden in den Prompt eingebettet (context window) ‚Äî das ist die praktische Umsetzung von RAG. Au√üerdem gibt es Varianten (hybride Retriever, reranking, h√∂chstwahrscheinlichkeitsbasierte Auswahl), und Datenschutz/Vertraulichkeit sind bei RAG wichtig, wenn interne Dokumente verwendet oder an externe APIs geschickt werden.\n\nWenn du magst, kann ich dir ein konkretes Architektur‚ÄëBeispiel oder eine einfache Schritt‚Äëf√ºr‚ÄëSchritt‚ÄëAnleitung zum Aufbau einer RAG‚ÄëPipeline geben."},"metadata":{}}]},{"cell_type":"markdown","source":["**üìä Vergleich: Chat-Modell vs. Reasoning-Modell**"],"metadata":{"id":"YJljcFw4Rz6W"},"id":"YJljcFw4Rz6W"},{"cell_type":"markdown","source":["| **Aspekt**                      | **Chat-Modell**                                                           | **Reasoning-Modell**                                                                                |\n","| ------------------------------- | ------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------- |\n","| **Stil**                        | Erkl√§rend, klassisch strukturiert mit Flie√ütext                           | Punktuell, technisch und systematisch                                                               |\n","| **Formatierung**                | Flie√ütext mit Abs√§tzen und Beispielen                                     | Aufz√§hlungen (‚Ä¢) und strukturierte Unterpunkte                                                      |\n","| **Definition Prompting**        | Kurze Definition als Text, danach Beispiel                                | Technisch klar: ‚Äûparametrisches Wissen‚Äú, Rolle der Prompt-Formulierung betont                       |\n","| **Definition RAG**              | Flie√ütext-Erkl√§rung mit Ablaufbeschreibung und Beispiel                   | Detaillierte Schritte: Abruf, Kontextintegration, Vorteile bzgl. Aktualit√§t und Pr√§zision erl√§utert |\n","| **Beispiele**                   | Ja ‚Äì einfache, praxisnahe Beispiele (z.‚ÄØB. Klimawandel, Quantencomputing) | Nein ‚Äì keine konkreten Beispiele, aber kontextbezogene Beschreibung                                 |\n","| **Vergleich Prompting vs. RAG** | Abstrakte Gegen√ºberstellung in einem abschlie√üenden Absatz                | Direkter Vergleich im letzten Abschnitt mit Betonung der Wissensquellen                             |\n","| **Technische Tiefe**            | Mittel ‚Äì f√ºr Einsteiger\\:innen gut verst√§ndlich                           | Hoch ‚Äì Begriffe wie ‚Äûparametrisches Wissen‚Äú, Fokus auf Architektur                                  |\n","| **Zielgruppe**                  | Allgemeines Publikum, Einstieg in das Thema                               | Fortgeschrittene Nutzer\\:innen, Technik-affine Leserschaft                                          |\n","| **St√§rken des Modells**         | Klare Erkl√§rung, gute didaktische Struktur                                | Pr√§zise Begriffsabgrenzung, hohe Informationsdichte                                                 |\n","\n"],"metadata":{"id":"OzPIlXnqR2Fq"},"id":"OzPIlXnqR2Fq"},{"cell_type":"markdown","source":["**üìä Vergleich: Chat-Modelle**"],"metadata":{"id":"GO40ayL6plxO"},"id":"GO40ayL6plxO"},{"cell_type":"markdown","source":["| Model              | Creator    | Open?  | API Name(s) (falls verf√ºgbar)                            | Context Window             |\n","| ------------------ | ---------- | ------ | -------------------------------------------------------- | -------------------------- |\n","| **GPT-5**          | OpenAI     | Closed | `gpt-5`, `gpt-5-mini`, `gpt-5-nano`, `gpt-5-chat-latest` | 400K Tokens (~296 Seiten)  |\n","| **GPT-4.1**        | OpenAI     | Closed | `gpt-4.1`, `gpt-4.1-mini`, `gpt-4.1-nano`                | 1M Tokens (~741 Seiten)    |\n","| **GPT-4o**         | OpenAI     | Closed | `gpt-4o`, `gpt-4o-mini`                                  | 128K Tokens (~95 Seiten)   |\n","| **GPT-4 Turbo**    | OpenAI     | Closed | `gpt-4-turbo`, `gpt-4-turbo-2024-04-09`                  | 128K Tokens (~95 Seiten)   |\n","| **gpt-oss-120b**   | OpenAI     | Open   | ‚Äî                                                        | 131K Tokens (~97 Seiten)   |\n","| **gpt-oss-20b**    | OpenAI     | Open   | ‚Äî                                                        | 131K Tokens (~97 Seiten)   |\n","| **Claude 4**       | Anthropic  | Closed | `claude-sonnet-4-20250514`, `claude-opus-4-20250514`     | 200K Tokens (~148 Seiten)  |\n","| **Gemini 2.5 Pro** | Google     | Closed | `gemini-2.5-pro`, `gemini-2.5-pro-exp-03-25`             | 1M Tokens (~741 Seiten)    |\n","| **Llama 3.1 405B** | Meta       | Open   | ‚Äî                                                        | 512K Tokens (~379 Seiten)  |\n","| **DeepSeek-V3**    | DeepSeek   | Open   | ‚Äî                                                        | 128K Tokens (~95 Seiten)   |\n","| **Mistral 7B**     | Mistral.AI | Open   | ‚Äî                                                        | 32K Tokens (~24 Seiten)    |\n","\n","<br>\n","\n","*Umrechnung Token/Seiten:  ~1.350 Tokens ‚âà 1 Seite deutschsprachiger Text*"],"metadata":{"id":"RpcsYzLApH7v"},"id":"RpcsYzLApH7v"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","\n","Ein anderer Modell-Provider: `groq`\n","</font></p>"],"metadata":{"id":"8y5xNtM1oMs3"},"id":"8y5xNtM1oMs3"},{"cell_type":"code","source":["install_packages([('langchain-groq', 'langchain_groq')])"],"metadata":{"id":"ec6V1NJRnjPv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764779383478,"user_tz":-60,"elapsed":385,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"b36bef32-4acb-4d75-9bc3-10139dd29b3f"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["üîÑ Installiere langchain-groq...\n","‚úÖ langchain-groq erfolgreich installiert und importiert\n"]}],"id":"ec6V1NJRnjPv"},{"cell_type":"code","source":["setup_api_keys(['GROQ_API_KEY'])"],"metadata":{"id":"wXDH1ysxfo_K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764779383958,"user_tz":-60,"elapsed":478,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"cc8bac52-4d12-419f-93ba-a063aae841e4"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úì GROQ_API_KEY erfolgreich gesetzt\n"]}],"id":"wXDH1ysxfo_K"},{"cell_type":"code","source":["# Kurznotation: \"provider:model\"\n","llm_groq = init_chat_model(\"groq:qwen/qwen3-32b\", temperature=0)"],"metadata":{"id":"cVSRS2Kaf1M7","executionInfo":{"status":"ok","timestamp":1764779384462,"user_tz":-60,"elapsed":486,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":45,"outputs":[],"id":"cVSRS2Kaf1M7"},{"cell_type":"code","source":["response_groq = llm_groq.invoke(\"Wer war Albert Einstein? Bitte antworte in deutscher Sprache.\")"],"metadata":{"id":"oRl7X3y-nm92","executionInfo":{"status":"ok","timestamp":1764779442033,"user_tz":-60,"elapsed":1818,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":63,"outputs":[],"id":"oRl7X3y-nm92"},{"cell_type":"code","source":["mprint(response_groq.content)"],"metadata":{"id":"fhwKoyS3n0ho","colab":{"base_uri":"https://localhost:8080/","height":505},"executionInfo":{"status":"ok","timestamp":1764780202986,"user_tz":-60,"elapsed":73,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"6cb596f2-e3e7-47c6-dbc2-b014ba845917"},"execution_count":71,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"<think>\nOkay, the user is asking who Albert Einstein was and wants the answer in German. Let me start by recalling the basics. Einstein was a physicist, right? He's famous for the theory of relativity. I should mention his birth and death dates. He was born in 1879 in Germany and died in 1955 in the US. He won the Nobel Prize in Physics in 1921 for the photoelectric effect. Oh, and he developed both the special and general theories of relativity. Maybe include E=mc¬≤ as a key equation. Also, he was a pacifist and a human rights advocate. He emigrated to the US to escape Nazi Germany. Need to make sure the German terms are correct, like \"Theorie der Relativit√§t\" and \"Photoelektrischer Effekt\". Check if there are any other important contributions or aspects to include. Maybe his work on Brownian motion? Also, his role in the Manhattan Project? Wait, he wrote a letter to FDR about nuclear weapons, but he wasn't directly involved in the project. Should clarify that. Make sure the answer is concise but covers the main points. Avoid any technical jargon that might be too complex. Use simple German sentences. Double-check the dates and facts for accuracy. Alright, structure the answer with his birth, education, major works, Nobel Prize, emigration, and legacy. Keep it in German as requested.\n</think>\n\nAlbert Einstein (*14. April 1879 in Ulm; ‚Ä†18. April 1955 in Princeton, New Jersey) war ein deutscher Physiker, der als einer der einflussreichsten Wissenschaftler des 20. Jahrhunderts gilt. Er revolutionierte die Physik mit seiner **Theorie der Relativit√§t**, die zwei grundlegende Konzepte umfasste:  \n\n1. **Spezielle Relativit√§tstheorie (1905)**: Erkl√§rte, dass Raum und Zeit relativ zum Beobachter sind und die Lichtgeschwindigkeit konstant bleibt. Bekannt ist die Formel **E = mc¬≤**, die die √Ñquivalenz von Masse und Energie beschreibt.  \n2. **Allgemeine Relativit√§tstheorie (1915)**: Beschrieb die Gravitation als Kr√ºmmung der Raumzeit durch Masse und Energie.  \n\nEinstein erhielt 1921 den **Nobelpreis f√ºr Physik** f√ºr seine Erkl√§rung des **photoelektrischen Effekts**, die die Quantentheorie ma√ügeblich vorantrieb.  \n\n### Weitere Aspekte seines Lebens:  \n- **Flucht vor dem Nationalsozialismus**: Er emigrierte 1933 nach den USA und lebte in Princeton.  \n- **Engagement f√ºr Frieden und Menschenrechte**: Er war ein lebenslanger Pazifist und setzte sich f√ºr soziale Gerechtigkeit ein.  \n- **Brief an Roosevelt (1939)**: Er warnte vor der Gefahr von Atomwaffen und regte indirekt die Entwicklung der Atombombe an, bedauerte dies sp√§ter aber.  \n\nEinstein gilt nicht nur als Wissenschaftler, sondern auch als Symbol f√ºr Kreativit√§t und kritisches Denken. Seine Arbeiten pr√§gten die moderne Physik und beeinflussten Philosophie, Technologie und Kultur."},"metadata":{}}],"id":"fhwKoyS3n0ho"},{"cell_type":"markdown","id":"c314ca71","metadata":{"id":"c314ca71"},"source":["# 7 | Chain üìå\n","---"]},{"cell_type":"markdown","source":["Chains verbinden mehrere Komponenten/Bausteine zu einer Verarbeitungskette. LangChain Expression Language (LCEL) ist das **neue Programmiermodell in LangChain v0.3+**, das die Entwicklung von LLM-Anwendungen vereinfacht.\n"],"metadata":{"id":"cEEMxhSBHrXA"},"id":"cEEMxhSBHrXA"},{"cell_type":"markdown","source":["\n","**Hier die Kernpunkte:**\n","\n","1. Was ist LCEL?\n","```python\n","# LCEL nutzt den Pipe-Operator (|) f√ºr klare Verkettungen\n","chain = prompt | model\n","```\n","- Eine deklarative Sprache zum Verketten von LangChain-Komponenten\n","- Erm√∂glicht linearen Datenfluss zwischen Komponenten\n","- Basiert auf dem Pipe-Operator (|) f√ºr intuitive Verbindungen\n","\n","2. Warum LCEL nutzen?\n","- Bessere Lesbarkeit des Codes\n","- Einfachere Wartung und Debugging\n","- Verbesserte Performance durch optimierte Ausf√ºhrung\n","- Bessere Typsicherheit und Fehlererkennung\n","- Unterst√ºtzt modernes Streaming und Async-Operationen\n","\n","3. Praktisches Beispiel:"],"metadata":{"id":"ltTQItwXH6w8"},"id":"ltTQItwXH6w8"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Simple Chain üìå\n","</font></p>\n"],"metadata":{"id":"BndZGbyWKs1C"},"id":"BndZGbyWKs1C"},{"cell_type":"code","id":"fsryxusc3p8","source":["#@markdown   <p><font size=\"4\" color='green'>   Mermaid - LCEL Chain-Komposition</font> </br></p>\n","\n","diagram = \"\"\"\n","graph LR\n","    A[\"Input Dictionary<br/>{user_input: ...}\"] --> B[\"Prompt Template\"]\n","    B --> C[\"Formatierter Prompt<br/>System + Human Message\"]\n","    C --> D[\"LLM\"]\n","    D --> E[\"AI Message\"]\n","    E --> F[\"StrOutputParser\"]\n","    F --> G[\"String Output\"]\n","\n","    H[\"chain = prompt | llm | parser\"] -.-> B\n","    H -.-> D\n","    H -.-> F\n","\n","    style A fill:#e1f5ff\n","    style B fill:#fff4e1\n","    style C fill:#f0f0f0\n","    style D fill:#ffe1f5\n","    style E fill:#f0f0f0\n","    style F fill:#e1ffe1\n","    style G fill:#f0f0f0\n","    style H fill:#ffe1e1\n","\"\"\"\n","mermaid(diagram, width=900, height=250)"],"metadata":{"id":"fsryxusc3p8","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1764779390825,"user_tz":-60,"elapsed":2621,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"caad370b-1089-461d-ce21-a39e9c120ca9"},"execution_count":48,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.SVG object>"],"image/svg+xml":"<svg xmlns=\"http://www.w3.org/2000/svg\" aria-roledescription=\"flowchart-v2\" role=\"graphics-document document\" viewBox=\"0 0 1567.953125 226\" class=\"flowchart\" id=\"container\" width=\"900\" height=\"250\"><style>#container{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:16px;fill:#333;}@keyframes edge-animation-frame{from{stroke-dashoffset:0;}}@keyframes dash{to{stroke-dashoffset:0;}}#container .edge-animation-slow{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 50s linear infinite;stroke-linecap:round;}#container .edge-animation-fast{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 20s linear infinite;stroke-linecap:round;}#container .error-icon{fill:#552222;}#container .error-text{fill:#552222;stroke:#552222;}#container .edge-thickness-normal{stroke-width:1px;}#container .edge-thickness-thick{stroke-width:3.5px;}#container .edge-pattern-solid{stroke-dasharray:0;}#container .edge-thickness-invisible{stroke-width:0;fill:none;}#container .edge-pattern-dashed{stroke-dasharray:3;}#container .edge-pattern-dotted{stroke-dasharray:2;}#container .marker{fill:#333333;stroke:#333333;}#container .marker.cross{stroke:#333333;}#container svg{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:16px;}#container p{margin:0;}#container .label{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;color:#333;}#container .cluster-label text{fill:#333;}#container .cluster-label span{color:#333;}#container .cluster-label span p{background-color:transparent;}#container .label text,#container span{fill:#333;color:#333;}#container .node rect,#container .node circle,#container .node ellipse,#container .node polygon,#container .node path{fill:#ECECFF;stroke:#9370DB;stroke-width:1px;}#container .rough-node .label text,#container .node .label text,#container .image-shape .label,#container .icon-shape .label{text-anchor:middle;}#container .node .katex path{fill:#000;stroke:#000;stroke-width:1px;}#container .rough-node .label,#container .node .label,#container .image-shape .label,#container .icon-shape .label{text-align:center;}#container .node.clickable{cursor:pointer;}#container .root .anchor path{fill:#333333!important;stroke-width:0;stroke:#333333;}#container .arrowheadPath{fill:#333333;}#container .edgePath .path{stroke:#333333;stroke-width:2.0px;}#container .flowchart-link{stroke:#333333;fill:none;}#container .edgeLabel{background-color:rgba(232,232,232, 0.8);text-align:center;}#container .edgeLabel p{background-color:rgba(232,232,232, 0.8);}#container .edgeLabel rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#container .labelBkg{background-color:rgba(232, 232, 232, 0.5);}#container .cluster rect{fill:#ffffde;stroke:#aaaa33;stroke-width:1px;}#container .cluster text{fill:#333;}#container .cluster span{color:#333;}#container div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:12px;background:hsl(80, 100%, 96.2745098039%);border:1px solid #aaaa33;border-radius:2px;pointer-events:none;z-index:100;}#container .flowchartTitleText{text-anchor:middle;font-size:18px;fill:#333;}#container rect.text{fill:none;stroke-width:0;}#container .icon-shape,#container .image-shape{background-color:rgba(232,232,232, 0.8);text-align:center;}#container .icon-shape p,#container .image-shape p{background-color:rgba(232,232,232, 0.8);padding:2px;}#container .icon-shape rect,#container .image-shape rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#container :root{--mermaid-font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;}</style><g><marker orient=\"auto\" markerHeight=\"8\" markerWidth=\"8\" markerUnits=\"userSpaceOnUse\" refY=\"5\" refX=\"5\" viewBox=\"0 0 10 10\" class=\"marker flowchart-v2\" id=\"container_flowchart-v2-pointEnd\"><path style=\"stroke-width: 1; stroke-dasharray: 1, 0;\" class=\"arrowMarkerPath\" d=\"M 0 0 L 10 5 L 0 10 z\"/></marker><marker orient=\"auto\" markerHeight=\"8\" markerWidth=\"8\" markerUnits=\"userSpaceOnUse\" refY=\"5\" refX=\"4.5\" viewBox=\"0 0 10 10\" class=\"marker flowchart-v2\" id=\"container_flowchart-v2-pointStart\"><path style=\"stroke-width: 1; stroke-dasharray: 1, 0;\" class=\"arrowMarkerPath\" d=\"M 0 5 L 10 10 L 10 0 z\"/></marker><marker orient=\"auto\" markerHeight=\"11\" markerWidth=\"11\" markerUnits=\"userSpaceOnUse\" refY=\"5\" refX=\"11\" viewBox=\"0 0 10 10\" class=\"marker flowchart-v2\" id=\"container_flowchart-v2-circleEnd\"><circle style=\"stroke-width: 1; stroke-dasharray: 1, 0;\" class=\"arrowMarkerPath\" r=\"5\" cy=\"5\" cx=\"5\"/></marker><marker orient=\"auto\" markerHeight=\"11\" markerWidth=\"11\" markerUnits=\"userSpaceOnUse\" refY=\"5\" refX=\"-1\" viewBox=\"0 0 10 10\" class=\"marker flowchart-v2\" id=\"container_flowchart-v2-circleStart\"><circle style=\"stroke-width: 1; stroke-dasharray: 1, 0;\" class=\"arrowMarkerPath\" r=\"5\" cy=\"5\" cx=\"5\"/></marker><marker orient=\"auto\" markerHeight=\"11\" markerWidth=\"11\" markerUnits=\"userSpaceOnUse\" refY=\"5.2\" refX=\"12\" viewBox=\"0 0 11 11\" class=\"marker cross flowchart-v2\" id=\"container_flowchart-v2-crossEnd\"><path style=\"stroke-width: 2; stroke-dasharray: 1, 0;\" class=\"arrowMarkerPath\" d=\"M 1,1 l 9,9 M 10,1 l -9,9\"/></marker><marker orient=\"auto\" markerHeight=\"11\" markerWidth=\"11\" markerUnits=\"userSpaceOnUse\" refY=\"5.2\" refX=\"-1\" viewBox=\"0 0 11 11\" class=\"marker cross flowchart-v2\" id=\"container_flowchart-v2-crossStart\"><path style=\"stroke-width: 2; stroke-dasharray: 1, 0;\" class=\"arrowMarkerPath\" d=\"M 1,1 l 9,9 M 10,1 l -9,9\"/></marker><g class=\"root\"><g class=\"clusters\"/><g class=\"edgePaths\"><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_A_B_0\" d=\"M226.578,47L237.648,47C248.719,47,270.859,47,285.43,47.12C300.001,47.239,307.002,47.478,310.502,47.598L314.002,47.718\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_B_C_0\" d=\"M502.156,51L506.323,51C510.49,51,518.823,51,526.49,51C534.156,51,541.156,51,544.656,51L548.156,51\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_C_D_0\" d=\"M797.734,51L801.901,51C806.068,51,814.401,51,822.288,53.315C830.175,55.629,837.615,60.258,841.336,62.573L845.056,64.887\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_D_E_0\" d=\"M935.969,94L940.135,94C944.302,94,952.635,94,960.302,94C967.969,94,974.969,94,978.469,94L981.969,94\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_E_F_0\" d=\"M1125.219,94L1129.385,94C1133.552,94,1141.885,94,1149.576,94.963C1157.266,95.926,1164.313,97.852,1167.837,98.815L1171.36,99.778\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_F_G_0\" d=\"M1352.078,125L1356.245,125C1360.411,125,1368.745,125,1376.411,125C1384.078,125,1391.078,125,1394.578,125L1398.078,125\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-dotted edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_H_B_0\" d=\"M226.897,140L237.914,135.167C248.931,130.333,270.966,120.667,292.122,110.637C313.278,100.608,333.555,90.216,343.694,85.02L353.833,79.824\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-dotted edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_H_D_0\" d=\"M268,161.387L272.167,160.823C276.333,160.258,284.667,159.129,308.346,158.565C332.026,158,371.052,158,410.078,158C449.104,158,488.13,158,532.275,158C576.419,158,625.682,158,674.945,158C724.208,158,773.471,158,804.273,152.286C835.076,146.573,847.417,135.145,853.587,129.431L859.758,123.718\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-dotted edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_H_F_0\" d=\"M268,195.774L272.167,196.312C276.333,196.849,284.667,197.925,308.346,198.462C332.026,199,371.052,199,410.078,199C449.104,199,488.13,199,532.275,199C576.419,199,625.682,199,674.945,199C724.208,199,773.471,199,809.622,199C845.773,199,868.813,199,891.852,199C914.891,199,937.93,199,965.22,199C992.51,199,1024.052,199,1055.594,199C1087.135,199,1118.677,199,1145.897,191.531C1173.116,184.062,1196.014,169.124,1207.463,161.655L1218.912,154.186\"/></g><g class=\"edgeLabels\"><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g></g><g class=\"nodes\"><g transform=\"translate(138, 47)\" id=\"flowchart-A-0\" class=\"node default\"><rect height=\"78\" width=\"177.15625\" y=\"-39\" x=\"-88.578125\" style=\"fill:#e1f5ff !important\" class=\"basic label-container\"/><g transform=\"translate(-58.578125, -24)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"48\" width=\"117.15625\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Input Dictionary<br/>{user_input: ...}</p></span></div></foreignObject></g></g><g transform=\"translate(410.078125, 51)\" id=\"flowchart-B-1\" class=\"node default\"><rect height=\"54\" width=\"184.15625\" y=\"-27\" x=\"-92.078125\" style=\"fill:#fff4e1 !important\" class=\"basic label-container\"/><g transform=\"translate(-62.078125, -12)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"24\" width=\"124.15625\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Prompt Template</p></span></div></foreignObject></g></g><g transform=\"translate(674.9453125, 51)\" id=\"flowchart-C-3\" class=\"node default\"><rect height=\"78\" width=\"245.578125\" y=\"-39\" x=\"-122.7890625\" style=\"fill:#f0f0f0 !important\" class=\"basic label-container\"/><g transform=\"translate(-92.7890625, -24)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"48\" width=\"185.578125\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Formatierter Prompt<br/>System + Human Message</p></span></div></foreignObject></g></g><g transform=\"translate(891.8515625, 94)\" id=\"flowchart-D-5\" class=\"node default\"><rect height=\"54\" width=\"88.234375\" y=\"-27\" x=\"-44.1171875\" style=\"fill:#ffe1f5 !important\" class=\"basic label-container\"/><g transform=\"translate(-14.1171875, -12)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"24\" width=\"28.234375\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>LLM</p></span></div></foreignObject></g></g><g transform=\"translate(1055.59375, 94)\" id=\"flowchart-E-7\" class=\"node default\"><rect height=\"54\" width=\"139.25\" y=\"-27\" x=\"-69.625\" style=\"fill:#f0f0f0 !important\" class=\"basic label-container\"/><g transform=\"translate(-39.625, -12)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"24\" width=\"79.25\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>AI Message</p></span></div></foreignObject></g></g><g transform=\"translate(1263.6484375, 125)\" id=\"flowchart-F-9\" class=\"node default\"><rect height=\"54\" width=\"176.859375\" y=\"-27\" x=\"-88.4296875\" style=\"fill:#e1ffe1 !important\" class=\"basic label-container\"/><g transform=\"translate(-58.4296875, -12)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"24\" width=\"116.859375\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>StrOutputParser</p></span></div></foreignObject></g></g><g transform=\"translate(1481.015625, 125)\" id=\"flowchart-G-11\" class=\"node default\"><rect height=\"54\" width=\"157.875\" y=\"-27\" x=\"-78.9375\" style=\"fill:#f0f0f0 !important\" class=\"basic label-container\"/><g transform=\"translate(-48.9375, -12)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"24\" width=\"97.875\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>String Output</p></span></div></foreignObject></g></g><g transform=\"translate(138, 179)\" id=\"flowchart-H-12\" class=\"node default\"><rect height=\"78\" width=\"260\" y=\"-39\" x=\"-130\" style=\"fill:#ffe1e1 !important\" class=\"basic label-container\"/><g transform=\"translate(-100, -24)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"48\" width=\"200\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\"><span class=\"nodeLabel\"><p>chain = prompt | llm | parser</p></span></div></foreignObject></g></g></g></g></g></svg>"},"metadata":{}}]},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","# 1. Prompt-Template\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"human\", \"Erkl√§re mir {user_input}\")\n","])\n","\n","# 2. Modell (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0)\n","\n","#  3. Einfache LCEL-Kette\n","chain = prompt | llm\n","\n","# 4. Ausf√ºhrung\n","response = chain.invoke({\"user_input\": \"LangChain Expression Language\"})"],"metadata":{"id":"PsWtjWROIW9d","executionInfo":{"status":"ok","timestamp":1764779401183,"user_tz":-60,"elapsed":10352,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"PsWtjWROIW9d","execution_count":49,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"VWYAG9aWJpZd","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1764779401196,"user_tz":-60,"elapsed":9,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"d9d26236-4c90-42a5-c707-69f526f44d04"},"id":"VWYAG9aWJpZd","execution_count":50,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"LangChain Expression Language (LCEL) ist eine spezielle Sprache, die in der LangChain-Bibliothek verwendet wird, um komplexe Datenverarbeitungs- und Abfrageoperationen zu vereinfachen. LangChain selbst ist eine Framework, das Entwicklern hilft, Anwendungen zu erstellen, die auf Sprachmodellen basieren, und LCEL ist ein Teil davon, der es erm√∂glicht, mit diesen Modellen auf eine strukturierte und ausdrucksstarke Weise zu interagieren.\n\nHier sind einige der Hauptmerkmale und Konzepte von LCEL:\n\n1. **Ausdr√ºcke**: LCEL erm√∂glicht es, Ausdr√ºcke zu definieren, die auf Daten angewendet werden k√∂nnen. Diese Ausdr√ºcke k√∂nnen einfache Berechnungen, logische Operationen oder komplexe Datenabfragen umfassen.\n\n2. **Variablen**: In LCEL k√∂nnen Variablen verwendet werden, um Werte zu speichern und wiederzuverwenden. Dies macht die Ausdr√ºcke flexibler und wiederverwendbar.\n\n3. **Funktionen**: LCEL unterst√ºtzt auch Funktionen, die auf Daten angewendet werden k√∂nnen. Diese Funktionen k√∂nnen vordefiniert oder benutzerdefiniert sein und erm√∂glichen eine erweiterte Datenmanipulation.\n\n4. **Integration mit Sprachmodellen**: LCEL ist so konzipiert, dass es nahtlos mit Sprachmodellen interagiert. Das bedeutet, dass du komplexe Anfragen an ein Sprachmodell stellen und die Ergebnisse in einer strukturierten Form zur√ºckerhalten kannst.\n\n5. **Einfache Syntax**: Die Syntax von LCEL ist darauf ausgelegt, intuitiv und leicht verst√§ndlich zu sein, sodass Entwickler schnell lernen k√∂nnen, wie sie sie effektiv nutzen.\n\nEin einfaches Beispiel k√∂nnte so aussehen:\n\n```lcel\nlet x = 10\nlet y = 20\nlet sum = x + y\n```\n\nIn diesem Beispiel werden zwei Variablen `x` und `y` definiert, und ihre Summe wird in der Variablen `sum` gespeichert.\n\nInsgesamt ist LCEL ein leistungsf√§higes Werkzeug, das Entwicklern hilft, die Interaktion mit Sprachmodellen zu optimieren und komplexe Datenoperationen auf eine klare und strukturierte Weise durchzuf√ºhren. Wenn du mehr √ºber spezifische Funktionen oder Anwendungsf√§lle wissen m√∂chtest, lass es mich wissen!"},"metadata":{}}]},{"cell_type":"markdown","source":["\n","LCEL ist der empfohlene Weg f√ºr alle neuen LangChain-Projekte, da es die Entwicklung vereinfacht und zukunftssicher macht."],"metadata":{"id":"49Wu_XreIZY2"},"id":"49Wu_XreIZY2"},{"cell_type":"markdown","source":["# 8 | Output-Handling\n","---"],"metadata":{"id":"LVqVL9CWRXU7"},"id":"LVqVL9CWRXU7"},{"cell_type":"markdown","source":["# 8.1 | OutputParser üìå"],"metadata":{"id":"89XLSUAvCE8P"},"id":"89XLSUAvCE8P"},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/langchain_prozess_03.png)"],"metadata":{"id":"erhTv-_o97q9"},"id":"erhTv-_o97q9"},{"cell_type":"markdown","source":["Ein OutputParser ist ein Konzept in LangChain, das hilft, die Antworten eines Modells richtig weiterzuverarbeiten. Es nimmt die **rohe Ausgabe** (zum Beispiel einen langen Text) und formt sie in ein **bestimmtes Format**, das sp√§ter in der Anwendung leichter benutzt werden kann."],"metadata":{"id":"EVBcCwJERfaj"},"id":"EVBcCwJERfaj"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Simple Chain mit Parser üìå\n","</font></p>"],"metadata":{"id":"oK4lZB3EGKFK"},"id":"oK4lZB3EGKFK"},{"cell_type":"code","source":["# 1. Prompt-Template\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"human\", \"Erkl√§re mir {user_input}\")\n","])\n","\n","# 2. Modell (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0)\n","\n","# 3. Parser\n","parser = StrOutputParser()\n","\n","# 4. Einfache LCEL-Kette\n","chain = prompt | llm | parser\n","\n","# 5. Ausf√ºhrung\n","response = chain.invoke({\"user_input\": \"LangChain Expression Language\"})"],"metadata":{"id":"PP_A_bGc7ovZ","executionInfo":{"status":"ok","timestamp":1764779412235,"user_tz":-60,"elapsed":11036,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":51,"outputs":[],"id":"PP_A_bGc7ovZ"},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response)"],"metadata":{"id":"qMcn5ctu7ova","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1764779412289,"user_tz":-60,"elapsed":49,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"7c9d88d1-065a-4c2d-fdee-48092755bbec"},"execution_count":52,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"LangChain Expression Language (LCEL) ist eine spezielle Sprache, die in der LangChain-Bibliothek verwendet wird, um komplexe Datenverarbeitungs- und Abfrageoperationen zu vereinfachen. LangChain selbst ist ein Framework, das Entwicklern hilft, Anwendungen mit Sprachmodellen zu erstellen, indem es verschiedene Komponenten wie Datenquellen, Agenten und Werkzeuge integriert.\n\nHier sind einige der Hauptmerkmale und Konzepte von LCEL:\n\n1. **Ausdr√ºcke**: LCEL erm√∂glicht es, Ausdr√ºcke zu definieren, die auf Daten angewendet werden k√∂nnen. Diese Ausdr√ºcke k√∂nnen einfache Berechnungen oder komplexe logische Operationen umfassen.\n\n2. **Datenmanipulation**: Mit LCEL k√∂nnen Entwickler Daten auf eine deklarative Weise manipulieren, was bedeutet, dass sie angeben, was sie erreichen m√∂chten, ohne sich um die genauen Schritte k√ºmmern zu m√ºssen, die zur Erreichung dieses Ziels erforderlich sind.\n\n3. **Integration mit Sprachmodellen**: LCEL ist so konzipiert, dass es nahtlos mit Sprachmodellen zusammenarbeitet, sodass Entwickler die Leistungsf√§higkeit von KI nutzen k√∂nnen, um Daten zu analysieren und zu verarbeiten.\n\n4. **Flexibilit√§t**: Die Sprache ist flexibel und kann an verschiedene Anwendungsf√§lle angepasst werden, sei es f√ºr einfache Datenabfragen oder komplexe Analysen.\n\n5. **Benutzerfreundlichkeit**: LCEL zielt darauf ab, die Lernkurve f√ºr Entwickler zu senken, indem es eine intuitive Syntax und klare Strukturen bietet.\n\nEin einfaches Beispiel k√∂nnte so aussehen:\n\n```lcel\nSELECT name, age FROM users WHERE age > 18\n```\n\nIn diesem Beispiel wird eine Abfrage formuliert, um die Namen und das Alter von Benutzern zu erhalten, die √§lter als 18 Jahre sind. \n\nInsgesamt ist LCEL ein leistungsf√§higes Werkzeug, das Entwicklern hilft, die Interaktion mit Daten zu vereinfachen und gleichzeitig die M√∂glichkeiten von KI-gest√ºtzten Anwendungen zu erweitern. Wenn du mehr √ºber spezifische Funktionen oder Anwendungsf√§lle wissen m√∂chtest, lass es mich wissen!"},"metadata":{}}],"id":"qMcn5ctu7ova"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Simple Chain mit Parser & Format im System-Prompt\n","</font></p>"],"metadata":{"id":"pNKOzTlQ8Kpl"},"id":"pNKOzTlQ8Kpl"},{"cell_type":"code","source":["# 1. Einfacher Prompt mit explizite Formatvorgabe\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent. Formatiere Deine Antwort im json-Format mit den Infos zu Name und Alter.\"),\n","    (\"human\", \"{user_input}\")\n","])\n","\n","# 2. Modell (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0)\n","\n","# 3. Parser\n","parser = StrOutputParser()\n","\n","# 4. Chain\n","chain = prompt | llm | parser\n","\n","# 5. Ausf√ºhrung\n","response = chain.invoke({\"user_input\": \"Bitte gibt den Namen einer fiktiven Person und das Alter\"})"],"metadata":{"id":"QKtYSBLhVlts","executionInfo":{"status":"ok","timestamp":1764779413238,"user_tz":-60,"elapsed":947,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":53,"outputs":[],"id":"QKtYSBLhVlts"},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response)"],"metadata":{"id":"BGceDe7Kw6ot","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1764779413312,"user_tz":-60,"elapsed":69,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"511c7e10-b9d1-4c10-ef8a-dac45bdbdab3"},"id":"BGceDe7Kw6ot","execution_count":54,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"```json\n{\n  \"Name\": \"Maximilian M√ºller\",\n  \"Alter\": 28\n}\n```"},"metadata":{}}]},{"cell_type":"markdown","id":"5efri5ponin","source":["# 8.2 | with_structured_output() ‚ú≥Ô∏è\n"],"metadata":{"id":"5efri5ponin"}},{"cell_type":"markdown","id":"9hnfp8g8bkj","source":["Im vorherigen Beispielen wurden Formatierung √ºber **System-Prompts** und/oder √ºber einen **OututParser** gesteuert. Das funktioniert, hat aber Nachteile:\n","\n","- ‚ùå Keine automatische Validierung (Typen k√∂nnen falsch sein)\n","- ‚ùå String-Parsing fehleranf√§llig\n","- ‚ùå Keine Typsicherheit im Code\n","- ‚ùå Prompt-Engineering f√ºr Format n√∂tig\n","\n","**Moderne L√∂sung: `with_structured_output()`**\n","\n","LangChain 1.0+ bietet eine elegante L√∂sung: Mit `with_structured_output()` kann man die gew√ºnschte Ausgabestruktur **direkt als Pydantic-Modell** definieren. Das Modell sorgt automatisch f√ºr:\n","\n","- ‚úÖ Validierte, typsichere Ausgaben\n","- ‚úÖ Automatische Format-Konvertierung\n","- ‚úÖ Klare Struktur ohne Prompt-Engineering\n","- ‚úÖ IDE-Unterst√ºtzung (Autocomplete, Type-Hints)"],"metadata":{"id":"9hnfp8g8bkj"}},{"cell_type":"markdown","id":"p8qe3tuhedi","source":["<p><font color='black' size=\"5\">\n","Einfaches Beispiel mit with_structured_output()\n","</font></p>"],"metadata":{"id":"p8qe3tuhedi"}},{"cell_type":"code","id":"a3xf7g08nw","source":["from pydantic import BaseModel, Field\n","\n","# 1. Pydantic-Modell definiert die gew√ºnschte Struktur\n","class PersonInfo(BaseModel):\n","    \"\"\"Informationen √ºber eine Person\"\"\"\n","    name: str = Field(description=\"Vollst√§ndiger Name der Person\")\n","    alter: int = Field(description=\"Alter der Person in Jahren\")\n","    beruf: str = Field(description=\"Aktueller Beruf der Person\")\n","\n","# 2. Modell mit strukturierter Ausgabe (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0)\n","structured_llm = llm.with_structured_output(PersonInfo)\n","\n","# 3. Einfache Anfrage - automatische Formatierung & Validierung\n","response = structured_llm.invoke(\n","    \"Erstelle eine fiktive Person: Name, Alter und Beruf\"\n",")\n","\n","# 4. Typsichere Ausgabe - response ist ein PersonInfo-Objekt\n","print(f\"Typ: {type(response)}\")\n","print(f\"Name: {response.name}\")\n","print(f\"Alter: {response.alter}\")\n","print(f\"Beruf: {response.beruf}\")"],"metadata":{"id":"a3xf7g08nw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764779414019,"user_tz":-60,"elapsed":702,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"88dd8187-af8c-4a16-d83b-cde30cc03a20"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Typ: <class '__main__.PersonInfo'>\n","Name: Lena M√ºller\n","Alter: 28\n","Beruf: Grafikdesignerin\n"]}]},{"cell_type":"markdown","id":"ybps752g89","source":["**Vergleich: Prompt-basierte Formatierung vs. StrOutputParser vs. with_structured_output()**\n","\n","| Aspekt | Prompt-basiert | StrOutputParser | with_structured_output() |\n","|--------|----------------|----------------|---------------------------|\n","| **Definition** | Via System-Prompt: ‚ÄûFormatiere im JSON-Format...‚Äú | Expliziter Parser, der LLM-Ausgabe als String extrahiert oder leicht nachbearbeitet | Automatische Strukturierung per Pydantic-Modell (`class PersonInfo(BaseModel)`) |\n","| **Validierung** | ‚ùå Keine ‚Äì Modell kann beliebig antworten | ‚ö†Ô∏è Nur minimale (z. B. String-Cleaning) | ‚úÖ Strikte Schema-Validierung durch Pydantic |\n","| **Typsicherheit** | ‚ùå Nur Strings | ‚ùå Nur Strings | ‚úÖ Typsichere Python-Objekte |\n","| **Fehlerbehandlung** | ‚ö†Ô∏è Manuelles Parsing und Regex n√∂tig | ‚ö†Ô∏è Begrenzte Kontrolle (nur einfache Parsing-Fehler) | ‚úÖ Automatische Validierung, Exception bei Schemafehlern |\n","| **Code-Qualit√§t** | ‚ö†Ô∏è Fragil, stark Prompt-abh√§ngig | ‚úÖ Etwas klarer, aber keine echte Struktur | ‚úÖ Robust, wartbar, klar definierte Typen |\n","| **IDE-Support** | ‚ùå Keine Autovervollst√§ndigung | ‚ùå Keine | ‚úÖ Vollst√§ndig durch Type-Hints |\n","| **Anwendungsfall** | Einfache oder explorative Ausgaben | Wenn nur reine Textausgabe n√∂tig ist | F√ºr strukturierte, verl√§ssliche Daten (JSON, Objekte) |\n","| **Lernkurve** | ‚úÖ Sehr einfach | ‚úÖ Einfach | ‚ö†Ô∏è Erfordert Grundkenntnisse in Pydantic und LangChain-Schemas |\n","| **Beispiel** | ‚ÄûNenne Name und Alter als JSON.‚Äú | `StrOutputParser().parse(output)` | `llm.with_structured_output(PersonInfo)` |\n","\n"],"metadata":{"id":"ybps752g89"}},{"cell_type":"markdown","source":["\n","**Wann welchen Ansatz?**\n","\n","- **Prompt-basiert**: F√ºr einfache Texte, Zusammenfassungen, kreative Inhalte  \n","- **StrOutputParser**: Wenn nur reine Textausgabe oder einfache Formatkorrektur n√∂tig ist  \n","- **with_structured_output()**: F√ºr strukturierte Daten, APIs, Datenbanken, komplexe Workflows"],"metadata":{"id":"uncqM3AxDjh9"},"id":"uncqM3AxDjh9"},{"cell_type":"markdown","source":["# 9 | Runnables (opt.)\n","---\n"],"metadata":{"id":"KRqrnpMx3Hwf"},"id":"KRqrnpMx3Hwf"},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/langchain_prozess_01.png)"],"metadata":{"id":"LFOITVoEzpkm"},"id":"LFOITVoEzpkm"},{"cell_type":"markdown","source":["Ein **Runnable** ist ein elementarer Baustein in LangChain. Er nimmt eine Eingabe und liefert eine Ausgabe - wie eine Funktion mit klaren Regeln. Man kann sich Konzepte wie Prompts, LLMs oder Parser als Runnables vorstellen. Runnable kann man als **Oberbegriff** f√ºr Prompts, LLMs, Parser und √§hnliche Komponenten verwenden.\n","\n","\n","\n","In der Grafik sieht man die **drei wichtigsten Runnables**: Prompt-Template, LLM und Parser, die zusammen eine Kette bilden.\n","\n","In LangChain sind die wichtigsten Komponenten als Runnables verf√ºgbar:\n","\n","+ Prompts/Templates (der erste Baustein in der Kette)\n","+ LLMs (das Herzst√ºck in der Mitte)\n","+ Output-Parser (verwandelt die LLM-Antwort in ein nutzbares Format)\n","+ Komplette Chains (die ganze Verarbeitungskette)\n","+ Tools (f√ºr spezielle Aufgaben)\n","+ Retriever (holen zus√§tzliche Informationen)\n","\n","\n","Vorteile von Runnables:\n","\n","+ Gleiche Bedienung: Alle Runnables (Prompt, LLM, Parser) funktionieren nach demselben Prinzip\n","+ Einfaches Verbinden: Man kann sie unkompliziert zu einer Kette zusammenf√ºgen - genau wie im Bild gezeigt\n","+ Anpassungsf√§hig: Sie funktionieren sowohl einzeln als auch in der Gruppe\n","Schrittweise Ausgabe: Ergebnisse k√∂nnen st√ºckweise weitergegeben werden"],"metadata":{"id":"DnUDR_983M4l"},"id":"DnUDR_983M4l"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Benutzerdefinierte Runnables erstellen\n","</font></p>"],"metadata":{"id":"WWJ7U1iJ6b8Q"},"id":"WWJ7U1iJ6b8Q"},{"cell_type":"markdown","source":["Man kann auch eigene Runnables erstellen, indem man eine Klasse definiert, die das Runnable-Interface implementiert oder eine Funktion mit `RunnableLambda` umsetzt:\n"],"metadata":{"id":"PAvbE_TR6g87"},"id":"PAvbE_TR6g87"},{"cell_type":"code","source":["from langchain_core.runnables import RunnableLambda\n","from langchain_core.output_parsers.string import StrOutputParser\n","\n","# 1. Mit RunnableLambda eine einfache Transformation erstellen\n","def text_verdoppeln(eingabe):\n","    return f\"{eingabe} <br> <br>  {eingabe}\"\n","\n","verdoppler = RunnableLambda(text_verdoppeln)\n","\n","# Verwendung des eigenen Runnables\n","verdoppler.invoke(\"Dies ist ein Text.\")"],"metadata":{"id":"_0wEf_hL6peM","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1764779414028,"user_tz":-60,"elapsed":6,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"8249aa95-4450-468a-b677-7cd07f7b4460"},"id":"_0wEf_hL6peM","execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Dies ist ein Text. <br> <br>  Dies ist ein Text.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["# Einfacher Prompt ohne explizite Formatvorgabe\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent. Antworte kurz.\"),\n","    (\"human\", \"Eingabe: {user_input}\")\n","])\n","\n","# In eine Kette einbauen - Korrektur der Pipe-Operatoren\n","chain_mit_verdoppler = prompt | llm | parser | verdoppler\n","\n","# Test der Kette\n","response = chain_mit_verdoppler.invoke({\"user_input\": \"Wer war Einstein?\"})"],"metadata":{"id":"NvZdOf1961xb","executionInfo":{"status":"ok","timestamp":1764779415496,"user_tz":-60,"elapsed":1441,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"NvZdOf1961xb","execution_count":57,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response)"],"metadata":{"id":"_elA0imx2HZA","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1764779415546,"user_tz":-60,"elapsed":32,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"ee081c96-0376-40e3-9f08-9284d9625bb5"},"id":"_elA0imx2HZA","execution_count":58,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Albert Einstein war ein theoretischer Physiker, bekannt f√ºr die Entwicklung der Relativit√§tstheorie und seine Beitr√§ge zur Quantenmechanik. Er gilt als einer der bedeutendsten Wissenschaftler des 20. Jahrhunderts. <br> <br>  Albert Einstein war ein theoretischer Physiker, bekannt f√ºr die Entwicklung der Relativit√§tstheorie und seine Beitr√§ge zur Quantenmechanik. Er gilt als einer der bedeutendsten Wissenschaftler des 20. Jahrhunderts."},"metadata":{}}]},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Runnable-Methoden\n","</font></p>\n","\n","Jedes Runnable unterst√ºtzt standardm√§√üig folgende Methoden:\n","\n","1. **invoke()**: F√ºr einzelne, synchrone Anfragen\n","2. **batch()**: Verarbeitet mehrere Eingaben parallel\n","3. **stream()**: Gibt Teilergebnisse zur√ºck, sobald sie verf√ºgbar sind\n","4. **ainvoke()**: Asynchrone Version von invoke\n","5. **abatch()**: Asynchrone Version von batch\n","6. **astream()**: Asynchrone Version von stream\n","\n"],"metadata":{"id":"CjFvHqWA3S2V"},"id":"CjFvHqWA3S2V"},{"cell_type":"markdown","source":["Hier ein einfaches Beispiel mit den verschiedenen Aufrufmethoden:\n"],"metadata":{"id":"a9zHZAMP3bCL"},"id":"a9zHZAMP3bCL"},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers.string import StrOutputParser\n","\n","# Einfaches Template erstellen\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"human\", \"{user_input}\")\n","])\n","\n","# Runnable-Kette erstellen\n","chain = prompt | llm | parser"],"metadata":{"id":"kQTKHK2D4DBb","executionInfo":{"status":"ok","timestamp":1764779415557,"user_tz":-60,"elapsed":2,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"kQTKHK2D4DBb","execution_count":59,"outputs":[]},{"cell_type":"code","source":["# 1. invoke - Einzelne Anfrage\n","response = chain.invoke({\"user_input\": \"Was ist ein Runnable in LangChain?\"})\n","\n","mprint(\"## üì£ Invoke response:\")\n","mprint(\"---\")\n","mprint(response)"],"metadata":{"id":"TVWtHSYw5Jla","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1764779420841,"user_tz":-60,"elapsed":5282,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"e44ab41f-39ea-437a-ea69-21497912e5d0"},"id":"TVWtHSYw5Jla","execution_count":60,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Invoke response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"In LangChain bezieht sich ein \"Runnable\" auf ein Konzept, das es erm√∂glicht, verschiedene Komponenten oder Funktionen in einer Kette von Operationen zu kombinieren und auszuf√ºhren. Es handelt sich dabei um eine abstrakte Schnittstelle, die es Entwicklern erm√∂glicht, ihre Logik modular und wiederverwendbar zu gestalten.\n\nEin Runnable kann verschiedene Aufgaben √ºbernehmen, wie z.B. Datenverarbeitung, API-Aufrufe oder die Ausf√ºhrung von KI-Modellen. Die Idee ist, dass man verschiedene Runnables miteinander verkn√ºpfen kann, um komplexe Workflows zu erstellen, ohne sich um die Details der Implementierung jeder einzelnen Komponente k√ºmmern zu m√ºssen.\n\nDas macht es einfacher, verschiedene Teile einer Anwendung zu testen, zu debuggen und zu optimieren. Wenn du also das n√§chste Mal einen Runnable in LangChain siehst, denk daran: Es ist wie ein Schweizer Taschenmesser f√ºr deine Programmierlogik ‚Äì vielseitig und immer bereit, dir das Leben leichter zu machen! üõ†Ô∏èüòÑ"},"metadata":{}}]},{"cell_type":"code","source":["# 2. batch - Mehrere Anfragen parallel\n","response = chain.batch([\n","    {\"user_input\": \"Erkl√§re den Begriff LCEL kurz.\"},\n","    {\"user_input\": \"Was sind die Hauptvorteile von Runnables?\"}\n","])\n","mprint(\"##  üì£ Batch response::\")\n","mprint(\"---\")\n","for i, ergebnis in enumerate(response, 1):\n","    mprint(f\"Anfrage {i}: {response[:50]}...\\n\")"],"metadata":{"id":"UAet4KSq5K2D","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1764779430310,"user_tz":-60,"elapsed":9446,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"34618c5d-3db5-4def-df44-f0aa2bb433d6"},"id":"UAet4KSq5K2D","execution_count":61,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"##  üì£ Batch response::"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Anfrage 1: ['LCEL steht f√ºr \"Low Carbon Energy Lab\" und bezieht sich auf Forschungs- und Entwicklungsinitiativen, die sich mit der Erzeugung und Nutzung von Energie mit geringem Kohlenstoffaussto√ü besch√§ftigen. Ziel ist es, nachhaltige und umweltfreundliche Energiel√∂sungen zu entwickeln, um den Klimawandel zu bek√§mpfen und die Abh√§ngigkeit von fossilen Brennstoffen zu reduzieren. Wenn du mehr Details oder spezifische Aspekte wissen m√∂chtest, sag einfach Bescheid!', 'Runnables in Java sind eine praktische M√∂glichkeit, um Aufgaben in einem separaten Thread auszuf√ºhren. Hier sind einige der Hauptvorteile von Runnables:\\n\\n1. **Thread-Sicherheit**: Runnables erm√∂glichen es, Aufgaben in einem eigenen Thread auszuf√ºhren, was die Hauptanwendung nicht blockiert. Das ist besonders n√ºtzlich f√ºr langwierige Aufgaben, die die Benutzeroberfl√§che nicht einfrieren lassen sollen.\\n\\n2. **Einfachheit**: Die Implementierung eines Runnables ist einfach. Du musst nur die `run()`-Methode √ºberschreiben und kannst dann das Runnable an einen Thread √ºbergeben. Das macht den Code √ºbersichtlich und leicht verst√§ndlich.\\n\\n3. **Flexibilit√§t**: Runnables k√∂nnen in verschiedenen Kontexten verwendet werden, z.B. in Thread-Pools oder mit Executor-Services. Das erm√∂glicht eine bessere Verwaltung von Threads und Ressourcen.\\n\\n4. **Wiederverwendbarkeit**: Ein Runnable kann mehrfach ausgef√ºhrt werden, indem es einfach an verschiedene Threads oder Executor-Services √ºbergeben wird. Das f√∂rdert die Wiederverwendbarkeit des Codes.\\n\\n5. **Keine R√ºckgabewerte**: Runnables sind ideal f√ºr Aufgaben, die keine R√ºckgabewerte ben√∂tigen. Wenn du nur eine Aufgabe ausf√ºhren m√∂chtest, ohne ein Ergebnis zur√ºckzugeben, ist ein Runnable die perfekte Wahl.\\n\\n6. **Leichtgewichtig**: Im Vergleich zu anderen Ans√§tzen wie `Callable` (das R√ºckgabewerte unterst√ºtzt) sind Runnables leichter und ben√∂tigen weniger Overhead.\\n\\n7. **Integration mit anderen Frameworks**: Runnables lassen sich gut in verschiedene Java-Frameworks integrieren, die Multithreading unterst√ºtzen, wie z.B. Swing f√ºr GUI-Anwendungen oder JavaFX.\\n\\nUnd nicht zu vergessen: Wenn du ein Runnable erstellst, kannst du immer einen Witz √ºber Threads einbauen ‚Äì schlie√ülich sind sie die einzigen, die sich nicht √ºber ihre Probleme beschweren! üòÑ']...\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Anfrage 2: ['LCEL steht f√ºr \"Low Carbon Energy Lab\" und bezieht sich auf Forschungs- und Entwicklungsinitiativen, die sich mit der Erzeugung und Nutzung von Energie mit geringem Kohlenstoffaussto√ü besch√§ftigen. Ziel ist es, nachhaltige und umweltfreundliche Energiel√∂sungen zu entwickeln, um den Klimawandel zu bek√§mpfen und die Abh√§ngigkeit von fossilen Brennstoffen zu reduzieren. Wenn du mehr Details oder spezifische Aspekte wissen m√∂chtest, sag einfach Bescheid!', 'Runnables in Java sind eine praktische M√∂glichkeit, um Aufgaben in einem separaten Thread auszuf√ºhren. Hier sind einige der Hauptvorteile von Runnables:\\n\\n1. **Thread-Sicherheit**: Runnables erm√∂glichen es, Aufgaben in einem eigenen Thread auszuf√ºhren, was die Hauptanwendung nicht blockiert. Das ist besonders n√ºtzlich f√ºr langwierige Aufgaben, die die Benutzeroberfl√§che nicht einfrieren lassen sollen.\\n\\n2. **Einfachheit**: Die Implementierung eines Runnables ist einfach. Du musst nur die `run()`-Methode √ºberschreiben und kannst dann das Runnable an einen Thread √ºbergeben. Das macht den Code √ºbersichtlich und leicht verst√§ndlich.\\n\\n3. **Flexibilit√§t**: Runnables k√∂nnen in verschiedenen Kontexten verwendet werden, z.B. in Thread-Pools oder mit Executor-Services. Das erm√∂glicht eine bessere Verwaltung von Threads und Ressourcen.\\n\\n4. **Wiederverwendbarkeit**: Ein Runnable kann mehrfach ausgef√ºhrt werden, indem es einfach an verschiedene Threads oder Executor-Services √ºbergeben wird. Das f√∂rdert die Wiederverwendbarkeit des Codes.\\n\\n5. **Keine R√ºckgabewerte**: Runnables sind ideal f√ºr Aufgaben, die keine R√ºckgabewerte ben√∂tigen. Wenn du nur eine Aufgabe ausf√ºhren m√∂chtest, ohne ein Ergebnis zur√ºckzugeben, ist ein Runnable die perfekte Wahl.\\n\\n6. **Leichtgewichtig**: Im Vergleich zu anderen Ans√§tzen wie `Callable` (das R√ºckgabewerte unterst√ºtzt) sind Runnables leichter und ben√∂tigen weniger Overhead.\\n\\n7. **Integration mit anderen Frameworks**: Runnables lassen sich gut in verschiedene Java-Frameworks integrieren, die Multithreading unterst√ºtzen, wie z.B. Swing f√ºr GUI-Anwendungen oder JavaFX.\\n\\nUnd nicht zu vergessen: Wenn du ein Runnable erstellst, kannst du immer einen Witz √ºber Threads einbauen ‚Äì schlie√ülich sind sie die einzigen, die sich nicht √ºber ihre Probleme beschweren! üòÑ']...\n"},"metadata":{}}]},{"cell_type":"code","source":["# 3. stream - Schrittweise Ausgabe\n","mprint(\"##  üì£ Stream response:\")\n","mprint(\"---\")\n","for chunk in chain.stream({\"user_input\": \"Erkl√§re die Abk√ºrzung LCEL kurz.\"}):\n","    print(chunk, end=\" ‚û° \")  # Zeichen trennt die via stream gelieferten Ergebnisse/Ausgaben\n","print(\"\\n\")"],"metadata":{"id":"VJn4e_IX5L-b","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1764779432922,"user_tz":-60,"elapsed":2584,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"ef5f6e67-bd18-46da-a189-d35e4333efbc"},"id":"VJn4e_IX5L-b","execution_count":62,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"##  üì£ Stream response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" ‚û° Die ‚û°  Ab ‚û° k ‚û° √ºr ‚û° zung ‚û°  LC ‚û° EL ‚û°  steht ‚û°  f√ºr ‚û°  \" ‚û° Low ‚û°  Carbon ‚û°  Energy ‚û°  Lab ‚û° \". ‚û°  Es ‚û°  handelt ‚û°  sich ‚û°  dabei ‚û°  um ‚û°  Einrichtungen ‚û°  oder ‚û°  Forschungs ‚û° zent ‚û° ren ‚û° , ‚û°  die ‚û°  sich ‚û°  mit ‚û°  der ‚û°  Entwicklung ‚û°  und ‚û°  F√∂rderung ‚û°  von ‚û°  Technologien ‚û°  und ‚û°  Strateg ‚û° ien ‚û°  zur ‚û°  Red ‚û° uz ‚û° ierung ‚û°  von ‚û°  Koh ‚û° len ‚û° stoff ‚û° em ‚û° ission ‚û° en ‚û°  in ‚û°  der ‚û°  Energie ‚û° er ‚û° zeug ‚û° ung ‚û°  und ‚û°  - ‚û° n ‚û° utzung ‚û°  besch√§ftigen ‚û° . ‚û°  Wenn ‚û°  du ‚û°  mehr ‚û°  Details ‚û°  oder ‚û°  spezif ‚û° ische ‚û°  Informationen ‚û°  zu ‚û°  einem ‚û°  bestimmten ‚û°  LC ‚û° EL ‚û°  ben√∂t ‚û° igst ‚û° , ‚û°  lass ‚û°  es ‚û°  mich ‚û°  wissen ‚û° ! ‚û°  ‚û°  ‚û°  ‚û° \n","\n"]}]},{"cell_type":"markdown","id":"F_5gn03b46rC","metadata":{"id":"F_5gn03b46rC"},"source":["# A | Aufgabe\n","---"]},{"cell_type":"markdown","id":"36LyGMmo5jb3","metadata":{"id":"36LyGMmo5jb3"},"source":["Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen."]},{"cell_type":"markdown","id":"Je-szbGU7IJ-","metadata":{"id":"Je-szbGU7IJ-"},"source":["\n","<p><font color='black' size=\"5\">\n","Erstellen einer LangChain-Kette zur Textanalyse\n","</font></p>"]},{"cell_type":"markdown","id":"kevePvA-7PzW","metadata":{"id":"kevePvA-7PzW"},"source":["Entwickeln Sie eine Verarbeitungskette, die einen Text analysiert und verschiedene Informationen dar√ºber extrahiert.\n","\n","**Schritte:**\n","1. Erstelle ein ChatPromptTemplate, das ein LLM anweist, einen Text zu analysieren\n","2. Die Analyse soll folgende Aspekte umfassen:\n","   - Hauptthema des Textes\n","   - Tonalit√§t (formal, informell, etc.)\n","   - Schl√ºsselw√∂rter (5-10)\n","   - Kurze Zusammenfassung (max. 3 S√§tze)\n","3. Formatiere die Ausgabe strukturiert mit Markdown-√úberschriften\n","4. Teste die Kette mit mindestens zwei verschiedenen Texten\n","\n","\n"]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["91ee89a4-477b-41ce-a511-aadd4c2fd547","fGV8FwGmIwmR","EouwC_wdZb-c","BMPwDNTCCOfx","-JwJD0rLIyT8","HqTUeMW20084","8akKmCCc06nm","Ps344Ejw0-ih","pt37u3cnbgc","8c859b9f","c314ca71","LVqVL9CWRXU7","89XLSUAvCE8P","5efri5ponin","KRqrnpMx3Hwf","F_5gn03b46rC"],"toc_visible":true},"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":5}
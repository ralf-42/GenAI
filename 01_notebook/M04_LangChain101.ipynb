{"cells":[{"cell_type":"markdown","id":"37rMyCQz2JNw","metadata":{"id":"37rMyCQz2JNw"},"source":["<p><font size=\"6\" color='grey'> <b>\n","\n","Generative KI. Verstehen. Anwenden. Gestalten.\n","</b></font> </br></p>"]},{"cell_type":"markdown","id":"R5CfUEMJdvFQ","metadata":{"id":"R5CfUEMJdvFQ"},"source":["<p><font size=\"5\" color='grey'> <b>\n","LangChain 101\n","</b></font> </br></p>\n","\n","---"]},{"cell_type":"code","source":["#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n","!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul\n","from genai_lib.utilities import (\n","    check_environment,\n","    get_ipinfo,\n","    setup_api_keys,\n","    mprint,\n","    install_packages,\n","    mermaid,\n","    get_model_profile,\n","    extract_thinking,\n","    load_prompt\n",")\n","setup_api_keys(['OPENAI_API_KEY'], create_globals=False)\n","print()\n","check_environment()\n","print()\n","get_ipinfo()"],"metadata":{"id":"PnT8m8TTUhF5","collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980394080,"user_tz":-60,"elapsed":59000,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"ad9797db-8598-419f-f544-2883d674cb59"},"id":"PnT8m8TTUhF5","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úì OPENAI_API_KEY erfolgreich gesetzt\n","\n","Python Version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n","\n","Installierte LangChain- und LangGraph-Bibliotheken:\n","langchain                                1.2.8\n","langchain-chroma                         1.1.0\n","langchain-classic                        1.0.1\n","langchain-community                      0.4.1\n","langchain-core                           1.2.9\n","langchain-ollama                         1.0.1\n","langchain-openai                         1.1.8\n","langchain-text-splitters                 1.1.0\n","langgraph                                1.0.7\n","langgraph-checkpoint                     4.0.0\n","langgraph-prebuilt                       1.0.7\n","langgraph-sdk                            0.3.3\n","\n","IP-Adresse: 34.125.145.78\n","Hostname: 78.145.125.34.bc.googleusercontent.com\n","Stadt: Las Vegas\n","Region: Nevada\n","Land: US\n","Koordinaten: 36.1750,-115.1372\n","Provider: AS396982 Google LLC\n","Postleitzahl: 89111\n","Zeitzone: America/Los_Angeles\n"]}]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","‚è∏Ô∏è 5-Minuten-Check:\n","</font></p>\n","\n","**Ziel:** Pr√ºfen, ob du das vorherige Kapitel verstanden hast ‚Äì nicht, ob es gerade l√§uft.\n","\n","**Aufgabe** (5 Minuten, ohne Vorlage):\n","\n","Rekonstruiere die zentrale Idee oder Code-Struktur des letzten Abschnitts selbstst√§ndig\n","(kein Copy & Paste, kein Nachschlagen).\n","\n","W√§hle eine der folgenden Optionen:\n","\n","+ Erkl√§re in 1‚Äì2 S√§tzen, was hier konzeptionell passiert.\n","\n","+ Ver√§ndere eine Kleinigkeit (z. B. Prompt, Parameter, Reihenfolge) und beschreibe die Auswirkung.\n","\n","+ Markiere eine Stelle, die du nicht sicher erkl√§ren kannst, und formuliere eine konkrete Frage dazu.\n","\n","**Hinweis:**\n","Nicht alles muss ‚Äûfertig‚Äú oder ‚Äûkorrekt‚Äú sein. Entscheidend ist, wo dein Verst√§ndnis gerade endet"],"metadata":{"id":"xSK4F8CBabVk"},"id":"xSK4F8CBabVk"},{"cell_type":"markdown","id":"91ee89a4-477b-41ce-a511-aadd4c2fd547","metadata":{"id":"91ee89a4-477b-41ce-a511-aadd4c2fd547"},"source":["# 1 | Was ist LangChain?\n","---"]},{"cell_type":"markdown","id":"b5004f8c","metadata":{"id":"b5004f8c"},"source":["LangChain ist ein Framework zur **Entwicklung von Anwendungen mit gro√üen Sprachmodellen** (LLMs). Es vereinfacht die Integration von LLMs in eigene Anwendungen durch:\n","\n","- Modulare Komponenten f√ºr verschiedene Aufgaben\n","- Vorgefertigte Ketten (Chains) f√ºr komplexe Workflows\n","- Einfache Integration externer Datenquellen\n","- Werkzeuge f√ºr das Speichern von Konversationskontexten\n","\n"]},{"cell_type":"markdown","id":"tl73bT-kKkia","metadata":{"id":"tl73bT-kKkia"},"source":["[Einf√ºhrung](https://python.langchain.com/docs/introduction/)   \n","[Konzepte](https://python.langchain.com/docs/concepts/)   \n","[API-References](https://python.langchain.com/api_reference/index.html)   \n","[Integrations](https://python.langchain.com/docs/integrations/providers/)\n","\n","---\n","\n","[Tutorials](https://python.langchain.com/docs/tutorials/)   \n","[How-to-Guides](https://python.langchain.com/docs/how_to/)   "]},{"cell_type":"markdown","id":"fGV8FwGmIwmR","metadata":{"id":"fGV8FwGmIwmR"},"source":["# 2 | √úberblick Konzepte\n","---\n","\n","\n"]},{"cell_type":"markdown","source":["Die Darstellung veranschaulicht das Grundprinzip von LangChain: die Verkettung verschiedener Konzepte zu einem durchg√§ngigen Verarbeitungsprozess, der Eingaben in strukturierte Ausgaben umwandelt."],"metadata":{"id":"nZ2ZMEd3ulKZ"},"id":"nZ2ZMEd3ulKZ"},{"cell_type":"code","source":["#@title üßú‚Äç‚ôÄÔ∏è Prozess-Diagramm { display-mode: \"form\" }\n","\n","diagram = \"\"\"\n","flowchart LR\n","    Input[\"Input<br/>(Message)\"] --> Prompt\n","\n","    subgraph Chain\n","        Prompt[\"Prompt<br/>(Template)\"]\n","        Model[\"Model<br/>(LLM)\"]\n","        Response[\"Response<br/>(Parser)\"]\n","        Prompt --> Model --> Response\n","    end\n","\n","    Response --> Result[\"Result\"]\n","\"\"\"\n","mermaid(diagram)"],"metadata":{"id":"yY1ypSdhbFUG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980395455,"user_tz":-60,"elapsed":1363,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"467502de-c436-4b15-fd2b-ec7c09b7687f"},"id":"yY1ypSdhbFUG","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.SVG object>"],"image/svg+xml":"<svg xmlns=\"http://www.w3.org/2000/svg\" id=\"container\" width=\"875.03125\" class=\"flowchart\" height=\"164\" viewBox=\"0 0 875.03125 164\" role=\"graphics-document document\" aria-roledescription=\"flowchart-v2\"><style>#container{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:16px;fill:#333;}@keyframes edge-animation-frame{from{stroke-dashoffset:0;}}@keyframes dash{to{stroke-dashoffset:0;}}#container .edge-animation-slow{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 50s linear infinite;stroke-linecap:round;}#container .edge-animation-fast{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 20s linear infinite;stroke-linecap:round;}#container .error-icon{fill:#552222;}#container .error-text{fill:#552222;stroke:#552222;}#container .edge-thickness-normal{stroke-width:1px;}#container .edge-thickness-thick{stroke-width:3.5px;}#container .edge-pattern-solid{stroke-dasharray:0;}#container .edge-thickness-invisible{stroke-width:0;fill:none;}#container .edge-pattern-dashed{stroke-dasharray:3;}#container .edge-pattern-dotted{stroke-dasharray:2;}#container .marker{fill:#333333;stroke:#333333;}#container .marker.cross{stroke:#333333;}#container svg{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:16px;}#container p{margin:0;}#container .label{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;color:#333;}#container .cluster-label text{fill:#333;}#container .cluster-label span{color:#333;}#container .cluster-label span p{background-color:transparent;}#container .label text,#container span{fill:#333;color:#333;}#container .node rect,#container .node circle,#container .node ellipse,#container .node polygon,#container .node path{fill:#ECECFF;stroke:#9370DB;stroke-width:1px;}#container .rough-node .label text,#container .node .label text,#container .image-shape .label,#container .icon-shape .label{text-anchor:middle;}#container .node .katex path{fill:#000;stroke:#000;stroke-width:1px;}#container .rough-node .label,#container .node .label,#container .image-shape .label,#container .icon-shape .label{text-align:center;}#container .node.clickable{cursor:pointer;}#container .root .anchor path{fill:#333333!important;stroke-width:0;stroke:#333333;}#container .arrowheadPath{fill:#333333;}#container .edgePath .path{stroke:#333333;stroke-width:2.0px;}#container .flowchart-link{stroke:#333333;fill:none;}#container .edgeLabel{background-color:rgba(232,232,232, 0.8);text-align:center;}#container .edgeLabel p{background-color:rgba(232,232,232, 0.8);}#container .edgeLabel rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#container .labelBkg{background-color:rgba(232, 232, 232, 0.5);}#container .cluster rect{fill:#ffffde;stroke:#aaaa33;stroke-width:1px;}#container .cluster text{fill:#333;}#container .cluster span{color:#333;}#container div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:12px;background:hsl(80, 100%, 96.2745098039%);border:1px solid #aaaa33;border-radius:2px;pointer-events:none;z-index:100;}#container .flowchartTitleText{text-anchor:middle;font-size:18px;fill:#333;}#container rect.text{fill:none;stroke-width:0;}#container .icon-shape,#container .image-shape{background-color:rgba(232,232,232, 0.8);text-align:center;}#container .icon-shape p,#container .image-shape p{background-color:rgba(232,232,232, 0.8);padding:2px;}#container .icon-shape rect,#container .image-shape rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#container .label-icon{display:inline-block;height:1em;overflow:visible;vertical-align:-0.125em;}#container .node .label-icon path{fill:currentColor;stroke:revert;stroke-width:revert;}#container :root{--mermaid-font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;}</style><g><marker id=\"container_flowchart-v2-pointEnd\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"5\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"8\" markerHeight=\"8\" orient=\"auto\"><path d=\"M 0 0 L 10 5 L 0 10 z\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-pointStart\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"4.5\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"8\" markerHeight=\"8\" orient=\"auto\"><path d=\"M 0 5 L 10 10 L 10 0 z\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-circleEnd\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"11\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><circle cx=\"5\" cy=\"5\" r=\"5\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-circleStart\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"-1\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><circle cx=\"5\" cy=\"5\" r=\"5\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-crossEnd\" class=\"marker cross flowchart-v2\" viewBox=\"0 0 11 11\" refX=\"12\" refY=\"5.2\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><path d=\"M 1,1 l 9,9 M 10,1 l -9,9\" class=\"arrowMarkerPath\" style=\"stroke-width: 2; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-crossStart\" class=\"marker cross flowchart-v2\" viewBox=\"0 0 11 11\" refX=\"-1\" refY=\"5.2\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><path d=\"M 1,1 l 9,9 M 10,1 l -9,9\" class=\"arrowMarkerPath\" style=\"stroke-width: 2; stroke-dasharray: 1, 0;\"/></marker><g class=\"root\"><g class=\"clusters\"><g class=\"cluster\" id=\"Chain\" data-look=\"classic\"><rect style=\"\" x=\"189.5\" y=\"8\" width=\"522.109375\" height=\"148\"/><g class=\"cluster-label\" transform=\"translate(430.1640625, 8)\"><foreignObject width=\"40.78125\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Chain</p></span></div></foreignObject></g></g></g><g class=\"edgePaths\"><path d=\"M139.5,82L143.667,82C147.833,82,156.167,82,164.5,82C172.833,82,181.167,82,188.833,82C196.5,82,203.5,82,207,82L210.5,82\" id=\"L_Input_Prompt_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_Input_Prompt_0\" data-points=\"W3sieCI6MTM5LjUsInkiOjgyfSx7IngiOjE2NC41LCJ5Ijo4Mn0seyJ4IjoxODkuNSwieSI6ODJ9LHsieCI6MjE0LjUsInkiOjgyfV0=\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M351.766,82L355.932,82C360.099,82,368.432,82,376.099,82C383.766,82,390.766,82,394.266,82L397.766,82\" id=\"L_Prompt_Model_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_Prompt_Model_0\" data-points=\"W3sieCI6MzUxLjc2NTYyNSwieSI6ODJ9LHsieCI6Mzc2Ljc2NTYyNSwieSI6ODJ9LHsieCI6NDAxLjc2NTYyNSwieSI6ODJ9XQ==\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M506.547,82L510.714,82C514.88,82,523.214,82,530.88,82C538.547,82,545.547,82,549.047,82L552.547,82\" id=\"L_Model_Response_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_Model_Response_0\" data-points=\"W3sieCI6NTA2LjU0Njg3NSwieSI6ODJ9LHsieCI6NTMxLjU0Njg3NSwieSI6ODJ9LHsieCI6NTU2LjU0Njg3NSwieSI6ODJ9XQ==\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M686.609,82L690.776,82C694.943,82,703.276,82,711.609,82C719.943,82,728.276,82,735.943,82C743.609,82,750.609,82,754.109,82L757.609,82\" id=\"L_Response_Result_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_Response_Result_0\" data-points=\"W3sieCI6Njg2LjYwOTM3NSwieSI6ODJ9LHsieCI6NzExLjYwOTM3NSwieSI6ODJ9LHsieCI6NzM2LjYwOTM3NSwieSI6ODJ9LHsieCI6NzYxLjYwOTM3NSwieSI6ODJ9XQ==\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/></g><g class=\"edgeLabels\"><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_Input_Prompt_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_Prompt_Model_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_Model_Response_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_Response_Result_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g></g><g class=\"nodes\"><g class=\"node default\" id=\"flowchart-Input-0\" transform=\"translate(73.75, 82)\"><rect class=\"basic label-container\" style=\"\" x=\"-65.75\" y=\"-39\" width=\"131.5\" height=\"78\"/><g class=\"label\" style=\"\" transform=\"translate(-35.75, -24)\"><rect/><foreignObject width=\"71.5\" height=\"48\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Input<br/>(Message)</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-Prompt-1\" transform=\"translate(283.1328125, 82)\"><rect class=\"basic label-container\" style=\"\" x=\"-68.6328125\" y=\"-39\" width=\"137.265625\" height=\"78\"/><g class=\"label\" style=\"\" transform=\"translate(-38.6328125, -24)\"><rect/><foreignObject width=\"77.265625\" height=\"48\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Prompt<br/>(Template)</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-Model-3\" transform=\"translate(454.15625, 82)\"><rect class=\"basic label-container\" style=\"\" x=\"-52.390625\" y=\"-39\" width=\"104.78125\" height=\"78\"/><g class=\"label\" style=\"\" transform=\"translate(-22.390625, -24)\"><rect/><foreignObject width=\"44.78125\" height=\"48\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Model<br/>(LLM)</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-Response-4\" transform=\"translate(621.578125, 82)\"><rect class=\"basic label-container\" style=\"\" x=\"-65.03125\" y=\"-39\" width=\"130.0625\" height=\"78\"/><g class=\"label\" style=\"\" transform=\"translate(-35.03125, -24)\"><rect/><foreignObject width=\"70.0625\" height=\"48\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Response<br/>(Parser)</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-Result-9\" transform=\"translate(814.3203125, 82)\"><rect class=\"basic label-container\" style=\"\" x=\"-52.7109375\" y=\"-27\" width=\"105.421875\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-22.7109375, -12)\"><rect/><foreignObject width=\"45.421875\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Result</p></span></div></foreignObject></g></g></g></g></g></svg>"},"metadata":{}}]},{"cell_type":"markdown","source":["Konzepte von LangChain sind grundlegende Bausteine und Prinzipien. Hier sind einige der wichtigsten Konzepte:\n","\n","+ Nachrichten (**Messages**): Kommunikationseinheiten in Chat-Modellen f√ºr Ein- und Ausgabe.\n","+ Prompt-Vorlagen (**Prompt template**s): Komponenten zur Erstellung strukturierter Prompts f√ºr LLMs.\n","+ Chat-Modelle (**Chat models**): LLMs, die √ºber eine Chat-API verf√ºgbar sind und Sequenzen von Nachrichten verarbeiten.\n","+ Chains (**Chains**): Verkn√ºpfungen mehrerer LLMs oder anderer Komponenten f√ºr komplexere Anwendungen.\n","+ Strukturierte Ausgabe (**Structured output**): Technik, um Chat-Modelle in strukturierten Formaten antworten zu lassen.\n","+ Chat-Verlauf (**Chat history**): Eine Sequenz von Nachrichten, die eine Konversation darstellt.\n","+ Retrieval Augmented Generation (**RAG**): Technik zur Verbesserung von Sprachmodellen durch Kombination mit externen Wissensbasen.\n","+ Retriever (**Retriever**): Komponenten, die relevante Dokumente aus einer Wissensbasis abrufen.\n","+ Agenten (**Agents**): Nutzen Sprachmodelle, um Aktionssequenzen auszuw√§hlen und mit externen Ressourcen zu interagieren.\n","+ Tools (**Tools**): Funktionen mit definierten Schemata f√ºr Name, Beschreibung und Argumente.\n"],"metadata":{"id":"cCuow5DOEsID"},"id":"cCuow5DOEsID"},{"cell_type":"markdown","id":"EouwC_wdZb-c","metadata":{"id":"EouwC_wdZb-c"},"source":["# 3 | Nachrichten-Typen\n","---"]},{"cell_type":"markdown","id":"yS2mM2J1aXlY","metadata":{"id":"yS2mM2J1aXlY"},"source":["Beim Einsatz von Large Language Models (LLMs) wie GPT gibt es drei Hauptarten von Nachrichten (Messages), die die Interaktion mit dem Modell steuern. Diese Messages definieren, wie das Modell reagiert, welche Eingaben es erh√§lt und welche Ausgaben es generiert.\n","\n"]},{"cell_type":"markdown","id":"sGqRXptbbQ2a","metadata":{"id":"sGqRXptbbQ2a"},"source":["**System Message:**   \n","Die `System Message` dient dazu, das **Verhalten des LLMs** zu steuern. Sie ist eine nicht sichtbare Nachricht, die dem Modell Anweisungen gibt, wie es sich verhalten soll. Dabei kann sie die Pers√∂nlichkeit, den Tonfall und die Einschr√§nkungen des Modells beeinflussen. Die Systemnachricht funktioniert wie eine Art \"Rollenspiel-Anweisung\", um das Modell in eine gew√ºnschte Rolle zu versetzen.\n","\n","*Merkmale:*\n","- Definiert, wie das Modell reagieren soll\n","- Legt Pers√∂nlichkeit, Verhalten und Einschr√§nkungen fest\n","- Wird zu Beginn einer Sitzung gesetzt und bleibt bestehen\n","- Nicht sichtbar f√ºr den Benutzer\n","\n","*Beispiel einer System Message:*\n","```json\n","{\n","  \"role\": \"system\",\n","  \"content\": \"Du bist ein KI-Assistent, der pr√§zise und informative Antworten in einem professionellen Ton liefert.\"\n","}\n","```\n","Dieses Beispiel weist das Modell an, in einem professionellen Tonfall kurze und informative Antworten zu geben.\n","\n","---\n","\n"]},{"cell_type":"markdown","id":"upsIzpcqbbP7","metadata":{"id":"upsIzpcqbbP7"},"source":["**User Message**   \n","Die `User Message` ist die eigentliche **Eingabe des Benutzer**s. Sie kann eine Frage, eine Aufforderung, kontextrelevanten Inhalt oder ein Befehl sein. Das Modell nutzt diese Nachricht als Ausgangspunkt f√ºr die Generierung einer Antwort.\n","\n","*Merkmale:*\n","- Direkte Eingabe des Benutzers\n","- Kann eine Frage, Aufforderung, kontextrelevanten Inhalt oder einen Befehl enthalten\n","- Basis f√ºr die Antwort des Modells\n","\n","*Beispiel einer User Message:*\n","```json\n","{\n","  \"role\": \"user\",\n","  \"content\": \"Was sind die wichtigsten Unterschiede zwischen KI und maschinellem Lernen?\"\n","}\n","```\n","Hier fragt der Benutzer nach den Unterschieden zwischen KI und maschinellem Lernen.\n","\n","---\n"]},{"cell_type":"markdown","id":"CozrrtYXbhZb","metadata":{"id":"CozrrtYXbhZb"},"source":["**AI Message**   \n","Die `AI Message` ist die **Antwort, die das Modell generiert**. Sie basiert auf der User Message und den Anweisungen aus der System Message. Die AI Message kann verschiedene Eigenschaften haben, ist aber haupts√§chlich auf den Inhalt fokussiert.\n","\n","*Merkmale:*\n","- Antwort des Modells auf die Benutzeranfrage\n","- Kann verschiedene Eigenschaften haben (z. B. L√§nge, Stil)\n","- Haupts√§chlich inhaltlich relevant\n","\n","*Beispiel einer AI Message:*\n","```json\n","{\n","  \"role\": \"assistant\",\n","  \"content\": \"K√ºnstliche Intelligenz (KI) ist ein weites Feld, das verschiedene Teilgebiete umfasst, darunter auch das maschinelle Lernen (ML). ML konzentriert sich auf die Erstellung von Modellen, die aus Daten lernen und Vorhersagen treffen.\"\n","}\n","```\n","Hier gibt das Modell eine inhaltliche Antwort auf die Frage des Benutzers.\n","\n","\n"]},{"cell_type":"markdown","source":["**System-Message vs. User-Message**\n","\n","\n","\n","*System-Message*\n","\n","**Funktion:** Persistente Verhaltensregeln √ºber verschiedene Aufgaben hinweg\n","\n","**Enth√§lt:**\n","- Identit√§t & Rolle des Modells\n","- Grunds√§tzliche Methodik & Denkansatz\n","- Durchg√§ngige Qualit√§tsstandards\n","- Tool-/Methodenkompetenz\n","- Generische Few-Shot-Examples\n","\n","**Metapher:** Das Betriebssystem ‚Äì selten ver√§ndert, verhaltensbestimmend\n","\n","\n"],"metadata":{"id":"a8QAtdYdg0Im"},"id":"a8QAtdYdg0Im"},{"cell_type":"markdown","source":["\n","*User-Message*\n","\n","**Funktion:** Konkrete Aufgabe und spezifischer Kontext f√ºr diese Interaktion\n","\n","**Enth√§lt:**\n","- Situativer Kontext & Hintergrund\n","- Klare Aufgabenstellung\n","- Erwartete Format/Umfang/Detailgrad\n","- Spezifische Daten & Randbedingungen\n","- Aufgabenspezifische Verhaltensanpassungen\n","\n","**Metapher:** Die Arbeitsanweisung ‚Äì variiert bei jeder Anfrage\n","\n"],"metadata":{"id":"cy765AHZhaWV"},"id":"cy765AHZhaWV"},{"cell_type":"markdown","id":"PiHnhKHVbofP","metadata":{"id":"PiHnhKHVbofP"},"source":["**Kurz gefasst**   \n","\n","+  **System Message** legt die Regeln und das Verhalten des Modells fest.\n","+ **User Message** stellt eine Anfrage oder einen Befehl.\n","+ **AI Message** gibt die generierte Antwort basierend auf den vorherigen Nachrichten.\n","\n","\n"]},{"cell_type":"code","id":"uwwloofx7h","source":["#@markdown   <p><font size=\"4\" color='green'>  üßú‚Äç‚ôÄÔ∏è Mermaid - Message-Typen Workflow</font> </br></p>\n","\n","diagram = \"\"\"\n","graph TD\n","    A[\"System Message\"] --> D[\"LLM Verarbeitung\"]\n","    B[\"User Message\"] --> D\n","    D --> C[\"AI Message\"]\n","\n","    A1[\"Definiert Verhalten<br/>Rolle & Pers√∂nlichkeit<br/>Einschr√§nkungen\"] -.-> A\n","    B1[\"Benutzer-Eingabe<br/>Frage oder Befehl<br/>Kontext\"] -.-> B\n","    C1[\"Generierte Antwort<br/>Basierend auf System + User<br/>Strukturiertes Format\"] -.-> C\n","\n","    style A fill:#ffe1e1\n","    style B fill:#e1f5ff\n","    style C fill:#e1ffe1\n","    style D fill:#fff4e1\n","\"\"\"\n","mermaid(diagram, width=700, height=450)"],"metadata":{"id":"uwwloofx7h","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980396502,"user_tz":-60,"elapsed":1073,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"7678ccc2-f202-4de5-985c-8929ea8edcbb"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.SVG object>"],"image/svg+xml":"<svg xmlns=\"http://www.w3.org/2000/svg\" id=\"container\" class=\"flowchart\" viewBox=\"0 0 655.7734375 502\" role=\"graphics-document document\" aria-roledescription=\"flowchart-v2\" width=\"700\" height=\"450\"><style>#container{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:16px;fill:#333;}@keyframes edge-animation-frame{from{stroke-dashoffset:0;}}@keyframes dash{to{stroke-dashoffset:0;}}#container .edge-animation-slow{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 50s linear infinite;stroke-linecap:round;}#container .edge-animation-fast{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 20s linear infinite;stroke-linecap:round;}#container .error-icon{fill:#552222;}#container .error-text{fill:#552222;stroke:#552222;}#container .edge-thickness-normal{stroke-width:1px;}#container .edge-thickness-thick{stroke-width:3.5px;}#container .edge-pattern-solid{stroke-dasharray:0;}#container .edge-thickness-invisible{stroke-width:0;fill:none;}#container .edge-pattern-dashed{stroke-dasharray:3;}#container .edge-pattern-dotted{stroke-dasharray:2;}#container .marker{fill:#333333;stroke:#333333;}#container .marker.cross{stroke:#333333;}#container svg{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:16px;}#container p{margin:0;}#container .label{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;color:#333;}#container .cluster-label text{fill:#333;}#container .cluster-label span{color:#333;}#container .cluster-label span p{background-color:transparent;}#container .label text,#container span{fill:#333;color:#333;}#container .node rect,#container .node circle,#container .node ellipse,#container .node polygon,#container .node path{fill:#ECECFF;stroke:#9370DB;stroke-width:1px;}#container .rough-node .label text,#container .node .label text,#container .image-shape .label,#container .icon-shape .label{text-anchor:middle;}#container .node .katex path{fill:#000;stroke:#000;stroke-width:1px;}#container .rough-node .label,#container .node .label,#container .image-shape .label,#container .icon-shape .label{text-align:center;}#container .node.clickable{cursor:pointer;}#container .root .anchor path{fill:#333333!important;stroke-width:0;stroke:#333333;}#container .arrowheadPath{fill:#333333;}#container .edgePath .path{stroke:#333333;stroke-width:2.0px;}#container .flowchart-link{stroke:#333333;fill:none;}#container .edgeLabel{background-color:rgba(232,232,232, 0.8);text-align:center;}#container .edgeLabel p{background-color:rgba(232,232,232, 0.8);}#container .edgeLabel rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#container .labelBkg{background-color:rgba(232, 232, 232, 0.5);}#container .cluster rect{fill:#ffffde;stroke:#aaaa33;stroke-width:1px;}#container .cluster text{fill:#333;}#container .cluster span{color:#333;}#container div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:12px;background:hsl(80, 100%, 96.2745098039%);border:1px solid #aaaa33;border-radius:2px;pointer-events:none;z-index:100;}#container .flowchartTitleText{text-anchor:middle;font-size:18px;fill:#333;}#container rect.text{fill:none;stroke-width:0;}#container .icon-shape,#container .image-shape{background-color:rgba(232,232,232, 0.8);text-align:center;}#container .icon-shape p,#container .image-shape p{background-color:rgba(232,232,232, 0.8);padding:2px;}#container .icon-shape rect,#container .image-shape rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#container .label-icon{display:inline-block;height:1em;overflow:visible;vertical-align:-0.125em;}#container .node .label-icon path{fill:currentColor;stroke:revert;stroke-width:revert;}#container :root{--mermaid-font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;}</style><g><marker id=\"container_flowchart-v2-pointEnd\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"5\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"8\" markerHeight=\"8\" orient=\"auto\"><path d=\"M 0 0 L 10 5 L 0 10 z\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-pointStart\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"4.5\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"8\" markerHeight=\"8\" orient=\"auto\"><path d=\"M 0 5 L 10 10 L 10 0 z\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-circleEnd\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"11\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><circle cx=\"5\" cy=\"5\" r=\"5\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-circleStart\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"-1\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><circle cx=\"5\" cy=\"5\" r=\"5\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-crossEnd\" class=\"marker cross flowchart-v2\" viewBox=\"0 0 11 11\" refX=\"12\" refY=\"5.2\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><path d=\"M 1,1 l 9,9 M 10,1 l -9,9\" class=\"arrowMarkerPath\" style=\"stroke-width: 2; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-crossStart\" class=\"marker cross flowchart-v2\" viewBox=\"0 0 11 11\" refX=\"-1\" refY=\"5.2\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><path d=\"M 1,1 l 9,9 M 10,1 l -9,9\" class=\"arrowMarkerPath\" style=\"stroke-width: 2; stroke-dasharray: 1, 0;\"/></marker><g class=\"root\"><g class=\"clusters\"/><g class=\"edgePaths\"><path d=\"M117.945,214L117.945,218.167C117.945,222.333,117.945,230.667,132.067,244.62C146.189,258.574,174.432,278.148,188.554,287.935L202.676,297.722\" id=\"L_A_D_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_A_D_0\" data-points=\"W3sieCI6MTE3Ljk0NTMxMjUsInkiOjIxNH0seyJ4IjoxMTcuOTQ1MzEyNSwieSI6MjM5fSx7IngiOjIwNS45NjMxNTY5NjAyMjcyOCwieSI6MzAwfV0=\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M371.898,214L371.898,218.167C371.898,222.333,371.898,230.667,357.777,244.62C343.655,258.574,315.412,278.148,301.29,287.935L287.168,297.722\" id=\"L_B_D_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_B_D_0\" data-points=\"W3sieCI6MzcxLjg5ODQzNzUsInkiOjIxNH0seyJ4IjozNzEuODk4NDM3NSwieSI6MjM5fSx7IngiOjI4My44ODA1OTMwMzk3NzI3NSwieSI6MzAwfV0=\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M244.922,354L244.922,364.167C244.922,374.333,244.922,394.667,255.432,408.84C265.943,423.012,286.964,431.025,297.474,435.031L307.985,439.037\" id=\"L_D_C_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_D_C_0\" data-points=\"W3sieCI6MjQ0LjkyMTg3NSwieSI6MzU0fSx7IngiOjI0NC45MjE4NzUsInkiOjQxNX0seyJ4IjozMTEuNzIyNjU2MjUsInkiOjQ0MC40NjE3NjA5MTYyNDkxfV0=\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M117.945,110L117.945,114.167C117.945,118.333,117.945,126.667,117.945,134.333C117.945,142,117.945,149,117.945,152.5L117.945,156\" id=\"L_A1_A_0\" class=\"edge-thickness-normal edge-pattern-dotted edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_A1_A_0\" data-points=\"W3sieCI6MTE3Ljk0NTMxMjUsInkiOjExMH0seyJ4IjoxMTcuOTQ1MzEyNSwieSI6MTM1fSx7IngiOjExNy45NDUzMTI1LCJ5IjoxNjB9XQ==\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M371.898,110L371.898,114.167C371.898,118.333,371.898,126.667,371.898,134.333C371.898,142,371.898,149,371.898,152.5L371.898,156\" id=\"L_B1_B_0\" class=\"edge-thickness-normal edge-pattern-dotted edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_B1_B_0\" data-points=\"W3sieCI6MzcxLjg5ODQzNzUsInkiOjExMH0seyJ4IjozNzEuODk4NDM3NSwieSI6MTM1fSx7IngiOjM3MS44OTg0Mzc1LCJ5IjoxNjB9XQ==\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M517.773,390L517.773,394.167C517.773,398.333,517.773,406.667,507.263,414.84C496.752,423.012,475.731,431.025,465.221,435.031L454.71,439.037\" id=\"L_C1_C_0\" class=\"edge-thickness-normal edge-pattern-dotted edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_C1_C_0\" data-points=\"W3sieCI6NTE3Ljc3MzQzNzUsInkiOjM5MH0seyJ4Ijo1MTcuNzczNDM3NSwieSI6NDE1fSx7IngiOjQ1MC45NzI2NTYyNSwieSI6NDQwLjQ2MTc2MDkxNjI0OTF9XQ==\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/></g><g class=\"edgeLabels\"><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_A_D_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_B_D_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_D_C_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_A1_A_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_B1_B_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_C1_C_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g></g><g class=\"nodes\"><g class=\"node default\" id=\"flowchart-A-0\" transform=\"translate(117.9453125, 187)\"><rect class=\"basic label-container\" style=\"fill:#ffe1e1 !important\" x=\"-88.5625\" y=\"-27\" width=\"177.125\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-58.5625, -12)\"><rect/><foreignObject width=\"117.125\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>System Message</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-D-1\" transform=\"translate(244.921875, 327)\"><rect class=\"basic label-container\" style=\"fill:#fff4e1 !important\" x=\"-92.8515625\" y=\"-27\" width=\"185.703125\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-62.8515625, -12)\"><rect/><foreignObject width=\"125.703125\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>LLM Verarbeitung</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-B-2\" transform=\"translate(371.8984375, 187)\"><rect class=\"basic label-container\" style=\"fill:#e1f5ff !important\" x=\"-79.125\" y=\"-27\" width=\"158.25\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-49.125, -12)\"><rect/><foreignObject width=\"98.25\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>User Message</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-C-5\" transform=\"translate(381.34765625, 467)\"><rect class=\"basic label-container\" style=\"fill:#e1ffe1 !important\" x=\"-69.625\" y=\"-27\" width=\"139.25\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-39.625, -12)\"><rect/><foreignObject width=\"79.25\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>AI Message</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-A1-6\" transform=\"translate(117.9453125, 59)\"><rect class=\"basic label-container\" style=\"\" x=\"-109.9453125\" y=\"-51\" width=\"219.890625\" height=\"102\"/><g class=\"label\" style=\"\" transform=\"translate(-79.9453125, -36)\"><rect/><foreignObject width=\"159.890625\" height=\"72\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Definiert Verhalten<br/>Rolle &amp; Pers√∂nlichkeit<br/>Einschr√§nkungen</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-B1-8\" transform=\"translate(371.8984375, 59)\"><rect class=\"basic label-container\" style=\"\" x=\"-94.0078125\" y=\"-51\" width=\"188.015625\" height=\"102\"/><g class=\"label\" style=\"\" transform=\"translate(-64.0078125, -36)\"><rect/><foreignObject width=\"128.015625\" height=\"72\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Benutzer-Eingabe<br/>Frage oder Befehl<br/>Kontext</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-C1-10\" transform=\"translate(517.7734375, 327)\"><rect class=\"basic label-container\" style=\"\" x=\"-130\" y=\"-63\" width=\"260\" height=\"126\"/><g class=\"label\" style=\"\" transform=\"translate(-100, -48)\"><rect/><foreignObject width=\"200\" height=\"96\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\"><span class=\"nodeLabel\"><p>Generierte Antwort<br/>Basierend auf System + User<br/>Strukturiertes Format</p></span></div></foreignObject></g></g></g></g></g></svg>"},"metadata":{}}]},{"cell_type":"markdown","source":["# 4 | Ein einfaches Modell\n","---\n"],"metadata":{"id":"BMPwDNTCCOfx"},"id":"BMPwDNTCCOfx"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e1926d03"},"outputs":[],"source":["# Importe\n","from langchain.chat_models import init_chat_model\n","from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n","from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n","from langchain_core.prompts.few_shot import FewShotPromptTemplate\n","from langchain_core.output_parsers.string import StrOutputParser"],"id":"e1926d03"},{"cell_type":"code","source":["# Modell definieren (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0.0)"],"metadata":{"id":"RG-y_bG8HOOi"},"execution_count":null,"outputs":[],"id":"RG-y_bG8HOOi"},{"cell_type":"markdown","id":"-JwJD0rLIyT8","metadata":{"id":"-JwJD0rLIyT8"},"source":["# 5 | Prompts\n","---"]},{"cell_type":"code","source":["#@title üßú‚Äç‚ôÄÔ∏è Prozess-Diagramm { display-mode: \"form\" }\n","\n","diagram = \"\"\"\n","flowchart LR\n","    Input[\"Input<br/>(Message)\"] --> Prompt\n","\n","    subgraph Chain\n","        Prompt[\"Prompt<br/>(Template)\"]\n","        Model[\"Model<br/>(LLM)\"]\n","        Response[\"Response<br/>(Parser)\"]\n","        Prompt --> Model --> Response\n","    end\n","\n","    Response --> Result[\"Result\"]\n","\n","    %% HIGHLIGHT einer einzelnen Box\n","    style Prompt fill:#f9c74f,stroke:#b36b00,stroke-width:2px\n","\n","\"\"\"\n","mermaid(diagram)"],"metadata":{"id":"Pho_i5xRbvRa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980401886,"user_tz":-60,"elapsed":1443,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"79aa2fec-7889-4574-cece-7a805723eb70"},"id":"Pho_i5xRbvRa","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.SVG object>"],"image/svg+xml":"<svg xmlns=\"http://www.w3.org/2000/svg\" id=\"container\" width=\"875.03125\" class=\"flowchart\" height=\"164\" viewBox=\"0 0 875.03125 164\" role=\"graphics-document document\" aria-roledescription=\"flowchart-v2\"><style>#container{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:16px;fill:#333;}@keyframes edge-animation-frame{from{stroke-dashoffset:0;}}@keyframes dash{to{stroke-dashoffset:0;}}#container .edge-animation-slow{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 50s linear infinite;stroke-linecap:round;}#container .edge-animation-fast{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 20s linear infinite;stroke-linecap:round;}#container .error-icon{fill:#552222;}#container .error-text{fill:#552222;stroke:#552222;}#container .edge-thickness-normal{stroke-width:1px;}#container .edge-thickness-thick{stroke-width:3.5px;}#container .edge-pattern-solid{stroke-dasharray:0;}#container .edge-thickness-invisible{stroke-width:0;fill:none;}#container .edge-pattern-dashed{stroke-dasharray:3;}#container .edge-pattern-dotted{stroke-dasharray:2;}#container .marker{fill:#333333;stroke:#333333;}#container .marker.cross{stroke:#333333;}#container svg{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:16px;}#container p{margin:0;}#container .label{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;color:#333;}#container .cluster-label text{fill:#333;}#container .cluster-label span{color:#333;}#container .cluster-label span p{background-color:transparent;}#container .label text,#container span{fill:#333;color:#333;}#container .node rect,#container .node circle,#container .node ellipse,#container .node polygon,#container .node path{fill:#ECECFF;stroke:#9370DB;stroke-width:1px;}#container .rough-node .label text,#container .node .label text,#container .image-shape .label,#container .icon-shape .label{text-anchor:middle;}#container .node .katex path{fill:#000;stroke:#000;stroke-width:1px;}#container .rough-node .label,#container .node .label,#container .image-shape .label,#container .icon-shape .label{text-align:center;}#container .node.clickable{cursor:pointer;}#container .root .anchor path{fill:#333333!important;stroke-width:0;stroke:#333333;}#container .arrowheadPath{fill:#333333;}#container .edgePath .path{stroke:#333333;stroke-width:2.0px;}#container .flowchart-link{stroke:#333333;fill:none;}#container .edgeLabel{background-color:rgba(232,232,232, 0.8);text-align:center;}#container .edgeLabel p{background-color:rgba(232,232,232, 0.8);}#container .edgeLabel rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#container .labelBkg{background-color:rgba(232, 232, 232, 0.5);}#container .cluster rect{fill:#ffffde;stroke:#aaaa33;stroke-width:1px;}#container .cluster text{fill:#333;}#container .cluster span{color:#333;}#container div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:12px;background:hsl(80, 100%, 96.2745098039%);border:1px solid #aaaa33;border-radius:2px;pointer-events:none;z-index:100;}#container .flowchartTitleText{text-anchor:middle;font-size:18px;fill:#333;}#container rect.text{fill:none;stroke-width:0;}#container .icon-shape,#container .image-shape{background-color:rgba(232,232,232, 0.8);text-align:center;}#container .icon-shape p,#container .image-shape p{background-color:rgba(232,232,232, 0.8);padding:2px;}#container .icon-shape rect,#container .image-shape rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#container .label-icon{display:inline-block;height:1em;overflow:visible;vertical-align:-0.125em;}#container .node .label-icon path{fill:currentColor;stroke:revert;stroke-width:revert;}#container :root{--mermaid-font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;}</style><g><marker id=\"container_flowchart-v2-pointEnd\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"5\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"8\" markerHeight=\"8\" orient=\"auto\"><path d=\"M 0 0 L 10 5 L 0 10 z\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-pointStart\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"4.5\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"8\" markerHeight=\"8\" orient=\"auto\"><path d=\"M 0 5 L 10 10 L 10 0 z\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-circleEnd\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"11\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><circle cx=\"5\" cy=\"5\" r=\"5\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-circleStart\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"-1\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><circle cx=\"5\" cy=\"5\" r=\"5\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-crossEnd\" class=\"marker cross flowchart-v2\" viewBox=\"0 0 11 11\" refX=\"12\" refY=\"5.2\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><path d=\"M 1,1 l 9,9 M 10,1 l -9,9\" class=\"arrowMarkerPath\" style=\"stroke-width: 2; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-crossStart\" class=\"marker cross flowchart-v2\" viewBox=\"0 0 11 11\" refX=\"-1\" refY=\"5.2\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><path d=\"M 1,1 l 9,9 M 10,1 l -9,9\" class=\"arrowMarkerPath\" style=\"stroke-width: 2; stroke-dasharray: 1, 0;\"/></marker><g class=\"root\"><g class=\"clusters\"><g class=\"cluster\" id=\"Chain\" data-look=\"classic\"><rect style=\"\" x=\"189.5\" y=\"8\" width=\"522.109375\" height=\"148\"/><g class=\"cluster-label\" transform=\"translate(430.1640625, 8)\"><foreignObject width=\"40.78125\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Chain</p></span></div></foreignObject></g></g></g><g class=\"edgePaths\"><path d=\"M139.5,82L143.667,82C147.833,82,156.167,82,164.5,82C172.833,82,181.167,82,188.833,82C196.5,82,203.5,82,207,82L210.5,82\" id=\"L_Input_Prompt_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_Input_Prompt_0\" data-points=\"W3sieCI6MTM5LjUsInkiOjgyfSx7IngiOjE2NC41LCJ5Ijo4Mn0seyJ4IjoxODkuNSwieSI6ODJ9LHsieCI6MjE0LjUsInkiOjgyfV0=\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M351.766,82L355.932,82C360.099,82,368.432,82,376.099,82C383.766,82,390.766,82,394.266,82L397.766,82\" id=\"L_Prompt_Model_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_Prompt_Model_0\" data-points=\"W3sieCI6MzUxLjc2NTYyNSwieSI6ODJ9LHsieCI6Mzc2Ljc2NTYyNSwieSI6ODJ9LHsieCI6NDAxLjc2NTYyNSwieSI6ODJ9XQ==\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M506.547,82L510.714,82C514.88,82,523.214,82,530.88,82C538.547,82,545.547,82,549.047,82L552.547,82\" id=\"L_Model_Response_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_Model_Response_0\" data-points=\"W3sieCI6NTA2LjU0Njg3NSwieSI6ODJ9LHsieCI6NTMxLjU0Njg3NSwieSI6ODJ9LHsieCI6NTU2LjU0Njg3NSwieSI6ODJ9XQ==\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M686.609,82L690.776,82C694.943,82,703.276,82,711.609,82C719.943,82,728.276,82,735.943,82C743.609,82,750.609,82,754.109,82L757.609,82\" id=\"L_Response_Result_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_Response_Result_0\" data-points=\"W3sieCI6Njg2LjYwOTM3NSwieSI6ODJ9LHsieCI6NzExLjYwOTM3NSwieSI6ODJ9LHsieCI6NzM2LjYwOTM3NSwieSI6ODJ9LHsieCI6NzYxLjYwOTM3NSwieSI6ODJ9XQ==\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/></g><g class=\"edgeLabels\"><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_Input_Prompt_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_Prompt_Model_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_Model_Response_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_Response_Result_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g></g><g class=\"nodes\"><g class=\"node default\" id=\"flowchart-Input-0\" transform=\"translate(73.75, 82)\"><rect class=\"basic label-container\" style=\"\" x=\"-65.75\" y=\"-39\" width=\"131.5\" height=\"78\"/><g class=\"label\" style=\"\" transform=\"translate(-35.75, -24)\"><rect/><foreignObject width=\"71.5\" height=\"48\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Input<br/>(Message)</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-Prompt-1\" transform=\"translate(283.1328125, 82)\"><rect class=\"basic label-container\" style=\"fill:#f9c74f !important;stroke:#b36b00 !important;stroke-width:2px !important\" x=\"-68.6328125\" y=\"-39\" width=\"137.265625\" height=\"78\"/><g class=\"label\" style=\"\" transform=\"translate(-38.6328125, -24)\"><rect/><foreignObject width=\"77.265625\" height=\"48\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Prompt<br/>(Template)</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-Model-3\" transform=\"translate(454.15625, 82)\"><rect class=\"basic label-container\" style=\"\" x=\"-52.390625\" y=\"-39\" width=\"104.78125\" height=\"78\"/><g class=\"label\" style=\"\" transform=\"translate(-22.390625, -24)\"><rect/><foreignObject width=\"44.78125\" height=\"48\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Model<br/>(LLM)</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-Response-4\" transform=\"translate(621.578125, 82)\"><rect class=\"basic label-container\" style=\"\" x=\"-65.03125\" y=\"-39\" width=\"130.0625\" height=\"78\"/><g class=\"label\" style=\"\" transform=\"translate(-35.03125, -24)\"><rect/><foreignObject width=\"70.0625\" height=\"48\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Response<br/>(Parser)</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-Result-9\" transform=\"translate(814.3203125, 82)\"><rect class=\"basic label-container\" style=\"\" x=\"-52.7109375\" y=\"-27\" width=\"105.421875\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-22.7109375, -12)\"><rect/><foreignObject width=\"45.421875\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Result</p></span></div></foreignObject></g></g></g></g></g></svg>"},"metadata":{}}]},{"cell_type":"markdown","source":["\n","\n","Prompts sind die Eingaben bzw. Auftr√§ge an das LLM. LangChain bietet verschiedene Template-Systeme. Ein **Template** ist eine **Vorlage** ‚Äì genauer gesagt ein Text mit **Platzhaltern**, die sp√§ter durch echte Werte ersetzt werden.\n","\n","\n","Ein **PromptTemplate** ist eine Vorlage f√ºr einen Eingabetext, der an ein Sprachmodell geschickt wird.\n","Er enth√§lt Platzhalter `{placeholder}`, die sp√§ter mit Nutzereingaben oder Kontext bef√ºllt werden.\n","\n","**Templates machen die KI-Prompts:**\n","\n","+ Wiederverwendbar (z.‚ÄØB. f√ºr viele Themen oder Fragen)\n","\n","+ Strukturiert (z.‚ÄØB. mit Rollen und Anweisungen)\n","\n","+ Dynamisch erweiterbar (mit Memory, Tools, etc.)"],"metadata":{"id":"QMVO4fzPFZ88"},"id":"QMVO4fzPFZ88"},{"cell_type":"markdown","id":"HqTUeMW20084","metadata":{"id":"HqTUeMW20084"},"source":["## 5.1 | Simple Prompt\n"]},{"cell_type":"markdown","source":["<p><font color='darkblue' size=\"4\">\n","<b>‚ùóHinweis:</b>\n","</font></p>\n","Ohne Memory-Management hat das Modell ein kurzes Ged√§chnis.\n"],"metadata":{"id":"5JF1_AvYNYY_"},"id":"5JF1_AvYNYY_"},{"cell_type":"markdown","source":["**Prompt wird als Zeichenkette √ºbergeben:**"],"metadata":{"id":"1azC75R5JpWC"},"id":"1azC75R5JpWC"},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(\"Was ist Generative KI?\")"],"metadata":{"id":"jPizca8FJV5b"},"id":"jPizca8FJV5b","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Antwort\n","type(response)"],"metadata":{"id":"EHjmjLgW3ENA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980409428,"user_tz":-60,"elapsed":19,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"e0cabc37-0aff-4c71-fca2-03f1655d505d"},"id":"EHjmjLgW3ENA","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["langchain_core.messages.ai.AIMessage"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.messages.ai.AIMessage</b><br/>def __init__(content: str | list[str | dict] | None=None, content_blocks: list[types.ContentBlock] | None=None, **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/langchain_core/messages/ai.py</a>Message from an AI.\n","\n","An `AIMessage` is returned from a chat model as a response to a prompt.\n","\n","This message represents the output of the model and consists of both\n","the raw output as returned by the model and standardized fields\n","(e.g., tool calls, usage metadata) added by the LangChain framework.</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 160);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["response"],"metadata":{"id":"4ZYngACo4RWN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980409507,"user_tz":-60,"elapsed":64,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"8784c07c-0525-4298-a273-285191c316de"},"id":"4ZYngACo4RWN","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='Generative KI, oder generative k√ºnstliche Intelligenz, bezieht sich auf eine Klasse von Algorithmen und Modellen, die in der Lage sind, neue Inhalte zu erzeugen. Diese Inhalte k√∂nnen in verschiedenen Formen vorliegen, darunter Texte, Bilder, Musik, Videos und mehr. Generative KI nutzt h√§ufig Techniken des maschinellen Lernens, insbesondere neuronale Netzwerke, um Muster in bestehenden Daten zu erkennen und darauf basierend neue, √§hnliche Daten zu generieren.\\n\\nEin bekanntes Beispiel f√ºr generative KI sind Sprachmodelle wie GPT (Generative Pre-trained Transformer), die in der Lage sind, menschen√§hnliche Texte zu schreiben, Fragen zu beantworten oder Geschichten zu erz√§hlen. In der Bildgenerierung gibt es Modelle wie DALL-E oder Midjourney, die aus Textbeschreibungen Bilder erstellen k√∂nnen.\\n\\nGenerative KI hat viele Anwendungen, darunter:\\n\\n1. **Kreative Inhalte**: Erstellung von Kunst, Musik, Literatur und Design.\\n2. **Datenaugmentation**: Generierung zus√§tzlicher Trainingsdaten f√ºr maschinelles Lernen.\\n3. **Simulationen**: Erzeugung realistischer Szenarien f√ºr Training und Forschung.\\n4. **Personalisierung**: Anpassung von Inhalten an individuelle Nutzerpr√§ferenzen.\\n\\nDie Technologie wirft jedoch auch ethische und rechtliche Fragen auf, insbesondere in Bezug auf Urheberrecht, F√§lschungen und die potenzielle Verbreitung von Fehlinformationen.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 297, 'prompt_tokens': 13, 'total_tokens': 310, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_373a14eb6f', 'id': 'chatcmpl-D8lByTH6AVDVEWCl4cFpfmp9zKcx0', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c56a8-64b2-71c0-9b62-19dc83357b63-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 13, 'output_tokens': 297, 'total_tokens': 310, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["for r in response:\n","    print(r)"],"metadata":{"id":"jveXqUjbpo0s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980409531,"user_tz":-60,"elapsed":33,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"9ce810ff-ea41-4814-c308-d38cc1cbac55"},"id":"jveXqUjbpo0s","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["('content', 'Generative KI, oder generative k√ºnstliche Intelligenz, bezieht sich auf eine Klasse von Algorithmen und Modellen, die in der Lage sind, neue Inhalte zu erzeugen. Diese Inhalte k√∂nnen in verschiedenen Formen vorliegen, darunter Texte, Bilder, Musik, Videos und mehr. Generative KI nutzt h√§ufig Techniken des maschinellen Lernens, insbesondere neuronale Netzwerke, um Muster in bestehenden Daten zu erkennen und darauf basierend neue, √§hnliche Daten zu generieren.\\n\\nEin bekanntes Beispiel f√ºr generative KI sind Sprachmodelle wie GPT (Generative Pre-trained Transformer), die in der Lage sind, menschen√§hnliche Texte zu schreiben, Fragen zu beantworten oder Geschichten zu erz√§hlen. In der Bildgenerierung gibt es Modelle wie DALL-E oder Midjourney, die aus Textbeschreibungen Bilder erstellen k√∂nnen.\\n\\nGenerative KI hat viele Anwendungen, darunter:\\n\\n1. **Kreative Inhalte**: Erstellung von Kunst, Musik, Literatur und Design.\\n2. **Datenaugmentation**: Generierung zus√§tzlicher Trainingsdaten f√ºr maschinelles Lernen.\\n3. **Simulationen**: Erzeugung realistischer Szenarien f√ºr Training und Forschung.\\n4. **Personalisierung**: Anpassung von Inhalten an individuelle Nutzerpr√§ferenzen.\\n\\nDie Technologie wirft jedoch auch ethische und rechtliche Fragen auf, insbesondere in Bezug auf Urheberrecht, F√§lschungen und die potenzielle Verbreitung von Fehlinformationen.')\n","('additional_kwargs', {'refusal': None})\n","('response_metadata', {'token_usage': {'completion_tokens': 297, 'prompt_tokens': 13, 'total_tokens': 310, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_373a14eb6f', 'id': 'chatcmpl-D8lByTH6AVDVEWCl4cFpfmp9zKcx0', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None})\n","('type', 'ai')\n","('name', None)\n","('id', 'lc_run--019c56a8-64b2-71c0-9b62-19dc83357b63-0')\n","('tool_calls', [])\n","('invalid_tool_calls', [])\n","('usage_metadata', {'input_tokens': 13, 'output_tokens': 297, 'total_tokens': 310, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n"]}]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"cOJGeiLGJb-g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980409576,"user_tz":-60,"elapsed":29,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"cd8e011b-8813-4b4e-e354-b2b6e4f6340b"},"id":"cOJGeiLGJb-g","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Generative KI, oder generative k√ºnstliche Intelligenz, bezieht sich auf eine Klasse von Algorithmen und Modellen, die in der Lage sind, neue Inhalte zu erzeugen. Diese Inhalte k√∂nnen in verschiedenen Formen vorliegen, darunter Texte, Bilder, Musik, Videos und mehr. Generative KI nutzt h√§ufig Techniken des maschinellen Lernens, insbesondere neuronale Netzwerke, um Muster in bestehenden Daten zu erkennen und darauf basierend neue, √§hnliche Daten zu generieren.\n\nEin bekanntes Beispiel f√ºr generative KI sind Sprachmodelle wie GPT (Generative Pre-trained Transformer), die in der Lage sind, menschen√§hnliche Texte zu schreiben, Fragen zu beantworten oder Geschichten zu erz√§hlen. In der Bildgenerierung gibt es Modelle wie DALL-E oder Midjourney, die aus Textbeschreibungen Bilder erstellen k√∂nnen.\n\nGenerative KI hat viele Anwendungen, darunter:\n\n1. **Kreative Inhalte**: Erstellung von Kunst, Musik, Literatur und Design.\n2. **Datenaugmentation**: Generierung zus√§tzlicher Trainingsdaten f√ºr maschinelles Lernen.\n3. **Simulationen**: Erzeugung realistischer Szenarien f√ºr Training und Forschung.\n4. **Personalisierung**: Anpassung von Inhalten an individuelle Nutzerpr√§ferenzen.\n\nDie Technologie wirft jedoch auch ethische und rechtliche Fragen auf, insbesondere in Bezug auf Urheberrecht, F√§lschungen und die potenzielle Verbreitung von Fehlinformationen."},"metadata":{}}]},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(\"Was war meine letzte Frage?\")"],"metadata":{"id":"9fayTzIrNN2b"},"id":"9fayTzIrNN2b","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"IahQJ0mFNUUV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980410505,"user_tz":-60,"elapsed":18,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"ce3118c4-d5f3-4eee-f43f-139c24047701"},"id":"IahQJ0mFNUUV","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Es tut mir leid, aber ich kann keine vorherigen Interaktionen oder Fragen speichern oder abrufen. Wie kann ich Ihnen heute helfen?"},"metadata":{}}]},{"cell_type":"markdown","source":["**Prompt wird als formatierte String-Variable √ºbergeben:**"],"metadata":{"id":"aKRhzB0oJnUG"},"id":"aKRhzB0oJnUG"},{"cell_type":"code","source":["# f-String f√ºr simple prompt mit template\n","thema = \"Machine Learning\"\n","prompt = f\"Erkl√§re {thema} in einfachen Worten.\""],"metadata":{"id":"8s5cQY2oHUvW"},"id":"8s5cQY2oHUvW","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(prompt)"],"metadata":{"id":"plYGfaoLJ8DI"},"execution_count":null,"outputs":[],"id":"plYGfaoLJ8DI"},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"SLX0zaONJ8DJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980414908,"user_tz":-60,"elapsed":66,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"93c7adbf-d98a-4fd6-c5d5-0dbff1f82cae"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine Learning, oder maschinelles Lernen, ist ein Teilbereich der K√ºnstlichen Intelligenz, der es Computern erm√∂glicht, aus Daten zu lernen und Muster zu erkennen, ohne dass sie daf√ºr explizit programmiert werden m√ºssen. \n\nStell dir vor, du hast viele Bilder von Katzen und Hunden. Anstatt einem Computer genau zu sagen, wie eine Katze oder ein Hund aussieht, zeigst du ihm einfach viele Beispiele. Der Computer analysiert diese Bilder und lernt, die Unterschiede zu erkennen. Nach einer Weile kann er dann selbstst√§ndig entscheiden, ob ein neues Bild eine Katze oder einen Hund zeigt.\n\nIm Grunde genommen geht es beim maschinellen Lernen darum, dass Computer aus Erfahrungen lernen, √§hnlich wie Menschen, und dadurch besser in bestimmten Aufgaben werden."},"metadata":{}}],"id":"SLX0zaONJ8DJ"},{"cell_type":"markdown","source":["**Prompt wird als template √ºbergeben:**"],"metadata":{"id":"n-VVZEHDKSIw"},"id":"n-VVZEHDKSIw"},{"cell_type":"code","source":["# Einfaches Template mit Platzhaltern\n","simple_prompt_template = PromptTemplate.from_template(\n","    \"Erkl√§re {thema} in einfachen Worten.\"\n",")"],"metadata":{"id":"koh0-_3r_HxG"},"id":"koh0-_3r_HxG","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prompt-Erstellung\n","thema = \"Generative KI\"\n","prompt = simple_prompt_template.format(thema=thema)\n","prompt"],"metadata":{"id":"nFrCjL6j_xo1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980414958,"user_tz":-60,"elapsed":15,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"a4b18d56-15de-4a98-a012-ed0e7fab2916"},"id":"nFrCjL6j_xo1","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Erkl√§re Generative KI in einfachen Worten.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(prompt)"],"metadata":{"id":"-of5FbM02qfZ"},"id":"-of5FbM02qfZ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"lmEE-3NDAE15","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980419449,"user_tz":-60,"elapsed":17,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"b347af29-c7cd-43e5-f0ea-bf5654e38f84"},"id":"lmEE-3NDAE15","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Generative KI ist eine Art von k√ºnstlicher Intelligenz, die in der Lage ist, neue Inhalte zu erstellen. Das k√∂nnen Texte, Bilder, Musik oder sogar Videos sein. Sie funktioniert, indem sie aus gro√üen Mengen von Daten lernt, wie bestimmte Inhalte aussehen oder klingen. \n\nStell dir vor, du hast viele Bilder von Katzen und Hunden. Eine generative KI kann lernen, wie diese Tiere aussehen, und dann ein ganz neues Bild von einem Tier erstellen, das es noch nie zuvor gegeben hat. \n\nIm Grunde genommen ist generative KI wie ein kreativer Computer, der neue Ideen und Inhalte basierend auf dem, was er gelernt hat, entwickeln kann."},"metadata":{}}]},{"cell_type":"markdown","id":"8akKmCCc06nm","metadata":{"id":"8akKmCCc06nm"},"source":["## 5.2 | ChatPromptTemplate üìå"]},{"cell_type":"markdown","source":["Das **ChatPromptTemplate** ist eine spezielle Prompt-Vorlage f√ºr Chatmodelle (z.‚ÄØB. GPT-3.5, GPT-4), die **mehrere Rollen und Nachrichten** unterst√ºtzt ‚Äì also genau das, was Chat-Modelle eigentlich brauchen.\n","\n","Es geh√∂rt zur LangChain-Bibliothek und baut auf dem Prinzip auf:\n","\n","\n","**Wer sagt was? ‚Üí system, user, assistant, etc.**"],"metadata":{"id":"SVoHwxtrMM_j"},"id":"SVoHwxtrMM_j"},{"cell_type":"markdown","source":["In LangChain unterst√ºtzte Rollen:\n","\n","+ \"system\" - f√ºr Systemnachrichten\n","+ \"assistant\" oder \"ai\" - f√ºr Assistenten-/KI-Antworten\n","+ \"user\" oder \"human\" - f√ºr Benutzernachrichten"],"metadata":{"id":"Po7W2FKz-VEd"},"id":"Po7W2FKz-VEd"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Variante 1: Liste & Tupel üìå\n","</font></p>"],"metadata":{"id":"yw4qr0mC5A6m"},"id":"yw4qr0mC5A6m"},{"cell_type":"code","execution_count":null,"id":"iW9URvON0IXr","metadata":{"id":"iW9URvON0IXr"},"outputs":[],"source":["# Template als Liste & Tupel\n","chat_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    (\"human\", \"Erkl√§re mir {thema}\"),\n","])"]},{"cell_type":"code","source":["# Variablen zuweisen\n","system_prompt = \"Du bist ein hilfreicher und humorvoller Assistent.\"\n","thema = \"Machine Learning\""],"metadata":{"id":"XrEDZ3Vz174e"},"id":"XrEDZ3Vz174e","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Template & Variablen kombinieren\n","prompt = chat_template.format_messages(system_prompt=system_prompt, thema=thema)"],"metadata":{"id":"x3pxKD_R2BvX"},"id":"x3pxKD_R2BvX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sprachmodell aufrufe\n","response = llm.invoke(prompt)"],"metadata":{"id":"CLU3o3dG4OGM"},"id":"CLU3o3dG4OGM","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"YnfES3Q24n0M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980435374,"user_tz":-60,"elapsed":24,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"064dd10d-335a-4cdc-9efe-d620e9733521"},"id":"YnfES3Q24n0M","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Klar, ich erkl√§re dir Machine Learning! \n\nStell dir vor, du hast einen Hund, der nicht wei√ü, wie man ‚ÄûSitz‚Äú macht. Du zeigst ihm immer wieder, was er tun soll, und wenn er es richtig macht, gibst du ihm ein Leckerli. Nach einer Weile lernt der Hund, dass ‚ÄûSitz‚Äú bedeutet, sich hinzusetzen, um das Leckerli zu bekommen. \n\nMachine Learning funktioniert √§hnlich, aber anstelle von Hunden und Leckerlis arbeiten wir mit Computern und Daten. Es ist ein Teilbereich der K√ºnstlichen Intelligenz, bei dem Computer aus Erfahrungen (Daten) lernen, ohne explizit programmiert zu werden.\n\nHier sind die grundlegenden Schritte:\n\n1. **Daten sammeln**: Du brauchst eine Menge Daten, die das Problem beschreiben, das du l√∂sen m√∂chtest. Das k√∂nnen Bilder, Texte, Zahlen oder alles M√∂gliche sein.\n\n2. **Modell w√§hlen**: Du w√§hlst ein Modell aus, das die Beziehung zwischen den Daten und dem gew√ºnschten Ergebnis lernen soll. Das kann ein einfaches lineares Modell oder ein komplexes neuronales Netzwerk sein.\n\n3. **Training**: Du f√ºtterst das Modell mit den gesammelten Daten und l√§sst es lernen. Dabei passt es seine internen Parameter an, um die besten Vorhersagen zu treffen.\n\n4. **Testen**: Du √ºberpr√ºfst, wie gut das Modell funktioniert, indem du es mit neuen, unbekannten Daten testest. So siehst du, ob es wirklich gelernt hat oder ob es nur auswendig gelernt hat (das nennt man √úberanpassung).\n\n5. **Anpassen und Optimieren**: Je nach den Ergebnissen kannst du das Modell anpassen, mehr Daten sammeln oder andere Techniken ausprobieren, um die Leistung zu verbessern.\n\n6. **Einsatz**: Wenn das Modell gut funktioniert, kannst du es in der realen Welt einsetzen, um Vorhersagen zu treffen oder Entscheidungen zu unterst√ºtzen.\n\nUnd das ist im Grunde genommen Machine Learning! Es ist wie das Training eines Hundes, nur dass der Hund ein Computer ist und die Leckerlis Daten sind. Wenn du noch mehr Fragen hast oder etwas genauer wissen m√∂chtest, sag einfach Bescheid! üê∂üíª"},"metadata":{}}]},{"cell_type":"markdown","source":["**... oder ...**"],"metadata":{"id":"6FrHMy1PYH0S"},"id":"6FrHMy1PYH0S"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Variante 2: Liste & Dictionary\n","</font></p>"],"metadata":{"id":"lkaCyUqq6OD0"},"id":"lkaCyUqq6OD0"},{"cell_type":"code","source":["# Template als Liste & Dictionary\n","chat_template = ChatPromptTemplate.from_messages([\n","    {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher und humorvoller Assistent.\"},\n","    {\"role\": \"human\", \"content\": \"Erkl√§re mir {thema}\"},\n","])"],"metadata":{"id":"kxEP8r3VDWwI"},"id":"kxEP8r3VDWwI","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Variablen zuweisen\n","system_prompt = \"Du bist ein hilfreicher und humorvoller Assistent.\"\n","thema = \"Machine Learning\""],"metadata":{"id":"naATTZyS4zEB"},"execution_count":null,"outputs":[],"id":"naATTZyS4zEB"},{"cell_type":"code","source":["# Template & Variablen kombinieren\n","prompt = chat_template.format_messages(system_prompt=system_prompt, thema=thema)"],"metadata":{"id":"O235HSRl4zEC"},"execution_count":null,"outputs":[],"id":"O235HSRl4zEC"},{"cell_type":"code","source":["# Sprachmodell aufrufe\n","response = llm.invoke(prompt)"],"metadata":{"id":"Ow6KjCLG4zEC"},"execution_count":null,"outputs":[],"id":"Ow6KjCLG4zEC"},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"6PtZzYUSC7-F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980447882,"user_tz":-60,"elapsed":25,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"4870a5fd-0481-409a-8dee-06e1840b5e73"},"id":"6PtZzYUSC7-F","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Klar, ich erkl√§re dir gerne Machine Learning! \n\nStell dir vor, du hast einen Roboterfreund, der lernen m√∂chte, wie man √Ñpfel von Orangen unterscheidet. Anstatt ihm einfach zu sagen, was was ist, zeigst du ihm viele Bilder von √Ñpfeln und Orangen. Der Roboter schaut sich die Bilder an, merkt sich die Unterschiede und versucht, Muster zu erkennen. \n\nMachine Learning funktioniert √§hnlich: Es ist ein Teilbereich der K√ºnstlichen Intelligenz, bei dem Computer aus Daten lernen, ohne explizit programmiert zu werden. Stattdessen verwenden sie Algorithmen, um Muster in den Daten zu finden und Vorhersagen zu treffen oder Entscheidungen zu treffen.\n\nHier sind die grundlegenden Schritte:\n\n1. **Daten sammeln**: Du brauchst eine Menge Daten, um deinem Modell beizubringen, was es lernen soll. Das k√∂nnen Bilder, Texte, Zahlen oder andere Informationen sein.\n\n2. **Daten vorbereiten**: Die Daten m√ºssen oft bereinigt und in ein passendes Format gebracht werden. Manchmal sind sie so chaotisch wie ein unordentliches Zimmer!\n\n3. **Modell w√§hlen**: Es gibt verschiedene Algorithmen, die du verwenden kannst, je nachdem, was du erreichen m√∂chtest. Einige sind wie ein guter Koch, der verschiedene Rezepte ausprobiert, um das beste Gericht zu finden.\n\n4. **Training**: Hier wird das Modell ‚Äûtrainiert‚Äú, indem es die Daten analysiert und lernt, Muster zu erkennen. Das ist wie das √úben f√ºr einen Wettbewerb ‚Äì je mehr du √ºbst, desto besser wirst du!\n\n5. **Testen**: Nach dem Training wird das Modell mit neuen, unbekannten Daten getestet, um zu sehen, wie gut es gelernt hat. Wenn es gut abschneidet, ist das wie ein A+ in der Schule!\n\n6. **Anpassen und Verbessern**: Manchmal muss man das Modell anpassen, um die Leistung zu verbessern. Das ist wie das Feintuning eines Instruments, um den perfekten Klang zu erreichen.\n\nMachine Learning wird in vielen Bereichen eingesetzt, von der Spracherkennung √ºber Bildverarbeitung bis hin zu Empfehlungen in Online-Shops. Es ist ein spannendes Feld, das st√§ndig w√§chst und sich weiterentwickelt ‚Äì fast so schnell wie die neuesten Internet-Memes! \n\nWenn du noch mehr Fragen hast oder etwas genauer wissen m√∂chtest, sag einfach Bescheid!"},"metadata":{}}]},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Vergleich: f-string vs. PromptTemplate vs. ChatPromptTemplate\n","</font></p>"],"metadata":{"id":"2uSD48umMXI7"},"id":"2uSD48umMXI7"},{"cell_type":"markdown","source":["\n","\n","| Vorteil                      | `f-string` (Python)           | `PromptTemplate`              | `ChatPromptTemplate`            |\n","| ---------------------------- | ----------------------------- | ----------------------------- | ------------------------------- |\n","| üöª Rollenstruktur            | ‚ùå Keine Unterst√ºtzung         | ‚ùå Nur einfacher Text          | ‚úÖ Klar: `system`, `user`, `ai`  |\n","| üìö Konversationsaufbau       | ‚ùå Nicht geeignet              | ‚ùå Nicht geeignet              | ‚úÖ Ideal f√ºr Multi-Turn-Dialoge <br>    (mehreren Gespr√§chsrunden)  |\n","| üîÑ Kombinierbar mit `Memory` | ‚ùå Nein                        | ‚ùå Nein                        | ‚úÖ Ja, via `MessagesPlaceholder` |\n","| üîå F√ºr Chatmodelle wie GPT   | ‚ö†Ô∏è Manuell als String         | ‚ö†Ô∏è Nur als Flie√ütext          | ‚úÖ Nativ & strukturiert          |\n","| üîç Klarheit & Wartbarkeit    | ‚ö†Ô∏è Inline, schwer erweiterbar | ‚ö†Ô∏è Nur Inline-Text            | ‚úÖ Saubere Message-Trennung      |\n","| üß™ Prompt-Testbarkeit        | ‚ùå Keine Validierung           | ‚ö†Ô∏è Eingeschr√§nkt               | ‚úÖ Besser strukturiert           |\n","| ‚ö° Performance                | ‚úÖ Schnell & direkt            | ‚úÖ Schnell                     | ‚úÖ Schnell                       |\n","| üîß Validierung von Variablen | ‚ùå Keine                       | ‚úÖ Automatisch                 | ‚úÖ Automatisch                   |\n","| üß© Wiederverwendbarkeit      | ‚ùå Eingeschr√§nkt               | ‚úÖ Als Template-Objekt         | ‚úÖ Als Template-Objekt           |\n","| üìù Partial Variables         | ‚ùå Nein                        | ‚úÖ Ja                          | ‚úÖ Ja                            |\n","| üîÄ Chains & Pipelines        | ‚ùå Nicht integrierbar          | ‚úÖ LangChain-kompatibel        | ‚úÖ LangChain-kompatibel          |\n","| üéØ Output Parser             | ‚ùå Manuell                     | ‚úÖ Integrierbar                | ‚úÖ Integrierbar                  |"],"metadata":{"id":"UvoCBqi0qc4B"},"id":"UvoCBqi0qc4B"},{"cell_type":"markdown","source":["\n","\n","**Anwendungsempfehlungen**\n","\n","| Methode               | Wann verwenden?                                                                 |\n","| --------------------- | ------------------------------------------------------------------------------- |\n","| **f-string**          | Einfache, einmalige Prompts ¬∑ Prototyping & schnelle Tests ¬∑ Keine LangChain-Integration |\n","| **PromptTemplate**    | Single-Turn Prompts mit Variablen ¬∑ Completion-Modelle (GPT-3, etc.) ¬∑ Keine Konversation |\n","| **ChatPromptTemplate** | Multi-Turn Dialoge ¬∑ Chat-Modelle (GPT-4, Claude, etc.) ¬∑ Komplexe Konversationen mit Memory |\n"],"metadata":{"id":"IrQUS1yvpsFQ"},"id":"IrQUS1yvpsFQ"},{"cell_type":"markdown","metadata":{"id":"Ps344Ejw0-ih"},"source":["## 5.3 | Few-Shot Prompt\n"],"id":"Ps344Ejw0-ih"},{"cell_type":"markdown","source":["Few-Shot-Prompting ist eine Technik, bei der man einem KI-Modell einige Beispiele (sogenannte \"Shots\") vorlegt, bevor es eine neue Aufgabe l√∂sen soll. Das Modell kann aus diesen Beispielen das Muster erkennen und auf neue, √§hnliche F√§lle anwenden."],"metadata":{"id":"LKY3IaJSDLvt"},"id":"LKY3IaJSDLvt"},{"cell_type":"markdown","source":["`FewShotPromptTemplate` ist eine Prompt-Vorlage, die automatisch mehrere Beispiele einf√ºgt, bevor das eigentliche Nutzer-Input kommt."],"metadata":{"id":"NnBh2dz8M2fi"},"id":"NnBh2dz8M2fi"},{"cell_type":"code","execution_count":null,"metadata":{"id":"cEopgOiS0KU_"},"outputs":[],"source":["beispiele = [\n","    {\"frage\": \"Was ist Python?\", \"antwort\": \"Eine interpretierende Programmiersprache.\"},\n","    {\"frage\": \"Was ist Java?\", \"antwort\": \"Eine objektorientierte Sprache.\"}\n","]"],"id":"cEopgOiS0KU_"},{"cell_type":"code","source":["beispiel_prompt_template = PromptTemplate.from_template(\n","    \"Frage: {frage}\\nAntwort: {antwort}\"\n",")"],"metadata":{"id":"fHEYrEPcDD9S"},"execution_count":null,"outputs":[],"id":"fHEYrEPcDD9S"},{"cell_type":"code","source":["system_prompt = \"Du bist ein Assistent, der Fragen zu Programmiersprachen beantwortet. Halte deine Antworten kurz und pr√§zise:\""],"metadata":{"id":"eAk2iqPrjynu"},"id":"eAk2iqPrjynu","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Few-Shot-Prompt\n","few_shot_prompt_template = FewShotPromptTemplate(\n","    examples=beispiele,\n","    example_prompt=beispiel_prompt_template,\n","    prefix=system_prompt,\n","    suffix=\"Frage: {frage}\\nAntwort:\",\n","    input_variables=[\"frage\"]\n",")\n","\n","prompt = few_shot_prompt_template.format(frage=\"Was ist Go?\")"],"metadata":{"id":"BpIbLhIhDcUp"},"execution_count":null,"outputs":[],"id":"BpIbLhIhDcUp"},{"cell_type":"markdown","source":["**Struktur des finalen Prompts:**\n","\n","```\n","[prefix]\n","[formatiertes Beispiel 1]\n","[formatiertes Beispiel 2]\n","...\n","[suffix mit aktueller Frage]\n","```\n","\n"],"metadata":{"id":"tpDH0jAgknKS"},"id":"tpDH0jAgknKS"},{"cell_type":"code","source":["response = llm.invoke(prompt)"],"metadata":{"id":"KZPoIO6gDo23"},"execution_count":null,"outputs":[],"id":"KZPoIO6gDo23"},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"Yr0YleeiD_x7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980448653,"user_tz":-60,"elapsed":24,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"84dcc0fd-6866-4779-82c4-e66b087481c7"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Eine kompilierte Programmiersprache, die f√ºr Effizienz und Einfachheit entwickelt wurde."},"metadata":{}}],"id":"Yr0YleeiD_x7"},{"cell_type":"markdown","source":["**Vorteile eines `FewShotPromptTemplate`**"],"metadata":{"id":"XFbved9KM_WJ"},"id":"XFbved9KM_WJ"},{"cell_type":"markdown","source":["| Vorteil                      | Beschreibung                                                      |\n","| ---------------------------- | ----------------------------------------------------------------- |\n","| üß† Lerneffekt f√ºr das Modell | Das Modell erkennt aus Beispielen, wie es antworten soll          |\n","| üß∞ Flexibilit√§t              | Beispiele k√∂nnen dynamisch gesetzt oder generiert werden          |\n","| üß± Struktur & Konsistenz     | Einheitlicher Aufbau aller Beispiele √ºber `example_prompt`        |\n","| ü™Ñ Kein Training n√∂tig       | Kein Fine-Tuning ‚Äì nur durch Prompting                            |\n","| üõ† Kombinierbar              | Kann mit Templates, Tools, Chains, OutputParser kombiniert werden |\n"],"metadata":{"id":"EWjuAd2_M-E7"},"id":"EWjuAd2_M-E7"},{"cell_type":"markdown","id":"pt37u3cnbgc","source":["## 5.4 | Eigenes Prompt-Template üìå"],"metadata":{"id":"pt37u3cnbgc"}},{"cell_type":"markdown","id":"ik5lbyv7vc","source":["Die `load_prompt()` Funktion aus der `genai_lib.utilities` erm√∂glicht das Laden von Prompt-Templates direkt von GitHub oder aus lokalen Dateien.\n","\n","**Vorteile:**\n","- Zentrale Verwaltung von Prompts im `05_prompt/` Verzeichnis\n","- Wiederverwendbarkeit √ºber mehrere Notebooks hinweg\n","- Versionskontrolle durch Git\n","- Einfaches Teilen von Best-Practice-Prompts\n","- **Markdown-Format** f√ºr bessere Lesbarkeit und einfache Bearbeitung\n","\n","**Modi:**\n","- `mode=\"T\"` (default): Gibt ein `ChatPromptTemplate` zur√ºck\n","- `mode=\"S\"`: Gibt den Inhalt ohne Frontmatter als String zur√ºck\n","\n","**Hinweis:** In Google Colab muss die GitHub-URL verwendet werden, da lokale Pfade nicht verf√ºgbar sind."],"metadata":{"id":"ik5lbyv7vc"}},{"cell_type":"markdown","id":"rjewn4pkze","source":["**Template-Struktur (`05_prompt/text_zusammenfassung.md`):**\n","\n","```markdown\n","---\n","name: text_zusammenfassung\n","description: Erstellt pr√§gnante Textzusammenfassungen\n","variables: [text]\n","---\n","\n","## system\n","\n","Du bist ein Experte f√ºr die Erstellung pr√§gnanter Textzusammenfassungen...\n","\n","## human\n","\n","Bitte fasse den folgenden Text zusammen: {text}\n","```\n","\n","**Format:** Templates werden als **Markdown-Dateien (.md)** mit YAML-Frontmatter erstellt. Die Message-Rollen (`system`, `human`) werden als `##`-Headings definiert."],"metadata":{"id":"rjewn4pkze"}},{"cell_type":"code","id":"fl5appj1wrf","source":["from genai_lib.utilities import load_prompt\n","\n","# Template von GitHub laden (Markdown-Format)\n","url = \"https://github.com/ralf-42/GenAI/blob/main/05_prompt/text_zusammenfassung.md\"\n","chat_template = load_prompt(url)\n","\n","# Template mit Beispieltext testen\n","beispiel_text = \"\"\"\n","LangChain ist ein Framework zur Entwicklung von Anwendungen mit gro√üen Sprachmodellen (LLMs).\n","Es bietet modulare Komponenten f√ºr verschiedene Aufgaben, vorgefertigte Ketten f√ºr komplexe\n","Workflows und eine einfache Integration externer Datenquellen. Mit LangChain k√∂nnen Entwickler\n","Chat-Modelle, Embeddings, Vektordatenbanken und Agents effizient kombinieren, um\n","leistungsstarke KI-Anwendungen zu erstellen.\n","\"\"\"\n","\n","# Prompt mit Variablen f√ºllen\n","prompt = chat_template.format_messages(text=beispiel_text)\n","\n","# Aufruf\n","response = llm.invoke(prompt)"],"metadata":{"id":"fl5appj1wrf"},"execution_count":null,"outputs":[]},{"cell_type":"code","id":"zqaw2rdob6b","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"zqaw2rdob6b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980453100,"user_tz":-60,"elapsed":16,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"e5312a7f-b679-4ff4-c005-395362732b9b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"LangChain ist ein Framework zur Entwicklung von Anwendungen mit gro√üen Sprachmodellen (LLMs), das modulare Komponenten und vorgefertigte Ketten f√ºr komplexe Workflows bietet. Es erm√∂glicht eine einfache Integration externer Datenquellen und die effiziente Kombination von Chat-Modellen, Embeddings, Vektordatenbanken und Agents. Dadurch k√∂nnen Entwickler leistungsstarke KI-Anwendungen erstellen."},"metadata":{}}]},{"cell_type":"markdown","id":"8c859b9f","metadata":{"id":"8c859b9f"},"source":["# 6 | Weitere Modelle üìå\n","---"]},{"cell_type":"code","source":["#@title üßú‚Äç‚ôÄÔ∏è Prozess-Diagramm { display-mode: \"form\" }\n","\n","diagram = \"\"\"\n","flowchart LR\n","    Input[\"Input<br/>(Message)\"] --> Prompt\n","\n","    subgraph Chain\n","        Prompt[\"Prompt<br/>(Template)\"]\n","        Model[\"Model<br/>(LLM)\"]\n","        Response[\"Response<br/>(Parser)\"]\n","        Prompt --> Model --> Response\n","    end\n","\n","    Response --> Result[\"Result\"]\n","\n","    %% HIGHLIGHT einer einzelnen Box\n","    style Model fill:#f9c74f,stroke:#b36b00,stroke-width:2px\n","\n","\"\"\"\n","mermaid(diagram)"],"metadata":{"id":"QaTXxdescavY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980455582,"user_tz":-60,"elapsed":2478,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"59677f52-917a-4a9a-db62-509b6eade254"},"id":"QaTXxdescavY","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.SVG object>"],"image/svg+xml":"<svg xmlns=\"http://www.w3.org/2000/svg\" id=\"container\" width=\"875.03125\" class=\"flowchart\" height=\"164\" viewBox=\"0 0 875.03125 164\" role=\"graphics-document document\" aria-roledescription=\"flowchart-v2\"><style>#container{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:16px;fill:#333;}@keyframes edge-animation-frame{from{stroke-dashoffset:0;}}@keyframes dash{to{stroke-dashoffset:0;}}#container .edge-animation-slow{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 50s linear infinite;stroke-linecap:round;}#container .edge-animation-fast{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 20s linear infinite;stroke-linecap:round;}#container .error-icon{fill:#552222;}#container .error-text{fill:#552222;stroke:#552222;}#container .edge-thickness-normal{stroke-width:1px;}#container .edge-thickness-thick{stroke-width:3.5px;}#container .edge-pattern-solid{stroke-dasharray:0;}#container .edge-thickness-invisible{stroke-width:0;fill:none;}#container .edge-pattern-dashed{stroke-dasharray:3;}#container .edge-pattern-dotted{stroke-dasharray:2;}#container .marker{fill:#333333;stroke:#333333;}#container .marker.cross{stroke:#333333;}#container svg{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:16px;}#container p{margin:0;}#container .label{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;color:#333;}#container .cluster-label text{fill:#333;}#container .cluster-label span{color:#333;}#container .cluster-label span p{background-color:transparent;}#container .label text,#container span{fill:#333;color:#333;}#container .node rect,#container .node circle,#container .node ellipse,#container .node polygon,#container .node path{fill:#ECECFF;stroke:#9370DB;stroke-width:1px;}#container .rough-node .label text,#container .node .label text,#container .image-shape .label,#container .icon-shape .label{text-anchor:middle;}#container .node .katex path{fill:#000;stroke:#000;stroke-width:1px;}#container .rough-node .label,#container .node .label,#container .image-shape .label,#container .icon-shape .label{text-align:center;}#container .node.clickable{cursor:pointer;}#container .root .anchor path{fill:#333333!important;stroke-width:0;stroke:#333333;}#container .arrowheadPath{fill:#333333;}#container .edgePath .path{stroke:#333333;stroke-width:2.0px;}#container .flowchart-link{stroke:#333333;fill:none;}#container .edgeLabel{background-color:rgba(232,232,232, 0.8);text-align:center;}#container .edgeLabel p{background-color:rgba(232,232,232, 0.8);}#container .edgeLabel rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#container .labelBkg{background-color:rgba(232, 232, 232, 0.5);}#container .cluster rect{fill:#ffffde;stroke:#aaaa33;stroke-width:1px;}#container .cluster text{fill:#333;}#container .cluster span{color:#333;}#container div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:12px;background:hsl(80, 100%, 96.2745098039%);border:1px solid #aaaa33;border-radius:2px;pointer-events:none;z-index:100;}#container .flowchartTitleText{text-anchor:middle;font-size:18px;fill:#333;}#container rect.text{fill:none;stroke-width:0;}#container .icon-shape,#container .image-shape{background-color:rgba(232,232,232, 0.8);text-align:center;}#container .icon-shape p,#container .image-shape p{background-color:rgba(232,232,232, 0.8);padding:2px;}#container .icon-shape rect,#container .image-shape rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#container .label-icon{display:inline-block;height:1em;overflow:visible;vertical-align:-0.125em;}#container .node .label-icon path{fill:currentColor;stroke:revert;stroke-width:revert;}#container :root{--mermaid-font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;}</style><g><marker id=\"container_flowchart-v2-pointEnd\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"5\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"8\" markerHeight=\"8\" orient=\"auto\"><path d=\"M 0 0 L 10 5 L 0 10 z\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-pointStart\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"4.5\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"8\" markerHeight=\"8\" orient=\"auto\"><path d=\"M 0 5 L 10 10 L 10 0 z\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-circleEnd\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"11\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><circle cx=\"5\" cy=\"5\" r=\"5\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-circleStart\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"-1\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><circle cx=\"5\" cy=\"5\" r=\"5\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-crossEnd\" class=\"marker cross flowchart-v2\" viewBox=\"0 0 11 11\" refX=\"12\" refY=\"5.2\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><path d=\"M 1,1 l 9,9 M 10,1 l -9,9\" class=\"arrowMarkerPath\" style=\"stroke-width: 2; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-crossStart\" class=\"marker cross flowchart-v2\" viewBox=\"0 0 11 11\" refX=\"-1\" refY=\"5.2\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><path d=\"M 1,1 l 9,9 M 10,1 l -9,9\" class=\"arrowMarkerPath\" style=\"stroke-width: 2; stroke-dasharray: 1, 0;\"/></marker><g class=\"root\"><g class=\"clusters\"><g class=\"cluster\" id=\"Chain\" data-look=\"classic\"><rect style=\"\" x=\"189.5\" y=\"8\" width=\"522.109375\" height=\"148\"/><g class=\"cluster-label\" transform=\"translate(430.1640625, 8)\"><foreignObject width=\"40.78125\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Chain</p></span></div></foreignObject></g></g></g><g class=\"edgePaths\"><path d=\"M139.5,82L143.667,82C147.833,82,156.167,82,164.5,82C172.833,82,181.167,82,188.833,82C196.5,82,203.5,82,207,82L210.5,82\" id=\"L_Input_Prompt_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_Input_Prompt_0\" data-points=\"W3sieCI6MTM5LjUsInkiOjgyfSx7IngiOjE2NC41LCJ5Ijo4Mn0seyJ4IjoxODkuNSwieSI6ODJ9LHsieCI6MjE0LjUsInkiOjgyfV0=\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M351.766,82L355.932,82C360.099,82,368.432,82,376.099,82C383.766,82,390.766,82,394.266,82L397.766,82\" id=\"L_Prompt_Model_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_Prompt_Model_0\" data-points=\"W3sieCI6MzUxLjc2NTYyNSwieSI6ODJ9LHsieCI6Mzc2Ljc2NTYyNSwieSI6ODJ9LHsieCI6NDAxLjc2NTYyNSwieSI6ODJ9XQ==\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M506.547,82L510.714,82C514.88,82,523.214,82,530.88,82C538.547,82,545.547,82,549.047,82L552.547,82\" id=\"L_Model_Response_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_Model_Response_0\" data-points=\"W3sieCI6NTA2LjU0Njg3NSwieSI6ODJ9LHsieCI6NTMxLjU0Njg3NSwieSI6ODJ9LHsieCI6NTU2LjU0Njg3NSwieSI6ODJ9XQ==\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M686.609,82L690.776,82C694.943,82,703.276,82,711.609,82C719.943,82,728.276,82,735.943,82C743.609,82,750.609,82,754.109,82L757.609,82\" id=\"L_Response_Result_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_Response_Result_0\" data-points=\"W3sieCI6Njg2LjYwOTM3NSwieSI6ODJ9LHsieCI6NzExLjYwOTM3NSwieSI6ODJ9LHsieCI6NzM2LjYwOTM3NSwieSI6ODJ9LHsieCI6NzYxLjYwOTM3NSwieSI6ODJ9XQ==\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/></g><g class=\"edgeLabels\"><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_Input_Prompt_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_Prompt_Model_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_Model_Response_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_Response_Result_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g></g><g class=\"nodes\"><g class=\"node default\" id=\"flowchart-Input-0\" transform=\"translate(73.75, 82)\"><rect class=\"basic label-container\" style=\"\" x=\"-65.75\" y=\"-39\" width=\"131.5\" height=\"78\"/><g class=\"label\" style=\"\" transform=\"translate(-35.75, -24)\"><rect/><foreignObject width=\"71.5\" height=\"48\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Input<br/>(Message)</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-Prompt-1\" transform=\"translate(283.1328125, 82)\"><rect class=\"basic label-container\" style=\"\" x=\"-68.6328125\" y=\"-39\" width=\"137.265625\" height=\"78\"/><g class=\"label\" style=\"\" transform=\"translate(-38.6328125, -24)\"><rect/><foreignObject width=\"77.265625\" height=\"48\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Prompt<br/>(Template)</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-Model-3\" transform=\"translate(454.15625, 82)\"><rect class=\"basic label-container\" style=\"fill:#f9c74f !important;stroke:#b36b00 !important;stroke-width:2px !important\" x=\"-52.390625\" y=\"-39\" width=\"104.78125\" height=\"78\"/><g class=\"label\" style=\"\" transform=\"translate(-22.390625, -24)\"><rect/><foreignObject width=\"44.78125\" height=\"48\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Model<br/>(LLM)</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-Response-4\" transform=\"translate(621.578125, 82)\"><rect class=\"basic label-container\" style=\"\" x=\"-65.03125\" y=\"-39\" width=\"130.0625\" height=\"78\"/><g class=\"label\" style=\"\" transform=\"translate(-35.03125, -24)\"><rect/><foreignObject width=\"70.0625\" height=\"48\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Response<br/>(Parser)</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-Result-9\" transform=\"translate(814.3203125, 82)\"><rect class=\"basic label-container\" style=\"\" x=\"-52.7109375\" y=\"-27\" width=\"105.421875\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-22.7109375, -12)\"><rect/><foreignObject width=\"45.421875\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Result</p></span></div></foreignObject></g></g></g></g></g></svg>"},"metadata":{}}]},{"cell_type":"markdown","source":["LangChain ist mit zahlreichen LLMs ‚Äì sowohl eines als auch verschiedener Anbieter ‚Äì kompatibel und bietet vielseitige Konfigurationsoptionen f√ºr individuelle Anwendungsf√§lle."],"metadata":{"id":"5y6J-chfEOPH"},"id":"5y6J-chfEOPH"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","\n","Model-Provider `OpenAI`\n","</font></p>"],"metadata":{"id":"0Vrxy9i9ZoP-"},"id":"0Vrxy9i9ZoP-"},{"cell_type":"markdown","source":["**Beispiel `gpt-4o` und `gpt-5-mini`**"],"metadata":{"id":"Oc0UPT5eTyTR"},"id":"Oc0UPT5eTyTR"},{"cell_type":"markdown","source":["+ gpt-4o:    \n","gpt-4o ist ein multimodales KI-Modell von OpenAI, das Text- und Bildeingaben verarbeiten kann und sich durch hohe Geschwindigkeit sowie eine nat√ºrliche, menschen√§hnliche Interaktion auszeichnet ‚Äì ideal f√ºr allt√§gliche Konversationen, kreative Aufgaben und allgemeine Chatanwendungen\n","+ gpt-5-mini:     \n","gpt-5-mini ist die mittlere Modellvariante der GPT-5-Familie, die ein ausgewogenes Verh√§ltnis zwischen Leistungsf√§higkeit und Kosten bietet und f√ºr allt√§gliche Aufgaben, allgemeine Automatisierung sowie kosteng√ºnstige Anwendungen im gro√üen Ma√üstab optimiert wurde"],"metadata":{"id":"f8yl7OApTvcQ"},"id":"f8yl7OApTvcQ"},{"cell_type":"code","source":["# Modelle konfigurieren (Kurznotation: \"provider:model\")\n","chat_model = init_chat_model(\"openai:gpt-4o\")\n","resoning_model = init_chat_model(\"openai:gpt-5-mini\")"],"metadata":{"id":"aDGqhlxoENvs"},"id":"aDGqhlxoENvs","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Gemeinsamer simple prompt\n","prompt = \"Was ist der Unterschied zwischen Prompting und Retrieval-Augmented-Generation?\""],"metadata":{"id":"PywoNdLrPCfO"},"id":"PywoNdLrPCfO","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aufruf\n","response = chat_model.invoke(prompt)"],"metadata":{"id":"iriw2JZPOazT"},"id":"iriw2JZPOazT","execution_count":null,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Chat-Model:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"9OlEy8jQPck1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980467893,"user_tz":-60,"elapsed":18,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"be966497-83f7-429c-928e-4235fe8b7070"},"id":"9OlEy8jQPck1","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Chat-Model:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Prompting und Retrieval-Augmented Generation (RAG) sind zwei verschiedene Ans√§tze, die im Bereich der k√ºnstlichen Intelligenz und insbesondere bei Sprachmodellen genutzt werden, um relevante und kontextspezifische Antworten zu generieren. Hier sind die Hauptunterschiede:\n\n1. **Prompting**:\n\n   - **Definition**: Prompting bezieht sich auf die Technik, bei der ein Sprachmodell durch Vorgabe eines spezifischen Eingabeprompts angeregt wird, eine bestimmte Art von Antwort oder Text zu generieren.\n   - **Arbeitsweise**: Der Benutzer formuliert einen Text oder eine Frage, der bzw. die als Eingabeprompt dient, und das Sprachmodell generiert basierend auf diesem Prompt eine Antwort.\n   - **Datenbasis**: Das Modell st√ºtzt sich dabei ausschlie√ülich auf die internen, w√§hrend des Trainings gelernten Informationen.\n   - **Anwendungsbereich**: Prompting kann genutzt werden, um spezifische Informationen abzurufen oder das Modell in gewisser Weise zu steuern, z.B. um Geschichten zu generieren, Fragen zu beantworten oder einfache logische Schl√ºsse zu ziehen.\n\n2. **Retrieval-Augmented Generation (RAG)**:\n\n   - **Definition**: RAG kombiniert die F√§higkeit, relevante externe Informationen aus einer gro√üen Datenbank oder Wissensbasis abzurufen, mit der Textgenerierung.\n   - **Arbeitsweise**: Dieser Ansatz verwendet ein duales System: Ein Retrieval-Modul sucht zun√§chst relevante Dokumente oder Informationen aus einer externen Datenquelle und leitet diese Informationen dann an ein Sprachmodell weiter, das eine koh√§rente und fundierte Antwort generiert.\n   - **Datenbasis**: RAG nutzt sowohl das vorab trainierte Sprachmodell als auch externe, aktualisierte Wissensbasen, um Informationen bereitzustellen.\n   - **Anwendungsbereich**: Besonders n√ºtzlich in dynamischen Wissensdom√§nen, in denen aktuelle Informationen erforderlich sind oder wo das Modell √ºber Informationen hinausgehen muss, die es w√§hrend seines Trainings gelernt hat.\n\nZusammengefasst dient Prompting dazu, Sprachmodelle direkt mit spezifischen Eingabesignalen zu steuern, w√§hrend RAG die Generierungsf√§higkeiten von Sprachmodellen mit einem vorgelagerten Schritt des gezielten Abrufens von Informationen kombiniert, um fundiertere und kontextuell relevantere Antworten zu liefern."},"metadata":{}}]},{"cell_type":"code","source":["# Aufruf\n","response = resoning_model.invoke(prompt)"],"metadata":{"id":"_Znkwm2SPOrX"},"id":"_Znkwm2SPOrX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Reasoning-Model:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"N0hcQkSfQyVK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980480756,"user_tz":-60,"elapsed":18,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"3edc15f5-6387-4b33-865a-078db080557e"},"id":"N0hcQkSfQyVK","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Reasoning-Model:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Kurzantwort\n- Prompting: Man gibt dem Sprachmodell nur Eingabetext (Prompt) und das Modell erzeugt eine Antwort allein aus seinem trainierten Wissen und der im Prompt bereitgestellten Instruktion/Kontext.\n- Retrieval‚ÄëAugmented Generation (RAG): Man erg√§nzt das Prompting dadurch, dass vor der Generierung relevante externe Dokumente oder Abschnitte abgerufen (retrieved) und als zus√§tzliche Kontexteingabe in das Modell eingespeist werden ‚Äî das Ergebnis ist also ‚Äûgrounded‚Äú in externen Quellen.\n\nWas ist Prompting?\n- Beschreibung: Direkte Interaktion mit dem LLM durch sorgf√§ltig formulierte Prompts, ggf. mit System- und Few‚ÄëShot‚ÄëBeispielen, Chain‚Äëof‚ÄëThought-Anweisungen, Temperatur‚ÄëEinstellungen etc.\n- Wissensquelle: ausschlie√ülich das im Modell gespeicherte Wissen + Input im Prompt.\n- Vorteile: sehr einfach zu implementieren, geringe Systemkomplexit√§t, niedrige Latenz.\n- Nachteile: beschr√§nkt durch das Modellwissen (Cutoff/Veraltetheit), Neigung zu Halluzinationen, limitiertes Kontextfenster.\n\nWas ist RAG?\n- Beschreibung: Pipeline aus (1) Retriever: Suche nach relevanten Dokumenten/Abschnitten in einer externen Wissensbasis (z. B. Vektor‚ÄëEmbeddings + ANN‚ÄëIndex, oder klassische IR), (2) Optionaler Reranker/Filter, (3) Generator: LLM erh√§lt Prompt plus die abgerufenen Texte und generiert die Antwort.\n- Wissensquelle: externe Dokumente + Modell.\n- Vorteile: bessere Faktenbasis, aktualisierbar ohne Neutraining, geringere Halluzinationsrate bei korrekter Integration, handhabbar gro√üer Wissensmengen jenseits des Kontextfensters.\n- Nachteile: mehr Systemkomponenten, h√∂here Latenz und Kosten, ben√∂tigt guten Retriever/Chunking und Prompt‚ÄëDesign, Risiko von ‚Äûcontext leakage‚Äú oder falscher Attribution wenn Retrieval fehlerhaft.\n\nVergleich nach Kriterien\n- Faktentreue: RAG > reines Prompting (wenn Retrieval korrekt).\n- Aktualit√§t: RAG (externe DB aktualisierbar) > Prompting (modellbedingt veraltet).\n- Implementationsaufwand: Prompting (niedrig) < RAG (h√∂her).\n- Kosten & Latenz: Prompting (niedriger) < RAG (h√∂her, wegen Retrieval und l√§ngerer Prompts).\n- Skalierbarkeit des Wissens: RAG skaliert gut (gro√üe Dokumentkorpora); Prompting nicht.\n- Erkl√§rbarkeit/Attribution: RAG erm√∂glicht Quellenangabe; Prompting nicht zuverl√§ssig.\n\nKurzes Beispiel\n- Prompting (einfach): ‚ÄûErkl√§re die Ursachen des Treibhauseffekts in einfachen Worten.‚Äú\n- RAG (Pipeline): Retriever holt aus einer wissenschaftlichen Wissensdatenbank drei relevante Abschnitte, dann Prompt an LLM: ‚ÄûNutze die folgenden Quellen (A, B, C) und erl√§utere die Ursachen des Treibhauseffekts; gib jeweils die Quelle an.‚Äú ‚Üí Antwort basiert auf den abgerufenen Textstellen.\n\nTypische Komponenten einer RAG‚ÄëImplementierung\n- Wissensspeicher: Dokumente in Chunks (z. B. 500‚Äì2000 Tokens).\n- Embeddings/Index: semantische Vektoren + ANN‚ÄëIndex (FAISS, Milvus, Pinecone, etc.).\n- Retriever: top‚Äëk semantische Suche (+optional BM25).\n- Reranker: ML‚Äëbasiert oder Heuristiken f√ºr bessere Precision.\n- Generator‚ÄëPrompting: Template, das die abgerufenen Snippets strukturiert einbettet.\n- Evaluation: Genauigkeit, Halluzinationsrate, Antwortzeit.\n\nWann welches w√§hlen?\n- Reines Prompting: Prototyping, kreative Aufgaben, generelle Konversation, wenn keine aktuellen/fakten Daten n√∂tig.\n- RAG: Anwendungen, die faktische Genauigkeit, Nachvollziehbarkeit oder aktuelle/unternehmensspezifische Informationen brauchen (Support‚ÄëBots, Wissensmanagement, Rechercheassistenz).\n\nRisiken und Best Practices bei RAG\n- Gute Chunk‚ÄëGr√∂√üe w√§hlen, irrelevante Matches filtern, Reranker nutzen.\n- Prompts so gestalten, dass das Modell Quellen zitiert und bei fehlender Info ‚Äûwei√ües Blinken‚Äú (kein Erfinden) macht.\n- Zugriffskontrollen und Datenschutz f√ºr interne Dokumente beachten.\n- Monitoring: Metriken f√ºr Halluzinationen und Quellenkonsistenz.\n\nFazit\nPrompting ist die einfache, direkte Nutzung des LLM; RAG erweitert das Modell durch Retrieval externer, aktualisierbarer Quellen und verbessert damit die Faktentreue und Nachvollziehbarkeit ‚Äî auf Kosten von Komplexit√§t, Latenz und Implementationsaufwand. Welche Methode sinnvoll ist, h√§ngt vom Anwendungsfall (Faktenbedarf, Aktualit√§t, Budget, Entwicklungsaufwand) ab."},"metadata":{}}]},{"cell_type":"markdown","source":["**üìä Vergleich: Chat-Modell vs. Reasoning-Modell**"],"metadata":{"id":"YJljcFw4Rz6W"},"id":"YJljcFw4Rz6W"},{"cell_type":"markdown","source":["| **Aspekt**                      | **Chat-Modell**                                                           | **Reasoning-Modell**                                                                                |\n","| ------------------------------- | ------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------- |\n","| **Stil**                        | Erkl√§rend, klassisch strukturiert mit Flie√ütext                           | Punktuell, technisch und systematisch                                                               |\n","| **Formatierung**                | Flie√ütext mit Abs√§tzen und Beispielen                                     | Aufz√§hlungen (‚Ä¢) und strukturierte Unterpunkte                                                      |\n","| **Definition Prompting**        | Kurze Definition als Text, danach Beispiel                                | Technisch klar: ‚Äûparametrisches Wissen‚Äú, Rolle der Prompt-Formulierung betont                       |\n","| **Definition RAG**              | Flie√ütext-Erkl√§rung mit Ablaufbeschreibung und Beispiel                   | Detaillierte Schritte: Abruf, Kontextintegration, Vorteile bzgl. Aktualit√§t und Pr√§zision erl√§utert |\n","| **Beispiele**                   | Ja ‚Äì einfache, praxisnahe Beispiele (z.‚ÄØB. Klimawandel, Quantencomputing) | Nein ‚Äì keine konkreten Beispiele, aber kontextbezogene Beschreibung                                 |\n","| **Vergleich Prompting vs. RAG** | Abstrakte Gegen√ºberstellung in einem abschlie√üenden Absatz                | Direkter Vergleich im letzten Abschnitt mit Betonung der Wissensquellen                             |\n","| **Technische Tiefe**            | Mittel ‚Äì f√ºr Einsteiger\\:innen gut verst√§ndlich                           | Hoch ‚Äì Begriffe wie ‚Äûparametrisches Wissen‚Äú, Fokus auf Architektur                                  |\n","| **Zielgruppe**                  | Allgemeines Publikum, Einstieg in das Thema                               | Fortgeschrittene Nutzer\\:innen, Technik-affine Leserschaft                                          |\n","| **St√§rken des Modells**         | Klare Erkl√§rung, gute didaktische Struktur                                | Pr√§zise Begriffsabgrenzung, hohe Informationsdichte                                                 |\n","\n"],"metadata":{"id":"OzPIlXnqR2Fq"},"id":"OzPIlXnqR2Fq"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","\n","Modell-Provider: `Google`\n","</font></p>"],"metadata":{"id":"xfC5UGI4WBrE"},"id":"xfC5UGI4WBrE"},{"cell_type":"markdown","source":["LangChain bietet eine robuste und einfache Integration mit Google Gemini, einschlie√ülich des leistungsf√§higen Modells `gemini-2.5-pro`. Dieses Modell zeichnet sich durch ein sehr gro√ües Kontextfenster mit multimodaler Verarbeitung (Text, Bild, Audio, Video) und starke Reasoning-F√§higkeiten aus. F√ºr den Zugang ben√∂tigen Sie einen kostenlosen API-Key, den Sie √ºber Google AI Studio oder die Google Cloud Console erhalten k√∂nnen. Mit dem offiziellen LangChain-Paket langchain-google-genai l√§sst sich Gemini nahtlos in Python-Anwendungen einbinden."],"metadata":{"id":"Kmc574udXFvU"},"id":"Kmc574udXFvU"},{"cell_type":"code","source":["install_packages([('langchain-google-genai', 'langchain_google_genai')])\n","setup_api_keys(['GOOGLE_API_KEY'])"],"metadata":{"id":"7xMyp01mS075","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980486399,"user_tz":-60,"elapsed":5631,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"a64b01a6-9bcf-4c32-f9e6-06dfe0e775d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üîÑ Installiere langchain-google-genai...\n","‚úÖ langchain-google-genai erfolgreich installiert und importiert\n","‚úì GOOGLE_API_KEY erfolgreich gesetzt\n"]}],"id":"7xMyp01mS075"},{"cell_type":"code","source":["llm_gemini = init_chat_model(\"google_genai:gemini-2.5-flash-lite\", temperature=0)"],"metadata":{"id":"SxvQlZ3UWLIe"},"execution_count":null,"outputs":[],"id":"SxvQlZ3UWLIe"},{"cell_type":"code","source":["prompt = (\n","    \"Was ist der Unterschied zwischen Prompting und Retrieval-Augmented-Generation?\"\n","    \"Zuerst, **schreibe deine Gedankenschritte unter einer √úberschrift 'Reasoning' auf**, \"\n","    \"und gib dann die endg√ºltige Antwort aus.\"\n",")\n","response_gemini = llm_gemini.invoke(prompt)"],"metadata":{"id":"sBb0dmCWXX2Q"},"execution_count":null,"outputs":[],"id":"sBb0dmCWXX2Q"},{"cell_type":"code","source":["mprint(\"### üì£ Antwort:\")\n","mprint(\"---\")\n","mprint(response_gemini.content)"],"metadata":{"id":"RwLENy0gKCmv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980493305,"user_tz":-60,"elapsed":16,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"2062b646-968f-4013-cebb-2907f77e148e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### üì£ Antwort:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## Reasoning\n\n1.  **Verst√§ndnis der Kernkonzepte:**\n    *   **Prompting:** Was bedeutet es, ein Sprachmodell zu \"prompten\"? Es geht darum, eine Eingabe (Text, Frage, Anweisung) zu geben, um eine gew√ºnschte Ausgabe zu erhalten. Der Prompt ist die direkte Anweisung oder der Kontext, den das Modell erh√§lt.\n    *   **Retrieval-Augmented Generation (RAG):** Was ist das? Der Name selbst gibt Hinweise: \"Retrieval\" (Abrufen) und \"Augmented Generation\" (erweiterte Generierung). Das bedeutet, dass vor der eigentlichen Generierung Informationen abgerufen werden, um die Generierung zu verbessern.\n\n2.  **Identifizierung der Hauptunterschiede:**\n    *   **Informationsquelle:** Woher bezieht das Modell seine Informationen?\n        *   Beim reinen Prompting verl√§sst sich das Modell auf sein *internes, vortrainiertes Wissen*.\n        *   Bei RAG bezieht das Modell *zus√§tzlich externe, spezifische Informationen* aus einer Wissensdatenbank oder einem Dokumentenkorpus.\n    *   **Prozess:** Wie funktioniert jeder Ansatz?\n        *   Prompting: Eingabe -> Modell -> Ausgabe. Ein direkter Prozess.\n        *   RAG: Eingabe -> Retrieval-System (sucht relevante Infos) -> Kombinierte Eingabe (Prompt + abgerufene Infos) -> Modell -> Ausgabe. Ein mehrstufiger Prozess.\n    *   **Ziel/Zweck:** Warum w√ºrde man einen Ansatz dem anderen vorziehen?\n        *   Prompting: F√ºr allgemeine Fragen, kreatives Schreiben, Zusammenfassungen von bekannten Themen, wo das vortrainierte Wissen ausreicht.\n        *   RAG: F√ºr Fragen, die spezifisches, aktuelles oder dom√§nenspezifisches Wissen erfordern, das nicht im urspr√ºnglichen Trainingsdatensatz des Modells enthalten war. Um Halluzinationen zu reduzieren und die Genauigkeit zu erh√∂hen.\n    *   **Komplexit√§t:** Welcher Ansatz ist aufwendiger zu implementieren?\n        *   Prompting: Relativ einfach, erfordert nur die Formulierung des Prompts.\n        *   RAG: Komplexer, erfordert ein Retrieval-System (z.B. Vektordatenbank, Suchindex) und eine Strategie zur Integration der abgerufenen Informationen in den Prompt.\n\n3.  **Strukturierung der Antwort:**\n    *   Beginne mit einer klaren Definition jedes Konzepts.\n    *   Hebe die Kernunterschiede hervor, indem du die oben identifizierten Punkte (Informationsquelle, Prozess, Zweck) ansprichst.\n    *   Verwende Analogien, falls hilfreich (z.B. Prompting ist wie ein Experte, der aus dem Ged√§chtnis spricht; RAG ist wie ein Experte, der zuerst in B√ºchern nachschl√§gt).\n    *   Fasse die Hauptunterschiede in einer pr√§gnanten Zusammenfassung zusammen.\n\n4.  **Formulierung der endg√ºltigen Antwort:**\n    *   Stelle sicher, dass die Sprache klar und verst√§ndlich ist.\n    *   Verwende Fachbegriffe korrekt.\n    *   Beantworte die Frage direkt: \"Was ist der Unterschied?\"\n\n---\n\n## Endg√ºltige Antwort\n\nDer Hauptunterschied zwischen **Prompting** und **Retrieval-Augmented Generation (RAG)** liegt in der **Informationsquelle**, die das Sprachmodell f√ºr die Generierung seiner Antwort nutzt.\n\nHier ist eine Aufschl√ºsselung:\n\n**1. Prompting (Reines Prompting)**\n\n*   **Definition:** Beim reinen Prompting gibst du dem Sprachmodell eine Eingabe (einen \"Prompt\"), die eine Frage, eine Anweisung oder einen Kontext darstellt. Das Modell generiert dann eine Antwort, indem es sich ausschlie√ülich auf sein **internes, vortrainiertes Wissen** st√ºtzt, das es w√§hrend seines Trainingsprozesses erworben hat.\n*   **Informationsquelle:** Das vortrainierte Wissen des Modells.\n*   **Prozess:** Direkte Eingabe -> Modell -> Ausgabe.\n*   **Anwendungsf√§lle:** Allgemeine Fragen, kreatives Schreiben, Zusammenfassungen bekannter Themen, Aufgaben, bei denen das Modell keine spezifischen, externen oder aktuellen Informationen ben√∂tigt.\n*   **Einschr√§nkungen:** Kann zu \"Halluzinationen\" f√ºhren (Erfinden von Fakten), wenn das interne Wissen unvollst√§ndig oder veraltet ist. Kann keine Informationen liefern, die nicht Teil des Trainingsdatensatzes waren.\n\n**2. Retrieval-Augmented Generation (RAG)**\n\n*   **Definition:** RAG ist ein fortgeschrittenerer Ansatz, bei dem das Sprachmodell **zus√§tzlich zu seinem internen Wissen externe Informationen abruft**, um seine Generierung zu verbessern. Bevor das Modell eine Antwort generiert, wird ein \"Retrieval\"-Schritt durchgef√ºhrt, um relevante Dokumente, Textpassagen oder Daten aus einer externen Wissensquelle (z.B. einer Datenbank, einem Dokumentenkorpus, dem Internet) zu finden. Diese abgerufenen Informationen werden dann zusammen mit dem urspr√ºnglichen Prompt an das Sprachmodell √ºbergeben.\n*   **Informationsquelle:**\n    *   Das interne, vortrainierte Wissen des Modells.\n    *   **Zus√§tzlich:** Spezifische, externe Informationen, die durch einen Retrieval-Mechanismus gefunden wurden.\n*   **Prozess:**\n    1.  Benutzer gibt einen Prompt ein.\n    2.  Ein **Retrieval-System** sucht in einer externen Wissensquelle nach relevanten Informationen basierend auf dem Prompt.\n    3.  Die gefundenen Informationen werden mit dem urspr√ºnglichen Prompt kombiniert.\n    4.  Das Sprachmodell erh√§lt diese **erweiterte Eingabe** und generiert die Antwort.\n    5.  Modell -> Ausgabe.\n*   **Anwendungsf√§lle:** Beantwortung von Fragen zu spezifischen Dokumenten, aktuellen Ereignissen, dom√§nenspezifischem Wissen (z.B. medizinische oder juristische Texte), Reduzierung von Halluzinationen, Bereitstellung von Antworten, die auf verifizierbaren Quellen basieren.\n*   **Vorteile:** Erh√∂ht die Genauigkeit, Relevanz und Aktualit√§t der Antworten. Erm√∂glicht die Nutzung von Wissen, das nicht im urspr√ºnglichen Trainingsdatensatz des Modells enthalten war.\n\n**Zusammenfassend:**\n\n*   **Prompting** ist die grundlegende Interaktion, bei der das Modell nur auf sein eigenes Wissen zur√ºckgreift.\n*   **RAG** ist eine Methode, die das Prompting erweitert, indem sie das Modell mit externen, relevanten Informationen \"f√ºttert\", bevor es die Antwort generiert, um die Qualit√§t und Genauigkeit zu verbessern."},"metadata":{}}],"id":"RwLENy0gKCmv"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","\n","Ein anderer Modell-Provider: `Alibaba/groq`\n","</font></p>"],"metadata":{"id":"8y5xNtM1oMs3"},"id":"8y5xNtM1oMs3"},{"cell_type":"markdown","source":["`Qwen3-32B` ist ein 32,8 Milliarden Parameter starker Transformer von Alibaba (Qwen3-Serie, April 2025), der durch Dual-Mode-Reasoning gl√§nzt: \"Thinking Mode\" f√ºr komplexe Logik, Mathe und Coding; \"Non-Thinking Mode\" f√ºr schnelle Dialoge"],"metadata":{"id":"umgN4oKXOr_f"},"id":"umgN4oKXOr_f"},{"cell_type":"code","source":["install_packages([('langchain-groq', 'langchain_groq')])\n","setup_api_keys(['GROQ_API_KEY'])"],"metadata":{"id":"ec6V1NJRnjPv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980494043,"user_tz":-60,"elapsed":725,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"f82c9643-90b6-4a04-998c-20e1451bf036"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üîÑ Installiere langchain-groq...\n","‚úÖ langchain-groq erfolgreich installiert und importiert\n","‚úì GROQ_API_KEY erfolgreich gesetzt\n"]}],"id":"ec6V1NJRnjPv"},{"cell_type":"code","source":["# Kurznotation: \"provider:model\"\n","llm_groq = init_chat_model(\"groq:qwen/qwen3-32b\", temperature=0)"],"metadata":{"id":"cVSRS2Kaf1M7"},"execution_count":null,"outputs":[],"id":"cVSRS2Kaf1M7"},{"cell_type":"code","source":["prompt = (\n","    \"Was ist der Unterschied zwischen Prompting und Retrieval-Augmented-Generation?\"\n","    \"Zuerst, **schreibe deine Gedankenschritte unter einer √úberschrift 'Reasoning' auf**, \"\n","    \"und gib dann die endg√ºltige Antwort aus.\"\n",")\n","response_groq = llm_groq.invoke(prompt)"],"metadata":{"id":"QDmosqYJLzeD"},"id":"QDmosqYJLzeD","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Separater Ausweis Reasoning m√∂glich\n","</font></p>"],"metadata":{"id":"dwwtPwJyLRE2"},"id":"dwwtPwJyLRE2"},{"cell_type":"code","source":["from genai_lib.utilities import extract_thinking\n","thinking, answer = extract_thinking(response_groq)\n","\n","mprint(\"### üß† Denken:\")\n","mprint(\"---\")\n","mprint(thinking)\n","mprint(\"### üì£ Antwort:\")\n","mprint(\"---\")\n","mprint(answer)"],"metadata":{"id":"WnZVyYPvKJpS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980498083,"user_tz":-60,"elapsed":14,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"6ba3fee2-a786-4670-e3af-bc0303025b7b"},"id":"WnZVyYPvKJpS","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### üß† Denken:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Okay, I need to explain the difference between Prompting and Retrieval-Augmented Generation (RAG). Let me start by recalling what each term means.\n\nPrompting is when you give a model a specific input, like a question or instruction, and it generates a response based on its training data. The model uses its internal knowledge to answer, right? So it's all about how the input is structured to get the desired output.\n\nNow, Retrieval-Augmented Generation. I remember that RAG combines retrieval from an external database with the model's generation. So when a query comes in, the system first retrieves relevant documents or information from a database, then uses that retrieved info to generate a more accurate or up-to-date response. It's like the model isn't just relying on its own knowledge but also looking up current data.\n\nWait, so the key difference is that prompting uses the model's existing knowledge, while RAG adds an external retrieval step. Prompting is about how you phrase the input to get the model to use what it knows. RAG enhances the model's response by fetching additional information from an external source before generating the answer.\n\nLet me check if I got that right. Prompting is about the input structure to elicit the model's knowledge. RAG is a method where the model retrieves external information to augment its generation. So RAG is a specific technique that uses retrieval to improve the generation process, whereas prompting is a broader concept about how inputs are designed to get the model to perform a task.\n\nAnother point: Prompting can be as simple as asking a question, while RAG involves a two-step process‚Äîretrieval followed by generation. Also, RAG can provide more accurate or updated information because it's not limited to the model's training data. Prompting might not have access to the latest data if the model's training data is outdated.\n\nSo summarizing, the main differences are in the process (prompting is input design, RAG includes retrieval), the source of information (model's knowledge vs. external data), and the potential for up-to-date information with RAG."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### üì£ Antwort:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"**Reasoning**  \nPrompting bezieht sich auf die Methode, wie ein KI-Modell durch gezielte Eingaben (Prompts) angeregt wird, auf Basis seines trainierten Wissens eine Antwort zu generieren. Das Modell nutzt ausschlie√ülich seine internen Kenntnisse, die w√§hrend der Trainingsphase erlernt wurden. Prompting ist also ein reiner Input-Output-Prozess, bei dem die Qualit√§t der Antwort von der Formulierung des Prompts und dem Modellwissen abh√§ngt.  \n\nRetrieval-Augmented Generation (RAG) hingegen kombiniert zwei Schritte: Zun√§chst wird relevante Information aus einer externen Datenbank oder einem Dokumentenspeicher abgerufen (Retrieval), und anschlie√üend wird diese Information vom Modell genutzt, um eine Antwort zu generieren. RAG erweitert das Modellwissen durch aktuelle oder spezifische Daten, die nicht zwangsl√§ufig Teil des Trainingsdatensatzes waren.  \n\n**Unterschiede im √úberblick:**  \n1. **Quelle des Wissens:** Prompting nutzt nur das interne Wissen des Modells, RAG integriert externe Daten.  \n2. **Prozess:** Prompting ist ein einstufiger Prozess, RAG besteht aus Retrieval und anschlie√üender Generation.  \n3. **Aktualit√§t:** RAG kann aktuelle oder spezialisierte Informationen liefern, Prompting ist auf das Trainingsdatum des Modells beschr√§nkt.  \n\n**Endg√ºltige Antwort**  \nDer Unterschied zwischen Prompting und Retrieval-Augmented Generation (RAG) liegt in der Quelle des Wissens und dem Prozess:  \n- **Prompting** nutzt ausschlie√ülich das interne Wissen des Modells, um auf Basis eines Prompts eine Antwort zu generieren.  \n- **RAG** f√ºgt einen Retrieval-Schritt hinzu, bei dem externe Dokumente oder Datenbanken abgefragt werden, um die Antwort zu erg√§nzen oder zu verbessern. RAG erm√∂glicht so pr√§zisere, aktuelle oder spezialisierte Antworten, w√§hrend Prompting auf das Trainingswissen des Modells angewiesen ist."},"metadata":{}}]},{"cell_type":"markdown","id":"c314ca71","metadata":{"id":"c314ca71"},"source":["# 7 | Chain üìå\n","---"]},{"cell_type":"code","source":["#@title üßú‚Äç‚ôÄÔ∏è Prozess-Diagramm { display-mode: \"form\" }\n","\n","diagram = \"\"\"\n","flowchart LR\n","    Input[\"Input<br/>(Message)\"] --> Prompt\n","\n","    subgraph Chain\n","        Prompt[\"Prompt<br/>(Template)\"]\n","        Model[\"Model<br/>(LLM)\"]\n","        Response[\"Response<br/>(Parser)\"]\n","        Prompt --> Model --> Response\n","    end\n","\n","    Response --> Result[\"Result\"]\n","\n","    %% HIGHLIGHT einer einzelnen Box\n","    style Chain fill:#f9c74f,stroke:#b36b00,stroke-width:2px\n","\n","\"\"\"\n","mermaid(diagram)"],"metadata":{"id":"OYL4O4sncgV1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980498854,"user_tz":-60,"elapsed":764,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"de83eb74-f3a3-4ff3-e242-3b80146e9c9a"},"id":"OYL4O4sncgV1","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.SVG object>"],"image/svg+xml":"<svg xmlns=\"http://www.w3.org/2000/svg\" id=\"container\" width=\"875.03125\" class=\"flowchart\" height=\"164\" viewBox=\"0 0 875.03125 164\" role=\"graphics-document document\" aria-roledescription=\"flowchart-v2\"><style>#container{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:16px;fill:#333;}@keyframes edge-animation-frame{from{stroke-dashoffset:0;}}@keyframes dash{to{stroke-dashoffset:0;}}#container .edge-animation-slow{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 50s linear infinite;stroke-linecap:round;}#container .edge-animation-fast{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 20s linear infinite;stroke-linecap:round;}#container .error-icon{fill:#552222;}#container .error-text{fill:#552222;stroke:#552222;}#container .edge-thickness-normal{stroke-width:1px;}#container .edge-thickness-thick{stroke-width:3.5px;}#container .edge-pattern-solid{stroke-dasharray:0;}#container .edge-thickness-invisible{stroke-width:0;fill:none;}#container .edge-pattern-dashed{stroke-dasharray:3;}#container .edge-pattern-dotted{stroke-dasharray:2;}#container .marker{fill:#333333;stroke:#333333;}#container .marker.cross{stroke:#333333;}#container svg{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:16px;}#container p{margin:0;}#container .label{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;color:#333;}#container .cluster-label text{fill:#333;}#container .cluster-label span{color:#333;}#container .cluster-label span p{background-color:transparent;}#container .label text,#container span{fill:#333;color:#333;}#container .node rect,#container .node circle,#container .node ellipse,#container .node polygon,#container .node path{fill:#ECECFF;stroke:#9370DB;stroke-width:1px;}#container .rough-node .label text,#container .node .label text,#container .image-shape .label,#container .icon-shape .label{text-anchor:middle;}#container .node .katex path{fill:#000;stroke:#000;stroke-width:1px;}#container .rough-node .label,#container .node .label,#container .image-shape .label,#container .icon-shape .label{text-align:center;}#container .node.clickable{cursor:pointer;}#container .root .anchor path{fill:#333333!important;stroke-width:0;stroke:#333333;}#container .arrowheadPath{fill:#333333;}#container .edgePath .path{stroke:#333333;stroke-width:2.0px;}#container .flowchart-link{stroke:#333333;fill:none;}#container .edgeLabel{background-color:rgba(232,232,232, 0.8);text-align:center;}#container .edgeLabel p{background-color:rgba(232,232,232, 0.8);}#container .edgeLabel rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#container .labelBkg{background-color:rgba(232, 232, 232, 0.5);}#container .cluster rect{fill:#ffffde;stroke:#aaaa33;stroke-width:1px;}#container .cluster text{fill:#333;}#container .cluster span{color:#333;}#container div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:12px;background:hsl(80, 100%, 96.2745098039%);border:1px solid #aaaa33;border-radius:2px;pointer-events:none;z-index:100;}#container .flowchartTitleText{text-anchor:middle;font-size:18px;fill:#333;}#container rect.text{fill:none;stroke-width:0;}#container .icon-shape,#container .image-shape{background-color:rgba(232,232,232, 0.8);text-align:center;}#container .icon-shape p,#container .image-shape p{background-color:rgba(232,232,232, 0.8);padding:2px;}#container .icon-shape rect,#container .image-shape rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#container .label-icon{display:inline-block;height:1em;overflow:visible;vertical-align:-0.125em;}#container .node .label-icon path{fill:currentColor;stroke:revert;stroke-width:revert;}#container :root{--mermaid-font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;}</style><g><marker id=\"container_flowchart-v2-pointEnd\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"5\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"8\" markerHeight=\"8\" orient=\"auto\"><path d=\"M 0 0 L 10 5 L 0 10 z\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-pointStart\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"4.5\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"8\" markerHeight=\"8\" orient=\"auto\"><path d=\"M 0 5 L 10 10 L 10 0 z\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-circleEnd\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"11\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><circle cx=\"5\" cy=\"5\" r=\"5\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-circleStart\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"-1\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><circle cx=\"5\" cy=\"5\" r=\"5\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-crossEnd\" class=\"marker cross flowchart-v2\" viewBox=\"0 0 11 11\" refX=\"12\" refY=\"5.2\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><path d=\"M 1,1 l 9,9 M 10,1 l -9,9\" class=\"arrowMarkerPath\" style=\"stroke-width: 2; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-crossStart\" class=\"marker cross flowchart-v2\" viewBox=\"0 0 11 11\" refX=\"-1\" refY=\"5.2\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><path d=\"M 1,1 l 9,9 M 10,1 l -9,9\" class=\"arrowMarkerPath\" style=\"stroke-width: 2; stroke-dasharray: 1, 0;\"/></marker><g class=\"root\"><g class=\"clusters\"><g class=\"cluster\" id=\"Chain\" data-look=\"classic\"><rect style=\"fill:#f9c74f !important;stroke:#b36b00 !important;stroke-width:2px !important\" x=\"189.5\" y=\"8\" width=\"522.109375\" height=\"148\"/><g class=\"cluster-label\" transform=\"translate(430.1640625, 8)\"><foreignObject width=\"40.78125\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Chain</p></span></div></foreignObject></g></g></g><g class=\"edgePaths\"><path d=\"M139.5,82L143.667,82C147.833,82,156.167,82,164.5,82C172.833,82,181.167,82,188.833,82C196.5,82,203.5,82,207,82L210.5,82\" id=\"L_Input_Prompt_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_Input_Prompt_0\" data-points=\"W3sieCI6MTM5LjUsInkiOjgyfSx7IngiOjE2NC41LCJ5Ijo4Mn0seyJ4IjoxODkuNSwieSI6ODJ9LHsieCI6MjE0LjUsInkiOjgyfV0=\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M351.766,82L355.932,82C360.099,82,368.432,82,376.099,82C383.766,82,390.766,82,394.266,82L397.766,82\" id=\"L_Prompt_Model_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_Prompt_Model_0\" data-points=\"W3sieCI6MzUxLjc2NTYyNSwieSI6ODJ9LHsieCI6Mzc2Ljc2NTYyNSwieSI6ODJ9LHsieCI6NDAxLjc2NTYyNSwieSI6ODJ9XQ==\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M506.547,82L510.714,82C514.88,82,523.214,82,530.88,82C538.547,82,545.547,82,549.047,82L552.547,82\" id=\"L_Model_Response_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_Model_Response_0\" data-points=\"W3sieCI6NTA2LjU0Njg3NSwieSI6ODJ9LHsieCI6NTMxLjU0Njg3NSwieSI6ODJ9LHsieCI6NTU2LjU0Njg3NSwieSI6ODJ9XQ==\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M686.609,82L690.776,82C694.943,82,703.276,82,711.609,82C719.943,82,728.276,82,735.943,82C743.609,82,750.609,82,754.109,82L757.609,82\" id=\"L_Response_Result_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_Response_Result_0\" data-points=\"W3sieCI6Njg2LjYwOTM3NSwieSI6ODJ9LHsieCI6NzExLjYwOTM3NSwieSI6ODJ9LHsieCI6NzM2LjYwOTM3NSwieSI6ODJ9LHsieCI6NzYxLjYwOTM3NSwieSI6ODJ9XQ==\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/></g><g class=\"edgeLabels\"><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_Input_Prompt_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_Prompt_Model_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_Model_Response_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_Response_Result_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g></g><g class=\"nodes\"><g class=\"node default\" id=\"flowchart-Input-0\" transform=\"translate(73.75, 82)\"><rect class=\"basic label-container\" style=\"\" x=\"-65.75\" y=\"-39\" width=\"131.5\" height=\"78\"/><g class=\"label\" style=\"\" transform=\"translate(-35.75, -24)\"><rect/><foreignObject width=\"71.5\" height=\"48\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Input<br/>(Message)</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-Prompt-1\" transform=\"translate(283.1328125, 82)\"><rect class=\"basic label-container\" style=\"\" x=\"-68.6328125\" y=\"-39\" width=\"137.265625\" height=\"78\"/><g class=\"label\" style=\"\" transform=\"translate(-38.6328125, -24)\"><rect/><foreignObject width=\"77.265625\" height=\"48\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Prompt<br/>(Template)</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-Model-3\" transform=\"translate(454.15625, 82)\"><rect class=\"basic label-container\" style=\"\" x=\"-52.390625\" y=\"-39\" width=\"104.78125\" height=\"78\"/><g class=\"label\" style=\"\" transform=\"translate(-22.390625, -24)\"><rect/><foreignObject width=\"44.78125\" height=\"48\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Model<br/>(LLM)</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-Response-4\" transform=\"translate(621.578125, 82)\"><rect class=\"basic label-container\" style=\"\" x=\"-65.03125\" y=\"-39\" width=\"130.0625\" height=\"78\"/><g class=\"label\" style=\"\" transform=\"translate(-35.03125, -24)\"><rect/><foreignObject width=\"70.0625\" height=\"48\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Response<br/>(Parser)</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-Result-9\" transform=\"translate(814.3203125, 82)\"><rect class=\"basic label-container\" style=\"\" x=\"-52.7109375\" y=\"-27\" width=\"105.421875\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-22.7109375, -12)\"><rect/><foreignObject width=\"45.421875\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Result</p></span></div></foreignObject></g></g></g></g></g></svg>"},"metadata":{}}]},{"cell_type":"markdown","source":["Chains verbinden mehrere Komponenten/Bausteine zu einer Verarbeitungskette. LangChain Expression Language (LCEL) ist das **neue Programmiermodell in LangChain v0.3+**, das die Entwicklung von LLM-Anwendungen vereinfacht.\n"],"metadata":{"id":"cEEMxhSBHrXA"},"id":"cEEMxhSBHrXA"},{"cell_type":"markdown","source":["\n","**Hier die Kernpunkte:**\n","\n","1. Was ist LCEL?\n","```python\n","# LCEL nutzt den Pipe-Operator (|) f√ºr klare Verkettungen\n","chain = prompt | model\n","```\n","- Eine deklarative Sprache zum Verketten von LangChain-Komponenten\n","- Erm√∂glicht linearen Datenfluss zwischen Komponenten\n","- Basiert auf dem Pipe-Operator (|) f√ºr intuitive Verbindungen\n","\n","2. Warum LCEL nutzen?\n","- Bessere Lesbarkeit des Codes\n","- Einfachere Wartung und Debugging\n","- Verbesserte Performance durch optimierte Ausf√ºhrung\n","- Bessere Typsicherheit und Fehlererkennung\n","- Unterst√ºtzt modernes Streaming und Async-Operationen\n","\n","3. Praktisches Beispiel:"],"metadata":{"id":"ltTQItwXH6w8"},"id":"ltTQItwXH6w8"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Simple Chain üìå\n","</font></p>\n"],"metadata":{"id":"BndZGbyWKs1C"},"id":"BndZGbyWKs1C"},{"cell_type":"code","id":"fsryxusc3p8","source":["#@markdown   <p><font size=\"4\" color='green'>   Mermaid - LCEL Chain-Komposition</font> </br></p>\n","\n","diagram = \"\"\"\n","graph LR\n","    A[\"Input Dictionary<br/>{user_input: ...}\"] --> B[\"Prompt Template\"]\n","    B --> C[\"Formatierter Prompt<br/>System + Human Message\"]\n","    C --> D[\"LLM\"]\n","    D --> E[\"AI Message\"]\n","    E --> F[\"StrOutputParser\"]\n","    F --> G[\"String Output\"]\n","\n","    H[\"chain = prompt | llm | parser\"] -.-> B\n","    H -.-> D\n","    H -.-> F\n","\n","    style A fill:#e1f5ff\n","    style B fill:#fff4e1\n","    style C fill:#f0f0f0\n","    style D fill:#ffe1f5\n","    style E fill:#f0f0f0\n","    style F fill:#e1ffe1\n","    style G fill:#f0f0f0\n","    style H fill:#ffe1e1\n","\"\"\"\n","mermaid(diagram, width=900, height=250)"],"metadata":{"id":"fsryxusc3p8","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980500276,"user_tz":-60,"elapsed":1406,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"fb30c9da-61c3-4d08-a112-60b7021a898e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.SVG object>"],"image/svg+xml":"<svg xmlns=\"http://www.w3.org/2000/svg\" id=\"container\" class=\"flowchart\" viewBox=\"0 0 1567.953125 226\" role=\"graphics-document document\" aria-roledescription=\"flowchart-v2\" width=\"900\" height=\"250\"><style>#container{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:16px;fill:#333;}@keyframes edge-animation-frame{from{stroke-dashoffset:0;}}@keyframes dash{to{stroke-dashoffset:0;}}#container .edge-animation-slow{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 50s linear infinite;stroke-linecap:round;}#container .edge-animation-fast{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 20s linear infinite;stroke-linecap:round;}#container .error-icon{fill:#552222;}#container .error-text{fill:#552222;stroke:#552222;}#container .edge-thickness-normal{stroke-width:1px;}#container .edge-thickness-thick{stroke-width:3.5px;}#container .edge-pattern-solid{stroke-dasharray:0;}#container .edge-thickness-invisible{stroke-width:0;fill:none;}#container .edge-pattern-dashed{stroke-dasharray:3;}#container .edge-pattern-dotted{stroke-dasharray:2;}#container .marker{fill:#333333;stroke:#333333;}#container .marker.cross{stroke:#333333;}#container svg{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:16px;}#container p{margin:0;}#container .label{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;color:#333;}#container .cluster-label text{fill:#333;}#container .cluster-label span{color:#333;}#container .cluster-label span p{background-color:transparent;}#container .label text,#container span{fill:#333;color:#333;}#container .node rect,#container .node circle,#container .node ellipse,#container .node polygon,#container .node path{fill:#ECECFF;stroke:#9370DB;stroke-width:1px;}#container .rough-node .label text,#container .node .label text,#container .image-shape .label,#container .icon-shape .label{text-anchor:middle;}#container .node .katex path{fill:#000;stroke:#000;stroke-width:1px;}#container .rough-node .label,#container .node .label,#container .image-shape .label,#container .icon-shape .label{text-align:center;}#container .node.clickable{cursor:pointer;}#container .root .anchor path{fill:#333333!important;stroke-width:0;stroke:#333333;}#container .arrowheadPath{fill:#333333;}#container .edgePath .path{stroke:#333333;stroke-width:2.0px;}#container .flowchart-link{stroke:#333333;fill:none;}#container .edgeLabel{background-color:rgba(232,232,232, 0.8);text-align:center;}#container .edgeLabel p{background-color:rgba(232,232,232, 0.8);}#container .edgeLabel rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#container .labelBkg{background-color:rgba(232, 232, 232, 0.5);}#container .cluster rect{fill:#ffffde;stroke:#aaaa33;stroke-width:1px;}#container .cluster text{fill:#333;}#container .cluster span{color:#333;}#container div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:12px;background:hsl(80, 100%, 96.2745098039%);border:1px solid #aaaa33;border-radius:2px;pointer-events:none;z-index:100;}#container .flowchartTitleText{text-anchor:middle;font-size:18px;fill:#333;}#container rect.text{fill:none;stroke-width:0;}#container .icon-shape,#container .image-shape{background-color:rgba(232,232,232, 0.8);text-align:center;}#container .icon-shape p,#container .image-shape p{background-color:rgba(232,232,232, 0.8);padding:2px;}#container .icon-shape rect,#container .image-shape rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#container .label-icon{display:inline-block;height:1em;overflow:visible;vertical-align:-0.125em;}#container .node .label-icon path{fill:currentColor;stroke:revert;stroke-width:revert;}#container :root{--mermaid-font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;}</style><g><marker id=\"container_flowchart-v2-pointEnd\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"5\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"8\" markerHeight=\"8\" orient=\"auto\"><path d=\"M 0 0 L 10 5 L 0 10 z\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-pointStart\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"4.5\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"8\" markerHeight=\"8\" orient=\"auto\"><path d=\"M 0 5 L 10 10 L 10 0 z\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-circleEnd\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"11\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><circle cx=\"5\" cy=\"5\" r=\"5\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-circleStart\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"-1\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><circle cx=\"5\" cy=\"5\" r=\"5\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-crossEnd\" class=\"marker cross flowchart-v2\" viewBox=\"0 0 11 11\" refX=\"12\" refY=\"5.2\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><path d=\"M 1,1 l 9,9 M 10,1 l -9,9\" class=\"arrowMarkerPath\" style=\"stroke-width: 2; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-crossStart\" class=\"marker cross flowchart-v2\" viewBox=\"0 0 11 11\" refX=\"-1\" refY=\"5.2\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><path d=\"M 1,1 l 9,9 M 10,1 l -9,9\" class=\"arrowMarkerPath\" style=\"stroke-width: 2; stroke-dasharray: 1, 0;\"/></marker><g class=\"root\"><g class=\"clusters\"/><g class=\"edgePaths\"><path d=\"M226.578,47L237.648,47C248.719,47,270.859,47,285.43,47.12C300.001,47.239,307.002,47.478,310.502,47.598L314.002,47.718\" id=\"L_A_B_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_A_B_0\" data-points=\"W3sieCI6MjI2LjU3ODEyNSwieSI6NDd9LHsieCI6MjkzLCJ5Ijo0N30seyJ4IjozMTgsInkiOjQ3Ljg1NDEzMDUyMTgyMDM2Nn1d\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M502.156,51L506.323,51C510.49,51,518.823,51,526.49,51C534.156,51,541.156,51,544.656,51L548.156,51\" id=\"L_B_C_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_B_C_0\" data-points=\"W3sieCI6NTAyLjE1NjI1LCJ5Ijo1MX0seyJ4Ijo1MjcuMTU2MjUsInkiOjUxfSx7IngiOjU1Mi4xNTYyNSwieSI6NTF9XQ==\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M797.734,51L801.901,51C806.068,51,814.401,51,822.288,53.315C830.175,55.629,837.615,60.258,841.336,62.573L845.056,64.887\" id=\"L_C_D_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_C_D_0\" data-points=\"W3sieCI6Nzk3LjczNDM3NSwieSI6NTF9LHsieCI6ODIyLjczNDM3NSwieSI6NTF9LHsieCI6ODQ4LjQ1MjM5ODI1NTgxMzksInkiOjY3fV0=\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M935.969,94L940.135,94C944.302,94,952.635,94,960.302,94C967.969,94,974.969,94,978.469,94L981.969,94\" id=\"L_D_E_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_D_E_0\" data-points=\"W3sieCI6OTM1Ljk2ODc1LCJ5Ijo5NH0seyJ4Ijo5NjAuOTY4NzUsInkiOjk0fSx7IngiOjk4NS45Njg3NSwieSI6OTR9XQ==\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M1125.219,94L1129.385,94C1133.552,94,1141.885,94,1149.576,94.963C1157.266,95.926,1164.313,97.852,1167.837,98.815L1171.36,99.778\" id=\"L_E_F_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_E_F_0\" data-points=\"W3sieCI6MTEyNS4yMTg3NSwieSI6OTR9LHsieCI6MTE1MC4yMTg3NSwieSI6OTR9LHsieCI6MTE3NS4yMTg3NSwieSI6MTAwLjgzMjQyNjQ3NTY1MjZ9XQ==\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M1352.078,125L1356.245,125C1360.411,125,1368.745,125,1376.411,125C1384.078,125,1391.078,125,1394.578,125L1398.078,125\" id=\"L_F_G_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_F_G_0\" data-points=\"W3sieCI6MTM1Mi4wNzgxMjUsInkiOjEyNX0seyJ4IjoxMzc3LjA3ODEyNSwieSI6MTI1fSx7IngiOjE0MDIuMDc4MTI1LCJ5IjoxMjV9XQ==\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M226.897,140L237.914,135.167C248.931,130.333,270.966,120.667,292.122,110.637C313.278,100.608,333.555,90.216,343.694,85.02L353.833,79.824\" id=\"L_H_B_0\" class=\"edge-thickness-normal edge-pattern-dotted edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_H_B_0\" data-points=\"W3sieCI6MjI2Ljg5NzA1ODgyMzUyOTQsInkiOjE0MH0seyJ4IjoyOTMsInkiOjExMX0seyJ4IjozNTcuMzkyOTY4NzUsInkiOjc4fV0=\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M268,161.387L272.167,160.823C276.333,160.258,284.667,159.129,308.346,158.565C332.026,158,371.052,158,410.078,158C449.104,158,488.13,158,532.275,158C576.419,158,625.682,158,674.945,158C724.208,158,773.471,158,804.273,152.286C835.076,146.573,847.417,135.145,853.587,129.431L859.758,123.718\" id=\"L_H_D_0\" class=\"edge-thickness-normal edge-pattern-dotted edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_H_D_0\" data-points=\"W3sieCI6MjY4LCJ5IjoxNjEuMzg3MDk2Nzc0MTkzNTR9LHsieCI6MjkzLCJ5IjoxNTh9LHsieCI6NDEwLjA3ODEyNSwieSI6MTU4fSx7IngiOjUyNy4xNTYyNSwieSI6MTU4fSx7IngiOjY3NC45NDUzMTI1LCJ5IjoxNTh9LHsieCI6ODIyLjczNDM3NSwieSI6MTU4fSx7IngiOjg2Mi42OTI3NDkwMjM0Mzc1LCJ5IjoxMjF9XQ==\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M268,195.774L272.167,196.312C276.333,196.849,284.667,197.925,308.346,198.462C332.026,199,371.052,199,410.078,199C449.104,199,488.13,199,532.275,199C576.419,199,625.682,199,674.945,199C724.208,199,773.471,199,809.622,199C845.773,199,868.813,199,891.852,199C914.891,199,937.93,199,965.22,199C992.51,199,1024.052,199,1055.594,199C1087.135,199,1118.677,199,1145.897,191.531C1173.116,184.062,1196.014,169.124,1207.463,161.655L1218.912,154.186\" id=\"L_H_F_0\" class=\"edge-thickness-normal edge-pattern-dotted edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_H_F_0\" data-points=\"W3sieCI6MjY4LCJ5IjoxOTUuNzc0MTkzNTQ4Mzg3MX0seyJ4IjoyOTMsInkiOjE5OX0seyJ4Ijo0MTAuMDc4MTI1LCJ5IjoxOTl9LHsieCI6NTI3LjE1NjI1LCJ5IjoxOTl9LHsieCI6Njc0Ljk0NTMxMjUsInkiOjE5OX0seyJ4Ijo4MjIuNzM0Mzc1LCJ5IjoxOTl9LHsieCI6ODkxLjg1MTU2MjUsInkiOjE5OX0seyJ4Ijo5NjAuOTY4NzUsInkiOjE5OX0seyJ4IjoxMDU1LjU5Mzc1LCJ5IjoxOTl9LHsieCI6MTE1MC4yMTg3NSwieSI6MTk5fSx7IngiOjEyMjIuMjYxOTI5ODk4NjQ4OCwieSI6MTUyfV0=\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/></g><g class=\"edgeLabels\"><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_A_B_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_B_C_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_C_D_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_D_E_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_E_F_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_F_G_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_H_B_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_H_D_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_H_F_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g></g><g class=\"nodes\"><g class=\"node default\" id=\"flowchart-A-0\" transform=\"translate(138, 47)\"><rect class=\"basic label-container\" style=\"fill:#e1f5ff !important\" x=\"-88.578125\" y=\"-39\" width=\"177.15625\" height=\"78\"/><g class=\"label\" style=\"\" transform=\"translate(-58.578125, -24)\"><rect/><foreignObject width=\"117.15625\" height=\"48\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Input Dictionary<br/>{user_input: ...}</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-B-1\" transform=\"translate(410.078125, 51)\"><rect class=\"basic label-container\" style=\"fill:#fff4e1 !important\" x=\"-92.078125\" y=\"-27\" width=\"184.15625\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-62.078125, -12)\"><rect/><foreignObject width=\"124.15625\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Prompt Template</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-C-3\" transform=\"translate(674.9453125, 51)\"><rect class=\"basic label-container\" style=\"fill:#f0f0f0 !important\" x=\"-122.7890625\" y=\"-39\" width=\"245.578125\" height=\"78\"/><g class=\"label\" style=\"\" transform=\"translate(-92.7890625, -24)\"><rect/><foreignObject width=\"185.578125\" height=\"48\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Formatierter Prompt<br/>System + Human Message</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-D-5\" transform=\"translate(891.8515625, 94)\"><rect class=\"basic label-container\" style=\"fill:#ffe1f5 !important\" x=\"-44.1171875\" y=\"-27\" width=\"88.234375\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-14.1171875, -12)\"><rect/><foreignObject width=\"28.234375\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>LLM</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-E-7\" transform=\"translate(1055.59375, 94)\"><rect class=\"basic label-container\" style=\"fill:#f0f0f0 !important\" x=\"-69.625\" y=\"-27\" width=\"139.25\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-39.625, -12)\"><rect/><foreignObject width=\"79.25\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>AI Message</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-F-9\" transform=\"translate(1263.6484375, 125)\"><rect class=\"basic label-container\" style=\"fill:#e1ffe1 !important\" x=\"-88.4296875\" y=\"-27\" width=\"176.859375\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-58.4296875, -12)\"><rect/><foreignObject width=\"116.859375\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>StrOutputParser</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-G-11\" transform=\"translate(1481.015625, 125)\"><rect class=\"basic label-container\" style=\"fill:#f0f0f0 !important\" x=\"-78.9375\" y=\"-27\" width=\"157.875\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-48.9375, -12)\"><rect/><foreignObject width=\"97.875\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>String Output</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-H-12\" transform=\"translate(138, 179)\"><rect class=\"basic label-container\" style=\"fill:#ffe1e1 !important\" x=\"-130\" y=\"-39\" width=\"260\" height=\"78\"/><g class=\"label\" style=\"\" transform=\"translate(-100, -24)\"><rect/><foreignObject width=\"200\" height=\"48\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\"><span class=\"nodeLabel\"><p>chain = prompt | llm | parser</p></span></div></foreignObject></g></g></g></g></g></svg>"},"metadata":{}}]},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","# 1. Prompt-Template\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"human\", \"Erkl√§re mir {user_input}\")\n","])\n","\n","# 2. Modell (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0)\n","\n","#  3. Einfache LCEL-Kette\n","chain = prompt | llm\n","\n","# 4. Ausf√ºhrung\n","response = chain.invoke({\"user_input\": \"LangChain Expression Language\"})"],"metadata":{"id":"PsWtjWROIW9d"},"id":"PsWtjWROIW9d","execution_count":null,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"VWYAG9aWJpZd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980510220,"user_tz":-60,"elapsed":26,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"cb1ada9b-727c-4556-d4d8-7473171f5786"},"id":"VWYAG9aWJpZd","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"LangChain Expression Language (LCEL) ist eine spezielle Sprache, die in der LangChain-Bibliothek verwendet wird, um komplexe Datenverarbeitungs- und Abfrageoperationen zu vereinfachen. LangChain selbst ist ein Framework, das Entwicklern hilft, Anwendungen zu erstellen, die auf Sprachmodellen basieren, und LCEL ist ein Teil davon, der es erm√∂glicht, mit Daten in einer flexiblen und ausdrucksstarken Weise zu interagieren.\n\nHier sind einige der Hauptmerkmale und Konzepte von LCEL:\n\n1. **Ausdrucksbasierte Syntax**: LCEL verwendet eine ausdrucksbasierte Syntax, die es Entwicklern erm√∂glicht, komplexe Abfragen und Transformationen auf Daten durchzuf√ºhren, ohne sich mit der zugrunde liegenden Logik auseinandersetzen zu m√ºssen.\n\n2. **Integration mit Sprachmodellen**: LCEL ist so konzipiert, dass es nahtlos mit Sprachmodellen interagiert. Das bedeutet, dass du die Ausdruckssprache verwenden kannst, um Anfragen an ein Sprachmodell zu formulieren und die Antworten zu verarbeiten.\n\n3. **Flexibilit√§t**: Die Sprache ist flexibel und erm√∂glicht es, verschiedene Datentypen und -strukturen zu verarbeiten. Du kannst mit Listen, Dictionaries und anderen Datenstrukturen arbeiten, um die gew√ºnschten Ergebnisse zu erzielen.\n\n4. **Einfache Integration**: LCEL l√§sst sich leicht in bestehende LangChain-Anwendungen integrieren, was es Entwicklern erleichtert, die Funktionalit√§t ihrer Anwendungen zu erweitern.\n\n5. **Beispielanwendungen**: Du kannst LCEL verwenden, um Daten zu filtern, zu transformieren oder zu aggregieren, was besonders n√ºtzlich ist, wenn du mit gro√üen Datenmengen arbeitest oder spezifische Informationen aus einem Text extrahieren m√∂chtest.\n\nWenn du mehr √ºber spezifische Funktionen oder Anwendungsf√§lle von LCEL erfahren m√∂chtest, lass es mich wissen! Und keine Sorge, ich verspreche, nicht zu viele technische Begriffe zu verwenden ‚Äì ich bin hier, um zu helfen, nicht um dich zu verwirren! üòÑ"},"metadata":{}}]},{"cell_type":"markdown","source":["\n","LCEL ist der empfohlene Weg f√ºr alle neuen LangChain-Projekte, da es die Entwicklung vereinfacht und zukunftssicher macht."],"metadata":{"id":"49Wu_XreIZY2"},"id":"49Wu_XreIZY2"},{"cell_type":"markdown","source":["# 8 | Output-Handling\n","---"],"metadata":{"id":"LVqVL9CWRXU7"},"id":"LVqVL9CWRXU7"},{"cell_type":"code","source":["#@title üßú‚Äç‚ôÄÔ∏è Prozess-Diagramm { display-mode: \"form\" }\n","\n","diagram = \"\"\"\n","flowchart LR\n","    Input[\"Input<br/>(Message)\"] --> Prompt\n","\n","    subgraph Chain\n","        Prompt[\"Prompt<br/>(Template)\"]\n","        Model[\"Model<br/>(LLM)\"]\n","        Response[\"Response<br/>(Parser)\"]\n","        Prompt --> Model --> Response\n","    end\n","\n","    Response --> Result[\"Result\"]\n","\n","    %% HIGHLIGHT einer einzelnen Box\n","    style Response fill:#f9c74f,stroke:#b36b00,stroke-width:2px\n","\n","\"\"\"\n","mermaid(diagram)"],"metadata":{"id":"uZ2nFRAgcqxe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980512048,"user_tz":-60,"elapsed":1816,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"9527898b-76dc-4248-c0c9-f5787615dcd1"},"id":"uZ2nFRAgcqxe","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.SVG object>"],"image/svg+xml":"<svg xmlns=\"http://www.w3.org/2000/svg\" id=\"container\" width=\"875.03125\" class=\"flowchart\" height=\"164\" viewBox=\"0 0 875.03125 164\" role=\"graphics-document document\" aria-roledescription=\"flowchart-v2\"><style>#container{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:16px;fill:#333;}@keyframes edge-animation-frame{from{stroke-dashoffset:0;}}@keyframes dash{to{stroke-dashoffset:0;}}#container .edge-animation-slow{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 50s linear infinite;stroke-linecap:round;}#container .edge-animation-fast{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 20s linear infinite;stroke-linecap:round;}#container .error-icon{fill:#552222;}#container .error-text{fill:#552222;stroke:#552222;}#container .edge-thickness-normal{stroke-width:1px;}#container .edge-thickness-thick{stroke-width:3.5px;}#container .edge-pattern-solid{stroke-dasharray:0;}#container .edge-thickness-invisible{stroke-width:0;fill:none;}#container .edge-pattern-dashed{stroke-dasharray:3;}#container .edge-pattern-dotted{stroke-dasharray:2;}#container .marker{fill:#333333;stroke:#333333;}#container .marker.cross{stroke:#333333;}#container svg{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:16px;}#container p{margin:0;}#container .label{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;color:#333;}#container .cluster-label text{fill:#333;}#container .cluster-label span{color:#333;}#container .cluster-label span p{background-color:transparent;}#container .label text,#container span{fill:#333;color:#333;}#container .node rect,#container .node circle,#container .node ellipse,#container .node polygon,#container .node path{fill:#ECECFF;stroke:#9370DB;stroke-width:1px;}#container .rough-node .label text,#container .node .label text,#container .image-shape .label,#container .icon-shape .label{text-anchor:middle;}#container .node .katex path{fill:#000;stroke:#000;stroke-width:1px;}#container .rough-node .label,#container .node .label,#container .image-shape .label,#container .icon-shape .label{text-align:center;}#container .node.clickable{cursor:pointer;}#container .root .anchor path{fill:#333333!important;stroke-width:0;stroke:#333333;}#container .arrowheadPath{fill:#333333;}#container .edgePath .path{stroke:#333333;stroke-width:2.0px;}#container .flowchart-link{stroke:#333333;fill:none;}#container .edgeLabel{background-color:rgba(232,232,232, 0.8);text-align:center;}#container .edgeLabel p{background-color:rgba(232,232,232, 0.8);}#container .edgeLabel rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#container .labelBkg{background-color:rgba(232, 232, 232, 0.5);}#container .cluster rect{fill:#ffffde;stroke:#aaaa33;stroke-width:1px;}#container .cluster text{fill:#333;}#container .cluster span{color:#333;}#container div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:12px;background:hsl(80, 100%, 96.2745098039%);border:1px solid #aaaa33;border-radius:2px;pointer-events:none;z-index:100;}#container .flowchartTitleText{text-anchor:middle;font-size:18px;fill:#333;}#container rect.text{fill:none;stroke-width:0;}#container .icon-shape,#container .image-shape{background-color:rgba(232,232,232, 0.8);text-align:center;}#container .icon-shape p,#container .image-shape p{background-color:rgba(232,232,232, 0.8);padding:2px;}#container .icon-shape rect,#container .image-shape rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#container .label-icon{display:inline-block;height:1em;overflow:visible;vertical-align:-0.125em;}#container .node .label-icon path{fill:currentColor;stroke:revert;stroke-width:revert;}#container :root{--mermaid-font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;}</style><g><marker id=\"container_flowchart-v2-pointEnd\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"5\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"8\" markerHeight=\"8\" orient=\"auto\"><path d=\"M 0 0 L 10 5 L 0 10 z\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-pointStart\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"4.5\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"8\" markerHeight=\"8\" orient=\"auto\"><path d=\"M 0 5 L 10 10 L 10 0 z\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-circleEnd\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"11\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><circle cx=\"5\" cy=\"5\" r=\"5\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-circleStart\" class=\"marker flowchart-v2\" viewBox=\"0 0 10 10\" refX=\"-1\" refY=\"5\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><circle cx=\"5\" cy=\"5\" r=\"5\" class=\"arrowMarkerPath\" style=\"stroke-width: 1; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-crossEnd\" class=\"marker cross flowchart-v2\" viewBox=\"0 0 11 11\" refX=\"12\" refY=\"5.2\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><path d=\"M 1,1 l 9,9 M 10,1 l -9,9\" class=\"arrowMarkerPath\" style=\"stroke-width: 2; stroke-dasharray: 1, 0;\"/></marker><marker id=\"container_flowchart-v2-crossStart\" class=\"marker cross flowchart-v2\" viewBox=\"0 0 11 11\" refX=\"-1\" refY=\"5.2\" markerUnits=\"userSpaceOnUse\" markerWidth=\"11\" markerHeight=\"11\" orient=\"auto\"><path d=\"M 1,1 l 9,9 M 10,1 l -9,9\" class=\"arrowMarkerPath\" style=\"stroke-width: 2; stroke-dasharray: 1, 0;\"/></marker><g class=\"root\"><g class=\"clusters\"><g class=\"cluster\" id=\"Chain\" data-look=\"classic\"><rect style=\"\" x=\"189.5\" y=\"8\" width=\"522.109375\" height=\"148\"/><g class=\"cluster-label\" transform=\"translate(430.1640625, 8)\"><foreignObject width=\"40.78125\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Chain</p></span></div></foreignObject></g></g></g><g class=\"edgePaths\"><path d=\"M139.5,82L143.667,82C147.833,82,156.167,82,164.5,82C172.833,82,181.167,82,188.833,82C196.5,82,203.5,82,207,82L210.5,82\" id=\"L_Input_Prompt_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_Input_Prompt_0\" data-points=\"W3sieCI6MTM5LjUsInkiOjgyfSx7IngiOjE2NC41LCJ5Ijo4Mn0seyJ4IjoxODkuNSwieSI6ODJ9LHsieCI6MjE0LjUsInkiOjgyfV0=\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M351.766,82L355.932,82C360.099,82,368.432,82,376.099,82C383.766,82,390.766,82,394.266,82L397.766,82\" id=\"L_Prompt_Model_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_Prompt_Model_0\" data-points=\"W3sieCI6MzUxLjc2NTYyNSwieSI6ODJ9LHsieCI6Mzc2Ljc2NTYyNSwieSI6ODJ9LHsieCI6NDAxLjc2NTYyNSwieSI6ODJ9XQ==\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M506.547,82L510.714,82C514.88,82,523.214,82,530.88,82C538.547,82,545.547,82,549.047,82L552.547,82\" id=\"L_Model_Response_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_Model_Response_0\" data-points=\"W3sieCI6NTA2LjU0Njg3NSwieSI6ODJ9LHsieCI6NTMxLjU0Njg3NSwieSI6ODJ9LHsieCI6NTU2LjU0Njg3NSwieSI6ODJ9XQ==\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/><path d=\"M686.609,82L690.776,82C694.943,82,703.276,82,711.609,82C719.943,82,728.276,82,735.943,82C743.609,82,750.609,82,754.109,82L757.609,82\" id=\"L_Response_Result_0\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" style=\";\" data-edge=\"true\" data-et=\"edge\" data-id=\"L_Response_Result_0\" data-points=\"W3sieCI6Njg2LjYwOTM3NSwieSI6ODJ9LHsieCI6NzExLjYwOTM3NSwieSI6ODJ9LHsieCI6NzM2LjYwOTM3NSwieSI6ODJ9LHsieCI6NzYxLjYwOTM3NSwieSI6ODJ9XQ==\" marker-end=\"url(#container_flowchart-v2-pointEnd)\"/></g><g class=\"edgeLabels\"><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_Input_Prompt_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_Prompt_Model_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_Model_Response_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g class=\"label\" data-id=\"L_Response_Result_0\" transform=\"translate(0, 0)\"><foreignObject width=\"0\" height=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"labelBkg\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"edgeLabel\"/></div></foreignObject></g></g></g><g class=\"nodes\"><g class=\"node default\" id=\"flowchart-Input-0\" transform=\"translate(73.75, 82)\"><rect class=\"basic label-container\" style=\"\" x=\"-65.75\" y=\"-39\" width=\"131.5\" height=\"78\"/><g class=\"label\" style=\"\" transform=\"translate(-35.75, -24)\"><rect/><foreignObject width=\"71.5\" height=\"48\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Input<br/>(Message)</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-Prompt-1\" transform=\"translate(283.1328125, 82)\"><rect class=\"basic label-container\" style=\"\" x=\"-68.6328125\" y=\"-39\" width=\"137.265625\" height=\"78\"/><g class=\"label\" style=\"\" transform=\"translate(-38.6328125, -24)\"><rect/><foreignObject width=\"77.265625\" height=\"48\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Prompt<br/>(Template)</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-Model-3\" transform=\"translate(454.15625, 82)\"><rect class=\"basic label-container\" style=\"\" x=\"-52.390625\" y=\"-39\" width=\"104.78125\" height=\"78\"/><g class=\"label\" style=\"\" transform=\"translate(-22.390625, -24)\"><rect/><foreignObject width=\"44.78125\" height=\"48\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Model<br/>(LLM)</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-Response-4\" transform=\"translate(621.578125, 82)\"><rect class=\"basic label-container\" style=\"fill:#f9c74f !important;stroke:#b36b00 !important;stroke-width:2px !important\" x=\"-65.03125\" y=\"-39\" width=\"130.0625\" height=\"78\"/><g class=\"label\" style=\"\" transform=\"translate(-35.03125, -24)\"><rect/><foreignObject width=\"70.0625\" height=\"48\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Response<br/>(Parser)</p></span></div></foreignObject></g></g><g class=\"node default\" id=\"flowchart-Result-9\" transform=\"translate(814.3203125, 82)\"><rect class=\"basic label-container\" style=\"\" x=\"-52.7109375\" y=\"-27\" width=\"105.421875\" height=\"54\"/><g class=\"label\" style=\"\" transform=\"translate(-22.7109375, -12)\"><rect/><foreignObject width=\"45.421875\" height=\"24\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Result</p></span></div></foreignObject></g></g></g></g></g></svg>"},"metadata":{}}]},{"cell_type":"markdown","source":["# 8.1 | OutputParser üìå"],"metadata":{"id":"89XLSUAvCE8P"},"id":"89XLSUAvCE8P"},{"cell_type":"markdown","source":["Ein OutputParser ist ein Konzept in LangChain, das hilft, die Antworten eines Modells richtig weiterzuverarbeiten. Es nimmt die **rohe Ausgabe** (zum Beispiel einen langen Text) und formt sie in ein **bestimmtes Format**, das sp√§ter in der Anwendung leichter benutzt werden kann."],"metadata":{"id":"EVBcCwJERfaj"},"id":"EVBcCwJERfaj"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Simple Chain mit Parser üìå\n","</font></p>"],"metadata":{"id":"oK4lZB3EGKFK"},"id":"oK4lZB3EGKFK"},{"cell_type":"code","source":["# 1. Prompt-Template\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"human\", \"Erkl√§re mir {user_input}\")\n","])\n","\n","# 2. Modell (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0)\n","\n","# 3. Parser\n","parser = StrOutputParser()\n","\n","# 4. Einfache LCEL-Kette\n","chain = prompt | llm | parser\n","\n","# 5. Ausf√ºhrung\n","response = chain.invoke({\"user_input\": \"LangChain Expression Language\"})"],"metadata":{"id":"PP_A_bGc7ovZ"},"execution_count":null,"outputs":[],"id":"PP_A_bGc7ovZ"},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response)"],"metadata":{"id":"qMcn5ctu7ova","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980519779,"user_tz":-60,"elapsed":35,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"4407f086-01b8-4689-e993-86779f86184a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"LangChain Expression Language (LCEL) ist eine spezielle Sprache, die in der LangChain-Bibliothek verwendet wird, um komplexe Datenverarbeitungs- und Abfrageoperationen zu vereinfachen. LangChain selbst ist ein Framework, das Entwicklern hilft, Anwendungen mit Sprachmodellen zu erstellen, indem es verschiedene Komponenten wie Datenquellen, Agenten und Chain-Logik integriert.\n\nHier sind einige der Hauptmerkmale und Konzepte von LCEL:\n\n1. **Ausdr√ºcke**: LCEL erm√∂glicht es, Ausdr√ºcke zu definieren, die auf Daten angewendet werden k√∂nnen. Diese Ausdr√ºcke k√∂nnen einfache Berechnungen, Filter oder komplexe Transformationen umfassen.\n\n2. **Variablen**: In LCEL k√∂nnen Variablen verwendet werden, um Werte zu speichern und wiederzuverwenden. Dies erleichtert die Handhabung von Daten und die Erstellung dynamischer Abfragen.\n\n3. **Funktionen**: LCEL unterst√ºtzt die Definition und Verwendung von Funktionen, die wiederverwendbare Logik kapseln. Dies ist besonders n√ºtzlich, um h√§ufige Operationen zu abstrahieren und den Code sauberer und wartbarer zu gestalten.\n\n4. **Integration mit Sprachmodellen**: LCEL ist so konzipiert, dass es nahtlos mit Sprachmodellen interagiert. Das bedeutet, dass du die Ausdruckssprache nutzen kannst, um die Eingaben und Ausgaben von Sprachmodellen zu steuern und zu verarbeiten.\n\n5. **Flexibilit√§t**: Die Sprache ist flexibel und anpassbar, sodass Entwickler sie an ihre spezifischen Anforderungen anpassen k√∂nnen. Das macht sie zu einem leistungsstarken Werkzeug f√ºr die Entwicklung von KI-Anwendungen.\n\nUm es humorvoll zu sagen: Wenn LCEL ein Kochrezept w√§re, dann w√§re es das geheime Gew√ºrz, das deinem Gericht den besonderen Pfiff verleiht ‚Äì es macht die Zubereitung einfacher und das Endergebnis k√∂stlicher! üçΩÔ∏è\n\nWenn du spezifische Fragen zu LCEL hast oder mehr √ºber bestimmte Aspekte wissen m√∂chtest, lass es mich wissen!"},"metadata":{}}],"id":"qMcn5ctu7ova"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Simple Chain mit Parser & Format im System-Prompt\n","</font></p>"],"metadata":{"id":"pNKOzTlQ8Kpl"},"id":"pNKOzTlQ8Kpl"},{"cell_type":"code","source":["# 1. Einfacher Prompt mit explizite Formatvorgabe\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent. Formatiere Deine Antwort im json-Format mit den Infos zu Name und Alter.\"),\n","    (\"human\", \"{user_input}\")\n","])\n","\n","# 2. Modell (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0)\n","\n","# 3. Parser\n","parser = StrOutputParser()\n","\n","# 4. Chain\n","chain = prompt | llm | parser\n","\n","# 5. Ausf√ºhrung\n","response = chain.invoke({\"user_input\": \"Bitte gibt den Namen einer fiktiven Person und das Alter\"})"],"metadata":{"id":"QKtYSBLhVlts"},"execution_count":null,"outputs":[],"id":"QKtYSBLhVlts"},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response)"],"metadata":{"id":"BGceDe7Kw6ot","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980520590,"user_tz":-60,"elapsed":20,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"eed2193e-ac8c-4fa1-e17a-fa66b379fea4"},"id":"BGceDe7Kw6ot","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"```json\n{\n  \"Name\": \"Maximilian M√ºller\",\n  \"Alter\": 28\n}\n```"},"metadata":{}}]},{"cell_type":"markdown","id":"5efri5ponin","source":["# 8.2 | with_structured_output() ‚ú≥Ô∏è\n"],"metadata":{"id":"5efri5ponin"}},{"cell_type":"markdown","id":"9hnfp8g8bkj","source":["Im vorherigen Beispielen wurden Formatierung √ºber **System-Prompts** und/oder √ºber einen **OututParser** gesteuert. Das funktioniert, hat aber Nachteile:\n","\n","- ‚ùå Keine automatische Validierung (Typen k√∂nnen falsch sein)\n","- ‚ùå String-Parsing fehleranf√§llig\n","- ‚ùå Keine Typsicherheit im Code\n","- ‚ùå Prompt-Engineering f√ºr Format n√∂tig\n","\n","**Moderne L√∂sung: `with_structured_output()`**\n","\n","LangChain 1.0+ bietet eine elegante L√∂sung: Mit `with_structured_output()` kann man die gew√ºnschte Ausgabestruktur **direkt als Pydantic-Modell** definieren. Das Modell sorgt automatisch f√ºr:\n","\n","- ‚úÖ Validierte, typsichere Ausgaben\n","- ‚úÖ Automatische Format-Konvertierung\n","- ‚úÖ Klare Struktur ohne Prompt-Engineering\n","- ‚úÖ IDE-Unterst√ºtzung (Autocomplete, Type-Hints)"],"metadata":{"id":"9hnfp8g8bkj"}},{"cell_type":"markdown","id":"p8qe3tuhedi","source":["<p><font color='black' size=\"5\">\n","Einfaches Beispiel mit with_structured_output()\n","</font></p>"],"metadata":{"id":"p8qe3tuhedi"}},{"cell_type":"code","id":"a3xf7g08nw","source":["from pydantic import BaseModel, Field\n","\n","# 1. Pydantic-Modell definiert die gew√ºnschte Struktur\n","class PersonInfo(BaseModel):\n","    \"\"\"Informationen √ºber eine Person\"\"\"\n","    name: str = Field(description=\"Vollst√§ndiger Name der Person\")\n","    alter: int = Field(description=\"Alter der Person in Jahren\")\n","    beruf: str = Field(description=\"Aktueller Beruf der Person\")\n","\n","# 2. Modell mit strukturierter Ausgabe (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0)\n","structured_llm = llm.with_structured_output(PersonInfo)\n","\n","# 3. Einfache Anfrage - automatische Formatierung & Validierung\n","response = structured_llm.invoke(\n","    \"Erstelle eine fiktive Person: Name, Alter und Beruf\"\n",")\n","\n","# 4. Typsichere Ausgabe - response ist ein PersonInfo-Objekt\n","print(f\"Typ: {type(response)}\")\n","print(f\"Name: {response.name}\")\n","print(f\"Alter: {response.alter}\")\n","print(f\"Beruf: {response.beruf}\")"],"metadata":{"id":"a3xf7g08nw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980521547,"user_tz":-60,"elapsed":909,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"0dde31fa-a7fc-46e2-af41-9c3fea361e82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Typ: <class '__main__.PersonInfo'>\n","Name: Lena M√ºller\n","Alter: 28\n","Beruf: Grafikdesignerin\n"]}]},{"cell_type":"markdown","id":"ybps752g89","source":["**Vergleich: Prompt-basierte Formatierung vs. StrOutputParser vs. with_structured_output()**\n","\n","| Aspekt | Prompt-basiert | StrOutputParser | with_structured_output() |\n","|--------|----------------|----------------|---------------------------|\n","| **Definition** | Via System-Prompt: ‚ÄûFormatiere im JSON-Format...‚Äú | Expliziter Parser, der LLM-Ausgabe als String extrahiert oder leicht nachbearbeitet | Automatische Strukturierung per Pydantic-Modell (`class PersonInfo(BaseModel)`) |\n","| **Validierung** | ‚ùå Keine ‚Äì Modell kann beliebig antworten | ‚ö†Ô∏è Nur minimale (z. B. String-Cleaning) | ‚úÖ Strikte Schema-Validierung durch Pydantic |\n","| **Typsicherheit** | ‚ùå Nur Strings | ‚ùå Nur Strings | ‚úÖ Typsichere Python-Objekte |\n","| **Fehlerbehandlung** | ‚ö†Ô∏è Manuelles Parsing und Regex n√∂tig | ‚ö†Ô∏è Begrenzte Kontrolle (nur einfache Parsing-Fehler) | ‚úÖ Automatische Validierung, Exception bei Schemafehlern |\n","| **Code-Qualit√§t** | ‚ö†Ô∏è Fragil, stark Prompt-abh√§ngig | ‚úÖ Etwas klarer, aber keine echte Struktur | ‚úÖ Robust, wartbar, klar definierte Typen |\n","| **IDE-Support** | ‚ùå Keine Autovervollst√§ndigung | ‚ùå Keine | ‚úÖ Vollst√§ndig durch Type-Hints |\n","| **Anwendungsfall** | Einfache oder explorative Ausgaben | Wenn nur reine Textausgabe n√∂tig ist | F√ºr strukturierte, verl√§ssliche Daten (JSON, Objekte) |\n","| **Lernkurve** | ‚úÖ Sehr einfach | ‚úÖ Einfach | ‚ö†Ô∏è Erfordert Grundkenntnisse in Pydantic und LangChain-Schemas |\n","| **Beispiel** | ‚ÄûNenne Name und Alter als JSON.‚Äú | `StrOutputParser().parse(output)` | `llm.with_structured_output(PersonInfo)` |\n","\n"],"metadata":{"id":"ybps752g89"}},{"cell_type":"markdown","source":["\n","**Wann welchen Ansatz?**\n","\n","- **Prompt-basiert**: F√ºr einfache Texte, Zusammenfassungen, kreative Inhalte  \n","- **StrOutputParser**: Wenn nur reine Textausgabe oder einfache Formatkorrektur n√∂tig ist  \n","- **with_structured_output()**: F√ºr strukturierte Daten, APIs, Datenbanken, komplexe Workflows"],"metadata":{"id":"uncqM3AxDjh9"},"id":"uncqM3AxDjh9"},{"cell_type":"markdown","source":["# 9 | Runnables (opt.)\n","---\n"],"metadata":{"id":"KRqrnpMx3Hwf"},"id":"KRqrnpMx3Hwf"},{"cell_type":"markdown","source":["Ein **Runnable** ist ein elementarer Baustein in LangChain. Er nimmt eine Eingabe und liefert eine Ausgabe - wie eine Funktion mit klaren Regeln. Man kann sich Konzepte wie Prompts, LLMs oder Parser als Runnables vorstellen. Runnable kann man als **Oberbegriff** f√ºr Prompts, LLMs, Parser und √§hnliche Komponenten verwenden.\n","\n","\n","\n","In der Grafik sieht man die **drei wichtigsten Runnables**: Prompt-Template, LLM und Parser, die zusammen eine Kette bilden.\n","\n","In LangChain sind die wichtigsten Komponenten als Runnables verf√ºgbar:\n","\n","+ Prompts/Templates (der erste Baustein in der Kette)\n","+ LLMs (das Herzst√ºck in der Mitte)\n","+ Output-Parser (verwandelt die LLM-Antwort in ein nutzbares Format)\n","+ Komplette Chains (die ganze Verarbeitungskette)\n","+ Tools (f√ºr spezielle Aufgaben)\n","+ Retriever (holen zus√§tzliche Informationen)\n","\n","\n","Vorteile von Runnables:\n","\n","+ Gleiche Bedienung: Alle Runnables (Prompt, LLM, Parser) funktionieren nach demselben Prinzip\n","+ Einfaches Verbinden: Man kann sie unkompliziert zu einer Kette zusammenf√ºgen - genau wie im Bild gezeigt\n","+ Anpassungsf√§hig: Sie funktionieren sowohl einzeln als auch in der Gruppe\n","Schrittweise Ausgabe: Ergebnisse k√∂nnen st√ºckweise weitergegeben werden"],"metadata":{"id":"DnUDR_983M4l"},"id":"DnUDR_983M4l"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Benutzerdefinierte Runnables erstellen\n","</font></p>"],"metadata":{"id":"WWJ7U1iJ6b8Q"},"id":"WWJ7U1iJ6b8Q"},{"cell_type":"markdown","source":["Man kann auch eigene Runnables erstellen, indem man eine Klasse definiert, die das Runnable-Interface implementiert oder eine Funktion mit `RunnableLambda` umsetzt:\n"],"metadata":{"id":"PAvbE_TR6g87"},"id":"PAvbE_TR6g87"},{"cell_type":"code","source":["from langchain_core.runnables import RunnableLambda\n","from langchain_core.output_parsers.string import StrOutputParser\n","\n","# 1. Mit RunnableLambda eine einfache Transformation erstellen\n","def text_verdoppeln(eingabe):\n","    return f\"{eingabe} <br> <br>  {eingabe}\"\n","\n","verdoppler = RunnableLambda(text_verdoppeln)\n","\n","# Verwendung des eigenen Runnables\n","verdoppler.invoke(\"Dies ist ein Text.\")"],"metadata":{"id":"_0wEf_hL6peM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980521572,"user_tz":-60,"elapsed":18,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"1d11c03d-cf6e-4204-c3fd-157ff0108a72"},"id":"_0wEf_hL6peM","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Dies ist ein Text. <br> <br>  Dies ist ein Text.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["# Einfacher Prompt ohne explizite Formatvorgabe\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent. Antworte kurz.\"),\n","    (\"human\", \"Eingabe: {user_input}\")\n","])\n","\n","# In eine Kette einbauen - Korrektur der Pipe-Operatoren\n","chain_mit_verdoppler = prompt | llm | parser | verdoppler\n","\n","# Test der Kette\n","response = chain_mit_verdoppler.invoke({\"user_input\": \"Wer war Einstein?\"})"],"metadata":{"id":"NvZdOf1961xb"},"id":"NvZdOf1961xb","execution_count":null,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response)"],"metadata":{"id":"_elA0imx2HZA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980523023,"user_tz":-60,"elapsed":33,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"06b2eb2a-4d54-4802-bd27-629dcf65d027"},"id":"_elA0imx2HZA","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Albert Einstein war ein theoretischer Physiker, der f√ºr seine Relativit√§tstheorie bekannt ist. Er wurde 1879 in Deutschland geboren und gilt als einer der bedeutendsten Wissenschaftler des 20. Jahrhunderts. 1921 erhielt er den Nobelpreis f√ºr Physik. <br> <br>  Albert Einstein war ein theoretischer Physiker, der f√ºr seine Relativit√§tstheorie bekannt ist. Er wurde 1879 in Deutschland geboren und gilt als einer der bedeutendsten Wissenschaftler des 20. Jahrhunderts. 1921 erhielt er den Nobelpreis f√ºr Physik."},"metadata":{}}]},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Runnable-Methoden\n","</font></p>\n","\n","Jedes Runnable unterst√ºtzt standardm√§√üig folgende Methoden:\n","\n","1. **invoke()**: F√ºr einzelne, synchrone Anfragen\n","2. **batch()**: Verarbeitet mehrere Eingaben parallel\n","3. **stream()**: Gibt Teilergebnisse zur√ºck, sobald sie verf√ºgbar sind\n","4. **ainvoke()**: Asynchrone Version von invoke\n","5. **abatch()**: Asynchrone Version von batch\n","6. **astream()**: Asynchrone Version von stream\n","\n"],"metadata":{"id":"CjFvHqWA3S2V"},"id":"CjFvHqWA3S2V"},{"cell_type":"markdown","source":["Hier ein einfaches Beispiel mit den verschiedenen Aufrufmethoden:\n"],"metadata":{"id":"a9zHZAMP3bCL"},"id":"a9zHZAMP3bCL"},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers.string import StrOutputParser\n","\n","# Einfaches Template erstellen\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"human\", \"{user_input}\")\n","])\n","\n","# Runnable-Kette erstellen\n","chain = prompt | llm | parser"],"metadata":{"id":"kQTKHK2D4DBb"},"id":"kQTKHK2D4DBb","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. invoke - Einzelne Anfrage\n","response = chain.invoke({\"user_input\": \"Was ist ein Runnable in LangChain?\"})\n","\n","mprint(\"## üì£ Invoke response:\")\n","mprint(\"---\")\n","mprint(response)"],"metadata":{"id":"TVWtHSYw5Jla","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980528437,"user_tz":-60,"elapsed":5358,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"0c9efa80-8192-40fd-fc2e-156285e78d42"},"id":"TVWtHSYw5Jla","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Invoke response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"In LangChain bezieht sich ein \"Runnable\" auf ein Konzept, das es erm√∂glicht, verschiedene Komponenten oder Funktionen in einer Kette von Operationen auszuf√ºhren. Es handelt sich dabei um eine abstrahierte Einheit, die eine bestimmte Aufgabe oder eine Reihe von Aufgaben ausf√ºhren kann. Runnables sind besonders n√ºtzlich, um komplexe Abl√§ufe zu strukturieren und zu organisieren, indem sie es erm√∂glichen, verschiedene Schritte in einem Prozess zu definieren und zu kombinieren.\n\nEin Runnable kann beispielsweise eine Funktion sein, die Daten verarbeitet, eine API-Anfrage stellt oder eine andere Art von Berechnung durchf√ºhrt. In LangChain k√∂nnen Runnables miteinander verkettet werden, sodass die Ausgabe eines Runnables als Eingabe f√ºr den n√§chsten dient. Dies f√∂rdert die Modularit√§t und Wiederverwendbarkeit von Code.\n\nUm es mit einem humorvollen Vergleich zu verdeutlichen: Stell dir vor, ein Runnable ist wie ein Koch in einer K√ºche. Der Koch (Runnable) hat eine spezielle Aufgabe, wie das Schneiden von Gem√ºse oder das Braten von Fleisch. Wenn du mehrere K√∂che (Runnables) hast, die zusammenarbeiten, kannst du ein k√∂stliches Gericht (komplexe Anwendung) zubereiten, indem jeder seine Aufgabe erledigt und die Ergebnisse weitergibt. Und am Ende gibt es ein leckeres Ergebnis ‚Äì oder zumindest weniger verbrannte Pfannen! üç≥üòÑ"},"metadata":{}}]},{"cell_type":"code","source":["# 2. batch - Mehrere Anfragen parallel\n","response = chain.batch([\n","    {\"user_input\": \"Erkl√§re den Begriff LCEL kurz.\"},\n","    {\"user_input\": \"Was sind die Hauptvorteile von Runnables?\"}\n","])\n","mprint(\"##  üì£ Batch response::\")\n","mprint(\"---\")\n","for i, ergebnis in enumerate(response, 1):\n","    mprint(f\"Anfrage {i}: {response[:50]}...\\n\")"],"metadata":{"id":"UAet4KSq5K2D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980538492,"user_tz":-60,"elapsed":10029,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"87bd10b0-7088-4988-d063-2b7f00ea3b56"},"id":"UAet4KSq5K2D","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"##  üì£ Batch response::"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Anfrage 1: ['LCEL steht f√ºr \"Low Carbon Energy Lab\" und bezieht sich auf Einrichtungen oder Initiativen, die sich mit der Forschung, Entwicklung und Umsetzung von Technologien zur Reduzierung von Kohlenstoffemissionen besch√§ftigen. Diese Labore konzentrieren sich oft auf erneuerbare Energien, Energieeffizienz und innovative L√∂sungen, um den √úbergang zu einer kohlenstoffarmen Wirtschaft zu f√∂rdern. Wenn du mehr Details oder spezifische Informationen ben√∂tigst, lass es mich wissen!', 'Runnables in Java sind eine praktische M√∂glichkeit, um Aufgaben in einem separaten Thread auszuf√ºhren. Hier sind einige der Hauptvorteile von Runnables:\\n\\n1. **Thread-Sicherheit**: Runnables erm√∂glichen es, Aufgaben in einem separaten Thread auszuf√ºhren, was die Hauptanwendung nicht blockiert. Das ist besonders n√ºtzlich f√ºr langwierige Aufgaben, die die Benutzeroberfl√§che nicht einfrieren lassen sollen.\\n\\n2. **Einfachheit**: Die Implementierung eines Runnables ist einfach. Man muss lediglich die `run()`-Methode √ºberschreiben und kann dann das Runnable an einen Thread √ºbergeben.\\n\\n3. **Flexibilit√§t**: Runnables k√∂nnen leicht an verschiedene Threads √ºbergeben werden, was die Wiederverwendbarkeit des Codes erh√∂ht. Man kann dasselbe Runnable in verschiedenen Kontexten verwenden.\\n\\n4. **Kombinierbarkeit**: Runnables k√∂nnen einfach kombiniert werden, um komplexere Aufgaben zu erstellen. Man kann mehrere Runnables in einem ExecutorService ausf√ºhren, um parallele Verarbeitung zu erm√∂glichen.\\n\\n5. **Keine R√ºckgabewerte**: Runnables sind ideal f√ºr Aufgaben, die keine R√ºckgabewerte ben√∂tigen. Wenn du einfach nur eine Aufgabe ausf√ºhren m√∂chtest, ohne ein Ergebnis zur√ºckzugeben, ist ein Runnable die richtige Wahl.\\n\\n6. **Leichtgewichtig**: Runnables sind leichtgewichtig im Vergleich zu anderen Ans√§tzen wie `Callable`, da sie keine R√ºckgabewerte oder Ausnahmen behandeln m√ºssen.\\n\\n7. **Integration mit Executor-Framework**: Runnables lassen sich gut in das Executor-Framework integrieren, was die Verwaltung von Threads und die Ausf√ºhrung von Aufgaben vereinfacht.\\n\\nUnd wenn du mal einen Thread zum Lachen bringen willst, kannst du ihm ein Runnable geben, das einen Witz erz√§hlt ‚Äì aber pass auf, dass er nicht vom Lachen umf√§llt! üòÑ']...\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Anfrage 2: ['LCEL steht f√ºr \"Low Carbon Energy Lab\" und bezieht sich auf Einrichtungen oder Initiativen, die sich mit der Forschung, Entwicklung und Umsetzung von Technologien zur Reduzierung von Kohlenstoffemissionen besch√§ftigen. Diese Labore konzentrieren sich oft auf erneuerbare Energien, Energieeffizienz und innovative L√∂sungen, um den √úbergang zu einer kohlenstoffarmen Wirtschaft zu f√∂rdern. Wenn du mehr Details oder spezifische Informationen ben√∂tigst, lass es mich wissen!', 'Runnables in Java sind eine praktische M√∂glichkeit, um Aufgaben in einem separaten Thread auszuf√ºhren. Hier sind einige der Hauptvorteile von Runnables:\\n\\n1. **Thread-Sicherheit**: Runnables erm√∂glichen es, Aufgaben in einem separaten Thread auszuf√ºhren, was die Hauptanwendung nicht blockiert. Das ist besonders n√ºtzlich f√ºr langwierige Aufgaben, die die Benutzeroberfl√§che nicht einfrieren lassen sollen.\\n\\n2. **Einfachheit**: Die Implementierung eines Runnables ist einfach. Man muss lediglich die `run()`-Methode √ºberschreiben und kann dann das Runnable an einen Thread √ºbergeben.\\n\\n3. **Flexibilit√§t**: Runnables k√∂nnen leicht an verschiedene Threads √ºbergeben werden, was die Wiederverwendbarkeit des Codes erh√∂ht. Man kann dasselbe Runnable in verschiedenen Kontexten verwenden.\\n\\n4. **Kombinierbarkeit**: Runnables k√∂nnen einfach kombiniert werden, um komplexere Aufgaben zu erstellen. Man kann mehrere Runnables in einem ExecutorService ausf√ºhren, um parallele Verarbeitung zu erm√∂glichen.\\n\\n5. **Keine R√ºckgabewerte**: Runnables sind ideal f√ºr Aufgaben, die keine R√ºckgabewerte ben√∂tigen. Wenn du einfach nur eine Aufgabe ausf√ºhren m√∂chtest, ohne ein Ergebnis zur√ºckzugeben, ist ein Runnable die richtige Wahl.\\n\\n6. **Leichtgewichtig**: Runnables sind leichtgewichtig im Vergleich zu anderen Ans√§tzen wie `Callable`, da sie keine R√ºckgabewerte oder Ausnahmen behandeln m√ºssen.\\n\\n7. **Integration mit Executor-Framework**: Runnables lassen sich gut in das Executor-Framework integrieren, was die Verwaltung von Threads und die Ausf√ºhrung von Aufgaben vereinfacht.\\n\\nUnd wenn du mal einen Thread zum Lachen bringen willst, kannst du ihm ein Runnable geben, das einen Witz erz√§hlt ‚Äì aber pass auf, dass er nicht vom Lachen umf√§llt! üòÑ']...\n"},"metadata":{}}]},{"cell_type":"code","source":["# 3. stream - Schrittweise Ausgabe\n","mprint(\"##  üì£ Stream response:\")\n","mprint(\"---\")\n","for chunk in chain.stream({\"user_input\": \"Erkl√§re die Abk√ºrzung LCEL kurz.\"}):\n","    print(chunk, end=\" ‚û° \")  # Zeichen trennt die via stream gelieferten Ergebnisse/Ausgaben\n","print(\"\\n\")"],"metadata":{"id":"VJn4e_IX5L-b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770980540287,"user_tz":-60,"elapsed":1787,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"37b038a9-7b2e-4b85-e076-0516e69dda47"},"id":"VJn4e_IX5L-b","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"##  üì£ Stream response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" ‚û° Die ‚û°  Ab ‚û° k ‚û° √ºr ‚û° zung ‚û°  LC ‚û° EL ‚û°  steht ‚û°  f√ºr ‚û°  \" ‚û° Low ‚û°  Carbon ‚û°  Energy ‚û°  Lab ‚û° \". ‚û°  Es ‚û°  handelt ‚û°  sich ‚û°  dabei ‚û°  um ‚û°  Einrichtungen ‚û°  oder ‚û°  Forschungs ‚û° zent ‚û° ren ‚û° , ‚û°  die ‚û°  sich ‚û°  mit ‚û°  der ‚û°  Entwicklung ‚û°  und ‚û°  F√∂rderung ‚û°  von ‚û°  Technologien ‚û°  und ‚û°  Strateg ‚û° ien ‚û°  zur ‚û°  Red ‚û° uz ‚û° ierung ‚û°  von ‚û°  Koh ‚û° len ‚û° stoff ‚û° em ‚û° ission ‚û° en ‚û°  im ‚û°  Ener ‚û° gies ‚û° ektor ‚û°  besch√§ftigen ‚û° . ‚û°  Wenn ‚û°  du ‚û°  mehr ‚û°  Details ‚û°  oder ‚û°  spezif ‚û° ische ‚û°  Informationen ‚û°  zu ‚û°  einem ‚û°  bestimmten ‚û°  LC ‚û° EL ‚û°  ben√∂t ‚û° igst ‚û° , ‚û°  lass ‚û°  es ‚û°  mich ‚û°  wissen ‚û° ! ‚û°  ‚û°  ‚û°  ‚û° \n","\n"]}]},{"cell_type":"markdown","id":"F_5gn03b46rC","metadata":{"id":"F_5gn03b46rC"},"source":["# A | Aufgabe\n","---"]},{"cell_type":"markdown","id":"36LyGMmo5jb3","metadata":{"id":"36LyGMmo5jb3"},"source":["Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen."]},{"cell_type":"markdown","id":"Je-szbGU7IJ-","metadata":{"id":"Je-szbGU7IJ-"},"source":["\n","<p><font color='black' size=\"5\">\n","Erstellen einer LangChain-Kette zur Textanalyse\n","</font></p>"]},{"cell_type":"markdown","id":"kevePvA-7PzW","metadata":{"id":"kevePvA-7PzW"},"source":["Entwickeln Sie eine Verarbeitungskette, die einen Text analysiert und verschiedene Informationen dar√ºber extrahiert.\n","\n","**Schritte:**\n","1. Erstelle ein ChatPromptTemplate, das ein LLM anweist, einen Text zu analysieren\n","2. Die Analyse soll folgende Aspekte umfassen:\n","   - Hauptthema des Textes\n","   - Tonalit√§t (formal, informell, etc.)\n","   - Schl√ºsselw√∂rter (5-10)\n","   - Kurze Zusammenfassung (max. 3 S√§tze)\n","3. Formatiere die Ausgabe strukturiert mit Markdown-√úberschriften\n","4. Teste die Kette mit mindestens zwei verschiedenen Texten\n","\n","\n"]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["91ee89a4-477b-41ce-a511-aadd4c2fd547","fGV8FwGmIwmR","EouwC_wdZb-c","BMPwDNTCCOfx","-JwJD0rLIyT8","HqTUeMW20084","8akKmCCc06nm","Ps344Ejw0-ih","8c859b9f","c314ca71","LVqVL9CWRXU7","89XLSUAvCE8P","5efri5ponin","KRqrnpMx3Hwf","F_5gn03b46rC"]},"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":5}
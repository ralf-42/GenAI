{"cells":[{"cell_type":"markdown","id":"37rMyCQz2JNw","metadata":{"id":"37rMyCQz2JNw"},"source":["<p><font size=\"6\" color='grey'> <b>\n","\n","Generative KI. Verstehen. Anwenden. Gestalten.\n","</b></font> </br></p>"]},{"cell_type":"markdown","id":"R5CfUEMJdvFQ","metadata":{"id":"R5CfUEMJdvFQ"},"source":["<p><font size=\"5\" color='grey'> <b>\n","LangChain 101\n","</b></font> </br></p>\n","\n","---"]},{"cell_type":"code","source":["#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n","!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul\n","from genai_lib.utilities import (\n","    check_environment,\n","    get_ipinfo,\n","    setup_api_keys,\n","    mprint,\n","    install_packages,\n","    mermaid,\n","    get_model_profile,\n","    extract_thinking,\n","    load_chat_prompt_template\n",")\n","setup_api_keys(['OPENAI_API_KEY'], create_globals=False)\n","print()\n","check_environment()\n","print()\n","get_ipinfo()"],"metadata":{"id":"PnT8m8TTUhF5","collapsed":true},"id":"PnT8m8TTUhF5","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","‚è∏Ô∏è 5-Minuten-Check:\n","</font></p>\n","\n","**Ziel:** Pr√ºfen, ob du das vorherige Kapitel verstanden hast ‚Äì nicht, ob es gerade l√§uft.\n","\n","**Aufgabe** (5 Minuten, ohne Vorlage):\n","\n","Rekonstruiere die zentrale Idee oder Code-Struktur des letzten Abschnitts selbstst√§ndig\n","(kein Copy & Paste, kein Nachschlagen).\n","\n","W√§hle eine der folgenden Optionen:\n","\n","+ Erkl√§re in 1‚Äì2 S√§tzen, was hier konzeptionell passiert.\n","\n","+ Ver√§ndere eine Kleinigkeit (z. B. Prompt, Parameter, Reihenfolge) und beschreibe die Auswirkung.\n","\n","+ Markiere eine Stelle, die du nicht sicher erkl√§ren kannst, und formuliere eine konkrete Frage dazu.\n","\n","**Hinweis:**\n","Nicht alles muss ‚Äûfertig‚Äú oder ‚Äûkorrekt‚Äú sein. Entscheidend ist, wo dein Verst√§ndnis gerade endet"],"metadata":{"id":"xSK4F8CBabVk"},"id":"xSK4F8CBabVk"},{"cell_type":"markdown","id":"91ee89a4-477b-41ce-a511-aadd4c2fd547","metadata":{"id":"91ee89a4-477b-41ce-a511-aadd4c2fd547"},"source":["# 1 | Was ist LangChain?\n","---"]},{"cell_type":"markdown","id":"b5004f8c","metadata":{"id":"b5004f8c"},"source":["LangChain ist ein Framework zur **Entwicklung von Anwendungen mit gro√üen Sprachmodellen** (LLMs). Es vereinfacht die Integration von LLMs in eigene Anwendungen durch:\n","\n","- Modulare Komponenten f√ºr verschiedene Aufgaben\n","- Vorgefertigte Ketten (Chains) f√ºr komplexe Workflows\n","- Einfache Integration externer Datenquellen\n","- Werkzeuge f√ºr das Speichern von Konversationskontexten\n","\n"]},{"cell_type":"markdown","id":"tl73bT-kKkia","metadata":{"id":"tl73bT-kKkia"},"source":["[Einf√ºhrung](https://python.langchain.com/docs/introduction/)   \n","[Konzepte](https://python.langchain.com/docs/concepts/)   \n","[API-References](https://python.langchain.com/api_reference/index.html)   \n","[Integrations](https://python.langchain.com/docs/integrations/providers/)\n","\n","---\n","\n","[Tutorials](https://python.langchain.com/docs/tutorials/)   \n","[How-to-Guides](https://python.langchain.com/docs/how_to/)   "]},{"cell_type":"markdown","id":"fGV8FwGmIwmR","metadata":{"id":"fGV8FwGmIwmR"},"source":["# 2 | √úberblick Konzepte\n","---\n","\n","\n"]},{"cell_type":"markdown","source":["Die Darstellung veranschaulicht das Grundprinzip von LangChain: die Verkettung verschiedener Konzepte zu einem durchg√§ngigen Verarbeitungsprozess, der Eingaben in strukturierte Ausgaben umwandelt."],"metadata":{"id":"nZ2ZMEd3ulKZ"},"id":"nZ2ZMEd3ulKZ"},{"cell_type":"code","source":["#@title üßú‚Äç‚ôÄÔ∏è Prozess-Diagramm { display-mode: \"form\" }\n","\n","diagram = \"\"\"\n","flowchart LR\n","    Input[\"Input<br/>(Message)\"] --> Prompt\n","\n","    subgraph Chain\n","        Prompt[\"Prompt<br/>(Template)\"]\n","        Model[\"Model<br/>(LLM)\"]\n","        Response[\"Response<br/>(Parser)\"]\n","        Prompt --> Model --> Response\n","    end\n","\n","    Response --> Result[\"Result\"]\n","\"\"\"\n","mermaid(diagram)"],"metadata":{"id":"yY1ypSdhbFUG"},"id":"yY1ypSdhbFUG","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Konzepte von LangChain sind grundlegende Bausteine und Prinzipien. Hier sind einige der wichtigsten Konzepte:\n","\n","+ Nachrichten (**Messages**): Kommunikationseinheiten in Chat-Modellen f√ºr Ein- und Ausgabe.\n","+ Prompt-Vorlagen (**Prompt template**s): Komponenten zur Erstellung strukturierter Prompts f√ºr LLMs.\n","+ Chat-Modelle (**Chat models**): LLMs, die √ºber eine Chat-API verf√ºgbar sind und Sequenzen von Nachrichten verarbeiten.\n","+ Chains (**Chains**): Verkn√ºpfungen mehrerer LLMs oder anderer Komponenten f√ºr komplexere Anwendungen.\n","+ Strukturierte Ausgabe (**Structured output**): Technik, um Chat-Modelle in strukturierten Formaten antworten zu lassen.\n","+ Chat-Verlauf (**Chat history**): Eine Sequenz von Nachrichten, die eine Konversation darstellt.\n","+ Retrieval Augmented Generation (**RAG**): Technik zur Verbesserung von Sprachmodellen durch Kombination mit externen Wissensbasen.\n","+ Retriever (**Retriever**): Komponenten, die relevante Dokumente aus einer Wissensbasis abrufen.\n","+ Agenten (**Agents**): Nutzen Sprachmodelle, um Aktionssequenzen auszuw√§hlen und mit externen Ressourcen zu interagieren.\n","+ Tools (**Tools**): Funktionen mit definierten Schemata f√ºr Name, Beschreibung und Argumente.\n"],"metadata":{"id":"cCuow5DOEsID"},"id":"cCuow5DOEsID"},{"cell_type":"markdown","id":"EouwC_wdZb-c","metadata":{"id":"EouwC_wdZb-c"},"source":["# 3 | Nachrichten-Typen\n","---"]},{"cell_type":"markdown","id":"yS2mM2J1aXlY","metadata":{"id":"yS2mM2J1aXlY"},"source":["Beim Einsatz von Large Language Models (LLMs) wie GPT gibt es drei Hauptarten von Nachrichten (Messages), die die Interaktion mit dem Modell steuern. Diese Messages definieren, wie das Modell reagiert, welche Eingaben es erh√§lt und welche Ausgaben es generiert.\n","\n"]},{"cell_type":"markdown","id":"sGqRXptbbQ2a","metadata":{"id":"sGqRXptbbQ2a"},"source":["**System Message:**   \n","Die `System Message` dient dazu, das **Verhalten des LLMs** zu steuern. Sie ist eine nicht sichtbare Nachricht, die dem Modell Anweisungen gibt, wie es sich verhalten soll. Dabei kann sie die Pers√∂nlichkeit, den Tonfall und die Einschr√§nkungen des Modells beeinflussen. Die Systemnachricht funktioniert wie eine Art \"Rollenspiel-Anweisung\", um das Modell in eine gew√ºnschte Rolle zu versetzen.\n","\n","*Merkmale:*\n","- Definiert, wie das Modell reagieren soll\n","- Legt Pers√∂nlichkeit, Verhalten und Einschr√§nkungen fest\n","- Wird zu Beginn einer Sitzung gesetzt und bleibt bestehen\n","- Nicht sichtbar f√ºr den Benutzer\n","\n","*Beispiel einer System Message:*\n","```json\n","{\n","  \"role\": \"system\",\n","  \"content\": \"Du bist ein KI-Assistent, der pr√§zise und informative Antworten in einem professionellen Ton liefert.\"\n","}\n","```\n","Dieses Beispiel weist das Modell an, in einem professionellen Tonfall kurze und informative Antworten zu geben.\n","\n","---\n","\n"]},{"cell_type":"markdown","id":"upsIzpcqbbP7","metadata":{"id":"upsIzpcqbbP7"},"source":["**User Message**   \n","Die `User Message` ist die eigentliche **Eingabe des Benutzer**s. Sie kann eine Frage, eine Aufforderung, kontextrelevanten Inhalt oder ein Befehl sein. Das Modell nutzt diese Nachricht als Ausgangspunkt f√ºr die Generierung einer Antwort.\n","\n","*Merkmale:*\n","- Direkte Eingabe des Benutzers\n","- Kann eine Frage, Aufforderung, kontextrelevanten Inhalt oder einen Befehl enthalten\n","- Basis f√ºr die Antwort des Modells\n","\n","*Beispiel einer User Message:*\n","```json\n","{\n","  \"role\": \"user\",\n","  \"content\": \"Was sind die wichtigsten Unterschiede zwischen KI und maschinellem Lernen?\"\n","}\n","```\n","Hier fragt der Benutzer nach den Unterschieden zwischen KI und maschinellem Lernen.\n","\n","---\n"]},{"cell_type":"markdown","id":"CozrrtYXbhZb","metadata":{"id":"CozrrtYXbhZb"},"source":["**AI Message**   \n","Die `AI Message` ist die **Antwort, die das Modell generiert**. Sie basiert auf der User Message und den Anweisungen aus der System Message. Die AI Message kann verschiedene Eigenschaften haben, ist aber haupts√§chlich auf den Inhalt fokussiert.\n","\n","*Merkmale:*\n","- Antwort des Modells auf die Benutzeranfrage\n","- Kann verschiedene Eigenschaften haben (z. B. L√§nge, Stil)\n","- Haupts√§chlich inhaltlich relevant\n","\n","*Beispiel einer AI Message:*\n","```json\n","{\n","  \"role\": \"assistant\",\n","  \"content\": \"K√ºnstliche Intelligenz (KI) ist ein weites Feld, das verschiedene Teilgebiete umfasst, darunter auch das maschinelle Lernen (ML). ML konzentriert sich auf die Erstellung von Modellen, die aus Daten lernen und Vorhersagen treffen.\"\n","}\n","```\n","Hier gibt das Modell eine inhaltliche Antwort auf die Frage des Benutzers.\n","\n","\n"]},{"cell_type":"markdown","source":["**System-Message vs. User-Message**\n","\n","\n","\n","*System-Message*\n","\n","**Funktion:** Persistente Verhaltensregeln √ºber verschiedene Aufgaben hinweg\n","\n","**Enth√§lt:**\n","- Identit√§t & Rolle des Modells\n","- Grunds√§tzliche Methodik & Denkansatz\n","- Durchg√§ngige Qualit√§tsstandards\n","- Tool-/Methodenkompetenz\n","- Generische Few-Shot-Examples\n","\n","**Metapher:** Das Betriebssystem ‚Äì selten ver√§ndert, verhaltensbestimmend\n","\n","\n"],"metadata":{"id":"a8QAtdYdg0Im"},"id":"a8QAtdYdg0Im"},{"cell_type":"markdown","source":["\n","*User-Message*\n","\n","**Funktion:** Konkrete Aufgabe und spezifischer Kontext f√ºr diese Interaktion\n","\n","**Enth√§lt:**\n","- Situativer Kontext & Hintergrund\n","- Klare Aufgabenstellung\n","- Erwartete Format/Umfang/Detailgrad\n","- Spezifische Daten & Randbedingungen\n","- Aufgabenspezifische Verhaltensanpassungen\n","\n","**Metapher:** Die Arbeitsanweisung ‚Äì variiert bei jeder Anfrage\n","\n"],"metadata":{"id":"cy765AHZhaWV"},"id":"cy765AHZhaWV"},{"cell_type":"markdown","id":"PiHnhKHVbofP","metadata":{"id":"PiHnhKHVbofP"},"source":["**Kurz gefasst**   \n","\n","+  **System Message** legt die Regeln und das Verhalten des Modells fest.\n","+ **User Message** stellt eine Anfrage oder einen Befehl.\n","+ **AI Message** gibt die generierte Antwort basierend auf den vorherigen Nachrichten.\n","\n","\n"]},{"cell_type":"code","id":"uwwloofx7h","source":["#@markdown   <p><font size=\"4\" color='green'>  üßú‚Äç‚ôÄÔ∏è Mermaid - Message-Typen Workflow</font> </br></p>\n","\n","diagram = \"\"\"\n","graph TD\n","    A[\"System Message\"] --> D[\"LLM Verarbeitung\"]\n","    B[\"User Message\"] --> D\n","    D --> C[\"AI Message\"]\n","\n","    A1[\"Definiert Verhalten<br/>Rolle & Pers√∂nlichkeit<br/>Einschr√§nkungen\"] -.-> A\n","    B1[\"Benutzer-Eingabe<br/>Frage oder Befehl<br/>Kontext\"] -.-> B\n","    C1[\"Generierte Antwort<br/>Basierend auf System + User<br/>Strukturiertes Format\"] -.-> C\n","\n","    style A fill:#ffe1e1\n","    style B fill:#e1f5ff\n","    style C fill:#e1ffe1\n","    style D fill:#fff4e1\n","\"\"\"\n","mermaid(diagram, width=700, height=450)"],"metadata":{"id":"uwwloofx7h","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4 | Ein einfaches Modell\n","---\n"],"metadata":{"id":"BMPwDNTCCOfx"},"id":"BMPwDNTCCOfx"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e1926d03"},"outputs":[],"source":["# Importe\n","from langchain.chat_models import init_chat_model\n","from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n","from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n","from langchain_core.prompts.few_shot import FewShotPromptTemplate\n","from langchain_core.output_parsers.string import StrOutputParser"],"id":"e1926d03"},{"cell_type":"code","source":["# Modell definieren (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0.0)"],"metadata":{"id":"RG-y_bG8HOOi"},"execution_count":null,"outputs":[],"id":"RG-y_bG8HOOi"},{"cell_type":"markdown","id":"-JwJD0rLIyT8","metadata":{"id":"-JwJD0rLIyT8"},"source":["# 5 | Prompts\n","---"]},{"cell_type":"code","source":["#@title üßú‚Äç‚ôÄÔ∏è Prozess-Diagramm { display-mode: \"form\" }\n","\n","diagram = \"\"\"\n","flowchart LR\n","    Input[\"Input<br/>(Message)\"] --> Prompt\n","\n","    subgraph Chain\n","        Prompt[\"Prompt<br/>(Template)\"]\n","        Model[\"Model<br/>(LLM)\"]\n","        Response[\"Response<br/>(Parser)\"]\n","        Prompt --> Model --> Response\n","    end\n","\n","    Response --> Result[\"Result\"]\n","\n","    %% HIGHLIGHT einer einzelnen Box\n","    style Prompt fill:#f9c74f,stroke:#b36b00,stroke-width:2px\n","\n","\"\"\"\n","mermaid(diagram)"],"metadata":{"id":"Pho_i5xRbvRa"},"id":"Pho_i5xRbvRa","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","Prompts sind die Eingaben bzw. Auftr√§ge an das LLM. LangChain bietet verschiedene Template-Systeme. Ein **Template** ist eine **Vorlage** ‚Äì genauer gesagt ein Text mit **Platzhaltern**, die sp√§ter durch echte Werte ersetzt werden.\n","\n","\n","Ein **PromptTemplate** ist eine Vorlage f√ºr einen Eingabetext, der an ein Sprachmodell geschickt wird.\n","Er enth√§lt Platzhalter `{placeholder}`, die sp√§ter mit Nutzereingaben oder Kontext bef√ºllt werden.\n","\n","**Templates machen die KI-Prompts:**\n","\n","+ Wiederverwendbar (z.‚ÄØB. f√ºr viele Themen oder Fragen)\n","\n","+ Strukturiert (z.‚ÄØB. mit Rollen und Anweisungen)\n","\n","+ Dynamisch erweiterbar (mit Memory, Tools, etc.)"],"metadata":{"id":"QMVO4fzPFZ88"},"id":"QMVO4fzPFZ88"},{"cell_type":"markdown","id":"HqTUeMW20084","metadata":{"id":"HqTUeMW20084"},"source":["## 5.1 | Simple Prompt\n"]},{"cell_type":"markdown","source":["<p><font color='darkblue' size=\"4\">\n","<b>‚ùóHinweis:</b>\n","</font></p>\n","Ohne Memory-Management hat das Modell ein kurzes Ged√§chnis.\n"],"metadata":{"id":"5JF1_AvYNYY_"},"id":"5JF1_AvYNYY_"},{"cell_type":"markdown","source":["**Prompt wird als Zeichenkette √ºbergeben:**"],"metadata":{"id":"1azC75R5JpWC"},"id":"1azC75R5JpWC"},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(\"Was ist Generative KI?\")"],"metadata":{"id":"jPizca8FJV5b"},"id":"jPizca8FJV5b","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Antwort\n","type(response)"],"metadata":{"id":"EHjmjLgW3ENA"},"id":"EHjmjLgW3ENA","execution_count":null,"outputs":[]},{"cell_type":"code","source":["response"],"metadata":{"id":"4ZYngACo4RWN"},"id":"4ZYngACo4RWN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["for r in response:\n","    print(r)"],"metadata":{"id":"jveXqUjbpo0s"},"id":"jveXqUjbpo0s","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"cOJGeiLGJb-g"},"id":"cOJGeiLGJb-g","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(\"Was war meine letzte Frage?\")"],"metadata":{"id":"9fayTzIrNN2b"},"id":"9fayTzIrNN2b","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"IahQJ0mFNUUV"},"id":"IahQJ0mFNUUV","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Prompt wird als formatierte String-Variable √ºbergeben:**"],"metadata":{"id":"aKRhzB0oJnUG"},"id":"aKRhzB0oJnUG"},{"cell_type":"code","source":["# f-String f√ºr simple prompt mit template\n","thema = \"Machine Learning\"\n","prompt = f\"Erkl√§re {thema} in einfachen Worten.\""],"metadata":{"id":"8s5cQY2oHUvW"},"id":"8s5cQY2oHUvW","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(prompt)"],"metadata":{"id":"plYGfaoLJ8DI"},"execution_count":null,"outputs":[],"id":"plYGfaoLJ8DI"},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"SLX0zaONJ8DJ"},"execution_count":null,"outputs":[],"id":"SLX0zaONJ8DJ"},{"cell_type":"markdown","source":["**Prompt wird als template √ºbergeben:**"],"metadata":{"id":"n-VVZEHDKSIw"},"id":"n-VVZEHDKSIw"},{"cell_type":"code","source":["# Einfaches Template mit Platzhaltern\n","simple_prompt_template = PromptTemplate.from_template(\n","    \"Erkl√§re {thema} in einfachen Worten.\"\n",")"],"metadata":{"id":"koh0-_3r_HxG"},"id":"koh0-_3r_HxG","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prompt-Erstellung\n","thema = \"Generative KI\"\n","prompt = simple_prompt_template.format(thema=thema)\n","prompt"],"metadata":{"id":"nFrCjL6j_xo1"},"id":"nFrCjL6j_xo1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(prompt)"],"metadata":{"id":"-of5FbM02qfZ"},"id":"-of5FbM02qfZ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"lmEE-3NDAE15"},"id":"lmEE-3NDAE15","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"8akKmCCc06nm","metadata":{"id":"8akKmCCc06nm"},"source":["## 5.2 | ChatPromptTemplate üìå"]},{"cell_type":"markdown","source":["Das **ChatPromptTemplate** ist eine spezielle Prompt-Vorlage f√ºr Chatmodelle (z.‚ÄØB. GPT-3.5, GPT-4), die **mehrere Rollen und Nachrichten** unterst√ºtzt ‚Äì also genau das, was Chat-Modelle eigentlich brauchen.\n","\n","Es geh√∂rt zur LangChain-Bibliothek und baut auf dem Prinzip auf:\n","\n","\n","**Wer sagt was? ‚Üí system, user, assistant, etc.**"],"metadata":{"id":"SVoHwxtrMM_j"},"id":"SVoHwxtrMM_j"},{"cell_type":"markdown","source":["In LangChain unterst√ºtzte Rollen:\n","\n","+ \"system\" - f√ºr Systemnachrichten\n","+ \"assistant\" oder \"ai\" - f√ºr Assistenten-/KI-Antworten\n","+ \"user\" oder \"human\" - f√ºr Benutzernachrichten"],"metadata":{"id":"Po7W2FKz-VEd"},"id":"Po7W2FKz-VEd"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Variante 1: Liste & Tupel üìå\n","</font></p>"],"metadata":{"id":"yw4qr0mC5A6m"},"id":"yw4qr0mC5A6m"},{"cell_type":"code","execution_count":null,"id":"iW9URvON0IXr","metadata":{"id":"iW9URvON0IXr"},"outputs":[],"source":["# Template als Liste & Tupel\n","chat_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    (\"human\", \"Erkl√§re mir {thema}\"),\n","])"]},{"cell_type":"code","source":["# Variablen zuweisen\n","system_prompt = \"Du bist ein hilfreicher und humorvoller Assistent.\"\n","thema = \"Machine Learning\""],"metadata":{"id":"XrEDZ3Vz174e"},"id":"XrEDZ3Vz174e","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Template & Variablen kombinieren\n","prompt = chat_template.format_messages(system_prompt=system_prompt, thema=thema)"],"metadata":{"id":"x3pxKD_R2BvX"},"id":"x3pxKD_R2BvX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sprachmodell aufrufe\n","response = llm.invoke(prompt)"],"metadata":{"id":"CLU3o3dG4OGM"},"id":"CLU3o3dG4OGM","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"YnfES3Q24n0M"},"id":"YnfES3Q24n0M","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**... oder ...**"],"metadata":{"id":"6FrHMy1PYH0S"},"id":"6FrHMy1PYH0S"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Variante 2: Liste & Dictionary\n","</font></p>"],"metadata":{"id":"lkaCyUqq6OD0"},"id":"lkaCyUqq6OD0"},{"cell_type":"code","source":["# Template als Liste & Dictionary\n","chat_template = ChatPromptTemplate.from_messages([\n","    {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher und humorvoller Assistent.\"},\n","    {\"role\": \"human\", \"content\": \"Erkl√§re mir {thema}\"},\n","])"],"metadata":{"id":"kxEP8r3VDWwI"},"id":"kxEP8r3VDWwI","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Variablen zuweisen\n","system_prompt = \"Du bist ein hilfreicher und humorvoller Assistent.\"\n","thema = \"Machine Learning\""],"metadata":{"id":"naATTZyS4zEB"},"execution_count":null,"outputs":[],"id":"naATTZyS4zEB"},{"cell_type":"code","source":["# Template & Variablen kombinieren\n","prompt = chat_template.format_messages(system_prompt=system_prompt, thema=thema)"],"metadata":{"id":"O235HSRl4zEC"},"execution_count":null,"outputs":[],"id":"O235HSRl4zEC"},{"cell_type":"code","source":["# Sprachmodell aufrufe\n","response = llm.invoke(prompt)"],"metadata":{"id":"Ow6KjCLG4zEC"},"execution_count":null,"outputs":[],"id":"Ow6KjCLG4zEC"},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"6PtZzYUSC7-F"},"id":"6PtZzYUSC7-F","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Vergleich: f-string vs. PromptTemplate vs. ChatPromptTemplate\n","</font></p>"],"metadata":{"id":"2uSD48umMXI7"},"id":"2uSD48umMXI7"},{"cell_type":"markdown","source":["\n","\n","| Vorteil                      | `f-string` (Python)           | `PromptTemplate`              | `ChatPromptTemplate`            |\n","| ---------------------------- | ----------------------------- | ----------------------------- | ------------------------------- |\n","| üöª Rollenstruktur            | ‚ùå Keine Unterst√ºtzung         | ‚ùå Nur einfacher Text          | ‚úÖ Klar: `system`, `user`, `ai`  |\n","| üìö Konversationsaufbau       | ‚ùå Nicht geeignet              | ‚ùå Nicht geeignet              | ‚úÖ Ideal f√ºr Multi-Turn-Dialoge <br>    (mehreren Gespr√§chsrunden)  |\n","| üîÑ Kombinierbar mit `Memory` | ‚ùå Nein                        | ‚ùå Nein                        | ‚úÖ Ja, via `MessagesPlaceholder` |\n","| üîå F√ºr Chatmodelle wie GPT   | ‚ö†Ô∏è Manuell als String         | ‚ö†Ô∏è Nur als Flie√ütext          | ‚úÖ Nativ & strukturiert          |\n","| üîç Klarheit & Wartbarkeit    | ‚ö†Ô∏è Inline, schwer erweiterbar | ‚ö†Ô∏è Nur Inline-Text            | ‚úÖ Saubere Message-Trennung      |\n","| üß™ Prompt-Testbarkeit        | ‚ùå Keine Validierung           | ‚ö†Ô∏è Eingeschr√§nkt               | ‚úÖ Besser strukturiert           |\n","| ‚ö° Performance                | ‚úÖ Schnell & direkt            | ‚úÖ Schnell                     | ‚úÖ Schnell                       |\n","| üîß Validierung von Variablen | ‚ùå Keine                       | ‚úÖ Automatisch                 | ‚úÖ Automatisch                   |\n","| üß© Wiederverwendbarkeit      | ‚ùå Eingeschr√§nkt               | ‚úÖ Als Template-Objekt         | ‚úÖ Als Template-Objekt           |\n","| üìù Partial Variables         | ‚ùå Nein                        | ‚úÖ Ja                          | ‚úÖ Ja                            |\n","| üîÄ Chains & Pipelines        | ‚ùå Nicht integrierbar          | ‚úÖ LangChain-kompatibel        | ‚úÖ LangChain-kompatibel          |\n","| üéØ Output Parser             | ‚ùå Manuell                     | ‚úÖ Integrierbar                | ‚úÖ Integrierbar                  |"],"metadata":{"id":"UvoCBqi0qc4B"},"id":"UvoCBqi0qc4B"},{"cell_type":"markdown","source":["\n","\n","**Anwendungsempfehlungen**\n","\n","| Methode               | Wann verwenden?                                                                 |\n","| --------------------- | ------------------------------------------------------------------------------- |\n","| **f-string**          | Einfache, einmalige Prompts ¬∑ Prototyping & schnelle Tests ¬∑ Keine LangChain-Integration |\n","| **PromptTemplate**    | Single-Turn Prompts mit Variablen ¬∑ Completion-Modelle (GPT-3, etc.) ¬∑ Keine Konversation |\n","| **ChatPromptTemplate** | Multi-Turn Dialoge ¬∑ Chat-Modelle (GPT-4, Claude, etc.) ¬∑ Komplexe Konversationen mit Memory |\n"],"metadata":{"id":"IrQUS1yvpsFQ"},"id":"IrQUS1yvpsFQ"},{"cell_type":"markdown","metadata":{"id":"Ps344Ejw0-ih"},"source":["## 5.3 | Few-Shot Prompt\n"],"id":"Ps344Ejw0-ih"},{"cell_type":"markdown","source":["Few-Shot-Prompting ist eine Technik, bei der man einem KI-Modell einige Beispiele (sogenannte \"Shots\") vorlegt, bevor es eine neue Aufgabe l√∂sen soll. Das Modell kann aus diesen Beispielen das Muster erkennen und auf neue, √§hnliche F√§lle anwenden."],"metadata":{"id":"LKY3IaJSDLvt"},"id":"LKY3IaJSDLvt"},{"cell_type":"markdown","source":["`FewShotPromptTemplate` ist eine Prompt-Vorlage, die automatisch mehrere Beispiele einf√ºgt, bevor das eigentliche Nutzer-Input kommt."],"metadata":{"id":"NnBh2dz8M2fi"},"id":"NnBh2dz8M2fi"},{"cell_type":"code","execution_count":null,"metadata":{"id":"cEopgOiS0KU_"},"outputs":[],"source":["beispiele = [\n","    {\"frage\": \"Was ist Python?\", \"antwort\": \"Eine interpretierende Programmiersprache.\"},\n","    {\"frage\": \"Was ist Java?\", \"antwort\": \"Eine objektorientierte Sprache.\"}\n","]"],"id":"cEopgOiS0KU_"},{"cell_type":"code","source":["beispiel_prompt_template = PromptTemplate.from_template(\n","    \"Frage: {frage}\\nAntwort: {antwort}\"\n",")"],"metadata":{"id":"fHEYrEPcDD9S"},"execution_count":null,"outputs":[],"id":"fHEYrEPcDD9S"},{"cell_type":"code","source":["system_prompt = \"Du bist ein Assistent, der Fragen zu Programmiersprachen beantwortet. Halte deine Antworten kurz und pr√§zise:\""],"metadata":{"id":"eAk2iqPrjynu"},"id":"eAk2iqPrjynu","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Few-Shot-Prompt\n","few_shot_prompt_template = FewShotPromptTemplate(\n","    examples=beispiele,\n","    example_prompt=beispiel_prompt_template,\n","    prefix=system_prompt,\n","    suffix=\"Frage: {frage}\\nAntwort:\",\n","    input_variables=[\"frage\"]\n",")\n","\n","prompt = few_shot_prompt_template.format(frage=\"Was ist Go?\")"],"metadata":{"id":"BpIbLhIhDcUp"},"execution_count":null,"outputs":[],"id":"BpIbLhIhDcUp"},{"cell_type":"markdown","source":["**Struktur des finalen Prompts:**\n","\n","```\n","[prefix]\n","[formatiertes Beispiel 1]\n","[formatiertes Beispiel 2]\n","...\n","[suffix mit aktueller Frage]\n","```\n","\n"],"metadata":{"id":"tpDH0jAgknKS"},"id":"tpDH0jAgknKS"},{"cell_type":"code","source":["response = llm.invoke(prompt)"],"metadata":{"id":"KZPoIO6gDo23"},"execution_count":null,"outputs":[],"id":"KZPoIO6gDo23"},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"Yr0YleeiD_x7"},"execution_count":null,"outputs":[],"id":"Yr0YleeiD_x7"},{"cell_type":"markdown","source":["**Vorteile eines `FewShotPromptTemplate`**"],"metadata":{"id":"XFbved9KM_WJ"},"id":"XFbved9KM_WJ"},{"cell_type":"markdown","source":["| Vorteil                      | Beschreibung                                                      |\n","| ---------------------------- | ----------------------------------------------------------------- |\n","| üß† Lerneffekt f√ºr das Modell | Das Modell erkennt aus Beispielen, wie es antworten soll          |\n","| üß∞ Flexibilit√§t              | Beispiele k√∂nnen dynamisch gesetzt oder generiert werden          |\n","| üß± Struktur & Konsistenz     | Einheitlicher Aufbau aller Beispiele √ºber `example_prompt`        |\n","| ü™Ñ Kein Training n√∂tig       | Kein Fine-Tuning ‚Äì nur durch Prompting                            |\n","| üõ† Kombinierbar              | Kann mit Templates, Tools, Chains, OutputParser kombiniert werden |\n"],"metadata":{"id":"EWjuAd2_M-E7"},"id":"EWjuAd2_M-E7"},{"cell_type":"markdown","id":"pt37u3cnbgc","source":["## 5.4 | Eigenes Prompt-Template üìå"],"metadata":{"id":"pt37u3cnbgc"}},{"cell_type":"markdown","id":"ik5lbyv7vc","source":["Die `load_chat_prompt_template()` Funktion aus der `genai_lib.utilities` erm√∂glicht das Laden von Prompt-Templates direkt von GitHub oder aus lokalen Dateien.\n","\n","**Vorteile:**\n","- Zentrale Verwaltung von Prompts im `05_prompt/` Verzeichnis\n","- Wiederverwendbarkeit √ºber mehrere Notebooks hinweg\n","- Versionskontrolle durch Git\n","- Einfaches Teilen von Best-Practice-Prompts\n","\n","**Hinweis:** In Google Colab muss die GitHub-URL verwendet werden, da lokale Pfade nicht verf√ºgbar sind."],"metadata":{"id":"ik5lbyv7vc"}},{"cell_type":"markdown","id":"rjewn4pkze","source":["**Template-Struktur (`05_prompt/text_zusammenfassung.py`):**\n","\n","```python\n","# Prompt Template als Python-Datei\n","messages = [\n","    (\n","        \"system\",\n","        \"Du bist ein Experte f√ºr die Erstellung pr√§gnanter Textzusammenfassungen...\"\n","    ),\n","    (\n","        \"human\",\n","        \"Bitte fasse den folgenden Text zusammen:\\n\\n{text}\\n\\n...\"\n","    )\n","]\n","```\n","\n","**Wichtig:** Templates m√ºssen als **Python-Dateien (.py)** erstellt werden und eine Variable `messages` definieren, die eine Liste von Tupeln mit `(role, content)` enth√§lt."],"metadata":{"id":"rjewn4pkze"}},{"cell_type":"code","id":"fl5appj1wrf","source":["from genai_lib.utilities import load_chat_prompt_template\n","\n","# Template von GitHub laden\n","url = \"https://github.com/ralf-42/GenAI/blob/main/05_prompt/text_zusammenfassung.py\"\n","chat_template = load_chat_prompt_template(url)\n","\n","# Template mit Beispieltext testen\n","beispiel_text = \"\"\"\n","LangChain ist ein Framework zur Entwicklung von Anwendungen mit gro√üen Sprachmodellen (LLMs).\n","Es bietet modulare Komponenten f√ºr verschiedene Aufgaben, vorgefertigte Ketten f√ºr komplexe\n","Workflows und eine einfache Integration externer Datenquellen. Mit LangChain k√∂nnen Entwickler\n","Chat-Modelle, Embeddings, Vektordatenbanken und Agents effizient kombinieren, um\n","leistungsstarke KI-Anwendungen zu erstellen.\n","\"\"\"\n","\n","# Prompt mit Variablen f√ºllen\n","prompt = chat_template.format_messages(text=beispiel_text)\n","\n","# Aufruf\n","response = llm.invoke(prompt)"],"metadata":{"id":"fl5appj1wrf"},"execution_count":null,"outputs":[]},{"cell_type":"code","id":"zqaw2rdob6b","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"zqaw2rdob6b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"8c859b9f","metadata":{"id":"8c859b9f"},"source":["# 6 | Weitere Modelle üìå\n","---"]},{"cell_type":"code","source":["#@title üßú‚Äç‚ôÄÔ∏è Prozess-Diagramm { display-mode: \"form\" }\n","\n","diagram = \"\"\"\n","flowchart LR\n","    Input[\"Input<br/>(Message)\"] --> Prompt\n","\n","    subgraph Chain\n","        Prompt[\"Prompt<br/>(Template)\"]\n","        Model[\"Model<br/>(LLM)\"]\n","        Response[\"Response<br/>(Parser)\"]\n","        Prompt --> Model --> Response\n","    end\n","\n","    Response --> Result[\"Result\"]\n","\n","    %% HIGHLIGHT einer einzelnen Box\n","    style Model fill:#f9c74f,stroke:#b36b00,stroke-width:2px\n","\n","\"\"\"\n","mermaid(diagram)"],"metadata":{"id":"QaTXxdescavY"},"id":"QaTXxdescavY","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["LangChain ist mit zahlreichen LLMs ‚Äì sowohl eines als auch verschiedener Anbieter ‚Äì kompatibel und bietet vielseitige Konfigurationsoptionen f√ºr individuelle Anwendungsf√§lle."],"metadata":{"id":"5y6J-chfEOPH"},"id":"5y6J-chfEOPH"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","\n","Model-Provider `OpenAI`\n","</font></p>"],"metadata":{"id":"0Vrxy9i9ZoP-"},"id":"0Vrxy9i9ZoP-"},{"cell_type":"markdown","source":["**Beispiel `gpt-4o` und `gpt-5-mini`**"],"metadata":{"id":"Oc0UPT5eTyTR"},"id":"Oc0UPT5eTyTR"},{"cell_type":"markdown","source":["+ gpt-4o:    \n","gpt-4o ist ein multimodales KI-Modell von OpenAI, das Text- und Bildeingaben verarbeiten kann und sich durch hohe Geschwindigkeit sowie eine nat√ºrliche, menschen√§hnliche Interaktion auszeichnet ‚Äì ideal f√ºr allt√§gliche Konversationen, kreative Aufgaben und allgemeine Chatanwendungen\n","+ gpt-5-mini:     \n","gpt-5-mini ist die mittlere Modellvariante der GPT-5-Familie, die ein ausgewogenes Verh√§ltnis zwischen Leistungsf√§higkeit und Kosten bietet und f√ºr allt√§gliche Aufgaben, allgemeine Automatisierung sowie kosteng√ºnstige Anwendungen im gro√üen Ma√üstab optimiert wurde"],"metadata":{"id":"f8yl7OApTvcQ"},"id":"f8yl7OApTvcQ"},{"cell_type":"code","source":["# Modelle konfigurieren (Kurznotation: \"provider:model\")\n","chat_model = init_chat_model(\"openai:gpt-4o\")\n","resoning_model = init_chat_model(\"openai:gpt-5-mini\")"],"metadata":{"id":"aDGqhlxoENvs"},"id":"aDGqhlxoENvs","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Gemeinsamer simple prompt\n","prompt = \"Was ist der Unterschied zwischen Prompting und Retrieval-Augmented-Generation?\""],"metadata":{"id":"PywoNdLrPCfO"},"id":"PywoNdLrPCfO","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aufruf\n","response = chat_model.invoke(prompt)"],"metadata":{"id":"iriw2JZPOazT"},"id":"iriw2JZPOazT","execution_count":null,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Chat-Model:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"9OlEy8jQPck1"},"id":"9OlEy8jQPck1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aufruf\n","response = resoning_model.invoke(prompt)"],"metadata":{"id":"_Znkwm2SPOrX"},"id":"_Znkwm2SPOrX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Reasoning-Model:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"N0hcQkSfQyVK"},"id":"N0hcQkSfQyVK","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**üìä Vergleich: Chat-Modell vs. Reasoning-Modell**"],"metadata":{"id":"YJljcFw4Rz6W"},"id":"YJljcFw4Rz6W"},{"cell_type":"markdown","source":["| **Aspekt**                      | **Chat-Modell**                                                           | **Reasoning-Modell**                                                                                |\n","| ------------------------------- | ------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------- |\n","| **Stil**                        | Erkl√§rend, klassisch strukturiert mit Flie√ütext                           | Punktuell, technisch und systematisch                                                               |\n","| **Formatierung**                | Flie√ütext mit Abs√§tzen und Beispielen                                     | Aufz√§hlungen (‚Ä¢) und strukturierte Unterpunkte                                                      |\n","| **Definition Prompting**        | Kurze Definition als Text, danach Beispiel                                | Technisch klar: ‚Äûparametrisches Wissen‚Äú, Rolle der Prompt-Formulierung betont                       |\n","| **Definition RAG**              | Flie√ütext-Erkl√§rung mit Ablaufbeschreibung und Beispiel                   | Detaillierte Schritte: Abruf, Kontextintegration, Vorteile bzgl. Aktualit√§t und Pr√§zision erl√§utert |\n","| **Beispiele**                   | Ja ‚Äì einfache, praxisnahe Beispiele (z.‚ÄØB. Klimawandel, Quantencomputing) | Nein ‚Äì keine konkreten Beispiele, aber kontextbezogene Beschreibung                                 |\n","| **Vergleich Prompting vs. RAG** | Abstrakte Gegen√ºberstellung in einem abschlie√üenden Absatz                | Direkter Vergleich im letzten Abschnitt mit Betonung der Wissensquellen                             |\n","| **Technische Tiefe**            | Mittel ‚Äì f√ºr Einsteiger\\:innen gut verst√§ndlich                           | Hoch ‚Äì Begriffe wie ‚Äûparametrisches Wissen‚Äú, Fokus auf Architektur                                  |\n","| **Zielgruppe**                  | Allgemeines Publikum, Einstieg in das Thema                               | Fortgeschrittene Nutzer\\:innen, Technik-affine Leserschaft                                          |\n","| **St√§rken des Modells**         | Klare Erkl√§rung, gute didaktische Struktur                                | Pr√§zise Begriffsabgrenzung, hohe Informationsdichte                                                 |\n","\n"],"metadata":{"id":"OzPIlXnqR2Fq"},"id":"OzPIlXnqR2Fq"},{"cell_type":"code","source":["mprint(\"### üì£ Antwort:\")\n","mprint(\"---\")\n","mprint(response_gemini.content)"],"metadata":{"id":"WmtfOD8iXa0p"},"id":"WmtfOD8iXa0p","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","\n","Ein anderer Modell-Provider: `Alibaba/groq`\n","</font></p>"],"metadata":{"id":"8y5xNtM1oMs3"},"id":"8y5xNtM1oMs3"},{"cell_type":"markdown","source":["`Qwen3-32B` ist ein 32,8 Milliarden Parameter starker Transformer von Alibaba (Qwen3-Serie, April 2025), der durch Dual-Mode-Reasoning gl√§nzt: \"Thinking Mode\" f√ºr komplexe Logik, Mathe und Coding; \"Non-Thinking Mode\" f√ºr schnelle Dialoge"],"metadata":{"id":"umgN4oKXOr_f"},"id":"umgN4oKXOr_f"},{"cell_type":"code","source":["install_packages([('langchain-groq', 'langchain_groq')])\n","setup_api_keys(['GROQ_API_KEY'])"],"metadata":{"id":"ec6V1NJRnjPv"},"execution_count":null,"outputs":[],"id":"ec6V1NJRnjPv"},{"cell_type":"code","source":["# Kurznotation: \"provider:model\"\n","llm_qwen = init_chat_model(\"groq:qwen/qwen3-32b\", temperature=0)"],"metadata":{"id":"cVSRS2Kaf1M7"},"execution_count":null,"outputs":[],"id":"cVSRS2Kaf1M7"},{"cell_type":"code","source":["response_groq = llm_qwen.invoke(\"Wer war Albert Einstein? Bitte antworte in deutscher Sprache.\")"],"metadata":{"id":"oRl7X3y-nm92"},"execution_count":null,"outputs":[],"id":"oRl7X3y-nm92"},{"cell_type":"code","source":["mprint(response_groq.content)"],"metadata":{"id":"fhwKoyS3n0ho"},"execution_count":null,"outputs":[],"id":"fhwKoyS3n0ho"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Separater Ausweis Reasoning m√∂glich\n","</font></p>"],"metadata":{"id":"dwwtPwJyLRE2"},"id":"dwwtPwJyLRE2"},{"cell_type":"code","source":["from genai_lib.utilities import extract_thinking\n","thinking, answer = extract_thinking(response_groq)\n","\n","mprint(\"### üß† Denken:\")\n","mprint(\"---\")\n","mprint(thinking)\n","mprint(\"### üì£ Antwort:\")\n","mprint(\"---\")\n","mprint(answer)"],"metadata":{"id":"WnZVyYPvKJpS"},"id":"WnZVyYPvKJpS","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"c314ca71","metadata":{"id":"c314ca71"},"source":["# 7 | Chain üìå\n","---"]},{"cell_type":"code","source":["#@title üßú‚Äç‚ôÄÔ∏è Prozess-Diagramm { display-mode: \"form\" }\n","\n","diagram = \"\"\"\n","flowchart LR\n","    Input[\"Input<br/>(Message)\"] --> Prompt\n","\n","    subgraph Chain\n","        Prompt[\"Prompt<br/>(Template)\"]\n","        Model[\"Model<br/>(LLM)\"]\n","        Response[\"Response<br/>(Parser)\"]\n","        Prompt --> Model --> Response\n","    end\n","\n","    Response --> Result[\"Result\"]\n","\n","    %% HIGHLIGHT einer einzelnen Box\n","    style Chain fill:#f9c74f,stroke:#b36b00,stroke-width:2px\n","\n","\"\"\"\n","mermaid(diagram)"],"metadata":{"id":"OYL4O4sncgV1"},"id":"OYL4O4sncgV1","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Chains verbinden mehrere Komponenten/Bausteine zu einer Verarbeitungskette. LangChain Expression Language (LCEL) ist das **neue Programmiermodell in LangChain v0.3+**, das die Entwicklung von LLM-Anwendungen vereinfacht.\n"],"metadata":{"id":"cEEMxhSBHrXA"},"id":"cEEMxhSBHrXA"},{"cell_type":"markdown","source":["\n","**Hier die Kernpunkte:**\n","\n","1. Was ist LCEL?\n","```python\n","# LCEL nutzt den Pipe-Operator (|) f√ºr klare Verkettungen\n","chain = prompt | model\n","```\n","- Eine deklarative Sprache zum Verketten von LangChain-Komponenten\n","- Erm√∂glicht linearen Datenfluss zwischen Komponenten\n","- Basiert auf dem Pipe-Operator (|) f√ºr intuitive Verbindungen\n","\n","2. Warum LCEL nutzen?\n","- Bessere Lesbarkeit des Codes\n","- Einfachere Wartung und Debugging\n","- Verbesserte Performance durch optimierte Ausf√ºhrung\n","- Bessere Typsicherheit und Fehlererkennung\n","- Unterst√ºtzt modernes Streaming und Async-Operationen\n","\n","3. Praktisches Beispiel:"],"metadata":{"id":"ltTQItwXH6w8"},"id":"ltTQItwXH6w8"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Simple Chain üìå\n","</font></p>\n"],"metadata":{"id":"BndZGbyWKs1C"},"id":"BndZGbyWKs1C"},{"cell_type":"code","id":"fsryxusc3p8","source":["#@markdown   <p><font size=\"4\" color='green'>   Mermaid - LCEL Chain-Komposition</font> </br></p>\n","\n","diagram = \"\"\"\n","graph LR\n","    A[\"Input Dictionary<br/>{user_input: ...}\"] --> B[\"Prompt Template\"]\n","    B --> C[\"Formatierter Prompt<br/>System + Human Message\"]\n","    C --> D[\"LLM\"]\n","    D --> E[\"AI Message\"]\n","    E --> F[\"StrOutputParser\"]\n","    F --> G[\"String Output\"]\n","\n","    H[\"chain = prompt | llm | parser\"] -.-> B\n","    H -.-> D\n","    H -.-> F\n","\n","    style A fill:#e1f5ff\n","    style B fill:#fff4e1\n","    style C fill:#f0f0f0\n","    style D fill:#ffe1f5\n","    style E fill:#f0f0f0\n","    style F fill:#e1ffe1\n","    style G fill:#f0f0f0\n","    style H fill:#ffe1e1\n","\"\"\"\n","mermaid(diagram, width=900, height=250)"],"metadata":{"id":"fsryxusc3p8","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","# 1. Prompt-Template\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"human\", \"Erkl√§re mir {user_input}\")\n","])\n","\n","# 2. Modell (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0)\n","\n","#  3. Einfache LCEL-Kette\n","chain = prompt | llm\n","\n","# 4. Ausf√ºhrung\n","response = chain.invoke({\"user_input\": \"LangChain Expression Language\"})"],"metadata":{"id":"PsWtjWROIW9d"},"id":"PsWtjWROIW9d","execution_count":null,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"VWYAG9aWJpZd"},"id":"VWYAG9aWJpZd","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","LCEL ist der empfohlene Weg f√ºr alle neuen LangChain-Projekte, da es die Entwicklung vereinfacht und zukunftssicher macht."],"metadata":{"id":"49Wu_XreIZY2"},"id":"49Wu_XreIZY2"},{"cell_type":"markdown","source":["# 8 | Output-Handling\n","---"],"metadata":{"id":"LVqVL9CWRXU7"},"id":"LVqVL9CWRXU7"},{"cell_type":"code","source":["#@title üßú‚Äç‚ôÄÔ∏è Prozess-Diagramm { display-mode: \"form\" }\n","\n","diagram = \"\"\"\n","flowchart LR\n","    Input[\"Input<br/>(Message)\"] --> Prompt\n","\n","    subgraph Chain\n","        Prompt[\"Prompt<br/>(Template)\"]\n","        Model[\"Model<br/>(LLM)\"]\n","        Response[\"Response<br/>(Parser)\"]\n","        Prompt --> Model --> Response\n","    end\n","\n","    Response --> Result[\"Result\"]\n","\n","    %% HIGHLIGHT einer einzelnen Box\n","    style Response fill:#f9c74f,stroke:#b36b00,stroke-width:2px\n","\n","\"\"\"\n","mermaid(diagram)"],"metadata":{"id":"uZ2nFRAgcqxe"},"id":"uZ2nFRAgcqxe","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 8.1 | OutputParser üìå"],"metadata":{"id":"89XLSUAvCE8P"},"id":"89XLSUAvCE8P"},{"cell_type":"markdown","source":["Ein OutputParser ist ein Konzept in LangChain, das hilft, die Antworten eines Modells richtig weiterzuverarbeiten. Es nimmt die **rohe Ausgabe** (zum Beispiel einen langen Text) und formt sie in ein **bestimmtes Format**, das sp√§ter in der Anwendung leichter benutzt werden kann."],"metadata":{"id":"EVBcCwJERfaj"},"id":"EVBcCwJERfaj"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Simple Chain mit Parser üìå\n","</font></p>"],"metadata":{"id":"oK4lZB3EGKFK"},"id":"oK4lZB3EGKFK"},{"cell_type":"code","source":["# 1. Prompt-Template\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"human\", \"Erkl√§re mir {user_input}\")\n","])\n","\n","# 2. Modell (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0)\n","\n","# 3. Parser\n","parser = StrOutputParser()\n","\n","# 4. Einfache LCEL-Kette\n","chain = prompt | llm | parser\n","\n","# 5. Ausf√ºhrung\n","response = chain.invoke({\"user_input\": \"LangChain Expression Language\"})"],"metadata":{"id":"PP_A_bGc7ovZ"},"execution_count":null,"outputs":[],"id":"PP_A_bGc7ovZ"},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response)"],"metadata":{"id":"qMcn5ctu7ova"},"execution_count":null,"outputs":[],"id":"qMcn5ctu7ova"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Simple Chain mit Parser & Format im System-Prompt\n","</font></p>"],"metadata":{"id":"pNKOzTlQ8Kpl"},"id":"pNKOzTlQ8Kpl"},{"cell_type":"code","source":["# 1. Einfacher Prompt mit explizite Formatvorgabe\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent. Formatiere Deine Antwort im json-Format mit den Infos zu Name und Alter.\"),\n","    (\"human\", \"{user_input}\")\n","])\n","\n","# 2. Modell (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0)\n","\n","# 3. Parser\n","parser = StrOutputParser()\n","\n","# 4. Chain\n","chain = prompt | llm | parser\n","\n","# 5. Ausf√ºhrung\n","response = chain.invoke({\"user_input\": \"Bitte gibt den Namen einer fiktiven Person und das Alter\"})"],"metadata":{"id":"QKtYSBLhVlts"},"execution_count":null,"outputs":[],"id":"QKtYSBLhVlts"},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response)"],"metadata":{"id":"BGceDe7Kw6ot"},"id":"BGceDe7Kw6ot","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"5efri5ponin","source":["# 8.2 | with_structured_output() ‚ú≥Ô∏è\n"],"metadata":{"id":"5efri5ponin"}},{"cell_type":"markdown","id":"9hnfp8g8bkj","source":["Im vorherigen Beispielen wurden Formatierung √ºber **System-Prompts** und/oder √ºber einen **OututParser** gesteuert. Das funktioniert, hat aber Nachteile:\n","\n","- ‚ùå Keine automatische Validierung (Typen k√∂nnen falsch sein)\n","- ‚ùå String-Parsing fehleranf√§llig\n","- ‚ùå Keine Typsicherheit im Code\n","- ‚ùå Prompt-Engineering f√ºr Format n√∂tig\n","\n","**Moderne L√∂sung: `with_structured_output()`**\n","\n","LangChain 1.0+ bietet eine elegante L√∂sung: Mit `with_structured_output()` kann man die gew√ºnschte Ausgabestruktur **direkt als Pydantic-Modell** definieren. Das Modell sorgt automatisch f√ºr:\n","\n","- ‚úÖ Validierte, typsichere Ausgaben\n","- ‚úÖ Automatische Format-Konvertierung\n","- ‚úÖ Klare Struktur ohne Prompt-Engineering\n","- ‚úÖ IDE-Unterst√ºtzung (Autocomplete, Type-Hints)"],"metadata":{"id":"9hnfp8g8bkj"}},{"cell_type":"markdown","id":"p8qe3tuhedi","source":["<p><font color='black' size=\"5\">\n","Einfaches Beispiel mit with_structured_output()\n","</font></p>"],"metadata":{"id":"p8qe3tuhedi"}},{"cell_type":"code","id":"a3xf7g08nw","source":["from pydantic import BaseModel, Field\n","\n","# 1. Pydantic-Modell definiert die gew√ºnschte Struktur\n","class PersonInfo(BaseModel):\n","    \"\"\"Informationen √ºber eine Person\"\"\"\n","    name: str = Field(description=\"Vollst√§ndiger Name der Person\")\n","    alter: int = Field(description=\"Alter der Person in Jahren\")\n","    beruf: str = Field(description=\"Aktueller Beruf der Person\")\n","\n","# 2. Modell mit strukturierter Ausgabe (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0)\n","structured_llm = llm.with_structured_output(PersonInfo)\n","\n","# 3. Einfache Anfrage - automatische Formatierung & Validierung\n","response = structured_llm.invoke(\n","    \"Erstelle eine fiktive Person: Name, Alter und Beruf\"\n",")\n","\n","# 4. Typsichere Ausgabe - response ist ein PersonInfo-Objekt\n","print(f\"Typ: {type(response)}\")\n","print(f\"Name: {response.name}\")\n","print(f\"Alter: {response.alter}\")\n","print(f\"Beruf: {response.beruf}\")"],"metadata":{"id":"a3xf7g08nw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"ybps752g89","source":["**Vergleich: Prompt-basierte Formatierung vs. StrOutputParser vs. with_structured_output()**\n","\n","| Aspekt | Prompt-basiert | StrOutputParser | with_structured_output() |\n","|--------|----------------|----------------|---------------------------|\n","| **Definition** | Via System-Prompt: ‚ÄûFormatiere im JSON-Format...‚Äú | Expliziter Parser, der LLM-Ausgabe als String extrahiert oder leicht nachbearbeitet | Automatische Strukturierung per Pydantic-Modell (`class PersonInfo(BaseModel)`) |\n","| **Validierung** | ‚ùå Keine ‚Äì Modell kann beliebig antworten | ‚ö†Ô∏è Nur minimale (z. B. String-Cleaning) | ‚úÖ Strikte Schema-Validierung durch Pydantic |\n","| **Typsicherheit** | ‚ùå Nur Strings | ‚ùå Nur Strings | ‚úÖ Typsichere Python-Objekte |\n","| **Fehlerbehandlung** | ‚ö†Ô∏è Manuelles Parsing und Regex n√∂tig | ‚ö†Ô∏è Begrenzte Kontrolle (nur einfache Parsing-Fehler) | ‚úÖ Automatische Validierung, Exception bei Schemafehlern |\n","| **Code-Qualit√§t** | ‚ö†Ô∏è Fragil, stark Prompt-abh√§ngig | ‚úÖ Etwas klarer, aber keine echte Struktur | ‚úÖ Robust, wartbar, klar definierte Typen |\n","| **IDE-Support** | ‚ùå Keine Autovervollst√§ndigung | ‚ùå Keine | ‚úÖ Vollst√§ndig durch Type-Hints |\n","| **Anwendungsfall** | Einfache oder explorative Ausgaben | Wenn nur reine Textausgabe n√∂tig ist | F√ºr strukturierte, verl√§ssliche Daten (JSON, Objekte) |\n","| **Lernkurve** | ‚úÖ Sehr einfach | ‚úÖ Einfach | ‚ö†Ô∏è Erfordert Grundkenntnisse in Pydantic und LangChain-Schemas |\n","| **Beispiel** | ‚ÄûNenne Name und Alter als JSON.‚Äú | `StrOutputParser().parse(output)` | `llm.with_structured_output(PersonInfo)` |\n","\n"],"metadata":{"id":"ybps752g89"}},{"cell_type":"markdown","source":["\n","**Wann welchen Ansatz?**\n","\n","- **Prompt-basiert**: F√ºr einfache Texte, Zusammenfassungen, kreative Inhalte  \n","- **StrOutputParser**: Wenn nur reine Textausgabe oder einfache Formatkorrektur n√∂tig ist  \n","- **with_structured_output()**: F√ºr strukturierte Daten, APIs, Datenbanken, komplexe Workflows"],"metadata":{"id":"uncqM3AxDjh9"},"id":"uncqM3AxDjh9"},{"cell_type":"markdown","source":["# 9 | Runnables (opt.)\n","---\n"],"metadata":{"id":"KRqrnpMx3Hwf"},"id":"KRqrnpMx3Hwf"},{"cell_type":"markdown","source":["Ein **Runnable** ist ein elementarer Baustein in LangChain. Er nimmt eine Eingabe und liefert eine Ausgabe - wie eine Funktion mit klaren Regeln. Man kann sich Konzepte wie Prompts, LLMs oder Parser als Runnables vorstellen. Runnable kann man als **Oberbegriff** f√ºr Prompts, LLMs, Parser und √§hnliche Komponenten verwenden.\n","\n","\n","\n","In der Grafik sieht man die **drei wichtigsten Runnables**: Prompt-Template, LLM und Parser, die zusammen eine Kette bilden.\n","\n","In LangChain sind die wichtigsten Komponenten als Runnables verf√ºgbar:\n","\n","+ Prompts/Templates (der erste Baustein in der Kette)\n","+ LLMs (das Herzst√ºck in der Mitte)\n","+ Output-Parser (verwandelt die LLM-Antwort in ein nutzbares Format)\n","+ Komplette Chains (die ganze Verarbeitungskette)\n","+ Tools (f√ºr spezielle Aufgaben)\n","+ Retriever (holen zus√§tzliche Informationen)\n","\n","\n","Vorteile von Runnables:\n","\n","+ Gleiche Bedienung: Alle Runnables (Prompt, LLM, Parser) funktionieren nach demselben Prinzip\n","+ Einfaches Verbinden: Man kann sie unkompliziert zu einer Kette zusammenf√ºgen - genau wie im Bild gezeigt\n","+ Anpassungsf√§hig: Sie funktionieren sowohl einzeln als auch in der Gruppe\n","Schrittweise Ausgabe: Ergebnisse k√∂nnen st√ºckweise weitergegeben werden"],"metadata":{"id":"DnUDR_983M4l"},"id":"DnUDR_983M4l"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Benutzerdefinierte Runnables erstellen\n","</font></p>"],"metadata":{"id":"WWJ7U1iJ6b8Q"},"id":"WWJ7U1iJ6b8Q"},{"cell_type":"markdown","source":["Man kann auch eigene Runnables erstellen, indem man eine Klasse definiert, die das Runnable-Interface implementiert oder eine Funktion mit `RunnableLambda` umsetzt:\n"],"metadata":{"id":"PAvbE_TR6g87"},"id":"PAvbE_TR6g87"},{"cell_type":"code","source":["from langchain_core.runnables import RunnableLambda\n","from langchain_core.output_parsers.string import StrOutputParser\n","\n","# 1. Mit RunnableLambda eine einfache Transformation erstellen\n","def text_verdoppeln(eingabe):\n","    return f\"{eingabe} <br> <br>  {eingabe}\"\n","\n","verdoppler = RunnableLambda(text_verdoppeln)\n","\n","# Verwendung des eigenen Runnables\n","verdoppler.invoke(\"Dies ist ein Text.\")"],"metadata":{"id":"_0wEf_hL6peM"},"id":"_0wEf_hL6peM","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Einfacher Prompt ohne explizite Formatvorgabe\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent. Antworte kurz.\"),\n","    (\"human\", \"Eingabe: {user_input}\")\n","])\n","\n","# In eine Kette einbauen - Korrektur der Pipe-Operatoren\n","chain_mit_verdoppler = prompt | llm | parser | verdoppler\n","\n","# Test der Kette\n","response = chain_mit_verdoppler.invoke({\"user_input\": \"Wer war Einstein?\"})"],"metadata":{"id":"NvZdOf1961xb"},"id":"NvZdOf1961xb","execution_count":null,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response)"],"metadata":{"id":"_elA0imx2HZA"},"id":"_elA0imx2HZA","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Runnable-Methoden\n","</font></p>\n","\n","Jedes Runnable unterst√ºtzt standardm√§√üig folgende Methoden:\n","\n","1. **invoke()**: F√ºr einzelne, synchrone Anfragen\n","2. **batch()**: Verarbeitet mehrere Eingaben parallel\n","3. **stream()**: Gibt Teilergebnisse zur√ºck, sobald sie verf√ºgbar sind\n","4. **ainvoke()**: Asynchrone Version von invoke\n","5. **abatch()**: Asynchrone Version von batch\n","6. **astream()**: Asynchrone Version von stream\n","\n"],"metadata":{"id":"CjFvHqWA3S2V"},"id":"CjFvHqWA3S2V"},{"cell_type":"markdown","source":["Hier ein einfaches Beispiel mit den verschiedenen Aufrufmethoden:\n"],"metadata":{"id":"a9zHZAMP3bCL"},"id":"a9zHZAMP3bCL"},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers.string import StrOutputParser\n","\n","# Einfaches Template erstellen\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"human\", \"{user_input}\")\n","])\n","\n","# Runnable-Kette erstellen\n","chain = prompt | llm | parser"],"metadata":{"id":"kQTKHK2D4DBb"},"id":"kQTKHK2D4DBb","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. invoke - Einzelne Anfrage\n","response = chain.invoke({\"user_input\": \"Was ist ein Runnable in LangChain?\"})\n","\n","mprint(\"## üì£ Invoke response:\")\n","mprint(\"---\")\n","mprint(response)"],"metadata":{"id":"TVWtHSYw5Jla"},"id":"TVWtHSYw5Jla","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. batch - Mehrere Anfragen parallel\n","response = chain.batch([\n","    {\"user_input\": \"Erkl√§re den Begriff LCEL kurz.\"},\n","    {\"user_input\": \"Was sind die Hauptvorteile von Runnables?\"}\n","])\n","mprint(\"##  üì£ Batch response::\")\n","mprint(\"---\")\n","for i, ergebnis in enumerate(response, 1):\n","    mprint(f\"Anfrage {i}: {response[:50]}...\\n\")"],"metadata":{"id":"UAet4KSq5K2D"},"id":"UAet4KSq5K2D","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. stream - Schrittweise Ausgabe\n","mprint(\"##  üì£ Stream response:\")\n","mprint(\"---\")\n","for chunk in chain.stream({\"user_input\": \"Erkl√§re die Abk√ºrzung LCEL kurz.\"}):\n","    print(chunk, end=\" ‚û° \")  # Zeichen trennt die via stream gelieferten Ergebnisse/Ausgaben\n","print(\"\\n\")"],"metadata":{"id":"VJn4e_IX5L-b"},"id":"VJn4e_IX5L-b","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"F_5gn03b46rC","metadata":{"id":"F_5gn03b46rC"},"source":["# A | Aufgabe\n","---"]},{"cell_type":"markdown","id":"36LyGMmo5jb3","metadata":{"id":"36LyGMmo5jb3"},"source":["Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen."]},{"cell_type":"markdown","id":"Je-szbGU7IJ-","metadata":{"id":"Je-szbGU7IJ-"},"source":["\n","<p><font color='black' size=\"5\">\n","Erstellen einer LangChain-Kette zur Textanalyse\n","</font></p>"]},{"cell_type":"markdown","id":"kevePvA-7PzW","metadata":{"id":"kevePvA-7PzW"},"source":["Entwickeln Sie eine Verarbeitungskette, die einen Text analysiert und verschiedene Informationen dar√ºber extrahiert.\n","\n","**Schritte:**\n","1. Erstelle ein ChatPromptTemplate, das ein LLM anweist, einen Text zu analysieren\n","2. Die Analyse soll folgende Aspekte umfassen:\n","   - Hauptthema des Textes\n","   - Tonalit√§t (formal, informell, etc.)\n","   - Schl√ºsselw√∂rter (5-10)\n","   - Kurze Zusammenfassung (max. 3 S√§tze)\n","3. Formatiere die Ausgabe strukturiert mit Markdown-√úberschriften\n","4. Teste die Kette mit mindestens zwei verschiedenen Texten\n","\n","\n"]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["91ee89a4-477b-41ce-a511-aadd4c2fd547","fGV8FwGmIwmR","EouwC_wdZb-c","BMPwDNTCCOfx","-JwJD0rLIyT8","HqTUeMW20084","8akKmCCc06nm","Ps344Ejw0-ih","pt37u3cnbgc","8c859b9f","c314ca71","LVqVL9CWRXU7","89XLSUAvCE8P","5efri5ponin","KRqrnpMx3Hwf","F_5gn03b46rC"],"toc_visible":true},"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":5}
{"cells":[{"cell_type":"markdown","id":"37rMyCQz2JNw","metadata":{"id":"37rMyCQz2JNw"},"source":["<p><font size=\"6\" color='grey'> <b>\n","\n","Generative KI. Verstehen. Anwenden. Gestalten.\n","</b></font> </br></p>"]},{"cell_type":"markdown","id":"R5CfUEMJdvFQ","metadata":{"id":"R5CfUEMJdvFQ"},"source":["<p><font size=\"5\" color='grey'> <b>\n","LangChain 101\n","</b></font> </br></p>\n","\n","---"]},{"cell_type":"code","source":["#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n","!uv pip install --system -q git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul\n","from genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages, load_chat_prompt_template, mermaid\n","setup_api_keys(['OPENAI_API_KEY'], create_globals=False)\n","print()\n","check_environment()\n","print()\n","get_ipinfo()"],"metadata":{"id":"PnT8m8TTUhF5","collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764781311146,"user_tz":-60,"elapsed":77895,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"e1e21e73-1c8f-41d6-bf53-885ecaab4306"},"id":"PnT8m8TTUhF5","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úì OPENAI_API_KEY erfolgreich gesetzt\n","\n","Python Version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n","\n","Installierte LangChain- und LangGraph-Bibliotheken:\n","langchain                                1.1.0\n","langchain-chroma                         1.0.0\n","langchain-classic                        1.0.0\n","langchain-community                      0.4.1\n","langchain-core                           1.1.0\n","langchain-ollama                         1.0.0\n","langchain-openai                         1.1.0\n","langchain-text-splitters                 1.0.0\n","langgraph                                1.0.3\n","langgraph-checkpoint                     3.0.1\n","langgraph-prebuilt                       1.0.5\n","langgraph-sdk                            0.2.10\n","\n","IP-Adresse: 34.56.148.208\n","Hostname: 208.148.56.34.bc.googleusercontent.com\n","Stadt: Council Bluffs\n","Region: Iowa\n","Land: US\n","Koordinaten: 41.2619,-95.8608\n","Provider: AS396982 Google LLC\n","Postleitzahl: 51502\n","Zeitzone: America/Chicago\n"]}]},{"cell_type":"markdown","id":"91ee89a4-477b-41ce-a511-aadd4c2fd547","metadata":{"id":"91ee89a4-477b-41ce-a511-aadd4c2fd547"},"source":["# 1 | Was ist LangChain?\n","---"]},{"cell_type":"markdown","id":"b5004f8c","metadata":{"id":"b5004f8c"},"source":["LangChain ist ein Framework zur **Entwicklung von Anwendungen mit gro√üen Sprachmodellen** (LLMs). Es vereinfacht die Integration von LLMs in eigene Anwendungen durch:\n","\n","- Modulare Komponenten f√ºr verschiedene Aufgaben\n","- Vorgefertigte Ketten (Chains) f√ºr komplexe Workflows\n","- Einfache Integration externer Datenquellen\n","- Werkzeuge f√ºr das Speichern von Konversationskontexten\n","\n"]},{"cell_type":"markdown","id":"tl73bT-kKkia","metadata":{"id":"tl73bT-kKkia"},"source":["[Einf√ºhrung](https://python.langchain.com/docs/introduction/)   \n","[Konzepte](https://python.langchain.com/docs/concepts/)   \n","[API-References](https://python.langchain.com/api_reference/index.html)   \n","[Integrations](https://python.langchain.com/docs/integrations/providers/)\n","\n","---\n","\n","[Tutorials](https://python.langchain.com/docs/tutorials/)   \n","[How-to-Guides](https://python.langchain.com/docs/how_to/)   "]},{"cell_type":"markdown","id":"fGV8FwGmIwmR","metadata":{"id":"fGV8FwGmIwmR"},"source":["# 2 | √úberblick Konzepte\n","---\n","\n","\n"]},{"cell_type":"markdown","source":["Die Darstellung veranschaulicht das Grundprinzip von LangChain: die Verkettung verschiedener Konzepte zu einem durchg√§ngigen Verarbeitungsprozess, der Eingaben in strukturierte Ausgaben umwandelt."],"metadata":{"id":"nZ2ZMEd3ulKZ"},"id":"nZ2ZMEd3ulKZ"},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/langchain_prozess_01.png)"],"metadata":{"id":"KdBKNv3gzJAq"},"id":"KdBKNv3gzJAq"},{"cell_type":"markdown","source":["Konzepte von LangChain sind grundlegende Bausteine und Prinzipien. Hier sind einige der wichtigsten Konzepte:\n","\n","+ Nachrichten (**Messages**): Kommunikationseinheiten in Chat-Modellen f√ºr Ein- und Ausgabe.\n","+ Prompt-Vorlagen (**Prompt template**s): Komponenten zur Erstellung strukturierter Prompts f√ºr LLMs.\n","+ Chat-Modelle (**Chat models**): LLMs, die √ºber eine Chat-API verf√ºgbar sind und Sequenzen von Nachrichten verarbeiten.\n","+ Chains (**Chains**): Verkn√ºpfungen mehrerer LLMs oder anderer Komponenten f√ºr komplexere Anwendungen.\n","+ Strukturierte Ausgabe (**Structured output**): Technik, um Chat-Modelle in strukturierten Formaten antworten zu lassen.\n","+ Chat-Verlauf (**Chat history**): Eine Sequenz von Nachrichten, die eine Konversation darstellt.\n","+ Retrieval Augmented Generation (**RAG**): Technik zur Verbesserung von Sprachmodellen durch Kombination mit externen Wissensbasen.\n","+ Retriever (**Retriever**): Komponenten, die relevante Dokumente aus einer Wissensbasis abrufen.\n","+ Agenten (**Agents**): Nutzen Sprachmodelle, um Aktionssequenzen auszuw√§hlen und mit externen Ressourcen zu interagieren.\n","+ Tools (**Tools**): Funktionen mit definierten Schemata f√ºr Name, Beschreibung und Argumente.\n"],"metadata":{"id":"cCuow5DOEsID"},"id":"cCuow5DOEsID"},{"cell_type":"markdown","id":"EouwC_wdZb-c","metadata":{"id":"EouwC_wdZb-c"},"source":["# 3 | Nachrichten-Typen\n","---"]},{"cell_type":"markdown","id":"yS2mM2J1aXlY","metadata":{"id":"yS2mM2J1aXlY"},"source":["Beim Einsatz von Large Language Models (LLMs) wie GPT gibt es drei Hauptarten von Nachrichten (Messages), die die Interaktion mit dem Modell steuern. Diese Messages definieren, wie das Modell reagiert, welche Eingaben es erh√§lt und welche Ausgaben es generiert.\n","\n"]},{"cell_type":"markdown","id":"sGqRXptbbQ2a","metadata":{"id":"sGqRXptbbQ2a"},"source":["**System Message:**   \n","Die `System Message` dient dazu, das **Verhalten des LLMs** zu steuern. Sie ist eine nicht sichtbare Nachricht, die dem Modell Anweisungen gibt, wie es sich verhalten soll. Dabei kann sie die Pers√∂nlichkeit, den Tonfall und die Einschr√§nkungen des Modells beeinflussen. Die Systemnachricht funktioniert wie eine Art \"Rollenspiel-Anweisung\", um das Modell in eine gew√ºnschte Rolle zu versetzen.\n","\n","*Merkmale:*\n","- Definiert, wie das Modell reagieren soll\n","- Legt Pers√∂nlichkeit, Verhalten und Einschr√§nkungen fest\n","- Wird zu Beginn einer Sitzung gesetzt und bleibt bestehen\n","- Nicht sichtbar f√ºr den Benutzer\n","\n","*Beispiel einer System Message:*\n","```json\n","{\n","  \"role\": \"system\",\n","  \"content\": \"Du bist ein KI-Assistent, der pr√§zise und informative Antworten in einem professionellen Ton liefert.\"\n","}\n","```\n","Dieses Beispiel weist das Modell an, in einem professionellen Tonfall kurze und informative Antworten zu geben.\n","\n","---\n","\n"]},{"cell_type":"markdown","id":"upsIzpcqbbP7","metadata":{"id":"upsIzpcqbbP7"},"source":["**User Message**   \n","Die `User Message` ist die eigentliche **Eingabe des Benutzer**s. Sie kann eine Frage, eine Aufforderung, kontextrelevanten Inhalt oder ein Befehl sein. Das Modell nutzt diese Nachricht als Ausgangspunkt f√ºr die Generierung einer Antwort.\n","\n","*Merkmale:*\n","- Direkte Eingabe des Benutzers\n","- Kann eine Frage, Aufforderung, kontextrelevanten Inhalt oder einen Befehl enthalten\n","- Basis f√ºr die Antwort des Modells\n","\n","*Beispiel einer User Message:*\n","```json\n","{\n","  \"role\": \"user\",\n","  \"content\": \"Was sind die wichtigsten Unterschiede zwischen KI und maschinellem Lernen?\"\n","}\n","```\n","Hier fragt der Benutzer nach den Unterschieden zwischen KI und maschinellem Lernen.\n","\n","---\n"]},{"cell_type":"markdown","id":"CozrrtYXbhZb","metadata":{"id":"CozrrtYXbhZb"},"source":["**AI Message**   \n","Die `AI Message` ist die **Antwort, die das Modell generiert**. Sie basiert auf der User Message und den Anweisungen aus der System Message. Die AI Message kann verschiedene Eigenschaften haben, ist aber haupts√§chlich auf den Inhalt fokussiert.\n","\n","*Merkmale:*\n","- Antwort des Modells auf die Benutzeranfrage\n","- Kann verschiedene Eigenschaften haben (z. B. L√§nge, Stil)\n","- Haupts√§chlich inhaltlich relevant\n","\n","*Beispiel einer AI Message:*\n","```json\n","{\n","  \"role\": \"assistant\",\n","  \"content\": \"K√ºnstliche Intelligenz (KI) ist ein weites Feld, das verschiedene Teilgebiete umfasst, darunter auch das maschinelle Lernen (ML). ML konzentriert sich auf die Erstellung von Modellen, die aus Daten lernen und Vorhersagen treffen.\"\n","}\n","```\n","Hier gibt das Modell eine inhaltliche Antwort auf die Frage des Benutzers.\n","\n","\n"]},{"cell_type":"markdown","source":["**System-Message vs. User-Message**\n","\n","\n","\n","*System-Message*\n","\n","**Funktion:** Persistente Verhaltensregeln √ºber verschiedene Aufgaben hinweg\n","\n","**Enth√§lt:**\n","- Identit√§t & Rolle des Modells\n","- Grunds√§tzliche Methodik & Denkansatz\n","- Durchg√§ngige Qualit√§tsstandards\n","- Tool-/Methodenkompetenz\n","- Generische Few-Shot-Examples\n","\n","**Metapher:** Das Betriebssystem ‚Äì selten ver√§ndert, verhaltensbestimmend\n","\n","\n"],"metadata":{"id":"a8QAtdYdg0Im"},"id":"a8QAtdYdg0Im"},{"cell_type":"markdown","source":["\n","*User-Message*\n","\n","**Funktion:** Konkrete Aufgabe und spezifischer Kontext f√ºr diese Interaktion\n","\n","**Enth√§lt:**\n","- Situativer Kontext & Hintergrund\n","- Klare Aufgabenstellung\n","- Erwartete Format/Umfang/Detailgrad\n","- Spezifische Daten & Randbedingungen\n","- Aufgabenspezifische Verhaltensanpassungen\n","\n","**Metapher:** Die Arbeitsanweisung ‚Äì variiert bei jeder Anfrage\n","\n"],"metadata":{"id":"cy765AHZhaWV"},"id":"cy765AHZhaWV"},{"cell_type":"markdown","id":"PiHnhKHVbofP","metadata":{"id":"PiHnhKHVbofP"},"source":["**Kurz gefasst**   \n","\n","+  **System Message** legt die Regeln und das Verhalten des Modells fest.\n","+ **User Message** stellt eine Anfrage oder einen Befehl.\n","+ **AI Message** gibt die generierte Antwort basierend auf den vorherigen Nachrichten.\n","\n","\n"]},{"cell_type":"code","id":"uwwloofx7h","source":["#@markdown   <p><font size=\"4\" color='green'>  üßú‚Äç‚ôÄÔ∏è Mermaid - Message-Typen Workflow</font> </br></p>\n","\n","diagram = \"\"\"\n","graph TD\n","    A[\"System Message\"] --> D[\"LLM Verarbeitung\"]\n","    B[\"User Message\"] --> D\n","    D --> C[\"AI Message\"]\n","\n","    A1[\"Definiert Verhalten<br/>Rolle & Pers√∂nlichkeit<br/>Einschr√§nkungen\"] -.-> A\n","    B1[\"Benutzer-Eingabe<br/>Frage oder Befehl<br/>Kontext\"] -.-> B\n","    C1[\"Generierte Antwort<br/>Basierend auf System + User<br/>Strukturiertes Format\"] -.-> C\n","\n","    style A fill:#ffe1e1\n","    style B fill:#e1f5ff\n","    style C fill:#e1ffe1\n","    style D fill:#fff4e1\n","\"\"\"\n","mermaid(diagram, width=700, height=450)"],"metadata":{"id":"uwwloofx7h","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"status":"ok","timestamp":1764781312358,"user_tz":-60,"elapsed":1208,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"feb242bd-e858-453a-9db4-8f009c20b60c"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.SVG object>"],"image/svg+xml":"<svg xmlns=\"http://www.w3.org/2000/svg\" aria-roledescription=\"flowchart-v2\" role=\"graphics-document document\" viewBox=\"0 0 655.7734375 502\" class=\"flowchart\" id=\"container\" width=\"700\" height=\"450\"><style>#container{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:16px;fill:#333;}@keyframes edge-animation-frame{from{stroke-dashoffset:0;}}@keyframes dash{to{stroke-dashoffset:0;}}#container .edge-animation-slow{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 50s linear infinite;stroke-linecap:round;}#container .edge-animation-fast{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 20s linear infinite;stroke-linecap:round;}#container .error-icon{fill:#552222;}#container .error-text{fill:#552222;stroke:#552222;}#container .edge-thickness-normal{stroke-width:1px;}#container .edge-thickness-thick{stroke-width:3.5px;}#container .edge-pattern-solid{stroke-dasharray:0;}#container .edge-thickness-invisible{stroke-width:0;fill:none;}#container .edge-pattern-dashed{stroke-dasharray:3;}#container .edge-pattern-dotted{stroke-dasharray:2;}#container .marker{fill:#333333;stroke:#333333;}#container .marker.cross{stroke:#333333;}#container svg{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:16px;}#container p{margin:0;}#container .label{font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;color:#333;}#container .cluster-label text{fill:#333;}#container .cluster-label span{color:#333;}#container .cluster-label span p{background-color:transparent;}#container .label text,#container span{fill:#333;color:#333;}#container .node rect,#container .node circle,#container .node ellipse,#container .node polygon,#container .node path{fill:#ECECFF;stroke:#9370DB;stroke-width:1px;}#container .rough-node .label text,#container .node .label text,#container .image-shape .label,#container .icon-shape .label{text-anchor:middle;}#container .node .katex path{fill:#000;stroke:#000;stroke-width:1px;}#container .rough-node .label,#container .node .label,#container .image-shape .label,#container .icon-shape .label{text-align:center;}#container .node.clickable{cursor:pointer;}#container .root .anchor path{fill:#333333!important;stroke-width:0;stroke:#333333;}#container .arrowheadPath{fill:#333333;}#container .edgePath .path{stroke:#333333;stroke-width:2.0px;}#container .flowchart-link{stroke:#333333;fill:none;}#container .edgeLabel{background-color:rgba(232,232,232, 0.8);text-align:center;}#container .edgeLabel p{background-color:rgba(232,232,232, 0.8);}#container .edgeLabel rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#container .labelBkg{background-color:rgba(232, 232, 232, 0.5);}#container .cluster rect{fill:#ffffde;stroke:#aaaa33;stroke-width:1px;}#container .cluster text{fill:#333;}#container .cluster span{color:#333;}#container div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;font-size:12px;background:hsl(80, 100%, 96.2745098039%);border:1px solid #aaaa33;border-radius:2px;pointer-events:none;z-index:100;}#container .flowchartTitleText{text-anchor:middle;font-size:18px;fill:#333;}#container rect.text{fill:none;stroke-width:0;}#container .icon-shape,#container .image-shape{background-color:rgba(232,232,232, 0.8);text-align:center;}#container .icon-shape p,#container .image-shape p{background-color:rgba(232,232,232, 0.8);padding:2px;}#container .icon-shape rect,#container .image-shape rect{opacity:0.5;background-color:rgba(232,232,232, 0.8);fill:rgba(232,232,232, 0.8);}#container :root{--mermaid-font-family:&quot;trebuchet ms&quot;,verdana,arial,sans-serif;}</style><g><marker orient=\"auto\" markerHeight=\"8\" markerWidth=\"8\" markerUnits=\"userSpaceOnUse\" refY=\"5\" refX=\"5\" viewBox=\"0 0 10 10\" class=\"marker flowchart-v2\" id=\"container_flowchart-v2-pointEnd\"><path style=\"stroke-width: 1; stroke-dasharray: 1, 0;\" class=\"arrowMarkerPath\" d=\"M 0 0 L 10 5 L 0 10 z\"/></marker><marker orient=\"auto\" markerHeight=\"8\" markerWidth=\"8\" markerUnits=\"userSpaceOnUse\" refY=\"5\" refX=\"4.5\" viewBox=\"0 0 10 10\" class=\"marker flowchart-v2\" id=\"container_flowchart-v2-pointStart\"><path style=\"stroke-width: 1; stroke-dasharray: 1, 0;\" class=\"arrowMarkerPath\" d=\"M 0 5 L 10 10 L 10 0 z\"/></marker><marker orient=\"auto\" markerHeight=\"11\" markerWidth=\"11\" markerUnits=\"userSpaceOnUse\" refY=\"5\" refX=\"11\" viewBox=\"0 0 10 10\" class=\"marker flowchart-v2\" id=\"container_flowchart-v2-circleEnd\"><circle style=\"stroke-width: 1; stroke-dasharray: 1, 0;\" class=\"arrowMarkerPath\" r=\"5\" cy=\"5\" cx=\"5\"/></marker><marker orient=\"auto\" markerHeight=\"11\" markerWidth=\"11\" markerUnits=\"userSpaceOnUse\" refY=\"5\" refX=\"-1\" viewBox=\"0 0 10 10\" class=\"marker flowchart-v2\" id=\"container_flowchart-v2-circleStart\"><circle style=\"stroke-width: 1; stroke-dasharray: 1, 0;\" class=\"arrowMarkerPath\" r=\"5\" cy=\"5\" cx=\"5\"/></marker><marker orient=\"auto\" markerHeight=\"11\" markerWidth=\"11\" markerUnits=\"userSpaceOnUse\" refY=\"5.2\" refX=\"12\" viewBox=\"0 0 11 11\" class=\"marker cross flowchart-v2\" id=\"container_flowchart-v2-crossEnd\"><path style=\"stroke-width: 2; stroke-dasharray: 1, 0;\" class=\"arrowMarkerPath\" d=\"M 1,1 l 9,9 M 10,1 l -9,9\"/></marker><marker orient=\"auto\" markerHeight=\"11\" markerWidth=\"11\" markerUnits=\"userSpaceOnUse\" refY=\"5.2\" refX=\"-1\" viewBox=\"0 0 11 11\" class=\"marker cross flowchart-v2\" id=\"container_flowchart-v2-crossStart\"><path style=\"stroke-width: 2; stroke-dasharray: 1, 0;\" class=\"arrowMarkerPath\" d=\"M 1,1 l 9,9 M 10,1 l -9,9\"/></marker><g class=\"root\"><g class=\"clusters\"/><g class=\"edgePaths\"><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_A_D_0\" d=\"M117.945,214L117.945,218.167C117.945,222.333,117.945,230.667,132.067,244.62C146.189,258.574,174.432,278.148,188.554,287.935L202.676,297.722\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_B_D_0\" d=\"M371.898,214L371.898,218.167C371.898,222.333,371.898,230.667,357.777,244.62C343.655,258.574,315.412,278.148,301.29,287.935L287.168,297.722\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-solid edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_D_C_0\" d=\"M244.922,354L244.922,364.167C244.922,374.333,244.922,394.667,255.432,408.84C265.943,423.012,286.964,431.025,297.474,435.031L307.985,439.037\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-dotted edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_A1_A_0\" d=\"M117.945,110L117.945,114.167C117.945,118.333,117.945,126.667,117.945,134.333C117.945,142,117.945,149,117.945,152.5L117.945,156\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-dotted edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_B1_B_0\" d=\"M371.898,110L371.898,114.167C371.898,118.333,371.898,126.667,371.898,134.333C371.898,142,371.898,149,371.898,152.5L371.898,156\"/><path marker-end=\"url(#container_flowchart-v2-pointEnd)\" style=\"\" class=\"edge-thickness-normal edge-pattern-dotted edge-thickness-normal edge-pattern-solid flowchart-link\" id=\"L_C1_C_0\" d=\"M517.773,390L517.773,394.167C517.773,398.333,517.773,406.667,507.263,414.84C496.752,423.012,475.731,431.025,465.221,435.031L454.71,439.037\"/></g><g class=\"edgeLabels\"><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g><g class=\"edgeLabel\"><g transform=\"translate(0, 0)\" class=\"label\"><foreignObject height=\"0\" width=\"0\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\" class=\"labelBkg\"><span class=\"edgeLabel\"/></div></foreignObject></g></g></g><g class=\"nodes\"><g transform=\"translate(117.9453125, 187)\" id=\"flowchart-A-0\" class=\"node default\"><rect height=\"54\" width=\"177.125\" y=\"-27\" x=\"-88.5625\" style=\"fill:#ffe1e1 !important\" class=\"basic label-container\"/><g transform=\"translate(-58.5625, -12)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"24\" width=\"117.125\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>System Message</p></span></div></foreignObject></g></g><g transform=\"translate(244.921875, 327)\" id=\"flowchart-D-1\" class=\"node default\"><rect height=\"54\" width=\"185.703125\" y=\"-27\" x=\"-92.8515625\" style=\"fill:#fff4e1 !important\" class=\"basic label-container\"/><g transform=\"translate(-62.8515625, -12)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"24\" width=\"125.703125\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>LLM Verarbeitung</p></span></div></foreignObject></g></g><g transform=\"translate(371.8984375, 187)\" id=\"flowchart-B-2\" class=\"node default\"><rect height=\"54\" width=\"158.25\" y=\"-27\" x=\"-79.125\" style=\"fill:#e1f5ff !important\" class=\"basic label-container\"/><g transform=\"translate(-49.125, -12)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"24\" width=\"98.25\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>User Message</p></span></div></foreignObject></g></g><g transform=\"translate(381.34765625, 467)\" id=\"flowchart-C-5\" class=\"node default\"><rect height=\"54\" width=\"139.25\" y=\"-27\" x=\"-69.625\" style=\"fill:#e1ffe1 !important\" class=\"basic label-container\"/><g transform=\"translate(-39.625, -12)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"24\" width=\"79.25\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>AI Message</p></span></div></foreignObject></g></g><g transform=\"translate(117.9453125, 59)\" id=\"flowchart-A1-6\" class=\"node default\"><rect height=\"102\" width=\"219.890625\" y=\"-51\" x=\"-109.9453125\" style=\"\" class=\"basic label-container\"/><g transform=\"translate(-79.9453125, -36)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"72\" width=\"159.890625\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Definiert Verhalten<br/>Rolle &amp; Pers√∂nlichkeit<br/>Einschr√§nkungen</p></span></div></foreignObject></g></g><g transform=\"translate(371.8984375, 59)\" id=\"flowchart-B1-8\" class=\"node default\"><rect height=\"102\" width=\"188.015625\" y=\"-51\" x=\"-94.0078125\" style=\"\" class=\"basic label-container\"/><g transform=\"translate(-64.0078125, -36)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"72\" width=\"128.015625\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table-cell; white-space: nowrap; line-height: 1.5; max-width: 200px; text-align: center;\"><span class=\"nodeLabel\"><p>Benutzer-Eingabe<br/>Frage oder Befehl<br/>Kontext</p></span></div></foreignObject></g></g><g transform=\"translate(517.7734375, 327)\" id=\"flowchart-C1-10\" class=\"node default\"><rect height=\"126\" width=\"260\" y=\"-63\" x=\"-130\" style=\"\" class=\"basic label-container\"/><g transform=\"translate(-100, -48)\" style=\"\" class=\"label\"><rect/><foreignObject height=\"96\" width=\"200\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: table; white-space: break-spaces; line-height: 1.5; max-width: 200px; text-align: center; width: 200px;\"><span class=\"nodeLabel\"><p>Generierte Antwort<br/>Basierend auf System + User<br/>Strukturiertes Format</p></span></div></foreignObject></g></g></g></g></g></svg>"},"metadata":{}}]},{"cell_type":"markdown","source":["# 4 | Ein einfaches Modell\n","---\n"],"metadata":{"id":"BMPwDNTCCOfx"},"id":"BMPwDNTCCOfx"},{"cell_type":"code","execution_count":3,"metadata":{"id":"e1926d03","executionInfo":{"status":"ok","timestamp":1764781312417,"user_tz":-60,"elapsed":42,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Importe\n","from langchain.chat_models import init_chat_model\n","from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n","from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n","from langchain_core.prompts.few_shot import FewShotPromptTemplate\n","from langchain_core.output_parsers.string import StrOutputParser"],"id":"e1926d03"},{"cell_type":"code","source":["# Modell definieren (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0.0)"],"metadata":{"id":"RG-y_bG8HOOi","executionInfo":{"status":"ok","timestamp":1764781316766,"user_tz":-60,"elapsed":4330,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":4,"outputs":[],"id":"RG-y_bG8HOOi"},{"cell_type":"markdown","id":"-JwJD0rLIyT8","metadata":{"id":"-JwJD0rLIyT8"},"source":["# 5 | Prompts\n","---"]},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/langchain_prozess_02.png)"],"metadata":{"id":"j9stNgKo93Eg"},"id":"j9stNgKo93Eg"},{"cell_type":"markdown","source":["\n","\n","Prompts sind die Eingaben bzw. Auftr√§ge an das LLM. LangChain bietet verschiedene Template-Systeme. Ein **Template** ist eine **Vorlage** ‚Äì genauer gesagt ein Text mit **Platzhaltern**, die sp√§ter durch echte Werte ersetzt werden.\n","\n","\n","Ein **PromptTemplate** ist eine Vorlage f√ºr einen Eingabetext, der an ein Sprachmodell geschickt wird.\n","Er enth√§lt Platzhalter `{placeholder}`, die sp√§ter mit Nutzereingaben oder Kontext bef√ºllt werden.\n","\n","**Templates machen die KI-Prompts:**\n","\n","+ Wiederverwendbar (z.‚ÄØB. f√ºr viele Themen oder Fragen)\n","\n","+ Strukturiert (z.‚ÄØB. mit Rollen und Anweisungen)\n","\n","+ Dynamisch erweiterbar (mit Memory, Tools, etc.)"],"metadata":{"id":"QMVO4fzPFZ88"},"id":"QMVO4fzPFZ88"},{"cell_type":"markdown","id":"HqTUeMW20084","metadata":{"id":"HqTUeMW20084"},"source":["## 5.1 | Simple Prompt\n"]},{"cell_type":"markdown","source":["<p><font color='darkblue' size=\"4\">\n","<b>‚ùóHinweis:</b>\n","</font></p>\n","Ohne Memory-Management hat das Modell ein kurzes Ged√§chnis.\n"],"metadata":{"id":"5JF1_AvYNYY_"},"id":"5JF1_AvYNYY_"},{"cell_type":"markdown","source":["**Prompt wird als Zeichenkette √ºbergeben:**"],"metadata":{"id":"1azC75R5JpWC"},"id":"1azC75R5JpWC"},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(\"Was ist Generative KI?\")"],"metadata":{"id":"jPizca8FJV5b","executionInfo":{"status":"ok","timestamp":1764781325550,"user_tz":-60,"elapsed":8753,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"jPizca8FJV5b","execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Antwort\n","type(response)"],"metadata":{"id":"EHjmjLgW3ENA","colab":{"base_uri":"https://localhost:8080/","height":203},"executionInfo":{"status":"ok","timestamp":1764781325594,"user_tz":-60,"elapsed":30,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"53a45cfd-23f9-4bb3-a281-3681237375a4"},"id":"EHjmjLgW3ENA","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["langchain_core.messages.ai.AIMessage"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.messages.ai.AIMessage</b><br/>def __init__(content: str | list[str | dict] | None=None, content_blocks: list[types.ContentBlock] | None=None, **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/langchain_core/messages/ai.py</a>Message from an AI.\n","\n","An `AIMessage` is returned from a chat model as a response to a prompt.\n","\n","This message represents the output of the model and consists of both\n","the raw output as returned by the model and standardized fields\n","(e.g., tool calls, usage metadata) added by the LangChain framework.</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 153);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["response"],"metadata":{"id":"4ZYngACo4RWN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764781325716,"user_tz":-60,"elapsed":109,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"4390ef34-0c98-4a25-ca8a-10e4007903c8"},"id":"4ZYngACo4RWN","execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='Generative KI, oder generative k√ºnstliche Intelligenz, bezieht sich auf eine Klasse von Algorithmen und Modellen, die in der Lage sind, neue Inhalte zu erzeugen. Diese Inhalte k√∂nnen in verschiedenen Formen vorliegen, darunter Texte, Bilder, Musik, Videos und mehr. Generative KI nutzt oft Techniken des maschinellen Lernens, insbesondere neuronale Netzwerke, um Muster in bestehenden Daten zu erkennen und darauf basierend neue, √§hnliche Daten zu generieren.\\n\\nEin bekanntes Beispiel f√ºr generative KI sind Sprachmodelle wie GPT (Generative Pre-trained Transformer), die in der Lage sind, menschen√§hnliche Texte zu schreiben, Fragen zu beantworten oder Geschichten zu erz√§hlen. In der Bildgenerierung gibt es Modelle wie DALL-E oder Midjourney, die aus Textbeschreibungen Bilder erstellen k√∂nnen.\\n\\nGenerative KI hat viele Anwendungen, darunter:\\n\\n1. **Kreative Inhalte**: Erstellung von Kunst, Musik, Literatur und Design.\\n2. **Datenaugmentation**: Generierung zus√§tzlicher Trainingsdaten f√ºr maschinelles Lernen.\\n3. **Simulationen**: Erzeugung realistischer Szenarien f√ºr Training und Forschung.\\n4. **Personalisierung**: Anpassung von Inhalten an individuelle Nutzerpr√§ferenzen.\\n\\nDie Technologie wirft jedoch auch ethische und rechtliche Fragen auf, insbesondere in Bezug auf Urheberrecht, F√§lschungen und die potenzielle Verbreitung von Fehlinformationen.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 297, 'prompt_tokens': 13, 'total_tokens': 310, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_50906f2aac', 'id': 'chatcmpl-CikWjHuyyaqPAPPYGo0xjMmRkABnV', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--38df1291-71f4-40d5-8e14-94c058698b4c-0', usage_metadata={'input_tokens': 13, 'output_tokens': 297, 'total_tokens': 310, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["for r in response:\n","    print(r)"],"metadata":{"id":"jveXqUjbpo0s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764781325791,"user_tz":-60,"elapsed":67,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"7da2e2ff-63cb-422e-d51b-1016d9c776e5"},"id":"jveXqUjbpo0s","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["('content', 'Generative KI, oder generative k√ºnstliche Intelligenz, bezieht sich auf eine Klasse von Algorithmen und Modellen, die in der Lage sind, neue Inhalte zu erzeugen. Diese Inhalte k√∂nnen in verschiedenen Formen vorliegen, darunter Texte, Bilder, Musik, Videos und mehr. Generative KI nutzt oft Techniken des maschinellen Lernens, insbesondere neuronale Netzwerke, um Muster in bestehenden Daten zu erkennen und darauf basierend neue, √§hnliche Daten zu generieren.\\n\\nEin bekanntes Beispiel f√ºr generative KI sind Sprachmodelle wie GPT (Generative Pre-trained Transformer), die in der Lage sind, menschen√§hnliche Texte zu schreiben, Fragen zu beantworten oder Geschichten zu erz√§hlen. In der Bildgenerierung gibt es Modelle wie DALL-E oder Midjourney, die aus Textbeschreibungen Bilder erstellen k√∂nnen.\\n\\nGenerative KI hat viele Anwendungen, darunter:\\n\\n1. **Kreative Inhalte**: Erstellung von Kunst, Musik, Literatur und Design.\\n2. **Datenaugmentation**: Generierung zus√§tzlicher Trainingsdaten f√ºr maschinelles Lernen.\\n3. **Simulationen**: Erzeugung realistischer Szenarien f√ºr Training und Forschung.\\n4. **Personalisierung**: Anpassung von Inhalten an individuelle Nutzerpr√§ferenzen.\\n\\nDie Technologie wirft jedoch auch ethische und rechtliche Fragen auf, insbesondere in Bezug auf Urheberrecht, F√§lschungen und die potenzielle Verbreitung von Fehlinformationen.')\n","('additional_kwargs', {'refusal': None})\n","('response_metadata', {'token_usage': {'completion_tokens': 297, 'prompt_tokens': 13, 'total_tokens': 310, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_50906f2aac', 'id': 'chatcmpl-CikWjHuyyaqPAPPYGo0xjMmRkABnV', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None})\n","('type', 'ai')\n","('name', None)\n","('id', 'lc_run--38df1291-71f4-40d5-8e14-94c058698b4c-0')\n","('tool_calls', [])\n","('invalid_tool_calls', [])\n","('usage_metadata', {'input_tokens': 13, 'output_tokens': 297, 'total_tokens': 310, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n"]}]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"cOJGeiLGJb-g","colab":{"base_uri":"https://localhost:8080/","height":365},"executionInfo":{"status":"ok","timestamp":1764781325882,"user_tz":-60,"elapsed":83,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"43f4f344-adc7-40ac-9329-51890dcc657c"},"id":"cOJGeiLGJb-g","execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Generative KI, oder generative k√ºnstliche Intelligenz, bezieht sich auf eine Klasse von Algorithmen und Modellen, die in der Lage sind, neue Inhalte zu erzeugen. Diese Inhalte k√∂nnen in verschiedenen Formen vorliegen, darunter Texte, Bilder, Musik, Videos und mehr. Generative KI nutzt oft Techniken des maschinellen Lernens, insbesondere neuronale Netzwerke, um Muster in bestehenden Daten zu erkennen und darauf basierend neue, √§hnliche Daten zu generieren.\n\nEin bekanntes Beispiel f√ºr generative KI sind Sprachmodelle wie GPT (Generative Pre-trained Transformer), die in der Lage sind, menschen√§hnliche Texte zu schreiben, Fragen zu beantworten oder Geschichten zu erz√§hlen. In der Bildgenerierung gibt es Modelle wie DALL-E oder Midjourney, die aus Textbeschreibungen Bilder erstellen k√∂nnen.\n\nGenerative KI hat viele Anwendungen, darunter:\n\n1. **Kreative Inhalte**: Erstellung von Kunst, Musik, Literatur und Design.\n2. **Datenaugmentation**: Generierung zus√§tzlicher Trainingsdaten f√ºr maschinelles Lernen.\n3. **Simulationen**: Erzeugung realistischer Szenarien f√ºr Training und Forschung.\n4. **Personalisierung**: Anpassung von Inhalten an individuelle Nutzerpr√§ferenzen.\n\nDie Technologie wirft jedoch auch ethische und rechtliche Fragen auf, insbesondere in Bezug auf Urheberrecht, F√§lschungen und die potenzielle Verbreitung von Fehlinformationen."},"metadata":{}}]},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(\"Was war meine letzte Frage?\")"],"metadata":{"id":"9fayTzIrNN2b","executionInfo":{"status":"ok","timestamp":1764781327235,"user_tz":-60,"elapsed":1346,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"9fayTzIrNN2b","execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"IahQJ0mFNUUV","colab":{"base_uri":"https://localhost:8080/","height":117},"executionInfo":{"status":"ok","timestamp":1764781327279,"user_tz":-60,"elapsed":26,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"20241b6f-7965-4d80-97ef-0149459086f3"},"id":"IahQJ0mFNUUV","execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Es tut mir leid, aber ich kann keine vorherigen Interaktionen oder Fragen speichern oder abrufen. Wenn du mir deine Frage erneut stellst, helfe ich dir gerne weiter!"},"metadata":{}}]},{"cell_type":"markdown","source":["**Prompt wird als formatierte String-Variable √ºbergeben:**"],"metadata":{"id":"aKRhzB0oJnUG"},"id":"aKRhzB0oJnUG"},{"cell_type":"code","source":["# f-String f√ºr simple prompt mit template\n","thema = \"Machine Learning\"\n","prompt = f\"Erkl√§re {thema} in einfachen Worten.\""],"metadata":{"id":"8s5cQY2oHUvW","executionInfo":{"status":"ok","timestamp":1764781327305,"user_tz":-60,"elapsed":16,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"8s5cQY2oHUvW","execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(prompt)"],"metadata":{"id":"plYGfaoLJ8DI","executionInfo":{"status":"ok","timestamp":1764781331679,"user_tz":-60,"elapsed":4358,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":13,"outputs":[],"id":"plYGfaoLJ8DI"},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"SLX0zaONJ8DJ","colab":{"base_uri":"https://localhost:8080/","height":239},"executionInfo":{"status":"ok","timestamp":1764781331714,"user_tz":-60,"elapsed":24,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"07e88c5c-7372-4ad8-fbf1-75225ce36a45"},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine Learning, oder maschinelles Lernen, ist ein Teilbereich der K√ºnstlichen Intelligenz, der es Computern erm√∂glicht, aus Daten zu lernen und Muster zu erkennen, ohne dass sie daf√ºr explizit programmiert werden m√ºssen. \n\nStell dir vor, du hast viele Bilder von Katzen und Hunden. Anstatt einem Computer genau zu sagen, wie eine Katze oder ein Hund aussieht, zeigst du ihm einfach viele Beispiele. Der Computer analysiert diese Bilder und lernt, die Unterschiede zwischen Katzen und Hunden zu erkennen. \n\nWenn du ihm dann ein neues Bild zeigst, kann er basierend auf dem, was er gelernt hat, vorhersagen, ob es sich um eine Katze oder einen Hund handelt. \n\nZusammengefasst: Machine Learning ist wie das Lernen eines Menschen ‚Äì je mehr Informationen und Erfahrungen er hat, desto besser kann er Entscheidungen treffen oder Vorhersagen machen."},"metadata":{}}],"id":"SLX0zaONJ8DJ"},{"cell_type":"markdown","source":["**Prompt wird als template √ºbergeben:**"],"metadata":{"id":"n-VVZEHDKSIw"},"id":"n-VVZEHDKSIw"},{"cell_type":"code","source":["# Einfaches Template mit Platzhaltern\n","simple_prompt_template = PromptTemplate.from_template(\n","    \"Erkl√§re {thema} in einfachen Worten.\"\n",")"],"metadata":{"id":"koh0-_3r_HxG","executionInfo":{"status":"ok","timestamp":1764781331729,"user_tz":-60,"elapsed":9,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"koh0-_3r_HxG","execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Prompt-Erstellung\n","thema = \"Generative KI\"\n","prompt = simple_prompt_template.format(thema=thema)\n","prompt"],"metadata":{"id":"nFrCjL6j_xo1","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1764781331762,"user_tz":-60,"elapsed":19,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"19fc55fd-df8b-4cc1-a70e-771c1472b512"},"id":"nFrCjL6j_xo1","execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Erkl√§re Generative KI in einfachen Worten.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(prompt)"],"metadata":{"id":"-of5FbM02qfZ","executionInfo":{"status":"ok","timestamp":1764781335287,"user_tz":-60,"elapsed":3518,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"-of5FbM02qfZ","execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"lmEE-3NDAE15","colab":{"base_uri":"https://localhost:8080/","height":198},"executionInfo":{"status":"ok","timestamp":1764781335314,"user_tz":-60,"elapsed":20,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"83104b26-a1f8-43ea-82f3-51e21ca7a2d9"},"id":"lmEE-3NDAE15","execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Generative KI ist eine Art von k√ºnstlicher Intelligenz, die in der Lage ist, neue Inhalte zu erstellen. Das k√∂nnen Texte, Bilder, Musik oder sogar Videos sein. Sie funktioniert, indem sie aus gro√üen Mengen von Daten lernt, wie bestimmte Inhalte aussehen oder klingen. \n\nStell dir vor, du hast viele Bilder von Katzen und Hunden. Eine generative KI kann dann lernen, wie diese Tiere aussehen, und neue Bilder von Katzen oder Hunden erstellen, die es so noch nicht gibt. \n\nIm Grunde genommen ist generative KI wie ein kreativer Computer, der neue Ideen und Inhalte basierend auf dem, was er gelernt hat, entwickeln kann."},"metadata":{}}]},{"cell_type":"markdown","id":"8akKmCCc06nm","metadata":{"id":"8akKmCCc06nm"},"source":["## 5.2 | ChatPromptTemplate üìå"]},{"cell_type":"markdown","source":["Das **ChatPromptTemplate** ist eine spezielle Prompt-Vorlage f√ºr Chatmodelle (z.‚ÄØB. GPT-3.5, GPT-4), die **mehrere Rollen und Nachrichten** unterst√ºtzt ‚Äì also genau das, was Chat-Modelle eigentlich brauchen.\n","\n","Es geh√∂rt zur LangChain-Bibliothek und baut auf dem Prinzip auf:\n","\n","\n","**Wer sagt was? ‚Üí system, user, assistant, etc.**"],"metadata":{"id":"SVoHwxtrMM_j"},"id":"SVoHwxtrMM_j"},{"cell_type":"markdown","source":["In LangChain unterst√ºtzte Rollen:\n","\n","+ \"system\" - f√ºr Systemnachrichten\n","+ \"assistant\" oder \"ai\" - f√ºr Assistenten-/KI-Antworten\n","+ \"user\" oder \"human\" - f√ºr Benutzernachrichten"],"metadata":{"id":"Po7W2FKz-VEd"},"id":"Po7W2FKz-VEd"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Variante 1: Liste & Tupel üìå\n","</font></p>"],"metadata":{"id":"yw4qr0mC5A6m"},"id":"yw4qr0mC5A6m"},{"cell_type":"code","execution_count":19,"id":"iW9URvON0IXr","metadata":{"id":"iW9URvON0IXr","executionInfo":{"status":"ok","timestamp":1764781335332,"user_tz":-60,"elapsed":12,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Template als Liste & Tupel\n","chat_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    (\"human\", \"Erkl√§re mir {thema}\"),\n","])"]},{"cell_type":"code","source":["# Variablen zuweisen\n","system_prompt = \"Du bist ein hilfreicher und humorvoller Assistent.\"\n","thema = \"Machine Learning\""],"metadata":{"id":"XrEDZ3Vz174e","executionInfo":{"status":"ok","timestamp":1764781335355,"user_tz":-60,"elapsed":10,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"XrEDZ3Vz174e","execution_count":20,"outputs":[]},{"cell_type":"code","source":["# Template & Variablen kombinieren\n","prompt = chat_template.format_messages(system_prompt=system_prompt, thema=thema)"],"metadata":{"id":"x3pxKD_R2BvX","executionInfo":{"status":"ok","timestamp":1764781335376,"user_tz":-60,"elapsed":13,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"x3pxKD_R2BvX","execution_count":21,"outputs":[]},{"cell_type":"code","source":["# Sprachmodell aufrufe\n","response = llm.invoke(prompt)"],"metadata":{"id":"CLU3o3dG4OGM","executionInfo":{"status":"ok","timestamp":1764781348177,"user_tz":-60,"elapsed":12794,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"CLU3o3dG4OGM","execution_count":22,"outputs":[]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"YnfES3Q24n0M","colab":{"base_uri":"https://localhost:8080/","height":482},"executionInfo":{"status":"ok","timestamp":1764781348207,"user_tz":-60,"elapsed":21,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"acc2c724-4491-4256-f4db-c9f02801429a"},"id":"YnfES3Q24n0M","execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Klar, ich erkl√§re dir gerne Machine Learning! \n\nMachine Learning (ML) ist ein Teilbereich der K√ºnstlichen Intelligenz (KI), der es Computern erm√∂glicht, aus Daten zu lernen und Muster zu erkennen, ohne explizit programmiert zu werden. Man k√∂nnte sagen, es ist wie das Lernen eines neuen Hobbys ‚Äì je mehr du √ºbst, desto besser wirst du darin!\n\nHier sind die grundlegenden Konzepte:\n\n1. **Daten**: Machine Learning ben√∂tigt Daten, um zu lernen. Das k√∂nnen Bilder, Texte, Zahlen oder sogar T√∂ne sein. Je mehr qualitativ hochwertige Daten du hast, desto besser kann das Modell lernen.\n\n2. **Modelle**: Ein ML-Modell ist wie ein Rezept. Es nimmt die Daten und ‚Äûlernt‚Äú daraus, um Vorhersagen oder Entscheidungen zu treffen. Es gibt verschiedene Arten von Modellen, je nach Aufgabe, wie z.B. Entscheidungsb√§ume, neuronale Netze oder Support Vector Machines.\n\n3. **Training**: Das Training ist der Prozess, bei dem das Modell aus den Daten lernt. Man zeigt dem Modell Beispiele und sagt ihm, was richtig oder falsch ist. Es ist ein bisschen so, als w√ºrde man einem Hund Tricks beibringen ‚Äì man zeigt ihm, was er tun soll, und belohnt ihn, wenn er es richtig macht.\n\n4. **Testen**: Nach dem Training wird das Modell mit neuen, unbekannten Daten getestet, um zu sehen, wie gut es gelernt hat. Hier zeigt sich, ob das Modell wirklich ‚Äûverstanden‚Äú hat oder ob es nur auswendig gelernt hat.\n\n5. **Anwendung**: Wenn das Modell gut funktioniert, kann es in der realen Welt eingesetzt werden, z.B. f√ºr Spracherkennung, Bildklassifikation oder sogar f√ºr Empfehlungen in Online-Shops.\n\nUnd das Beste daran? Machine Learning kann sich st√§ndig verbessern, je mehr Daten es erh√§lt. Es ist wie ein Sch√ºler, der nie aufh√∂rt zu lernen ‚Äì und das kann manchmal ganz sch√∂n chaotisch werden!\n\nWenn du noch spezifischere Fragen hast oder mehr √ºber einen bestimmten Aspekt wissen m√∂chtest, lass es mich wissen!"},"metadata":{}}]},{"cell_type":"markdown","source":["**... oder ...**"],"metadata":{"id":"6FrHMy1PYH0S"},"id":"6FrHMy1PYH0S"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Variante 2: Liste & Dictionary\n","</font></p>"],"metadata":{"id":"lkaCyUqq6OD0"},"id":"lkaCyUqq6OD0"},{"cell_type":"code","source":["# Template als Liste & Dictionary\n","chat_template = ChatPromptTemplate.from_messages([\n","    {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher und humorvoller Assistent.\"},\n","    {\"role\": \"human\", \"content\": \"Erkl√§re mir {thema}\"},\n","])"],"metadata":{"id":"kxEP8r3VDWwI","executionInfo":{"status":"ok","timestamp":1764781348232,"user_tz":-60,"elapsed":18,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"kxEP8r3VDWwI","execution_count":24,"outputs":[]},{"cell_type":"code","source":["# Variablen zuweisen\n","system_prompt = \"Du bist ein hilfreicher und humorvoller Assistent.\"\n","thema = \"Machine Learning\""],"metadata":{"id":"naATTZyS4zEB","executionInfo":{"status":"ok","timestamp":1764781348257,"user_tz":-60,"elapsed":12,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":25,"outputs":[],"id":"naATTZyS4zEB"},{"cell_type":"code","source":["# Template & Variablen kombinieren\n","prompt = chat_template.format_messages(system_prompt=system_prompt, thema=thema)"],"metadata":{"id":"O235HSRl4zEC","executionInfo":{"status":"ok","timestamp":1764781348279,"user_tz":-60,"elapsed":12,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":26,"outputs":[],"id":"O235HSRl4zEC"},{"cell_type":"code","source":["# Sprachmodell aufrufe\n","response = llm.invoke(prompt)"],"metadata":{"id":"Ow6KjCLG4zEC","executionInfo":{"status":"ok","timestamp":1764781361071,"user_tz":-60,"elapsed":12784,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":27,"outputs":[],"id":"Ow6KjCLG4zEC"},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"6PtZzYUSC7-F","colab":{"base_uri":"https://localhost:8080/","height":488},"executionInfo":{"status":"ok","timestamp":1764781361113,"user_tz":-60,"elapsed":29,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"18ae1912-a4a9-4382-816d-582afde9e876"},"id":"6PtZzYUSC7-F","execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Klar, ich erkl√§re dir gerne Machine Learning! \n\nStell dir vor, du hast einen Hund, der nicht wei√ü, wie man ‚ÄûSitz‚Äú macht. Du zeigst ihm immer wieder, was er tun soll, und wenn er es richtig macht, gibst du ihm ein Leckerli. Nach einer Weile lernt der Hund, dass ‚ÄûSitz‚Äú bedeutet, sich hinzusetzen, um das Leckerli zu bekommen. Das ist im Grunde genommen, wie Machine Learning funktioniert!\n\nMachine Learning (ML) ist ein Teilbereich der K√ºnstlichen Intelligenz (KI), bei dem Computer aus Daten lernen, ohne explizit programmiert zu werden. Anstatt einem Computer genau zu sagen, was er tun soll, geben wir ihm viele Beispiele (Daten), und er findet selbst Muster und Zusammenh√§nge.\n\nHier sind die grundlegenden Schritte:\n\n1. **Daten sammeln**: Du brauchst eine Menge Daten, die relevant f√ºr das Problem sind, das du l√∂sen m√∂chtest. Das k√∂nnen Bilder, Texte, Zahlen oder alles M√∂gliche sein.\n\n2. **Modell w√§hlen**: Du w√§hlst ein Modell aus, das die Daten analysieren soll. Das kann ein einfacher Algorithmus sein oder ein komplexes neuronales Netzwerk.\n\n3. **Training**: Das Modell wird mit den gesammelten Daten trainiert. Es lernt, indem es die Daten analysiert und versucht, Vorhersagen zu treffen. Wenn es falsch liegt, wird es angepasst, um beim n√§chsten Mal besser zu sein.\n\n4. **Testen**: Nach dem Training testest du das Modell mit neuen, unbekannten Daten, um zu sehen, wie gut es funktioniert.\n\n5. **Anpassen und Optimieren**: Je nach den Ergebnissen kannst du das Modell weiter anpassen, um die Genauigkeit zu verbessern.\n\nMachine Learning wird in vielen Bereichen eingesetzt, von der Spracherkennung √ºber Bildverarbeitung bis hin zu Empfehlungen in Online-Shops. Es ist wie ein sehr kluger Hund, der immer besser wird, je mehr er lernt!\n\nWenn du noch mehr Details oder spezifische Beispiele m√∂chtest, sag einfach Bescheid! üê∂üíª"},"metadata":{}}]},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Vergleich: f-string vs. PromptTemplate vs. ChatPromptTemplate\n","</font></p>"],"metadata":{"id":"2uSD48umMXI7"},"id":"2uSD48umMXI7"},{"cell_type":"markdown","source":["\n","\n","| Vorteil                      | `f-string` (Python)           | `PromptTemplate`              | `ChatPromptTemplate`            |\n","| ---------------------------- | ----------------------------- | ----------------------------- | ------------------------------- |\n","| üöª Rollenstruktur            | ‚ùå Keine Unterst√ºtzung         | ‚ùå Nur einfacher Text          | ‚úÖ Klar: `system`, `user`, `ai`  |\n","| üìö Konversationsaufbau       | ‚ùå Nicht geeignet              | ‚ùå Nicht geeignet              | ‚úÖ Ideal f√ºr Multi-Turn-Dialoge <br>    (mehreren Gespr√§chsrunden)  |\n","| üîÑ Kombinierbar mit `Memory` | ‚ùå Nein                        | ‚ùå Nein                        | ‚úÖ Ja, via `MessagesPlaceholder` |\n","| üîå F√ºr Chatmodelle wie GPT   | ‚ö†Ô∏è Manuell als String         | ‚ö†Ô∏è Nur als Flie√ütext          | ‚úÖ Nativ & strukturiert          |\n","| üîç Klarheit & Wartbarkeit    | ‚ö†Ô∏è Inline, schwer erweiterbar | ‚ö†Ô∏è Nur Inline-Text            | ‚úÖ Saubere Message-Trennung      |\n","| üß™ Prompt-Testbarkeit        | ‚ùå Keine Validierung           | ‚ö†Ô∏è Eingeschr√§nkt               | ‚úÖ Besser strukturiert           |\n","| ‚ö° Performance                | ‚úÖ Schnell & direkt            | ‚úÖ Schnell                     | ‚úÖ Schnell                       |\n","| üîß Validierung von Variablen | ‚ùå Keine                       | ‚úÖ Automatisch                 | ‚úÖ Automatisch                   |\n","| üß© Wiederverwendbarkeit      | ‚ùå Eingeschr√§nkt               | ‚úÖ Als Template-Objekt         | ‚úÖ Als Template-Objekt           |\n","| üìù Partial Variables         | ‚ùå Nein                        | ‚úÖ Ja                          | ‚úÖ Ja                            |\n","| üîÄ Chains & Pipelines        | ‚ùå Nicht integrierbar          | ‚úÖ LangChain-kompatibel        | ‚úÖ LangChain-kompatibel          |\n","| üéØ Output Parser             | ‚ùå Manuell                     | ‚úÖ Integrierbar                | ‚úÖ Integrierbar                  |"],"metadata":{"id":"UvoCBqi0qc4B"},"id":"UvoCBqi0qc4B"},{"cell_type":"markdown","source":["\n","\n","**Anwendungsempfehlungen**\n","\n","| Methode               | Wann verwenden?                                                                 |\n","| --------------------- | ------------------------------------------------------------------------------- |\n","| **f-string**          | Einfache, einmalige Prompts ¬∑ Prototyping & schnelle Tests ¬∑ Keine LangChain-Integration |\n","| **PromptTemplate**    | Single-Turn Prompts mit Variablen ¬∑ Completion-Modelle (GPT-3, etc.) ¬∑ Keine Konversation |\n","| **ChatPromptTemplate** | Multi-Turn Dialoge ¬∑ Chat-Modelle (GPT-4, Claude, etc.) ¬∑ Komplexe Konversationen mit Memory |\n"],"metadata":{"id":"IrQUS1yvpsFQ"},"id":"IrQUS1yvpsFQ"},{"cell_type":"markdown","metadata":{"id":"Ps344Ejw0-ih"},"source":["## 5.3 | Few-Shot Prompt ‚ú≥Ô∏è\n"],"id":"Ps344Ejw0-ih"},{"cell_type":"markdown","source":["Few-Shot-Prompting ist eine Technik, bei der man einem KI-Modell einige Beispiele (sogenannte \"Shots\") vorlegt, bevor es eine neue Aufgabe l√∂sen soll. Das Modell kann aus diesen Beispielen das Muster erkennen und auf neue, √§hnliche F√§lle anwenden."],"metadata":{"id":"LKY3IaJSDLvt"},"id":"LKY3IaJSDLvt"},{"cell_type":"markdown","source":["`FewShotPromptTemplate` ist eine Prompt-Vorlage, die automatisch mehrere Beispiele einf√ºgt, bevor das eigentliche Nutzer-Input kommt."],"metadata":{"id":"NnBh2dz8M2fi"},"id":"NnBh2dz8M2fi"},{"cell_type":"code","execution_count":29,"metadata":{"id":"cEopgOiS0KU_","executionInfo":{"status":"ok","timestamp":1764781361143,"user_tz":-60,"elapsed":16,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["beispiele = [\n","    {\"frage\": \"Was ist Python?\", \"antwort\": \"Eine interpretierende Programmiersprache.\"},\n","    {\"frage\": \"Was ist Java?\", \"antwort\": \"Eine objektorientierte Sprache.\"}\n","]"],"id":"cEopgOiS0KU_"},{"cell_type":"code","source":["beispiel_prompt_template = PromptTemplate.from_template(\n","    \"Frage: {frage}\\nAntwort: {antwort}\"\n",")"],"metadata":{"id":"fHEYrEPcDD9S","executionInfo":{"status":"ok","timestamp":1764781361171,"user_tz":-60,"elapsed":10,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":30,"outputs":[],"id":"fHEYrEPcDD9S"},{"cell_type":"code","source":["system_prompt = \"Du bist ein Assistent, der Fragen zu Programmiersprachen beantwortet. Halte deine Antworten kurz und pr√§zise:\""],"metadata":{"id":"eAk2iqPrjynu","executionInfo":{"status":"ok","timestamp":1764781361195,"user_tz":-60,"elapsed":17,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"eAk2iqPrjynu","execution_count":31,"outputs":[]},{"cell_type":"code","source":["# Few-Shot-Prompt\n","few_shot_prompt_template = FewShotPromptTemplate(\n","    examples=beispiele,\n","    example_prompt=beispiel_prompt_template,\n","    prefix=system_prompt,\n","    suffix=\"Frage: {frage}\\nAntwort:\",\n","    input_variables=[\"frage\"]\n",")\n","\n","prompt = few_shot_prompt_template.format(frage=\"Was ist Go?\")"],"metadata":{"id":"BpIbLhIhDcUp","executionInfo":{"status":"ok","timestamp":1764781361201,"user_tz":-60,"elapsed":16,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":32,"outputs":[],"id":"BpIbLhIhDcUp"},{"cell_type":"markdown","source":["**Struktur des finalen Prompts:**\n","\n","```\n","[prefix]\n","[formatiertes Beispiel 1]\n","[formatiertes Beispiel 2]\n","...\n","[suffix mit aktueller Frage]\n","```\n","\n"],"metadata":{"id":"tpDH0jAgknKS"},"id":"tpDH0jAgknKS"},{"cell_type":"code","source":["response = llm.invoke(prompt)"],"metadata":{"id":"KZPoIO6gDo23","executionInfo":{"status":"ok","timestamp":1764781361888,"user_tz":-60,"elapsed":683,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":33,"outputs":[],"id":"KZPoIO6gDo23"},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"Yr0YleeiD_x7","colab":{"base_uri":"https://localhost:8080/","height":99},"executionInfo":{"status":"ok","timestamp":1764781361915,"user_tz":-60,"elapsed":20,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"5754b3e2-176b-4610-a0cb-95a5d7a87984"},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Eine kompilierte Sprache, die f√ºr Effizienz und Parallelit√§t entwickelt wurde."},"metadata":{}}],"id":"Yr0YleeiD_x7"},{"cell_type":"markdown","source":["**Vorteile eines `FewShotPromptTemplate`**"],"metadata":{"id":"XFbved9KM_WJ"},"id":"XFbved9KM_WJ"},{"cell_type":"markdown","source":["| Vorteil                      | Beschreibung                                                      |\n","| ---------------------------- | ----------------------------------------------------------------- |\n","| üß† Lerneffekt f√ºr das Modell | Das Modell erkennt aus Beispielen, wie es antworten soll          |\n","| üß∞ Flexibilit√§t              | Beispiele k√∂nnen dynamisch gesetzt oder generiert werden          |\n","| üß± Struktur & Konsistenz     | Einheitlicher Aufbau aller Beispiele √ºber `example_prompt`        |\n","| ü™Ñ Kein Training n√∂tig       | Kein Fine-Tuning ‚Äì nur durch Prompting                            |\n","| üõ† Kombinierbar              | Kann mit Templates, Tools, Chains, OutputParser kombiniert werden |\n"],"metadata":{"id":"EWjuAd2_M-E7"},"id":"EWjuAd2_M-E7"},{"cell_type":"markdown","id":"pt37u3cnbgc","source":["## 5.4 | Eigenes Prompt-Template üìå"],"metadata":{"id":"pt37u3cnbgc"}},{"cell_type":"markdown","id":"ik5lbyv7vc","source":["Die `load_chat_prompt_template()` Funktion aus der `genai_lib.utilities` erm√∂glicht das Laden von Prompt-Templates direkt von GitHub oder aus lokalen Dateien.\n","\n","**Vorteile:**\n","- Zentrale Verwaltung von Prompts im `05_prompt/` Verzeichnis\n","- Wiederverwendbarkeit √ºber mehrere Notebooks hinweg\n","- Versionskontrolle durch Git\n","- Einfaches Teilen von Best-Practice-Prompts\n","\n","**Hinweis:** In Google Colab muss die GitHub-URL verwendet werden, da lokale Pfade nicht verf√ºgbar sind."],"metadata":{"id":"ik5lbyv7vc"}},{"cell_type":"markdown","id":"rjewn4pkze","source":["**Template-Struktur (`05_prompt/text_zusammenfassung.py`):**\n","\n","```python\n","# Prompt Template als Python-Datei\n","messages = [\n","    (\n","        \"system\",\n","        \"Du bist ein Experte f√ºr die Erstellung pr√§gnanter Textzusammenfassungen...\"\n","    ),\n","    (\n","        \"human\",\n","        \"Bitte fasse den folgenden Text zusammen:\\n\\n{text}\\n\\n...\"\n","    )\n","]\n","```\n","\n","**Wichtig:** Templates m√ºssen als **Python-Dateien (.py)** erstellt werden und eine Variable `messages` definieren, die eine Liste von Tupeln mit `(role, content)` enth√§lt."],"metadata":{"id":"rjewn4pkze"}},{"cell_type":"code","id":"fl5appj1wrf","source":["from genai_lib.utilities import load_chat_prompt_template\n","\n","# Template von GitHub laden\n","url = \"https://github.com/ralf-42/GenAI/blob/main/05_prompt/text_zusammenfassung.py\"\n","chat_template = load_chat_prompt_template(url)\n","\n","# Template mit Beispieltext testen\n","beispiel_text = \"\"\"\n","LangChain ist ein Framework zur Entwicklung von Anwendungen mit gro√üen Sprachmodellen (LLMs).\n","Es bietet modulare Komponenten f√ºr verschiedene Aufgaben, vorgefertigte Ketten f√ºr komplexe\n","Workflows und eine einfache Integration externer Datenquellen. Mit LangChain k√∂nnen Entwickler\n","Chat-Modelle, Embeddings, Vektordatenbanken und Agents effizient kombinieren, um\n","leistungsstarke KI-Anwendungen zu erstellen.\n","\"\"\"\n","\n","# Prompt mit Variablen f√ºllen\n","prompt = chat_template.format_messages(text=beispiel_text)\n","\n","# Aufruf\n","response = llm.invoke(prompt)"],"metadata":{"id":"fl5appj1wrf","executionInfo":{"status":"ok","timestamp":1764781364426,"user_tz":-60,"elapsed":2501,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","id":"zqaw2rdob6b","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"zqaw2rdob6b","colab":{"base_uri":"https://localhost:8080/","height":134},"executionInfo":{"status":"ok","timestamp":1764781364480,"user_tz":-60,"elapsed":24,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"2a7cafb1-74bc-4d45-ca84-7bb9d4ab9092"},"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"LangChain ist ein Framework zur Entwicklung von Anwendungen mit gro√üen Sprachmodellen (LLMs), das modulare Komponenten und vorgefertigte Ketten f√ºr komplexe Workflows bietet. Es erm√∂glicht eine einfache Integration externer Datenquellen und die effiziente Kombination von Chat-Modellen, Embeddings, Vektordatenbanken und Agents. Dadurch k√∂nnen Entwickler leistungsstarke KI-Anwendungen erstellen."},"metadata":{}}]},{"cell_type":"markdown","id":"8c859b9f","metadata":{"id":"8c859b9f"},"source":["# 6 | Weitere Modelle üìå\n","---"]},{"cell_type":"markdown","source":["LangChain ist mit zahlreichen LLMs ‚Äì sowohl eines als auch verschiedener Anbieter ‚Äì kompatibel und bietet vielseitige Konfigurationsoptionen f√ºr individuelle Anwendungsf√§lle."],"metadata":{"id":"5y6J-chfEOPH"},"id":"5y6J-chfEOPH"},{"cell_type":"markdown","source":["**Beispiel `gpt-4o` und `gpt-5-mini`**"],"metadata":{"id":"Oc0UPT5eTyTR"},"id":"Oc0UPT5eTyTR"},{"cell_type":"markdown","source":["+ gpt-4o:    \n","gpt-4o ist ein multimodales KI-Modell von OpenAI, das Text- und Bildeingaben verarbeiten kann und sich durch hohe Geschwindigkeit sowie eine nat√ºrliche, menschen√§hnliche Interaktion auszeichnet ‚Äì ideal f√ºr allt√§gliche Konversationen, kreative Aufgaben und allgemeine Chatanwendungen\n","+ gpt-5-mini:     \n","gpt-5-mini ist die mittlere Modellvariante der GPT-5-Familie, die ein ausgewogenes Verh√§ltnis zwischen Leistungsf√§higkeit und Kosten bietet und f√ºr allt√§gliche Aufgaben, allgemeine Automatisierung sowie kosteng√ºnstige Anwendungen im gro√üen Ma√üstab optimiert wurde"],"metadata":{"id":"f8yl7OApTvcQ"},"id":"f8yl7OApTvcQ"},{"cell_type":"code","source":["# Modelle konfigurieren (Kurznotation: \"provider:model\")\n","chat_model = init_chat_model(\"openai:gpt-4o\")\n","resoning_model = init_chat_model(\"openai:gpt-5-mini\")"],"metadata":{"id":"aDGqhlxoENvs","executionInfo":{"status":"ok","timestamp":1764781364505,"user_tz":-60,"elapsed":17,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"aDGqhlxoENvs","execution_count":37,"outputs":[]},{"cell_type":"code","source":["# Gemeinsamer simple prompt\n","prompt = \"Was ist der Unterschied zwischen Prompting und Retrieval-Augmented-Generation?\""],"metadata":{"id":"PywoNdLrPCfO","executionInfo":{"status":"ok","timestamp":1764781364565,"user_tz":-60,"elapsed":34,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"PywoNdLrPCfO","execution_count":38,"outputs":[]},{"cell_type":"code","source":["# Aufruf\n","response = chat_model.invoke(prompt)"],"metadata":{"id":"iriw2JZPOazT","executionInfo":{"status":"ok","timestamp":1764781379180,"user_tz":-60,"elapsed":14638,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"iriw2JZPOazT","execution_count":39,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Chat-Model:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"9OlEy8jQPck1","colab":{"base_uri":"https://localhost:8080/","height":555},"executionInfo":{"status":"ok","timestamp":1764781379201,"user_tz":-60,"elapsed":19,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"81c5834c-a631-4401-ca0c-bcc0435aa282"},"id":"9OlEy8jQPck1","execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Chat-Model:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Prompting und Retrieval-Augmented Generation (RAG) sind zwei verschiedene Ans√§tze im Bereich der Verarbeitung nat√ºrlicher Sprache, die oft mit gro√üen Sprachmodellen zusammenarbeiten, aber sie dienen unterschiedlichen Zwecken und haben unterschiedliche Mechanismen.\n\n### Prompting:\n- **Definition**: Prompting bezieht sich darauf, wie man ein Sprachmodell dazu bringt, bestimmte Arten von Antworten zu generieren. Es bedeutet im Grunde, eine Eingabe (Prompt) zu formulieren, die das Modell dazu bringt, die gew√ºnschte Ausgabe zu erzeugen.\n- **Anwendung**: Einfache Formen des Promptings beinhalten das direkte Stellen von Fragen oder das Vorgeben von Anweisungen, um einen bestimmten Kontext oder eine bestimmte Art von Antwort zu erhalten. Dies kann von einfachen Eingabeaufforderungen bis hin zu komplexeren, kontextreichen Szenarien reichen.\n- **Vorteil**: Prompting ist direkt und unkompliziert, oft sehr n√ºtzlich f√ºr Interaktionen, die eine schnelle und einfache Antwort erfordern.\n- **Limitierung**: Der Erfolg h√§ngt stark von der Formulierung der Eingabeaufforderung ab, und das Modell hat keinen Zugriff auf externe Informationen au√üer dem, was in seinen Trainingsdaten enthalten ist.\n\n### Retrieval-Augmented Generation (RAG):\n- **Definition**: RAG kombiniert die F√§higkeit von Sprachmodellen, Sprache zu generieren, mit einem Informationsabrufsystem, das externe Datenquellen durchsucht, um die Generierung zu verbessern.\n- **Prozess**:\n  1. **Retrieval-Phase**: Ein Abrufmodul durchsucht eine gro√üe Sammlung von Dokumenten, um die relevantesten Informationen basierend auf der Eingabe zu finden.\n  2. **Generation-Phase**: Das Sprachmodell verwendet diese abgerufenen Informationen, um eine fundiertere und pr√§zisere Antwort zu generieren.\n- **Vorteil**: RAG kann genauere und aktuellere Informationen liefern, da es auf externe Wissensquellen zur√ºckgreift, anstatt sich ausschlie√ülich auf die im Modell gespeicherten Informationen zu verlassen.\n- **Limitierung**: Die Leistung h√§ngt von der Qualit√§t der Suchalgorithmen und der Verf√ºgbarkeit relevanter, aktueller Daten ab.\n\nZusammenfassend l√§sst sich sagen, dass Prompting mehr auf die Formulierung der Eingabe an das Modell abzielt, w√§hrend RAG den Prozess erweitert, indem es das Modell mit zus√§tzlichen, aktuelleren Informationen aus externen Quellen versorgt, um die Generierung zu verbessern."},"metadata":{}}]},{"cell_type":"code","source":["# Aufruf\n","response = resoning_model.invoke(prompt)"],"metadata":{"id":"_Znkwm2SPOrX","executionInfo":{"status":"ok","timestamp":1764781402222,"user_tz":-60,"elapsed":23014,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"_Znkwm2SPOrX","execution_count":41,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Reasoning-Model:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"N0hcQkSfQyVK","colab":{"base_uri":"https://localhost:8080/","height":983},"executionInfo":{"status":"ok","timestamp":1764781402310,"user_tz":-60,"elapsed":60,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"c0632f85-3dce-4dd1-b9f7-674cf1173573"},"id":"N0hcQkSfQyVK","execution_count":42,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Reasoning-Model:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Kurzfassung\n- Prompting: Du stellst einer gro√üen Sprach‚ÄëKI (LLM) direkt eine Eingabe (Prompt) und sie erzeugt darauf eine Ausgabe nur aus ihrem internen Wissen und ihren Trainingsmustern.\n- Retrieval‚ÄëAugmented Generation (RAG): Die KI holt zuerst relevante externe Dokumente (Retrieval) und nutzt diese als Kontext, um die Antwort zu generieren (Generation). Dadurch wird die Ausgabe an aktuelle oder private Quellen ‚Äûgeerdet‚Äú.\n\nDefinitionen und Ablauf\n- Prompting\n  - Definition: Formular von Interaktion mit einem LLM durch gut gestaltete Eingabe (z. B. Zero‚Äë/Few‚ÄëShot, Instruktions‚ÄëPrompts, Chain‚Äëof‚ÄëThought).\n  - Ablauf: Prompt ‚Üí LLM ‚Üí Antwort (nur Modellwissen).\n  - Beispiel: ‚ÄûErkl√§re Quantenverschr√§nkung f√ºr Studierende im ersten Semester.‚Äú\n\n- RAG\n  - Definition: Kombination aus Such-/Retrieval‚ÄëKomponente und einem generativen Modell. Der Retriever liefert relevante Textpassagen (z. B. aus Dokumenten, Datenbanken, Web), die dann als Kontext im Prompt verwendet werden.\n  - Ablauf: Anfrage ‚Üí Retriever (BM25 / Vektor‚ÄëEmbeddings) findet Dokumente ‚Üí Dokumente + Anfrage ‚Üí Generator (LLM) ‚Üí Antwort (ggf. mit Zitaten).\n  - Beispiel: Nutzerfrage zur Firmenrichtlinie ‚Üí System sucht interne Handbuchseiten ‚Üí LLM erstellt Antwort mit Referenz auf die gefundenen Abschnitte.\n\nWesentliche Unterschiede\n- Datenquelle: Prompting nutzt nur das im Modell gespeicherte Wissen; RAG nutzt zus√§tzlich externe, aktuelle oder private Daten.\n- Grounding/Faithfulness: RAG kann Antworten besser belegen und Halluzinationen reduzieren (abh√§ngig von Retriever‚ÄëQualit√§t). Prompting kann st√§rker halluzinieren, besonders bei Faktenfragen.\n- Aktualit√§t: RAG erlaubt Zugriff auf aktuelle Informationen; reines Prompting ist limitiert durch das Trainingsdatum.\n- Architektur: Prompting ist einfacher (nur LLM + Prompt); RAG ben√∂tigt Index, Retriever, ggf. Embedding‚ÄëModelle und Orchestrierung.\n- Kosten & Latenz: RAG hat zus√§tzlichen Kostenaufwand (Indexierung, Retrieval) und Latenz, kann aber Gesamtkosten reduzieren, weil k√ºrzere generative Prompts oder spezialisierte Modelle genutzt werden.\n- Datenschutz: RAG erlaubt gezielten Zugriff auf private Datenquellen, kontrollierbar durch Indexzugriffsrechte.\n\nVor‚Äë und Nachteile (knapp)\n- Prompting\n  - + Einfach zu implementieren, schnell f√ºr kreative/konversationelle Aufgaben.\n  - ‚àí Schlechter bei aktuellen/private Fakten, h√∂here Halluzinationsgefahr bei Detailfragen.\n- RAG\n  - + Bessere Faktentreue, Zugriff auf aktuelle und private Inhalte, Zitation m√∂glich.\n  - ‚àí Komplexer aufzubauen, h√∂here Infrastrukturkosten und Latenz.\n\nWann welches w√§hlen?\n- Prompting: kreative Texte, Sprachstil, generelle Erkl√§rungen, wenn Modellwissen + kein externer Kontext ausreichend sind.\n- RAG: Fragen zu firmenspezifischen Dokumenten, aktuelle Fakten, Compliance, wenn Belege/Zitate n√∂tig sind oder das Modell veraltetes Wissen hat.\n- Hybrid: Oft am sinnvollsten ‚Äî gutes Prompting innerhalb des RAG‚ÄëWorkflows (der Generator wird durch Retrieval kontextualisiert).\n\nKurze Best Practices\n- F√ºr Prompting: klare Instruktionen, Few‚ÄëShot‚ÄëBeispiele, Temperatursteuerung, evtl. Chain‚Äëof‚ÄëThought nur zur Erkl√§rung verwenden.\n- F√ºr RAG: qualitativ guten Retriever (Embeddings + Faiss/Annoy oder BM25), Relevanzfeedback, Prompt‚ÄëTemplate f√ºr Kontext‚ÄëInjektion (z. B. Dokumente zuerst, dann Frage), Quellenangaben in der Ausgabe, Fallback wenn keine relevanten Dokumente gefunden wurden.\n\nWenn du magst, kann ich dir ein konkretes Beispiel zeigen (Prompt vs. RAG) mit echten Textschnipseln oder eine Architektur‚ÄëSkizze f√ºr eine RAG‚ÄëPipeline."},"metadata":{}}]},{"cell_type":"markdown","source":["**üìä Vergleich: Chat-Modell vs. Reasoning-Modell**"],"metadata":{"id":"YJljcFw4Rz6W"},"id":"YJljcFw4Rz6W"},{"cell_type":"markdown","source":["| **Aspekt**                      | **Chat-Modell**                                                           | **Reasoning-Modell**                                                                                |\n","| ------------------------------- | ------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------- |\n","| **Stil**                        | Erkl√§rend, klassisch strukturiert mit Flie√ütext                           | Punktuell, technisch und systematisch                                                               |\n","| **Formatierung**                | Flie√ütext mit Abs√§tzen und Beispielen                                     | Aufz√§hlungen (‚Ä¢) und strukturierte Unterpunkte                                                      |\n","| **Definition Prompting**        | Kurze Definition als Text, danach Beispiel                                | Technisch klar: ‚Äûparametrisches Wissen‚Äú, Rolle der Prompt-Formulierung betont                       |\n","| **Definition RAG**              | Flie√ütext-Erkl√§rung mit Ablaufbeschreibung und Beispiel                   | Detaillierte Schritte: Abruf, Kontextintegration, Vorteile bzgl. Aktualit√§t und Pr√§zision erl√§utert |\n","| **Beispiele**                   | Ja ‚Äì einfache, praxisnahe Beispiele (z.‚ÄØB. Klimawandel, Quantencomputing) | Nein ‚Äì keine konkreten Beispiele, aber kontextbezogene Beschreibung                                 |\n","| **Vergleich Prompting vs. RAG** | Abstrakte Gegen√ºberstellung in einem abschlie√üenden Absatz                | Direkter Vergleich im letzten Abschnitt mit Betonung der Wissensquellen                             |\n","| **Technische Tiefe**            | Mittel ‚Äì f√ºr Einsteiger\\:innen gut verst√§ndlich                           | Hoch ‚Äì Begriffe wie ‚Äûparametrisches Wissen‚Äú, Fokus auf Architektur                                  |\n","| **Zielgruppe**                  | Allgemeines Publikum, Einstieg in das Thema                               | Fortgeschrittene Nutzer\\:innen, Technik-affine Leserschaft                                          |\n","| **St√§rken des Modells**         | Klare Erkl√§rung, gute didaktische Struktur                                | Pr√§zise Begriffsabgrenzung, hohe Informationsdichte                                                 |\n","\n"],"metadata":{"id":"OzPIlXnqR2Fq"},"id":"OzPIlXnqR2Fq"},{"cell_type":"markdown","source":["**üìä Vergleich: Chat-Modelle**"],"metadata":{"id":"GO40ayL6plxO"},"id":"GO40ayL6plxO"},{"cell_type":"markdown","source":["| Model              | Creator    | Open?  | API Name(s) (falls verf√ºgbar)                            | Context Window             |\n","| ------------------ | ---------- | ------ | -------------------------------------------------------- | -------------------------- |\n","| **GPT-5**          | OpenAI     | Closed | `gpt-5`, `gpt-5-mini`, `gpt-5-nano`, `gpt-5-chat-latest` | 400K Tokens (~296 Seiten)  |\n","| **GPT-4.1**        | OpenAI     | Closed | `gpt-4.1`, `gpt-4.1-mini`, `gpt-4.1-nano`                | 1M Tokens (~741 Seiten)    |\n","| **GPT-4o**         | OpenAI     | Closed | `gpt-4o`, `gpt-4o-mini`                                  | 128K Tokens (~95 Seiten)   |\n","| **GPT-4 Turbo**    | OpenAI     | Closed | `gpt-4-turbo`, `gpt-4-turbo-2024-04-09`                  | 128K Tokens (~95 Seiten)   |\n","| **gpt-oss-120b**   | OpenAI     | Open   | ‚Äî                                                        | 131K Tokens (~97 Seiten)   |\n","| **gpt-oss-20b**    | OpenAI     | Open   | ‚Äî                                                        | 131K Tokens (~97 Seiten)   |\n","| **Claude 4**       | Anthropic  | Closed | `claude-sonnet-4-20250514`, `claude-opus-4-20250514`     | 200K Tokens (~148 Seiten)  |\n","| **Gemini 2.5 Pro** | Google     | Closed | `gemini-2.5-pro`, `gemini-2.5-pro-exp-03-25`             | 1M Tokens (~741 Seiten)    |\n","| **Llama 3.1 405B** | Meta       | Open   | ‚Äî                                                        | 512K Tokens (~379 Seiten)  |\n","| **DeepSeek-V3**    | DeepSeek   | Open   | ‚Äî                                                        | 128K Tokens (~95 Seiten)   |\n","| **Mistral 7B**     | Mistral.AI | Open   | ‚Äî                                                        | 32K Tokens (~24 Seiten)    |\n","\n","<br>\n","\n","*Umrechnung Token/Seiten:  ~1.350 Tokens ‚âà 1 Seite deutschsprachiger Text*"],"metadata":{"id":"RpcsYzLApH7v"},"id":"RpcsYzLApH7v"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","\n","Ein anderer Modell-Provider: `groq`\n","</font></p>"],"metadata":{"id":"8y5xNtM1oMs3"},"id":"8y5xNtM1oMs3"},{"cell_type":"code","source":["install_packages([('langchain-groq', 'langchain_groq')])"],"metadata":{"id":"ec6V1NJRnjPv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764781402682,"user_tz":-60,"elapsed":258,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"f8ea3354-96fa-4b4b-e249-7efff56968cc"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["üîÑ Installiere langchain-groq...\n","‚úÖ langchain-groq erfolgreich installiert und importiert\n"]}],"id":"ec6V1NJRnjPv"},{"cell_type":"code","source":["setup_api_keys(['GROQ_API_KEY'])"],"metadata":{"id":"wXDH1ysxfo_K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764781403150,"user_tz":-60,"elapsed":460,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"439b2948-a454-4540-ba14-ebce54a45c67"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úì GROQ_API_KEY erfolgreich gesetzt\n"]}],"id":"wXDH1ysxfo_K"},{"cell_type":"code","source":["# Kurznotation: \"provider:model\"\n","llm_groq = init_chat_model(\"groq:qwen/qwen3-32b\", temperature=0)"],"metadata":{"id":"cVSRS2Kaf1M7","executionInfo":{"status":"ok","timestamp":1764781403469,"user_tz":-60,"elapsed":315,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":45,"outputs":[],"id":"cVSRS2Kaf1M7"},{"cell_type":"code","source":["response_groq = llm_groq.invoke(\"Wer war Albert Einstein? Bitte antworte in deutscher Sprache.\")"],"metadata":{"id":"oRl7X3y-nm92","executionInfo":{"status":"ok","timestamp":1764781406473,"user_tz":-60,"elapsed":2987,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":46,"outputs":[],"id":"oRl7X3y-nm92"},{"cell_type":"code","source":["mprint(response_groq.content)"],"metadata":{"id":"fhwKoyS3n0ho","colab":{"base_uri":"https://localhost:8080/","height":521},"executionInfo":{"status":"ok","timestamp":1764781406520,"user_tz":-60,"elapsed":51,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"5775d9e5-5d2b-4199-ec3b-5388ca32867c"},"execution_count":47,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"<think>\nOkay, the user is asking who Albert Einstein was and wants the answer in German. Let me start by recalling the basics. Einstein was a physicist, right? He's famous for the theory of relativity. I should mention his birth and death dates. He was born in 1879 in Germany and died in 1955 in the US. He won the Nobel Prize in Physics in 1921, but I need to remember the exact reason. Oh, it was for the photoelectric effect, not relativity. That's a common misconception.\n\nHe also contributed to quantum theory and the famous equation E=mc¬≤. I should explain that equation briefly. Maybe mention his work on Brownian motion and the photoelectric effect. Also, his role in the Manhattan Project, though he was against the use of atomic weapons. He was a pacifist and civil rights advocate. His later years were in Princeton, and he worked on a unified field theory. Need to make sure the dates and facts are correct. Let me check the Nobel Prize year again‚Äîyes, 1921. Also, he emigrated to the US in 1933 due to the rise of the Nazis. \n\nI should structure the answer in German, starting with his full name, birth and death, nationality, main contributions, Nobel Prize, and his other activities. Keep it concise but informative. Avoid technical jargon but include key terms like \"Relativit√§tstheorie\" and \"Energie-Masse-√Ñquivalenz\". Make sure to highlight his impact on physics and his humanitarian efforts. Double-check any names or terms in German to ensure accuracy. For example, \"photoelectric effect\" is \"Photoeffekt\" in German. Also, mention his Swiss and American citizenships. Alright, that should cover the main points without being too lengthy.\n</think>\n\nAlbert Einstein (*14. M√§rz 1879 in Ulm; ‚Ä† 18. April 1955 in Princeton, New Jersey) war ein deutscher Physiker, der sp√§ter Schweizer und US-amerikanischer Staatsb√ºrger wurde. Er gilt als einer der einflussreichsten Wissenschaftler des 20. Jahrhunderts.  \n\nEinstein ist vor allem f√ºr seine **Relativit√§tstheorie** bekannt, die er in zwei Teilen entwickelte:  \n1. **Spezielle Relativit√§tstheorie (1905)**: Erkl√§rte, dass Raum und Zeit relativ zum Beobachter sind und die Lichtgeschwindigkeit konstant bleibt.  \n2. **Allgemeine Relativit√§tstheorie (1915)**: Beschrieb die Gravitation als Kr√ºmmung der Raumzeit durch Masse und Energie.  \n\nZu seinen weiteren bahnbrechenden Beitr√§gen geh√∂rten:  \n- Die Erkl√§rung des **Photoeffekts** (1905), f√ºr die er 1921 den **Nobelpreis f√ºr Physik** erhielt.  \n- Die Formel **E = mc¬≤**, die die √Ñquivalenz von Masse und Energie beschreibt.  \n- Die Theorie der **Brown‚Äôschen Bewegung**, die die Existenz von Atomen best√§tigte.  \n\nEinstein war auch ein engagierter Pazifist und Vork√§mpfer f√ºr die B√ºrgerrechte. Er floh 1933 vor dem NS-Regime nach Princeton (USA), wo er bis zu seinem Tod lebte. Seine Arbeit pr√§gte die moderne Physik grundlegend und beeinflusste Bereiche wie Astrophysik, Kosmologie und Quantenmechanik."},"metadata":{}}],"id":"fhwKoyS3n0ho"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Modelle mit Reasoning/Chain-of-Thought\n","</font></p>"],"metadata":{"id":"dwwtPwJyLRE2"},"id":"dwwtPwJyLRE2"},{"cell_type":"code","source":["from genai_lib.utilities import extract_thinking\n","thinking, answer = extract_thinking(response_groq)\n","\n","mprint(\"### Thinking:\")\n","mprint(\"---\")\n","mprint(thinking)\n","mprint(\"### Answer:\")\n","mprint(\"---\")\n","mprint(answer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":619},"id":"WnZVyYPvKJpS","executionInfo":{"status":"ok","timestamp":1764781659933,"user_tz":-60,"elapsed":79,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"850006f0-2155-44b9-9522-edb0c5a68a88"},"id":"WnZVyYPvKJpS","execution_count":50,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### Thinking:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Okay, the user is asking who Albert Einstein was and wants the answer in German. Let me start by recalling the basics. Einstein was a physicist, right? He's famous for the theory of relativity. I should mention his birth and death dates. He was born in 1879 in Germany and died in 1955 in the US. He won the Nobel Prize in Physics in 1921, but I need to remember the exact reason. Oh, it was for the photoelectric effect, not relativity. That's a common misconception.\n\nHe also contributed to quantum theory and the famous equation E=mc¬≤. I should explain that equation briefly. Maybe mention his work on Brownian motion and the photoelectric effect. Also, his role in the Manhattan Project, though he was against the use of atomic weapons. He was a pacifist and civil rights advocate. His later years were in Princeton, and he worked on a unified field theory. Need to make sure the dates and facts are correct. Let me check the Nobel Prize year again‚Äîyes, 1921. Also, he emigrated to the US in 1933 due to the rise of the Nazis. \n\nI should structure the answer in German, starting with his full name, birth and death, nationality, main contributions, Nobel Prize, and his other activities. Keep it concise but informative. Avoid technical jargon but include key terms like \"Relativit√§tstheorie\" and \"Energie-Masse-√Ñquivalenz\". Make sure to highlight his impact on physics and his humanitarian efforts. Double-check any names or terms in German to ensure accuracy. For example, \"photoelectric effect\" is \"Photoeffekt\" in German. Also, mention his Swiss and American citizenships. Alright, that should cover the main points without being too lengthy."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### Ansqwer:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Albert Einstein (*14. M√§rz 1879 in Ulm; ‚Ä† 18. April 1955 in Princeton, New Jersey) war ein deutscher Physiker, der sp√§ter Schweizer und US-amerikanischer Staatsb√ºrger wurde. Er gilt als einer der einflussreichsten Wissenschaftler des 20. Jahrhunderts.  \n\nEinstein ist vor allem f√ºr seine **Relativit√§tstheorie** bekannt, die er in zwei Teilen entwickelte:  \n1. **Spezielle Relativit√§tstheorie (1905)**: Erkl√§rte, dass Raum und Zeit relativ zum Beobachter sind und die Lichtgeschwindigkeit konstant bleibt.  \n2. **Allgemeine Relativit√§tstheorie (1915)**: Beschrieb die Gravitation als Kr√ºmmung der Raumzeit durch Masse und Energie.  \n\nZu seinen weiteren bahnbrechenden Beitr√§gen geh√∂rten:  \n- Die Erkl√§rung des **Photoeffekts** (1905), f√ºr die er 1921 den **Nobelpreis f√ºr Physik** erhielt.  \n- Die Formel **E = mc¬≤**, die die √Ñquivalenz von Masse und Energie beschreibt.  \n- Die Theorie der **Brown‚Äôschen Bewegung**, die die Existenz von Atomen best√§tigte.  \n\nEinstein war auch ein engagierter Pazifist und Vork√§mpfer f√ºr die B√ºrgerrechte. Er floh 1933 vor dem NS-Regime nach Princeton (USA), wo er bis zu seinem Tod lebte. Seine Arbeit pr√§gte die moderne Physik grundlegend und beeinflusste Bereiche wie Astrophysik, Kosmologie und Quantenmechanik."},"metadata":{}}]},{"cell_type":"markdown","id":"c314ca71","metadata":{"id":"c314ca71"},"source":["# 7 | Chain üìå\n","---"]},{"cell_type":"markdown","source":["Chains verbinden mehrere Komponenten/Bausteine zu einer Verarbeitungskette. LangChain Expression Language (LCEL) ist das **neue Programmiermodell in LangChain v0.3+**, das die Entwicklung von LLM-Anwendungen vereinfacht.\n"],"metadata":{"id":"cEEMxhSBHrXA"},"id":"cEEMxhSBHrXA"},{"cell_type":"markdown","source":["\n","**Hier die Kernpunkte:**\n","\n","1. Was ist LCEL?\n","```python\n","# LCEL nutzt den Pipe-Operator (|) f√ºr klare Verkettungen\n","chain = prompt | model\n","```\n","- Eine deklarative Sprache zum Verketten von LangChain-Komponenten\n","- Erm√∂glicht linearen Datenfluss zwischen Komponenten\n","- Basiert auf dem Pipe-Operator (|) f√ºr intuitive Verbindungen\n","\n","2. Warum LCEL nutzen?\n","- Bessere Lesbarkeit des Codes\n","- Einfachere Wartung und Debugging\n","- Verbesserte Performance durch optimierte Ausf√ºhrung\n","- Bessere Typsicherheit und Fehlererkennung\n","- Unterst√ºtzt modernes Streaming und Async-Operationen\n","\n","3. Praktisches Beispiel:"],"metadata":{"id":"ltTQItwXH6w8"},"id":"ltTQItwXH6w8"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Simple Chain üìå\n","</font></p>\n"],"metadata":{"id":"BndZGbyWKs1C"},"id":"BndZGbyWKs1C"},{"cell_type":"code","id":"fsryxusc3p8","source":["#@markdown   <p><font size=\"4\" color='green'>   Mermaid - LCEL Chain-Komposition</font> </br></p>\n","\n","diagram = \"\"\"\n","graph LR\n","    A[\"Input Dictionary<br/>{user_input: ...}\"] --> B[\"Prompt Template\"]\n","    B --> C[\"Formatierter Prompt<br/>System + Human Message\"]\n","    C --> D[\"LLM\"]\n","    D --> E[\"AI Message\"]\n","    E --> F[\"StrOutputParser\"]\n","    F --> G[\"String Output\"]\n","\n","    H[\"chain = prompt | llm | parser\"] -.-> B\n","    H -.-> D\n","    H -.-> F\n","\n","    style A fill:#e1f5ff\n","    style B fill:#fff4e1\n","    style C fill:#f0f0f0\n","    style D fill:#ffe1f5\n","    style E fill:#f0f0f0\n","    style F fill:#e1ffe1\n","    style G fill:#f0f0f0\n","    style H fill:#ffe1e1\n","\"\"\"\n","mermaid(diagram, width=900, height=250)"],"metadata":{"id":"fsryxusc3p8","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","# 1. Prompt-Template\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"human\", \"Erkl√§re mir {user_input}\")\n","])\n","\n","# 2. Modell (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0)\n","\n","#  3. Einfache LCEL-Kette\n","chain = prompt | llm\n","\n","# 4. Ausf√ºhrung\n","response = chain.invoke({\"user_input\": \"LangChain Expression Language\"})"],"metadata":{"id":"PsWtjWROIW9d"},"id":"PsWtjWROIW9d","execution_count":null,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"VWYAG9aWJpZd"},"id":"VWYAG9aWJpZd","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","LCEL ist der empfohlene Weg f√ºr alle neuen LangChain-Projekte, da es die Entwicklung vereinfacht und zukunftssicher macht."],"metadata":{"id":"49Wu_XreIZY2"},"id":"49Wu_XreIZY2"},{"cell_type":"markdown","source":["# 8 | Output-Handling\n","---"],"metadata":{"id":"LVqVL9CWRXU7"},"id":"LVqVL9CWRXU7"},{"cell_type":"markdown","source":["# 8.1 | OutputParser üìå"],"metadata":{"id":"89XLSUAvCE8P"},"id":"89XLSUAvCE8P"},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/langchain_prozess_03.png)"],"metadata":{"id":"erhTv-_o97q9"},"id":"erhTv-_o97q9"},{"cell_type":"markdown","source":["Ein OutputParser ist ein Konzept in LangChain, das hilft, die Antworten eines Modells richtig weiterzuverarbeiten. Es nimmt die **rohe Ausgabe** (zum Beispiel einen langen Text) und formt sie in ein **bestimmtes Format**, das sp√§ter in der Anwendung leichter benutzt werden kann."],"metadata":{"id":"EVBcCwJERfaj"},"id":"EVBcCwJERfaj"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Simple Chain mit Parser üìå\n","</font></p>"],"metadata":{"id":"oK4lZB3EGKFK"},"id":"oK4lZB3EGKFK"},{"cell_type":"code","source":["# 1. Prompt-Template\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"human\", \"Erkl√§re mir {user_input}\")\n","])\n","\n","# 2. Modell (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0)\n","\n","# 3. Parser\n","parser = StrOutputParser()\n","\n","# 4. Einfache LCEL-Kette\n","chain = prompt | llm | parser\n","\n","# 5. Ausf√ºhrung\n","response = chain.invoke({\"user_input\": \"LangChain Expression Language\"})"],"metadata":{"id":"PP_A_bGc7ovZ"},"execution_count":null,"outputs":[],"id":"PP_A_bGc7ovZ"},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response)"],"metadata":{"id":"qMcn5ctu7ova"},"execution_count":null,"outputs":[],"id":"qMcn5ctu7ova"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Simple Chain mit Parser & Format im System-Prompt\n","</font></p>"],"metadata":{"id":"pNKOzTlQ8Kpl"},"id":"pNKOzTlQ8Kpl"},{"cell_type":"code","source":["# 1. Einfacher Prompt mit explizite Formatvorgabe\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent. Formatiere Deine Antwort im json-Format mit den Infos zu Name und Alter.\"),\n","    (\"human\", \"{user_input}\")\n","])\n","\n","# 2. Modell (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0)\n","\n","# 3. Parser\n","parser = StrOutputParser()\n","\n","# 4. Chain\n","chain = prompt | llm | parser\n","\n","# 5. Ausf√ºhrung\n","response = chain.invoke({\"user_input\": \"Bitte gibt den Namen einer fiktiven Person und das Alter\"})"],"metadata":{"id":"QKtYSBLhVlts"},"execution_count":null,"outputs":[],"id":"QKtYSBLhVlts"},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response)"],"metadata":{"id":"BGceDe7Kw6ot"},"id":"BGceDe7Kw6ot","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"5efri5ponin","source":["# 8.2 | with_structured_output() ‚ú≥Ô∏è\n"],"metadata":{"id":"5efri5ponin"}},{"cell_type":"markdown","id":"9hnfp8g8bkj","source":["Im vorherigen Beispielen wurden Formatierung √ºber **System-Prompts** und/oder √ºber einen **OututParser** gesteuert. Das funktioniert, hat aber Nachteile:\n","\n","- ‚ùå Keine automatische Validierung (Typen k√∂nnen falsch sein)\n","- ‚ùå String-Parsing fehleranf√§llig\n","- ‚ùå Keine Typsicherheit im Code\n","- ‚ùå Prompt-Engineering f√ºr Format n√∂tig\n","\n","**Moderne L√∂sung: `with_structured_output()`**\n","\n","LangChain 1.0+ bietet eine elegante L√∂sung: Mit `with_structured_output()` kann man die gew√ºnschte Ausgabestruktur **direkt als Pydantic-Modell** definieren. Das Modell sorgt automatisch f√ºr:\n","\n","- ‚úÖ Validierte, typsichere Ausgaben\n","- ‚úÖ Automatische Format-Konvertierung\n","- ‚úÖ Klare Struktur ohne Prompt-Engineering\n","- ‚úÖ IDE-Unterst√ºtzung (Autocomplete, Type-Hints)"],"metadata":{"id":"9hnfp8g8bkj"}},{"cell_type":"markdown","id":"p8qe3tuhedi","source":["<p><font color='black' size=\"5\">\n","Einfaches Beispiel mit with_structured_output()\n","</font></p>"],"metadata":{"id":"p8qe3tuhedi"}},{"cell_type":"code","id":"a3xf7g08nw","source":["from pydantic import BaseModel, Field\n","\n","# 1. Pydantic-Modell definiert die gew√ºnschte Struktur\n","class PersonInfo(BaseModel):\n","    \"\"\"Informationen √ºber eine Person\"\"\"\n","    name: str = Field(description=\"Vollst√§ndiger Name der Person\")\n","    alter: int = Field(description=\"Alter der Person in Jahren\")\n","    beruf: str = Field(description=\"Aktueller Beruf der Person\")\n","\n","# 2. Modell mit strukturierter Ausgabe (Kurznotation: \"provider:model\")\n","llm = init_chat_model(\"openai:gpt-4o-mini\", temperature=0)\n","structured_llm = llm.with_structured_output(PersonInfo)\n","\n","# 3. Einfache Anfrage - automatische Formatierung & Validierung\n","response = structured_llm.invoke(\n","    \"Erstelle eine fiktive Person: Name, Alter und Beruf\"\n",")\n","\n","# 4. Typsichere Ausgabe - response ist ein PersonInfo-Objekt\n","print(f\"Typ: {type(response)}\")\n","print(f\"Name: {response.name}\")\n","print(f\"Alter: {response.alter}\")\n","print(f\"Beruf: {response.beruf}\")"],"metadata":{"id":"a3xf7g08nw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"ybps752g89","source":["**Vergleich: Prompt-basierte Formatierung vs. StrOutputParser vs. with_structured_output()**\n","\n","| Aspekt | Prompt-basiert | StrOutputParser | with_structured_output() |\n","|--------|----------------|----------------|---------------------------|\n","| **Definition** | Via System-Prompt: ‚ÄûFormatiere im JSON-Format...‚Äú | Expliziter Parser, der LLM-Ausgabe als String extrahiert oder leicht nachbearbeitet | Automatische Strukturierung per Pydantic-Modell (`class PersonInfo(BaseModel)`) |\n","| **Validierung** | ‚ùå Keine ‚Äì Modell kann beliebig antworten | ‚ö†Ô∏è Nur minimale (z. B. String-Cleaning) | ‚úÖ Strikte Schema-Validierung durch Pydantic |\n","| **Typsicherheit** | ‚ùå Nur Strings | ‚ùå Nur Strings | ‚úÖ Typsichere Python-Objekte |\n","| **Fehlerbehandlung** | ‚ö†Ô∏è Manuelles Parsing und Regex n√∂tig | ‚ö†Ô∏è Begrenzte Kontrolle (nur einfache Parsing-Fehler) | ‚úÖ Automatische Validierung, Exception bei Schemafehlern |\n","| **Code-Qualit√§t** | ‚ö†Ô∏è Fragil, stark Prompt-abh√§ngig | ‚úÖ Etwas klarer, aber keine echte Struktur | ‚úÖ Robust, wartbar, klar definierte Typen |\n","| **IDE-Support** | ‚ùå Keine Autovervollst√§ndigung | ‚ùå Keine | ‚úÖ Vollst√§ndig durch Type-Hints |\n","| **Anwendungsfall** | Einfache oder explorative Ausgaben | Wenn nur reine Textausgabe n√∂tig ist | F√ºr strukturierte, verl√§ssliche Daten (JSON, Objekte) |\n","| **Lernkurve** | ‚úÖ Sehr einfach | ‚úÖ Einfach | ‚ö†Ô∏è Erfordert Grundkenntnisse in Pydantic und LangChain-Schemas |\n","| **Beispiel** | ‚ÄûNenne Name und Alter als JSON.‚Äú | `StrOutputParser().parse(output)` | `llm.with_structured_output(PersonInfo)` |\n","\n"],"metadata":{"id":"ybps752g89"}},{"cell_type":"markdown","source":["\n","**Wann welchen Ansatz?**\n","\n","- **Prompt-basiert**: F√ºr einfache Texte, Zusammenfassungen, kreative Inhalte  \n","- **StrOutputParser**: Wenn nur reine Textausgabe oder einfache Formatkorrektur n√∂tig ist  \n","- **with_structured_output()**: F√ºr strukturierte Daten, APIs, Datenbanken, komplexe Workflows"],"metadata":{"id":"uncqM3AxDjh9"},"id":"uncqM3AxDjh9"},{"cell_type":"markdown","source":["# 9 | Runnables (opt.)\n","---\n"],"metadata":{"id":"KRqrnpMx3Hwf"},"id":"KRqrnpMx3Hwf"},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/GenAI/main/07_image/langchain_prozess_01.png)"],"metadata":{"id":"LFOITVoEzpkm"},"id":"LFOITVoEzpkm"},{"cell_type":"markdown","source":["Ein **Runnable** ist ein elementarer Baustein in LangChain. Er nimmt eine Eingabe und liefert eine Ausgabe - wie eine Funktion mit klaren Regeln. Man kann sich Konzepte wie Prompts, LLMs oder Parser als Runnables vorstellen. Runnable kann man als **Oberbegriff** f√ºr Prompts, LLMs, Parser und √§hnliche Komponenten verwenden.\n","\n","\n","\n","In der Grafik sieht man die **drei wichtigsten Runnables**: Prompt-Template, LLM und Parser, die zusammen eine Kette bilden.\n","\n","In LangChain sind die wichtigsten Komponenten als Runnables verf√ºgbar:\n","\n","+ Prompts/Templates (der erste Baustein in der Kette)\n","+ LLMs (das Herzst√ºck in der Mitte)\n","+ Output-Parser (verwandelt die LLM-Antwort in ein nutzbares Format)\n","+ Komplette Chains (die ganze Verarbeitungskette)\n","+ Tools (f√ºr spezielle Aufgaben)\n","+ Retriever (holen zus√§tzliche Informationen)\n","\n","\n","Vorteile von Runnables:\n","\n","+ Gleiche Bedienung: Alle Runnables (Prompt, LLM, Parser) funktionieren nach demselben Prinzip\n","+ Einfaches Verbinden: Man kann sie unkompliziert zu einer Kette zusammenf√ºgen - genau wie im Bild gezeigt\n","+ Anpassungsf√§hig: Sie funktionieren sowohl einzeln als auch in der Gruppe\n","Schrittweise Ausgabe: Ergebnisse k√∂nnen st√ºckweise weitergegeben werden"],"metadata":{"id":"DnUDR_983M4l"},"id":"DnUDR_983M4l"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Benutzerdefinierte Runnables erstellen\n","</font></p>"],"metadata":{"id":"WWJ7U1iJ6b8Q"},"id":"WWJ7U1iJ6b8Q"},{"cell_type":"markdown","source":["Man kann auch eigene Runnables erstellen, indem man eine Klasse definiert, die das Runnable-Interface implementiert oder eine Funktion mit `RunnableLambda` umsetzt:\n"],"metadata":{"id":"PAvbE_TR6g87"},"id":"PAvbE_TR6g87"},{"cell_type":"code","source":["from langchain_core.runnables import RunnableLambda\n","from langchain_core.output_parsers.string import StrOutputParser\n","\n","# 1. Mit RunnableLambda eine einfache Transformation erstellen\n","def text_verdoppeln(eingabe):\n","    return f\"{eingabe} <br> <br>  {eingabe}\"\n","\n","verdoppler = RunnableLambda(text_verdoppeln)\n","\n","# Verwendung des eigenen Runnables\n","verdoppler.invoke(\"Dies ist ein Text.\")"],"metadata":{"id":"_0wEf_hL6peM"},"id":"_0wEf_hL6peM","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Einfacher Prompt ohne explizite Formatvorgabe\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent. Antworte kurz.\"),\n","    (\"human\", \"Eingabe: {user_input}\")\n","])\n","\n","# In eine Kette einbauen - Korrektur der Pipe-Operatoren\n","chain_mit_verdoppler = prompt | llm | parser | verdoppler\n","\n","# Test der Kette\n","response = chain_mit_verdoppler.invoke({\"user_input\": \"Wer war Einstein?\"})"],"metadata":{"id":"NvZdOf1961xb"},"id":"NvZdOf1961xb","execution_count":null,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response)"],"metadata":{"id":"_elA0imx2HZA"},"id":"_elA0imx2HZA","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Runnable-Methoden\n","</font></p>\n","\n","Jedes Runnable unterst√ºtzt standardm√§√üig folgende Methoden:\n","\n","1. **invoke()**: F√ºr einzelne, synchrone Anfragen\n","2. **batch()**: Verarbeitet mehrere Eingaben parallel\n","3. **stream()**: Gibt Teilergebnisse zur√ºck, sobald sie verf√ºgbar sind\n","4. **ainvoke()**: Asynchrone Version von invoke\n","5. **abatch()**: Asynchrone Version von batch\n","6. **astream()**: Asynchrone Version von stream\n","\n"],"metadata":{"id":"CjFvHqWA3S2V"},"id":"CjFvHqWA3S2V"},{"cell_type":"markdown","source":["Hier ein einfaches Beispiel mit den verschiedenen Aufrufmethoden:\n"],"metadata":{"id":"a9zHZAMP3bCL"},"id":"a9zHZAMP3bCL"},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers.string import StrOutputParser\n","\n","# Einfaches Template erstellen\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"human\", \"{user_input}\")\n","])\n","\n","# Runnable-Kette erstellen\n","chain = prompt | llm | parser"],"metadata":{"id":"kQTKHK2D4DBb"},"id":"kQTKHK2D4DBb","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. invoke - Einzelne Anfrage\n","response = chain.invoke({\"user_input\": \"Was ist ein Runnable in LangChain?\"})\n","\n","mprint(\"## üì£ Invoke response:\")\n","mprint(\"---\")\n","mprint(response)"],"metadata":{"id":"TVWtHSYw5Jla"},"id":"TVWtHSYw5Jla","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. batch - Mehrere Anfragen parallel\n","response = chain.batch([\n","    {\"user_input\": \"Erkl√§re den Begriff LCEL kurz.\"},\n","    {\"user_input\": \"Was sind die Hauptvorteile von Runnables?\"}\n","])\n","mprint(\"##  üì£ Batch response::\")\n","mprint(\"---\")\n","for i, ergebnis in enumerate(response, 1):\n","    mprint(f\"Anfrage {i}: {response[:50]}...\\n\")"],"metadata":{"id":"UAet4KSq5K2D"},"id":"UAet4KSq5K2D","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. stream - Schrittweise Ausgabe\n","mprint(\"##  üì£ Stream response:\")\n","mprint(\"---\")\n","for chunk in chain.stream({\"user_input\": \"Erkl√§re die Abk√ºrzung LCEL kurz.\"}):\n","    print(chunk, end=\" ‚û° \")  # Zeichen trennt die via stream gelieferten Ergebnisse/Ausgaben\n","print(\"\\n\")"],"metadata":{"id":"VJn4e_IX5L-b"},"id":"VJn4e_IX5L-b","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"F_5gn03b46rC","metadata":{"id":"F_5gn03b46rC"},"source":["# A | Aufgabe\n","---"]},{"cell_type":"markdown","id":"36LyGMmo5jb3","metadata":{"id":"36LyGMmo5jb3"},"source":["Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen."]},{"cell_type":"markdown","id":"Je-szbGU7IJ-","metadata":{"id":"Je-szbGU7IJ-"},"source":["\n","<p><font color='black' size=\"5\">\n","Erstellen einer LangChain-Kette zur Textanalyse\n","</font></p>"]},{"cell_type":"markdown","id":"kevePvA-7PzW","metadata":{"id":"kevePvA-7PzW"},"source":["Entwickeln Sie eine Verarbeitungskette, die einen Text analysiert und verschiedene Informationen dar√ºber extrahiert.\n","\n","**Schritte:**\n","1. Erstelle ein ChatPromptTemplate, das ein LLM anweist, einen Text zu analysieren\n","2. Die Analyse soll folgende Aspekte umfassen:\n","   - Hauptthema des Textes\n","   - Tonalit√§t (formal, informell, etc.)\n","   - Schl√ºsselw√∂rter (5-10)\n","   - Kurze Zusammenfassung (max. 3 S√§tze)\n","3. Formatiere die Ausgabe strukturiert mit Markdown-√úberschriften\n","4. Teste die Kette mit mindestens zwei verschiedenen Texten\n","\n","\n"]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["91ee89a4-477b-41ce-a511-aadd4c2fd547","fGV8FwGmIwmR","EouwC_wdZb-c","BMPwDNTCCOfx","-JwJD0rLIyT8","HqTUeMW20084","8akKmCCc06nm","Ps344Ejw0-ih","pt37u3cnbgc","8c859b9f","c314ca71","LVqVL9CWRXU7","89XLSUAvCE8P","5efri5ponin","KRqrnpMx3Hwf","F_5gn03b46rC"],"toc_visible":true},"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":5}
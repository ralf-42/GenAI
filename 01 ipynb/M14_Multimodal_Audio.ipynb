{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"collapsed_sections":["oYvUY6gMBKO1","B27m4yz8h8cu","wRvUmkuGh4Qm","44jdmbAHipr3","uE5RXwpTjPcw","N7ofsiPv12qi","xx2rVTSB4yiM"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<p><font size=\"6\" color='grey'> <b>\n","Anwendung Generativer KI\n","</b></font> </br></p>"],"metadata":{"id":"Ih2CTVBnArVZ"}},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b>\n","Multimodal - Audio\n","</b></font> </br></p>\n","\n","\n","---"],"metadata":{"id":"6jJZ7wbdArVc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfdhPIzcEYRG","cellView":"form","collapsed":true},"outputs":[],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  Colab-Umfeld</font> </br></p>\n","# Installierte Python Version\n","import sys\n","print(f\"Python Version: \",sys.version)\n","\n","# Installierte LangChain Bibliotheken\n","print()\n","print(\"Installierte LangChain Bibliotheken:\")\n","!pip list | grep '^langchain'\n","# Unterdr√ºckt die \"DeprecationWarning\" von LangChain f√ºr die Memory-Funktionden\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")"]},{"cell_type":"code","source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  SetUp API-Keys (setup_api_keys)</font> </br></p>\n","def setup_api_keys():\n","    \"\"\"Konfiguriert alle ben√∂tigten API-Keys aus Google Colab userdata\"\"\"\n","    from google.colab import userdata\n","    import os\n","    from os import environ\n","\n","    # Dictionary der ben√∂tigten API-Keys\n","    keys = {\n","        'OPENAI_API_KEY': 'OPENAI_API_KEY',\n","        'HF_TOKEN': 'HF_TOKEN',\n","        # Weitere Keys bei Bedarf\n","    }\n","\n","    # Keys in Umgebungsvariablen setzen\n","    for env_var, key_name in keys.items():\n","        environ[env_var] = userdata.get(key_name)\n","\n","    return {k: environ[k] for k in keys.keys()}\n","\n","# Verwendung\n","all_keys = setup_api_keys()\n","# Bei Bedarf einzelne Keys direkt zugreifen\n","# WEATHER_API_KEY = all_keys['WEATHER_API_KEY']"],"metadata":{"cellView":"form","id":"WD3Wwr6sESX8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1 | Einf√ºhrung\n","---"],"metadata":{"id":"oYvUY6gMBKO1"}},{"cell_type":"markdown","metadata":{"id":"v2MPPX0c1pHi"},"source":["Audio spielt eine zentrale Rolle in vielen Anwendungen, sei es in der Sprachsteuerung, der automatischen Spracherkennung oder der Analyse von Inhalten. Durch moderne KI-Technologien k√∂nnen gesprochene Inhalte effizient verarbeitet und ausgewertet werden. In diesem Abschnitt werden vier wesentliche Anwendungsbereiche der Audioverarbeitung vorgestellt:  \n","\n","- **Sprach-zu-Text (Speech-to-Text, STT)**: Gesprochene Sprache wird in Text umgewandelt, um Gespr√§che, Interviews oder Vortr√§ge schriftlich festzuhalten. Dies erleichtert die Dokumentation und Weiterverarbeitung von gesprochenen Inhalten.  \n","- **Sprachanalyse**: Durch die Analyse von Inhalten, Tonlage, Betonung und Sprachmelodie kann die emotionale Stimmung in einer Aufnahme erkannt werden. Diese Methode wird h√§ufig in der Kundenkommunikation, im Marketing oder in der psychologischen Forschung eingesetzt.  \n","- **Audio-Summary**: Wichtige Informationen aus langen Audiodateien werden extrahiert und in einer kompakten Form dargestellt. Dies erm√∂glicht eine schnelle Erfassung der Kernaussage, ohne das gesamte Material anh√∂ren zu m√ºssen.  \n","- **Text-zu-Sprache (Text-to-Speech, TTS)**: Geschriebene Inhalte werden in nat√ºrlich klingende Sprache umgewandelt. Dies findet Anwendung in H√∂rb√ºchern, Sprachassistenten oder barrierefreien L√∂sungen. Ein popul√§res Beispiel ist die automatische Erstellung von Podcasts aus schriftlichen Artikeln, wodurch Texte einem breiteren Publikum zug√§nglich gemacht werden.  \n","\n","Diese Techniken erleichtern die Verarbeitung und Nutzung gro√üer Mengen gesprochener Inhalte und er√∂ffnen neue M√∂glichkeiten f√ºr die Automatisierung und Analyse von Audio-Daten.  \n","\n","---\n","\n","Die Begriffe **Speech-to-Text (STT)** und **Text-to-Speech (TTS)** sind g√§ngige Fachausdr√ºcke, die in der KI- und Audioverarbeitung h√§ufig verwendet werden. Auch ‚ÄûSprachanalyse‚Äú und ‚ÄûStimmungsdetektion‚Äú sind gebr√§uchlicher als ‚ÄûAudio-Stimmungsanalyse‚Äú. Die Anpassungen sorgen f√ºr mehr Klarheit und eine breitere Verst√§ndlichkeit.\n","\n"]},{"cell_type":"markdown","source":["# 2 | Speech-to-Text (STT)\n","---"],"metadata":{"id":"B27m4yz8h8cu"}},{"cell_type":"markdown","source":["Die Umwandlung gesprochener Sprache in Text, bekannt als Audio-Transkription, erm√∂glicht die automatische Verschriftlichung von Gespr√§chen, Interviews oder Vortr√§gen. Dies erleichtert die Dokumentation, Archivierung und Weiterverarbeitung von Audioinhalten. Moderne Spracherkennungstechnologien nutzen KI-Modelle, um Sprache pr√§zise zu erfassen und in lesbaren Text umzuwandeln.  \n","\n","Transkriptionen sind in vielen Bereichen von Bedeutung ‚Äì von der journalistischen Berichterstattung √ºber die Erstellung von Untertiteln bis hin zur Analyse von Kundenfeedback. Durch den Einsatz automatisierter Systeme kann dieser Prozess erheblich beschleunigt werden, wodurch gro√üe Mengen an Audio effizient verarbeitet werden k√∂nnen."],"metadata":{"id":"sV3WQC5OyF5i"}},{"cell_type":"code","source":["import openai\n","from google.colab import files\n","from IPython.display import display, Markdown\n","\n","# Nutzer w√§hlt eine Datei aus und l√§dt sie hoch\n","uploaded = files.upload()\n","\n","# Den ersten Dateinamen extrahieren\n","audio_file_path = list(uploaded.keys())[0]\n","\n","# Audiodatei √∂ffnen und Whisper nutzen\n","with open(audio_file_path, \"rb\") as audio_file:\n","    response = openai.audio.transcriptions.create(\n","        model=\"whisper-1\",\n","        file=audio_file\n","    )\n","\n","# Markdown-Formatierte Ausgabe\n","markdown_output = f\"\"\"\n","# üéôÔ∏è Transkript der Audiodatei\n","\n","**üìÇ Datei:** `{audio_file_path}`\n","**ü§ñ Modell:** `Whisper-1`\n","\n","---\n","\n","{response.text}\n","\"\"\"\n","\n","# Markdown in Google Colab anzeigen\n","display(Markdown(markdown_output))"],"metadata":{"id":"FcVxaTwgf8ze"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# 3 | Sprachanalyse\n","---"],"metadata":{"id":"wRvUmkuGh4Qm"}},{"cell_type":"markdown","source":["Durch die Analyse von Inhalten, Tonlage, Betonung und Sprachmelodie kann die emotionale Stimmung in einer Aufnahme erkannt werden. Die Fokus in diesem Modul ist die Stimmungsanalyse aus Texten, auch **Sentiment Analysis** genannt, sie ist eine Methode zur automatischen Erkennung der emotionalen Haltung in geschriebenen Inhalten. Mithilfe nat√ºrlicher Sprachverarbeitung (NLP) werden Texte auf positive, neutrale oder negative Stimmungen untersucht. Diese Technik wird h√§ufig im Kundenfeedback, in sozialen Medien oder in Produktbewertungen eingesetzt, um Meinungen systematisch auszuwerten.  \n","\n","Moderne KI-Modelle analysieren nicht nur einzelne W√∂rter, sondern ber√ºcksichtigen auch den Kontext, um Stimmungen pr√§ziser zu erfassen. Unternehmen nutzen diese Technologie, um Trends zu erkennen, Stimmungen zu messen oder die Kundenzufriedenheit zu verbessern."],"metadata":{"id":"GZ70N-_-zIQy"}},{"cell_type":"code","source":["import openai\n","from google.colab import files\n","from IPython.display import display, Markdown\n","\n","# üìÇ Nutzer l√§dt eine Audiodatei hoch\n","uploaded = files.upload()\n","audio_file_path = list(uploaded.keys())[0]\n","\n","# üéôÔ∏è Audiodatei transkribieren mit Whisper\n","with open(audio_file_path, \"rb\") as audio_file:\n","    response = openai.audio.transcriptions.create(\n","        model=\"whisper-1\",\n","        file=audio_file\n","    )\n","\n","# üìù Transkribierter Text\n","transcribed_text = response.text\n","\n","# üß† Stimmungsanalyse mit GPT-4o-mini\n","sentiment_prompt = f\"\"\"\n","Analysiere die Stimmung des folgenden Textes und gib eine klare Bewertung zur√ºck:\n","- Positiv üòä\n","- Neutral üòê\n","- Negativ üòû\n","\n","Falls m√∂glich, gib eine kurze Begr√ºndung f√ºr deine Einsch√§tzung.\n","\n","### Text:\n","{transcribed_text}\n","\"\"\"\n","\n","gpt_response = openai.chat.completions.create(\n","    model=\"gpt-4o-mini\",\n","    messages=[{\"role\": \"system\", \"content\": \"Du bist ein Experte f√ºr Stimmungsanalysen.\"},\n","              {\"role\": \"user\", \"content\": sentiment_prompt}],\n","    temperature=0.2\n",")\n","\n","# üí° GPT-4o-mini-Antwort extrahieren\n","sentiment_result = gpt_response.choices[0].message.content\n","\n","# üìú Markdown-Formatierte Ausgabe in Google Colab\n","markdown_output = f\"\"\"\n","# üéôÔ∏è Transkript der Audiodatei\n","\n","**üìÇ Datei:** `{audio_file_path}`\n","**ü§ñ Modell:** `Whisper-1`\n","\n","---\n","\n","{transcribed_text}\n","\n","---\n","\n","## üßê Stimmungsanalyse mit GPT-4o-mini:\n","{sentiment_result}\n","\"\"\"\n","\n","# üìå Markdown in Google Colab anzeigen\n","display(Markdown(markdown_output))"],"metadata":{"id":"EHv1PSywfSzo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4 | Audio-Summary\n","---"],"metadata":{"id":"44jdmbAHipr3"}},{"cell_type":"markdown","source":["Die automatische Audio-Zusammenfassung erm√∂glicht es, lange Sprachaufnahmen auf ihre wesentlichen Inhalte zu reduzieren. Mithilfe von KI-gest√ºtzten Algorithmen werden zentrale Aussagen erkannt und in kompakter Form wiedergegeben. Dies erleichtert die schnelle Erfassung wichtiger Informationen, ohne das gesamte Audio anh√∂ren zu m√ºssen.  \n","\n","Diese Technik wird h√§ufig f√ºr Podcasts, Meetings oder Interviews eingesetzt, um relevante Inhalte effizient zusammenzufassen. Durch den Einsatz moderner Spracherkennung und nat√ºrlicher Sprachverarbeitung (NLP) k√∂nnen pr√§zise und verst√§ndliche Zusammenfassungen erstellt werden."],"metadata":{"id":"NvSnvX0fz1Nu"}},{"cell_type":"code","source":["import openai\n","from google.colab import files\n","from IPython.display import display, Markdown\n","\n","# üìÇ Nutzer l√§dt eine Audiodatei hoch\n","uploaded = files.upload()\n","audio_file_path = list(uploaded.keys())[0]\n","\n","# üéôÔ∏è Audiodatei transkribieren mit Whisper\n","with open(audio_file_path, \"rb\") as audio_file:\n","    response = openai.audio.transcriptions.create(\n","        model=\"whisper-1\",\n","        file=audio_file\n","    )\n","\n","# üìù Transkribierter Text\n","transcribed_text = response.text\n","\n","# üß† GPT-4o-mini: Zusammenfassung\n","gpt_prompt = f\"\"\"\n","Erstelle eine kurze Zusammenfassung des folgenden Textes. Die Zusammenfassung sollte maximal 3-4 S√§tze umfassen und die Hauptpunkte klar darstellen.\n","\n","### Transkript:\n","{transcribed_text}\n","\"\"\"\n","\n","gpt_response = openai.chat.completions.create(\n","    model=\"gpt-4o-mini\",\n","    messages=[{\"role\": \"system\", \"content\": \"Du bist ein Experte f√ºr Textzusammenfassungen.\"},\n","              {\"role\": \"user\", \"content\": gpt_prompt}],\n","    temperature=0.2\n",")\n","\n","# üí° GPT-4o-mini-Antwort extrahieren\n","summary_result = gpt_response.choices[0].message.content\n","\n","# üìú Markdown-Formatierte Ausgabe in Google Colab\n","markdown_output = f\"\"\"\n","# üéôÔ∏è Transkript der Audiodatei\n","\n","**üìÇ Datei:** `{audio_file_path}`\n","**ü§ñ Modell:** `Whisper-1`\n","\n","---\n","\n","{transcribed_text}\n","\n","---\n","\n","## ‚úçÔ∏è Zusammenfassung mit GPT-4o-mini:\n","\n","{summary_result}\n","\"\"\"\n","\n","# üìå Markdown in Google Colab anzeigen\n","display(Markdown(markdown_output))\n"],"metadata":{"id":"4JTY4ikLis7Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5 | Text-to-Speech (TTS)\n","---"],"metadata":{"id":"uE5RXwpTjPcw"}},{"cell_type":"markdown","source":["Fokus ist die Anwendung von Text-to-Speech (TTS) Large Language Models (LLMs) mit den fortschrittlichen L√∂sungen von OpenAI. Im Mittelpunkt steht das Modell TTS-1, ein leistungsf√§higes und vielseitiges System zur Umwandlung von geschriebenem Text in nat√ºrliche Sprache. TTS-1 wurde f√ºr den Echtzeiteinsatz optimiert und eignet sich daher besonders f√ºr Anwendungen, die eine geringe Latenz erfordern. Das Modell repr√§sentiert einen bedeutenden Fortschritt in der Sprachsynthese, da es mithilfe von Deep-Learning-Techniken qualitativ hochwertige und realistisch klingende Sprachausgaben erzeugt. Im Rahmen dieses Moduls werden die Funktionalit√§ten von TTS-1 untersucht, seine praktischen Einsatzm√∂glichkeiten analysiert und sein Einfluss auf verschiedene Branchen betrachtet ‚Äì von barrierefreien Kommunikationsl√∂sungen bis hin zu interaktiven Sprachantwortsystemen."],"metadata":{"id":"bzlL3ITv0a-b"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Stimmen\n","</font></p>"],"metadata":{"id":"Ave6gDyM1hkw"}},{"cell_type":"markdown","source":["Dieser Codeausschnitt demonstriert die Nutzung der Text-to-Speech-API von OpenAI zur Erzeugung gesprochener Sprache aus Text. Zun√§chst werden die ben√∂tigten Bibliotheken importiert: **openai** f√ºr die API-Kommunikation, **IPython.display** zur Audiowiedergabe in Jupyter-Notebooks und **base64** f√ºr die Kodierung. Die Variable `TEXT` enth√§lt den zu konvertierenden Text. Anschlie√üend wird die Funktion `openai.audio.speech.create()` mit drei Parametern aufgerufen: dem Modell (`\"tts-1\"`), der gew√§hlten Stimme (`\"alloy\"`) und dem Eingabetext.  \n","\n","OpenAI stellt mehrere Stimmen mit unterschiedlichen Charakteristiken zur Verf√ºgung:  \n","\n","* **Alloy** ‚Äì neutral  \n","* **Echo** ‚Äì jugendlich  \n","* **Fable** ‚Äì m√§nnlich  \n","* **Onyx** ‚Äì tiefe m√§nnliche Stimme  \n","* **Nova** ‚Äì weiblich  \n","* **Shimmer** ‚Äì warme weibliche Stimme  \n","\n","Jede dieser Stimmen hat eigene klangliche Eigenschaften, sodass je nach Anwendung die passende ausgew√§hlt werden kann. Zudem bietet OpenAI mit `\"tts-1-hd\"` eine hochaufl√∂sende Variante f√ºr eine verbesserte Audioqualit√§t, die jedoch mit einer h√∂heren Latenz verbunden sein kann. Die Funktion liefert ein Antwortobjekt zur√ºck, aus dem der generierte Audioinhalt extrahiert und in der Variable `audio_data` zur weiteren Verarbeitung oder Wiedergabe gespeichert wird."],"metadata":{"id":"jot1zKM71Mvn"}},{"cell_type":"code","source":["import openai\n","from IPython.display import display, Audio\n","\n","# üìù Text\n","summary_text = \"\"\"\n","In dem Gedicht reitet ein Vater mit seinem Sohn durch die Nacht, w√§hrend der Junge von dem Erlk√∂nig tr√§umt, der ihn mit verlockenden Angeboten anlockt. Der Vater versucht, seinen Sohn zu beruhigen und erkl√§rt, dass die Erscheinungen nur Nebel sind. Doch der Erlk√∂nig wird bedrohlich und fordert den Jungen, was schlie√ülich zu einem tragischen Ende f√ºhrt, als der Vater mit seinem tot geglaubten Kind nach Hause zur√ºckkehrt.\n","\"\"\"\n","\n","# üé§ Verf√ºgbare OpenAI-Stimmen\n","voices = [\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"]\n","\n","# üìÇ Audiodateien f√ºr jede Stimme generieren und speichern\n","audio_files = []\n","for voice in voices:\n","    tts_response = openai.audio.speech.create(\n","        model=\"tts-1\",\n","        voice=voice,\n","        input=summary_text\n","    )\n","\n","    # üìÇ Speichern der Audiodatei\n","    audio_filename = f\"summary_{voice}.mp3\"\n","    with open(audio_filename, \"wb\") as audio_file:\n","        audio_file.write(tts_response.content)\n","\n","    audio_files.append(audio_filename)\n","\n","# üîä Ausgabe der Audiodateien\n","for file in audio_files:\n","    print(f\"üéß {file} erstellt und abspielbar!\")\n","    display(Audio(file, autoplay=False))\n","    print()"],"metadata":{"id":"zb8mIeNIjS4I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6 | Audio-Pipeline\n","---"],"metadata":{"id":"N7ofsiPv12qi"}},{"cell_type":"markdown","metadata":{"id":"8zQRIrJrLxj3"},"source":["Eine Audio-Pipleine kombiniert verschiedene Verarbeitungsschritte, um gesprochene Inhalte effizient zu analysieren, zu verarbeiten und bereitzustellen. Durch die Verkn√ºpfung mehrerer Technologien entstehen leistungsstarke Anwendungen, die Sprache in mehreren Stufen umwandeln und auswerten k√∂nnen.\n","\n","`Audio ‚Üí STT ‚Üí Chat ‚Üí TTS  `\n","\n","**Hinweis:**\n","\n","Die hier vorgestellte Technik wird genutzt, um Audioaufnahmen in Colab zu erstellen und wurde mit ChatGPT und [Link](https://gist.github.com/korakot/c21c3476c024ad6d56d5f48b0bca92be) erstellt.\n","\n"]},{"cell_type":"code","source":["!uv pip install --system -q pydub gradio"],"metadata":{"id":"Qz9kZen2oVlt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Audio aufzeichnen\n","</font></p>"],"metadata":{"id":"1hpBx1-mu7Qt"}},{"cell_type":"markdown","source":["Dieser Code erstellt eine einfache Audioaufnahme-Anwendung mit Gradio. Hier ist was er macht:\n","\n","1. Der Code importiert ben√∂tigte Bibliotheken:\n","   - `gradio` f√ºr die Erstellung der Benutzeroberfl√§che\n","   - `scipy.io.wavfile` zum Speichern von Audiodateien\n","   - `numpy` f√ºr die Verarbeitung numerischer Daten\n","   - `files` aus Google Colab zum Herunterladen von Dateien\n","\n","2. Die Funktion `aufnehmen_und_speichern()` verarbeitet Audiodaten:\n","   - Sie pr√ºft, ob Audio aufgenommen wurde\n","   - Bei vorhandenen Audiodaten extrahiert sie die Abtastrate und die Audiodaten\n","   - Speichert die Aufnahme als \"audio.wav\"\n","   - L√∂st den Download der Datei aus (in Google Colab)\n","   - Gibt die Audiodaten zur√ºck, damit sie in der Oberfl√§che abgespielt werden k√∂nnen\n","\n","3. Eine Gradio-Oberfl√§che wird erstellt mit:\n","   - Einem Audio-Input (Mikrofon) f√ºr die Aufnahme\n","   - Einem Audio-Output zum Abspielen der aufgenommenen Datei\n","   - Einem Titel und einer Beschreibung\n","\n","4. Schlie√ülich wird die Anwendung mit `demo.launch()` gestartet\n","\n","Zusammengefasst ist dies eine kompakte Webanwendung, die Audio aufnimmt, es speichert, automatisch zum Download anbietet und gleichzeitig die M√∂glichkeit bietet, die Aufnahme anzuh√∂ren."],"metadata":{"id":"OpFZmA_K4Id8"}},{"cell_type":"code","source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  Starten vor Gradio - keine Aufnahme</font> </br></p>\n","#\n","# Walkaround Gradio/Colab nach der Ausf√ºhrung funktioniert auch Gradio Mikro üòäüòä\n","# Code erstellt keine Audio-Aufnahme\n","#\n","from IPython.display import Javascript\n","from google.colab import output\n","from base64 import b64decode\n","import io\n","from IPython.display import Audio\n","from pydub import AudioSegment\n","\n","RECORD = \"\"\"\n","const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n","const b2text = blob => new Promise(resolve => {\n","    const reader = new FileReader()\n","    reader.onloadend = e => resolve(e.srcElement.result)\n","    reader.readAsDataURL(blob)\n","})\n","var record = time => new Promise(async resolve => {\n","    stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n","    recorder = new MediaRecorder(stream)\n","    chunks = []\n","    recorder.ondataavailable = e => chunks.push(e.data)\n","    recorder.start()\n","    await sleep(time)\n","    recorder.onstop = async ()=>{\n","        blob = new Blob(chunks)\n","        text = await b2text(blob)\n","        resolve(text)\n","    }\n","    recorder.stop()\n","})\n","\"\"\"\n","\n","def record(seconds=3):\n","    # input(\"Start nach Enter f√ºr 5 Sekunden.\")\n","    # print(f\"Aufzeichnung l√§uft jetzt f√ºr {seconds} Sekunden. Bitte sprechen!\")\n","    display(Javascript(RECORD))\n","    s = output.eval_js('record(%d)' % (seconds * 1000))\n","    binary = b64decode(s.split(',')[1])\n","\n","    # In AudioSegment konvertieren\n","    audio = AudioSegment.from_file(io.BytesIO(binary), format=\"webm\")\n","\n","    # Als WAV exportieren\n","    # audio.export(\"audio.wav\", format=\"wav\")\n","    # print(\"Recording done.\")\n","    return audio\n","\n","# Nehmen Sie 5 Sekunden Audio auf\n","audio = record(1)"],"metadata":{"cellView":"form","id":"ZvOJsLP7wgIE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gradio as gr\n","import scipy.io.wavfile as wav\n","import numpy as np\n","from google.colab import files\n","from IPython.display import display, HTML, Audio\n","\n","\n","def aufnehmen_und_speichern(audio):\n","    if audio is not None:\n","        # audio ist ein Tupel (sample_rate, data)\n","        sample_rate, data = audio\n","\n","        # Speichern der aufgenommenen Datei\n","        wav.write(\"audio.wav\", sample_rate, data)\n","\n","        # Download ausl√∂sen\n","        files.download(\"audio.wav\")\n","\n","        # Wichtig: Die aufgenommene Audiodatei korrekt zur√ºckgeben\n","        return (sample_rate, data)\n","    else:\n","        return None\n","\n","demo = gr.Interface(\n","    fn=aufnehmen_und_speichern,\n","    inputs=gr.Audio(sources=\"microphone\", type=\"numpy\"),\n","    outputs=gr.Audio(type=\"numpy\"),  # Setze den gleichen Typ f√ºr Output\n","    title=\"üéôÔ∏è Audio Aufnahme\",\n","    description=\"Nimm ein Audio auf und und speichert es.\",\n","    flagging_mode=\"never\"\n",")\n","demo.launch()"],"metadata":{"collapsed":true,"id":"zc8Yg0gDZn1S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","STT\n","</font></p>"],"metadata":{"id":"8PpQ-h_4u2kn"}},{"cell_type":"markdown","metadata":{"id":"jurTAwUCKRM-"},"source":["Dieser Code verwendet OpenAI's Whisper-Modell, um eine automatische Transkription einer Audiodatei durchzuf√ºhren."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qrf898AGONJ6"},"outputs":[],"source":["from IPython.display import display, Markdown, Audio\n","from openai import OpenAI\n","\n","client = OpenAI()\n","\n","def transcribe_audio(filename):\n","    with open(filename, \"rb\") as audio_file:\n","        transcription = client.audio.transcriptions.create(\n","            model=\"whisper-1\",\n","            file=audio_file\n","        )\n","    return transcription.text\n","\n","# Transkribieren Sie die aufgezeichneten Audiodaten\n","transcription = transcribe_audio(\"/content/audio.wav\")\n","\n","display(Markdown(\"## üìÑ Transkribiert:\"))\n","display(Markdown(f\"{transcription}\"))"]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Chat\n","</font></p>"],"metadata":{"id":"KP-QD1--RhNE"}},{"cell_type":"code","source":["def query_llm(prompt):\n","  response = client.chat.completions.create(\n","      model=\"gpt-4o-mini\",  # Use the specified model\n","      messages=[{\"role\": \"user\", \"content\": prompt}],\n","      temperature=0.2  # Adjust temperature as needed\n","  )\n","  return response.choices[0].message.content\n","\n","#\n","user_prompt = transcription\n","llm_response = query_llm(user_prompt)\n","\n","display(Markdown(\"## ü§ñ Chat\"))\n","display(Markdown(f\"**Prompt:** {user_prompt}\"))\n","display(Markdown(f\"**LLM Antwort:** {llm_response}\"))"],"metadata":{"id":"lUeuQi8fRmr6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","TTS\n","</font></p>"],"metadata":{"id":"LUF4xhCi6os4"}},{"cell_type":"markdown","source":["Der Code nimmt einen Text als Eingabe, wandelt ihn mithilfe eines KI-gest√ºtzten Text-to-Speech-Modells (`tts-1`) in gesprochene Sprache um und gibt die generierte Audiodatei aus."],"metadata":{"id":"8Oj6aF_y7gIQ"}},{"cell_type":"code","source":["def generate_text(text):\n","    response = client.audio.speech.create(\n","        model=\"tts-1\",\n","        voice=\"onyx\",\n","        input=text\n","    )\n","    audio_data = response.content\n","    return audio_data  # Audiodaten direkt zur√ºckgeben\n","\n","def speak_text(text):\n","    audio_data = generate_text(text)\n","    display(Markdown(\"## üîä ... Speech\"))\n","    display(Audio(audio_data, autoplay=True))\n","\n","display(Markdown(\"## üìÑ Text to ...\"))\n","display(Markdown(f\"---\"))\n","display(Markdown(f\"{llm_response}\"))\n","display(Markdown(f\"---\"))\n","speak_text(llm_response)"],"metadata":{"id":"BZp9rMrG6q5f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# A | Aufgabe\n","---"],"metadata":{"id":"xx2rVTSB4yiM"}},{"cell_type":"markdown","source":["Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen."],"metadata":{"id":"5eOEsXz_6lG0"}},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Sprach√ºbersetzung mit KI\n","</font></p>"],"metadata":{"id":"Iwyc5dhx5zjW"}},{"cell_type":"markdown","source":["Angenommen, eine Reise ins Ausland steht an, und es gilt, sich mit der lokalen Bev√∂lkerung zu verst√§ndigen, obwohl deren Sprache nicht beherrscht wird. Oder es findet eine Videokonferenz mit internationalen Teilnehmern statt, die jeweils in ihrer eigenen Sprache sprechen. In dieser Aufgabe wird eine KI-basierte √úbersetzungs-Pipeline entwickelt, die gesprochene Sprache erfasst, automatisch in eine andere Sprache √ºbertr√§gt und die √úbersetzung als Audiodatei ausgibt."],"metadata":{"id":"6VU7YdK76CLb"}}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["-ID_iebz9wyq","CSGjgTJz-cpi","QH9WDuH8_e8y","9d9cb086","c477fff3","x0S24SQRPc-o","HvLa5GWZfgVH","-47CBLdZbLr0","gRJbfUG9-ujC"],"machine_shape":"hm","gpuType":"L4","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)"],"metadata":{"id":"Ih2CTVBnArVZ"}},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b>\n","Multimodal - Video\n","</b></font> </br></p>\n","\n","\n","---"],"metadata":{"id":"6jJZ7wbdArVc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfdhPIzcEYRG","cellView":"form","collapsed":true},"outputs":[],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  Colab-Umfeld</font> </br></p>\n","# Installierte Python Version\n","import sys\n","print(f\"Python Version: \",sys.version)\n","\n","# Installierte LangChain Bibliotheken\n","print()\n","print(\"Installierte LangChain Bibliotheken:\")\n","!pip list | grep '^langchain'\n","# Unterdr√ºckt die \"DeprecationWarning\" von LangChain f√ºr die Memory-Funktionden\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")"]},{"cell_type":"code","source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  SetUp API-Keys (setup_api_keys)</font> </br></p>\n","def setup_api_keys():\n","    \"\"\"Konfiguriert alle ben√∂tigten API-Keys aus Google Colab userdata\"\"\"\n","    from google.colab import userdata\n","    import os\n","    from os import environ\n","\n","    # Dictionary der ben√∂tigten API-Keys\n","    keys = {\n","        'OPENAI_API_KEY': 'OPENAI_API_KEY',\n","        'HF_TOKEN': 'HF_TOKEN',\n","        # Weitere Keys bei Bedarf\n","    }\n","\n","    # Keys in Umgebungsvariablen setzen\n","    for env_var, key_name in keys.items():\n","        environ[env_var] = userdata.get(key_name)\n","\n","    return {k: environ[k] for k in keys.keys()}\n","\n","# Verwendung\n","all_keys = setup_api_keys()\n","# Bei Bedarf einzelne Keys direkt zugreifen\n","# WEATHER_API_KEY = all_keys['WEATHER_API_KEY']"],"metadata":{"cellView":"form","id":"WD3Wwr6sESX8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# 1 | Einf√ºhrung\n","\n","---"],"metadata":{"id":"-ID_iebz9wyq"}},{"cell_type":"markdown","source":["\n","\n","Video spielt eine zentrale Rolle in vielen Anwendungen, sei es in der Inhaltsanalyse, der automatischen Objekterkennung oder der Generierung von Inhalten. Durch moderne KI-Technologien k√∂nnen Videoinhalte effizient verarbeitet und ausgewertet werden. In diesem Abschnitt werden vier wesentliche Anwendungsbereiche der Videoverarbeitung vorgestellt:\n","\n","- **Video-zu-Text (Video-to-Text, VTT)**: Videoinhalte werden in Text umgewandelt, um visuelle Szenen, Handlungen oder Pr√§sentationen schriftlich festzuhalten. Dies erleichtert die Dokumentation und Weiterverarbeitung von visuellen Inhalten.\n","- **Videoanalyse**: Durch die Analyse von Bildinhalten, Bewegungen, Objekten und Szenen k√∂nnen relevante Informationen aus Videos extrahiert werden. Diese Methode wird h√§ufig in der Sicherheits√ºberwachung, im Marketing oder in der Forschung eingesetzt.\n","- **Video-Summary**: Wichtige Informationen aus langen Videodateien werden extrahiert und in einer kompakten Form dargestellt. Dies erm√∂glicht eine schnelle Erfassung der Kernaussage, ohne das gesamte Material ansehen zu m√ºssen.\n","- **Bild-zu-Video (Image-to-Video, ITV)**: Statische Bilder werden in bewegte Videosequenzen umgewandelt. Diese Technik wird f√ºr die Animation von Illustrationen, die Erstellung dynamischer Erkl√§rvideos oder die visuelle Inszenierung von Konzepten genutzt.\n","\n","Diese Techniken erleichtern die Verarbeitung und Nutzung gro√üer Mengen visueller Inhalte und er√∂ffnen neue M√∂glichkeiten f√ºr die Automatisierung und Analyse von Video-Daten.\n","\n","\n"],"metadata":{"id":"6hHK5izsH-cM"}},{"cell_type":"markdown","source":["# 2 | Video-to-Text (VTT)\n","\n","---"],"metadata":{"id":"CSGjgTJz-cpi"}},{"cell_type":"markdown","source":["\n","\n","Die Umwandlung von Videoinhalten in Text, bekannt als Video-Transkription, erm√∂glicht die automatische Verschriftlichung von Videoszenen, Vortr√§gen oder Handlungen. Dies erleichtert die Dokumentation, Archivierung und Weiterverarbeitung von Videoinhalten. Moderne Bilderkennung und Spracherkennungstechnologien nutzen KI-Modelle, um visuelle und auditive Elemente pr√§zise zu erfassen und in lesbaren Text umzuwandeln.\n","\n","Videotranskriptionen sind in vielen Bereichen von Bedeutung ‚Äì von der Erstellung von Untertiteln und der Zug√§nglichkeit f√ºr Menschen mit Behinderungen √ºber die Erstellung durchsuchbarer Videoinhalte bis hin zur Analyse von Marktforschungsdaten. Durch den Einsatz automatisierter Systeme kann dieser Prozess erheblich beschleunigt werden, wodurch gro√üe Mengen an Videomaterial effizient verarbeitet werden k√∂nnen."],"metadata":{"id":"rZM_rb5ZmAYk"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Version auf Basis Transkript\n","</font></p>"],"metadata":{"id":"cVG8SXbzFa5u"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2192f76c"},"outputs":[],"source":["# Import\n","import openai\n","from google.colab import files\n","from IPython.display import display, Markdown\n","import os\n","\n","# Nutzer w√§hlt eine Datei aus und l√§dt sie hoch\n","uploaded = files.upload()\n","\n","# Den ersten Dateinamen extrahieren\n","video_file_path = list(uploaded.keys())[0]\n","\n","# Tempor√§ren Pfad f√ºr die Audio-Extraktion erstellen\n","audio_path = \"extracted_audio.mp3\"\n","\n","# FFmpeg verwenden, um Audio aus dem Video zu extrahieren\n","!ffmpeg -i \"{video_file_path}\" -q:a 0 -map a \"{audio_path}\" -y\n","\n","# Audiodatei √∂ffnen und Whisper nutzen\n","with open(audio_path, \"rb\") as audio_file:\n","    response = openai.audio.transcriptions.create(\n","        model=\"whisper-1\",\n","        file=audio_file\n","    )"]},{"cell_type":"code","source":["# Markdown-Formatierte Ausgabe\n","markdown_output = f\"\"\"\n","## üé• Transkript des Videos\n","\n","**üìÇ Datei:** `{video_file_path}`\n","**ü§ñ Modell:** `Whisper-1` (f√ºr Audiotranskription)\n","\n","---\n","\n","{response.text}\n","\"\"\"\n","\n","# Markdown in Google Colab anzeigen\n","display(Markdown(markdown_output))\n","\n","# Tempor√§re Audiodatei l√∂schen\n","# os.remove(audio_path)"],"metadata":{"id":"9Ui5P84-SFJF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Version auf Basis Bilder\n","</font></p>"],"metadata":{"id":"bxTaXF99FV6q"}},{"cell_type":"markdown","source":["In Anlehnung an: [Quelle](https://cookbook.openai.com/examples/gpt_with_vision_for_video_understanding)."],"metadata":{"id":"1e4QPJfzFt9s"}},{"cell_type":"code","source":["from IPython.display import display, Image, Markdown\n","from google.colab import files\n","import cv2\n","import base64\n","import time\n","from openai import OpenAI\n","import os\n","import requests\n","\n","client = OpenAI()"],"metadata":{"id":"t9ENnWPNC3KX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Datei-Upload erm√∂glichen\n","uploaded = files.upload()  # √ñffnet einen Dialog zum Datei-Upload\n","\n","# Name der hochgeladenen Datei ermitteln\n","filename = list(uploaded.keys())[0]\n","print(f\"Datei '{filename}' wurde hochgeladen.\")\n","\n","# Video mit OpenCV √∂ffnen\n","video = cv2.VideoCapture(filename)\n","\n","# Frames in base64 umwandeln\n","base64Frames = []\n","while video.isOpened():\n","    success, frame = video.read()\n","    if not success:\n","        break\n","    _, buffer = cv2.imencode(\".jpg\", frame)\n","    base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n","\n","# Video-Ressourcen freigeben\n","video.release()\n","print(len(base64Frames), \"Frames wurden eingelesen.\")"],"metadata":{"id":"jtcGOhf8DALD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prompt und Chat-Aufruf\n","PROMPT_MESSAGES = [\n","    {\n","        \"role\": \"user\",\n","        \"content\": [\n","            \"These are frames from a video that I want to upload. Generate a compelling description that I can upload along with the video. Please answer in German.\",\n","            *map(lambda x: {\"image\": x, \"resize\": 768}, base64Frames[0::50]),\n","        ],\n","    },\n","]\n","params = {\n","    \"model\": \"gpt-4o-mini\",\n","    \"messages\": PROMPT_MESSAGES,\n","    \"max_tokens\": 200,\n","}\n","\n","result = client.chat.completions.create(**params)"],"metadata":{"id":"U1xP9p6SDTcW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Markdown-Formatierte Ausgabe\n","markdown_output = f\"\"\"\n","## üé• Analyse des Videos auf Basis Frames\n","\n","**üìÇ Datei:** `{filename}`\n","**ü§ñ Modell:** gpt-4o-mini\n","\n","---\n","\"\"\"\n","\n","display(Markdown(markdown_output))\n","display(Markdown(result.choices[0].message.content))"],"metadata":{"id":"1_BD4Al9SsLf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3 | Image-to-Video (I2V)\n","\n","---"],"metadata":{"id":"QH9WDuH8_e8y"}},{"cell_type":"markdown","source":["Image-zu-Video (I2V) mit generativer KI erm√∂glicht die Erstellung realistischer Videos auf Basis einzelner Bilder oder Bildsequenzen. Modelle wie Sora (OpenAI), Gen-2 (Runway) und Lumiere (Google) verwandeln statische Bilder in dynamische, visuell konsistente Videoclips ‚Äì h√§ufig erg√§nzt durch passende Audiospuren oder die M√∂glichkeit, eigenen Ton hinzuzuf√ºgen.\n","\n","Diese KI-Systeme nutzen multimodale Architekturen und Deep Learning, um Bewegungen, Kamerafahrten und visuelle Effekte glaubw√ºrdig zu simulieren. Anwendungsfelder sind u.‚ÄØa. Marketing, Bildung, digitale Kunst und Simulation.\n","\n","F√ºhrende I2V-Modelle:\n","\n","+ Sora (OpenAI) ‚Äì Erzeugt aus Bildern und Bildfolgen realistische Szenen mit hoher Koh√§renz.\n","\n","+ Gen-2 (Runway) ‚Äì Unterst√ºtzt Bild-, Text- und Hybridprompts f√ºr vielseitige Anwendungen.\n","\n","+ Pika Labs ‚Äì Ideal f√ºr einfache Animationen aus Einzelbildern.\n","\n","+ Stable Video Diffusion ‚Äì Quelloffen, mit wachsender Community.\n","\n","+ Lumiere (Google) ‚Äì Experimentell, mit Fokus auf Bewegungsdynamik und Stil√ºbertragung.\n","\n","+ Hedra ‚Äì Erm√∂glicht die Erstellung von Videos aus statischen Bildern mit synchronisiertem Audio, ideal f√ºr die Generierung sprechender oder singender Avatare.\n","\n","+ HeyGen ‚Äì Spezialisiert auf die schnelle Erstellung personalisierter, sprechender Avatare aus Fotos oder Videos. Bietet pr√§zise Lippensynchronisation, Voice-Cloning, Text-zu-Video und Video√ºbersetzung in √ºber 40 Sprachen. Besonders geeignet f√ºr Marketing, E-Learning und Social Media; erm√∂glicht Multi-Avatar-Videos und individuelle Anpassungen bis hin zu 4K-Qualit√§t.\n","\n","\n","Viele Tools bieten zus√§tzlich Funktionen zur Audiointegration, etwa durch automatische Tonunterlegung oder individuelle Sprachkommentare, um das Ergebnis noch lebendiger zu gestalten."],"metadata":{"id":"cw-XodgbBJ43"}},{"cell_type":"markdown","source":["Hier ist eine √úbersicht, welche der genannten Modelle mit/ohne **API-Zugang** nutzbar sind:\n","\n","| Modell                   | API-Zugang m√∂glich?               | Hinweise |\n","| :--                      | :--                              | :-- |\n","| **Sora (OpenAI)**        | Eingeschr√§nkt, nur nach Bewerbung | Sora ist nicht √∂ffentlich per API verf√ºgbar. Zugang gibt es nur nach individueller Bewerbung und Freischaltung durch OpenAI. |\n","| **Gen-2 (Runway)**       | Ja                               | Runway bietet eine API f√ºr Gen-3 und Gen-4 Modelle, die f√ºr Entwickler offen zug√§nglich ist. |\n","| **Pika Labs**            | Ja                               | Die Pika API ist verf√ºgbar, allerdings mit bestimmten Einschr√§nkungen und nach Registrierung. |\n","| **Stable Video Diffusion** | Ja                             | √úber die Developer Platform von Stability AI ist eine API f√ºr Stable Video Diffusion verf√ºgbar. |\n","| **Lumiere (Google)**     | Nein, aktuell nicht              | Lumiere befindet sich noch in der Entwicklung und ist nicht √∂ffentlich oder per API nutzbar. |\n","| **Hedra**                | Nein, aktuell nicht              | Hedra ist als Web-App nutzbar, aber es gibt derzeit keine √∂ffentlich dokumentierte API. |\n","| **HeyGen**               | Ja                               | HeyGen bietet einen dokumentierten API-Zugang f√ºr Video- und Avatar-Erstellung. Authentifizierung erfolgt per API-Key oder OAuth2. Die API ist f√ºr Entwickler offen zug√§nglich, Details in der offiziellen Doku[1][3][8]. |\n","\n","\n","**Zusammengefasst:**  \n","Per API lassen sich aktuell **Gen-2 (Runway)**, **Pika Labs**, **Stable Video Diffusion** und **HeyGen** direkt nutzen.  \n","**Sora** ist nur nach individueller Bewerbung und Freischaltung per API zug√§nglich.  \n","**Lumiere** und **Hedra** bieten derzeit keinen √∂ffentlichen API-Zugang.\n","\n","\n","Stand: 05.2025"],"metadata":{"id":"-bG3ZNZSFmhj"}},{"cell_type":"markdown","source":["[Hedra](https://www.hedra.com/app/video)\n","\n","[HeyGen](https://app.heygen.com/get-started)"],"metadata":{"id":"cuANbDNpc4p0"}},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/under_construction_dall_e_klein.png)"],"metadata":{"id":"uv-Nursv_mqg"}},{"cell_type":"markdown","source":["[Diffuser](https://huggingface.co/docs/diffusers/v0.33.1/en/index)\n","\n","\n","[Videogeneration](https://huggingface.co/docs/diffusers/using-diffusers/text-img2vid?popular-models=CogVideoX)"],"metadata":{"id":"ydCJj0xgP-u7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lIYdn1woOS1n"},"outputs":[],"source":["!uv pip install --system --prerelease allow -q -U diffusers transformers accelerate"]},{"cell_type":"code","source":["!uv pip install --system --prerelease allow -q --upgrade torchvision\n","!pip install torchvision --no-cache-dir --force-reinstall \"torchvision-cuda==11.8\""],"metadata":{"id":"JA7GIPc4JboI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Restart Kernel**  `Ctrl+M.`"],"metadata":{"id":"XK4jMNTGJubZ"}},{"cell_type":"code","source":["!uv pip install --system --prerelease allow -q --upgrade torchvision\n","!uv pip install --system --prerelease allow -q --upgrade torch"],"metadata":{"id":"YjO8QZblJnTj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Stable Diffusion Video also accepts micro-conditioning, in addition to the conditioning image, which allows more control over the generated video:\n","\n","+ `fps`: the frames per second of the generated video.\n","+ `motion_bucket_id`: the motion bucket id to use for the generated video. This can be used to control the motion of the generated video. Increasing the motion bucket id increases the motion of the generated video.\n","+ `noise_aug_strength`: the amount of noise added to the conditioning image. The higher the values the less the video resembles the conditioning image. Increasing this value also increases the motion of the generated video.\n","\n","For example, to generate a video with more motion, use the motion_bucket_id and noise_aug_strength micro-conditioning parameters:"],"metadata":{"id":"NYXLbxIDLFNJ"}},{"cell_type":"code","source":["# Import\n","import torch\n","\n","# Importiere die ben√∂tigten Module aus diffusers\n","from diffusers import StableVideoDiffusionPipeline\n","from diffusers.utils import load_image, export_to_video"],"metadata":{"id":"wvxo9HqJKyzm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lade die vortrainierte Stable Video Diffusion Pipeline\n","pipe = StableVideoDiffusionPipeline.from_pretrained(\n","  \"stabilityai/stable-video-diffusion-img2vid-xt\",  # Modellname von Hugging Face\n","  torch_dtype=torch.float16,                        # Reduzierter Speicherbedarf durch Verwendung von float16\n","  variant=\"fp16\"                                    # Modellvariante f√ºr float16-Unterst√ºtzung\n",")"],"metadata":{"id":"C-qn5UeoRu6m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Optional: Verschiebe Modellkomponenten automatisch zwischen CPU und GPU bei Bedarf (auskommentiert)\n","# pipe.enable_model_cpu_offload() - > F√ºhrt zu einem Fehler"],"metadata":{"id":"WoUustfSRxYV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lade ein Bild von einer URL als Ausgangspunkt f√ºr die Videoerzeugung\n","image = load_image(\"https://raw.githubusercontent.com/ralf-42/GenAI/main/02%20data/a_retro-futuristic_robot_dall_e.jpg\")\n","image = image.resize((1024, 576))  # Passe die Bildgr√∂√üe an das Modellinput-Format an"],"metadata":{"id":"38U7mflyR4VM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Erzeuge ein Video basierend auf dem Eingabebild\n","frames = pipe(\n","    image,                          # Eingabebild\n","    decode_chunk_size=8,            # Anzahl der Frames, die gleichzeitig decodiert werden (Speicheroptimierung)\n","    generator=generator,            # Zufallsgenerator f√ºr Reproduzierbarkeit\n","    motion_bucket_id=180,           # Steuert die Bewegung im Video (h√∂her = mehr Bewegung)\n","    noise_aug_strength=0.1          # Rauschst√§rke w√§hrend der Augmentation (steigert Variation)\n",").frames[0]                         # Extrahiere die erste Liste von Frames (ein Video)"],"metadata":{"id":"Wh_MADf_R6qA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Exportiere die Frames als Video-Datei mit 7 Bildern pro Sekunde\n","export_to_video(frames, \"output_itv.mp4\", fps=7)"],"metadata":{"id":"AW7z_nvnR_5M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9d9cb086"},"source":["# 4 | Text-to-Video (T2V)\n","\n","---"]},{"cell_type":"markdown","source":["Text-zu-Video (T2V) mit generativer KI erm√∂glicht die Erstellung realistischer Videos allein auf Basis von Texteingaben. Modelle wie Sora (OpenAI), Gen-2 (Runway) und Lumiere (Google) wandeln sprachliche Beschreibungen in dynamische, visuell konsistente Videoclips um ‚Äì oft erg√§nzt durch passende Audiospuren oder die M√∂glichkeit, eigenen Ton hinzuzuf√ºgen.\n","\n","Diese KI-Systeme nutzen multimodale Architekturen und tiefes Lernen, um Bewegungen, Perspektiven und visuelle Effekte realit√§tsnah zu generieren. Sie finden Anwendung in Bereichen wie Marketing, Bildung, Storytelling und Simulation.\n","\n","F√ºhrende T2V-Modelle:\n","\n","\n","+ Sora (OpenAI) ‚Äì Erzeugt aus Bildern und Bildfolgen realistische Szenen mit hoher Koh√§renz.\n","\n","+ Gen-2 (Runway) ‚Äì Unterst√ºtzt Bild-, Text- und Hybridprompts f√ºr vielseitige Anwendungen.\n","\n","+ Pika Labs ‚Äì Ideal f√ºr einfache Animationen aus Einzelbildern.\n","\n","+ Stable Video Diffusion ‚Äì Quelloffen, mit wachsender Community.\n","\n","+ Lumiere (Google) ‚Äì Experimentell, mit Fokus auf Bewegungsdynamik und Stil√ºbertragung.\n","\n","+ Hedra ‚Äì Erm√∂glicht die Erstellung von Videos aus statischen Bildern mit synchronisiertem Audio, ideal f√ºr die Generierung sprechender oder singender Avatare.\n","\n","+ HeyGen ‚Äì Spezialisiert auf die schnelle Erstellung personalisierter, sprechender Avatare aus Fotos oder Videos. Bietet pr√§zise Lippensynchronisation, Voice-Cloning, Text-zu-Video und Video√ºbersetzung in √ºber 40 Sprachen. Besonders geeignet f√ºr Marketing, E-Learning und Social Media; erm√∂glicht Multi-Avatar-Videos und individuelle Anpassungen bis hin zu 4K-Qualit√§t.\n","\n","Viele Tools bieten zus√§tzlich Funktionen zur Audiointegration, etwa durch automatische Tonunterlegung oder individuelle Sprachkommentare, um das Ergebnis noch lebendiger zu gestalten.\n","\n"],"metadata":{"id":"ya_B4h3PA6I6"}},{"cell_type":"markdown","source":["Hier ist eine √úbersicht, welche der genannten Modelle mit/ohne **API-Zugang** nutzbar sind:\n","\n","| Modell                   | API-Zugang m√∂glich?               | Hinweise |\n","| :--                      | :--                              | :-- |\n","| **Sora (OpenAI)**        | Eingeschr√§nkt, nur nach Bewerbung | Sora ist nicht √∂ffentlich per API verf√ºgbar. Zugang gibt es nur nach individueller Bewerbung und Freischaltung durch OpenAI. |\n","| **Gen-2 (Runway)**       | Ja                               | Runway bietet eine API f√ºr Gen-3 und Gen-4 Modelle, die f√ºr Entwickler offen zug√§nglich ist. |\n","| **Pika Labs**            | Ja                               | Die Pika API ist verf√ºgbar, allerdings mit bestimmten Einschr√§nkungen und nach Registrierung. |\n","| **Stable Video Diffusion** | Ja                             | √úber die Developer Platform von Stability AI ist eine API f√ºr Stable Video Diffusion verf√ºgbar. |\n","| **Lumiere (Google)**     | Nein, aktuell nicht              | Lumiere befindet sich noch in der Entwicklung und ist nicht √∂ffentlich oder per API nutzbar. |\n","| **Hedra**                | Nein, aktuell nicht              | Hedra ist als Web-App nutzbar, aber es gibt derzeit keine √∂ffentlich dokumentierte API. |\n","| **HeyGen**               | Ja                               | HeyGen bietet einen dokumentierten API-Zugang f√ºr Video- und Avatar-Erstellung. Authentifizierung erfolgt per API-Key oder OAuth2. Die API ist f√ºr Entwickler offen zug√§nglich, Details in der offiziellen Doku[1][3][8]. |\n","\n","\n","**Zusammengefasst:**  \n","Per API lassen sich aktuell **Gen-2 (Runway)**, **Pika Labs**, **Stable Video Diffusion** und **HeyGen** direkt nutzen.  \n","**Sora** ist nur nach individueller Bewerbung und Freischaltung per API zug√§nglich.  \n","**Lumiere** und **Hedra** bieten derzeit keinen √∂ffentlichen API-Zugang.\n","\n","\n","Stand: 05.2025"],"metadata":{"id":"uiP1vkgsI0IS"}},{"cell_type":"markdown","source":["[Hedra](https://www.hedra.com/app/video)\n","\n","[HeyGen](https://app.heygen.com/get-started)"],"metadata":{"id":"DlWW-HxTI0IU"}},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/under_construction_dall_e_klein.png)"],"metadata":{"id":"Hwrf9njg9gSH"}},{"cell_type":"code","source":["# Import\n","import torch\n","from diffusers import CogVideoXImageToVideoPipeline\n","from diffusers.utils import export_to_video, load_image"],"metadata":{"id":"WSK_oAbOPiSp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Bild laden\n","image = load_image(image=\"/content/a_retro-futuristic_robot_dall_e.jpg\")"],"metadata":{"id":"a3QvQaKNPxy9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prompt f√ºr die Animation\n","prompt = \"\"\"\n","A cheerful retro-futuristic robot arrives at a chrome-plated welcome center on a distant planet, its polished body gleaming in the warm glow of a golden sunset. Neon lights flicker above the entrance, reading \"Robots Welcome\" as a crowd of similarly styled robots moves inside. The robot turns its head, smiles, and waves at the camera . Inside, metallic walls reflect colorful lights, while robots converse, exchange parts, and charge up. The scene captures a vibrant utopia where robots live in harmony, animated in a smooth, cinematic style with a nostalgic 1950s sci-fi aesthetic.\n","\"\"\""],"metadata":{"id":"oQmJvkoEPldk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Importiere das CogVideoX-Pipeline-Modul (dieser Teil muss vorher erfolgt sein)\n","pipe = CogVideoXImageToVideoPipeline.from_pretrained(\n","    \"THUDM/CogVideoX-5b-I2V\",          # Name des vortrainierten Modells\n","    torch_dtype=torch.bfloat16        # Reduziere den Speicherverbrauch durch Verwendung des bfloat16-Datentyps\n",")"],"metadata":{"id":"gsXszceiPrsk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reduziere den Speicherbedarf w√§hrend der Verarbeitung\n","pipe.vae.enable_tiling()             # Aktiviere Tiling (Bildaufteilung in kleinere Kacheln)\n","pipe.vae.enable_slicing()            # Aktiviere Slicing (Schichtenweise Verarbeitung)"],"metadata":{"id":"3zP2LCczQGzi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verschiebe das Modell auf die CUDA-Grafikkarte f√ºr schnellere Berechnungen\n","pipe.to(\"cuda\")"],"metadata":{"id":"_JE6N2NZQIXn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Erzeuge ein Video aus einem Bild und einem Prompt\n","video = pipe(\n","    prompt=prompt,                   # Textbeschreibung, die die Bild-zu-Video-Generierung steuert\n","    image=image,                     # Eingabebild, das als Ausgangspunkt dient\n","    num_videos_per_prompt=1,         # Anzahl der Videos, die f√ºr den Prompt generiert werden sollen\n","    num_inference_steps=50,          # Anzahl der Inferenzschritte (mehr Schritte = bessere Qualit√§t, aber langsamer)\n","    num_frames=49,                   # Anzahl der Bilder (Frames) im resultierenden Video\n","    guidance_scale=6,                # St√§rke der Prompt-Steuerung (h√∂her = n√§her am Textprompt)\n","    generator=torch.Generator(device=\"cuda\").manual_seed(42),  # Zufallszahlengenerator f√ºr reproduzierbare Ergebnisse\n",").frames[0]                          # Extrahiere das erste generierte Video (als Liste von Frames)"],"metadata":{"id":"agHmqRXPQJ3i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Export des Videos\n","export_to_video(video, \"output_ttv.mp4\", fps=8)"],"metadata":{"id":"6hAV0nL8QZZg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c477fff3"},"source":["# 5 | Objekterkennung\n","\n","---"]},{"cell_type":"markdown","source":["Die Video-Objekterkennung ist ein wichtiger Bereich der Computer Vision, der sich mit der Identifikation und Klassifikation von Objekten in Videomaterial befasst. Im Gegensatz zur statischen Bilderkennung m√ºssen dabei nicht nur einzelne Frames analysiert, sondern auch zeitliche Zusammenh√§nge ber√ºcksichtigt werden.  \n","\n","Ziel dieses Kapitels ist es, ein grundlegendes Verst√§ndnis f√ºr die Herausforderungen und L√∂sungsans√§tze der Video-Objekterkennung zu vermitteln und praxisnahe Anwendungen zu demonstrieren. Zum Einsatz kommt **YOLO** (You Only Look Once), ein Echtzeit-Objekterkennungsmodell, das durch seine hohe Geschwindigkeit und Genauigkeit besonders f√ºr Anwendungen wie autonome Fahrzeuge, √úberwachungssysteme und interaktive KI-Systeme geeignet ist."],"metadata":{"id":"onWjUpoV0Vy8"}},{"cell_type":"markdown","source":["[Ultralytics | Yolo](https://www.ultralytics.com/de)"],"metadata":{"id":"2zBB6_P9hEkF"}},{"cell_type":"code","source":["!uv pip install --system --prerelease allow -q ultralytics opencv-python"],"metadata":{"id":"03nrbETSmmwu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import\n","from ultralytics import YOLO\n","from google.colab import files\n","import cv2\n","from IPython.display import HTML\n","from base64 import b64encode\n","\n","# YOLO-Modell laden\n","model = YOLO(\"yolo11s.pt\")\n","\n","# Video-Datei hochladen\n","uploaded = files.upload()\n","\n","# Dateinamen abrufen (das erste hochgeladene Video)\n","video_path = list(uploaded.keys())[0]\n","\n","# YOLO-Vorhersage auf dem Video durchf√ºhren\n","# Hier werden Objekte erkannt und das Video mit Bounding Boxes gespeichert\n","results = model.predict(source=video_path, save=True, conf=0.25, save_txt=True, format='avi')\n","\n","# Das verarbeitete Video anzeigen\n","video_path = video_path.replace(\"mp4\", \"avi\")\n","processed_video_path = f\"{results[0].save_dir}/{video_path}\"\n","\n","# Video herunterladen\n","print(f\"Video mit Objekterkennung: {processed_video_path}\")\n","files.download(processed_video_path)"],"metadata":{"id":"P9ZQmS_OmzQB","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x0S24SQRPc-o"},"source":["# 6 | Video-/Bild-/Sprachanalyse\n","\n","---"]},{"cell_type":"markdown","source":["\n","\n","Durch die Analyse von Bildinhalten, Objekten, Bewegungen und Szenen k√∂nnen relevante Informationen aus Videos extrahiert werden. Die Fokus in diesem Modul ist die Videoanalyse mit Computer Vision, auch **Visual Analysis** genannt, sie ist eine Methode zur automatischen Erkennung und Klassifizierung von Objekten, Personen und Handlungen in Videosequenzen. Mithilfe maschinellen Sehens (Computer Vision) werden Videos auf verschiedene Elemente und Muster untersucht. Diese Technik wird h√§ufig in der Sicherheits√ºberwachung, in der Produktionslinie oder bei der Analyse von Social-Media-Inhalten eingesetzt.\n","\n","Moderne KI-Modelle analysieren nicht nur einzelne Frames, sondern ber√ºcksichtigen auch zeitliche Zusammenh√§nge, um Bewegungen und Handlungen pr√§ziser zu erfassen. Unternehmen nutzen diese Technologie, um Kundenverhalten zu verstehen, Sicherheitsrisiken zu erkennen oder Prozesse zu optimieren."],"metadata":{"id":"kLrmSculPc-o"}},{"cell_type":"code","source":["import openai\n","from google.colab import files\n","from IPython.display import display, Markdown, HTML\n","import cv2\n","import os\n","import numpy as np\n","import base64\n","import tempfile"],"metadata":{"id":"FBBH8k4bPc-p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Video hochladen und Frame-Extraction\n","uploaded = files.upload()\n","video_file_path = list(uploaded.keys())[0]"],"metadata":{"id":"H6RafyZYaCtF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Frame-Extraction\n","def extract_frames(video_path, num_frames=5):\n","    \"\"\"Extrahiert Frames aus einem Video\"\"\"\n","    cap = cv2.VideoCapture(video_path)\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    frame_indices = np.linspace(0, total_frames-1, num_frames, dtype=int)\n","\n","    frames = []\n","    for idx in frame_indices:\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n","        ret, frame = cap.read()\n","        if ret:\n","            frames.append(frame)\n","    cap.release()\n","    return frames\n","\n","# Hilfsfunktion f√ºr Base64-Kodierung\n","def get_base64_encoded(image_path):\n","    \"\"\"Konvertiert ein Bild in Base64-String\"\"\"\n","    with open(image_path, \"rb\") as image_file:\n","        return base64.b64encode(image_file.read()).decode('utf-8')\n","\n","# Frames extrahieren und speichern\n","frames = extract_frames(video_file_path)\n","frame_paths = [f\"frame_{i}.jpg\" for i in range(len(frames))]\n","for i, frame in enumerate(frames):\n","    cv2.imwrite(frame_paths[i], frame)\n","\n","# Audio extrahieren und transkribieren\n","audio_path = \"extracted_audio.mp3\"\n","!ffmpeg -i \"{video_file_path}\" -q:a 0 -map a \"{audio_path}\" -y\n","\n","with open(audio_path, \"rb\") as audio_file:\n","    transcribed_text = openai.audio.transcriptions.create(\n","        model=\"whisper-1\",\n","        file=audio_file\n","    ).text\n","\n","# Frame-Analyse mit GPT-4o\n","frame_descriptions = []\n","for frame_path in frame_paths:\n","    base64_img = get_base64_encoded(frame_path)\n","    response = openai.chat.completions.create(\n","        model=\"gpt-4o\",\n","        messages=[\n","            {\"role\": \"system\", \"content\": \"Du bist ein Experte f√ºr Videoanalyse. Beschreibe kurz und pr√§zise, was auf diesem Frame zu sehen ist.\"},\n","            {\"role\": \"user\", \"content\": [\n","                {\"type\": \"text\", \"text\": \"Beschreibe diesen Video-Frame mit einem kurzen Satz:\"},\n","                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img}\"}}\n","            ]}\n","        ],\n","        max_tokens=150\n","    )\n","    frame_descriptions.append(response.choices[0].message.content)"],"metadata":{"id":"OsiwldHYaSR3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Gesamtanalyse\n","analysis_prompt = f\"\"\"\n","Analysiere den folgenden Videoinhalt basierend auf dem Transkript und den Frame-Beschreibungen.\n","Gib eine detaillierte Zusammenfassung dessen, was im Video passiert.\n","\n","Hauptpunkt: Inhaltsanalyse:\n","    Themen und Argumentationsstruktur: Was wird gesagt? Wie ist das Gesagte aufgebaut?\n","    Sprachstil: Alltagssprache vs. Fachsprache, formell vs. informell\n","    Rhetorische Mittel: Metaphern, Wiederholungen, Fragen, Ironie, Pausen\n","    Satzstruktur & Wortwahl: Einfach oder komplex? Viele F√ºllw√∂rter?\n","    Erz√§hlperspektive: Ich-Form, Du-Ansprache, distanziert?\n","    Tonfall und Haltung: Neutral, ironisch, kritisch, emotional?\n","\n","Hauptpunkt: Parasprachliche Merkmale (Stimme, Ton, Sprechweise):\n","    Sprechtempo: Schnell, langsam, variierend\n","    Intonation: Monoton oder lebendig? Betonung gezielt eingesetzt?\n","    Lautst√§rke: Konstant, wechselnd, passend zur Situation?\n","    Stimmqualit√§t: Klar, nasal, heiser, angespannt?\n","    Pausen: Nat√ºrlich oder unnat√ºrlich? Strategisch gesetzt?\n","    Versprecher oder Selbstkorrekturen: H√§ufig? Sympathisch oder irritierend?\n","\n","Hauptpunkt: Visuelle Elemente:\n","    Bildkomposition: Einstellungsgr√∂√üen, Perspektiven, Bildausschnitte\n","    Kameraf√ºhrung: Statisch, dynamisch, Schwenks, Zooms\n","    Schnittrhythmus: Schnell, langsam, variierend, passend zum Inhalt?\n","    Lichtgestaltung: Nat√ºrliches vs. k√ºnstliches Licht, Farbstimmung\n","    Setting/Location: Bedeutung und Wirkung des Hintergrunds\n","    Kleidung/Styling: Formell, informell, thematisch passend?\n","\n","Hauptpunkt: Nonverbale Kommunikation:\n","    K√∂rperhaltung: Offen, geschlossen, angespannt, entspannt?\n","    Gestik: H√§ufigkeit, Nat√ºrlichkeit, unterst√ºtzend zum Gesagten?\n","    Mimik: Ausdrucksstark, zur√ºckhaltend, authentisch?\n","    Blickkontakt: Direkt in die Kamera oder ablenkend?\n","    Bewegung im Raum: Statisch oder dynamisch?\n","\n","Hauptpunkt: Zusammenspiel von Bild und Ton:\n","    Synchronit√§t: Passen Bild und Ton zusammen?\n","    Erg√§nzung: Unterst√ºtzen visuelle Elemente das Gesagte?\n","    Musik/Soundeffekte: Einsatz und Wirkung auf die Gesamtwahrnehmung\n","    Grafiken/Einblendungen: Sinnvoll integriert und informativ?\n","\n","Hauptpunkt: Wirkung auf die Zuschauer:innen\n","    Verst√§ndlichkeit: Wird der Inhalt klar transportiert?\n","    Authentizit√§t: Wirkt die Person/Szene glaubw√ºrdig und echt?\n","    Emotionale Ansprache: Ber√ºhrt der Inhalt? Motiviert? Regt zum Denken an?\n","    Zielgruppenorientierung: Passt Sprache, Ton und Bild zum Publikum?\n","\n","Hauptpunkt: Stimmungsanalyse:\n","    Stimmung: Positiv üòä, Neutral üòê, Negativ üòû\n","    Begr√ºnde Deine Einsch√§tzung unter Ber√ºcksichtigung visueller und auditiver Elemente\n","\n","Hauptpunkt: Technische Aspekte\n","    Audioqualit√§t: Rauschfrei, Hall, Hintergrundger√§usche?\n","    Mikrofonierung: Richtiger Abstand, Pop-Ger√§usche, Aussteuerung?\n","    Bildqualit√§t: Sch√§rfe, Aufl√∂sung, Kontrast, Farbwiedergabe\n","    Schnitt & Nachbearbeitung: Erkennbar geschnitten? √úberg√§nge sauber?\n","    Spezialeffekte/CGI: Sinnvoll eingesetzt und qualitativ hochwertig?\n","\n","Hauptpunkt: Weitere Aspekte:\n","    Gibt es ethische Aspekte die angesprochen werden m√ºssten?\n","    Gibt es versteckte Botschaften in Bild oder Ton?\n","    Wie ist die Konsistenz zwischen verbaler und visueller Kommunikation?\n","    Welche Punkte k√∂nnten noch interessant sein, an die ich nicht gedacht habe?\n","\n","Vermeide unscharfe Formulierungen  wie wahrscheinlich, m√∂glicherweise, scheint, k√∂nnte oder sollte.\n","Nimm konkret Stellung.\n","\n","Stelle bei der Ausgabe der Ergebnisse nur bei den Hauptpunkten zur besseren Unterscheidung ein \"üîπ\" voran.\n","Zeige die Unterpunkte als dot-Aufz√§hlung an.\n","\n","### Video-Transkript:\n","{transcribed_text}\n","\n","### Frame-Beschreibungen:\n","{' | '.join([f\"Frame {i+1}: {desc}\" for i, desc in enumerate(frame_descriptions)])}\n","\"\"\""],"metadata":{"id":"fDXzlAwrQbQQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["analysis_result = openai.chat.completions.create(\n","    model=\"gpt-4o-mini\",\n","    messages=[\n","        {\"role\": \"system\", \"content\": \"Du bist ein Experte f√ºr Videoanalyse.\"},\n","        {\"role\": \"user\", \"content\": analysis_prompt}\n","    ],\n","    temperature=0.2\n",").choices[0].message.content"],"metadata":{"id":"egq3jA03QgXW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# HTML f√ºr Ergebnisse-Anzeige definieren\n","\n","frames_html = \"\".join([\n","    f\"\"\"\n","    <div style=\"display: inline-block; margin: 10px; text-align: center;\">\n","        <img src=\"data:image/jpeg;base64,{get_base64_encoded(frame_path)}\" style=\"width: 200px; border: 1px solid #ddd;\">\n","        <p style=\"width: 200px; font-size: 12px;\">{frame_descriptions[i]}</p>\n","    </div>\n","    \"\"\" for i, frame_path in enumerate(frame_paths)\n","    ])\n","\n","# Markdown-Anzeige\n","display(Markdown(f\"\"\"\n","## üé• Videoanalyse\n","---\n","\n","**üìÇ Datei:** `{video_file_path}`\n","**ü§ñ Modelle:** `Whisper-1` (Audio), `GPT-4o` (Frames), `GPT-4o-mini` (Gesamtanalyse)\n","\n","<br>\n","\n","### üìù Transkript\n","---\n","{transcribed_text}\n","\n","<br>\n","\n","### üñºÔ∏è Extrahierte Frames und Beschreibungen\n","---\n","\"\"\"))\n","\n","display(HTML(frames_html))\n","display(Markdown(f\"\"\"\n","### üß† Videoanalyse-Ergebnis\n","{analysis_result}\n","\"\"\"))"],"metadata":{"id":"55TU4u1QRD8r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aufr√§umen - Dateien l√∂schen\n","for path in frame_paths + [audio_path]:\n","    os.remove(path)"],"metadata":{"id":"dPOMymT1aaEc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 7 | Dubbing\n","---\n"],"metadata":{"id":"HvLa5GWZfgVH"}},{"cell_type":"markdown","source":["Dubbing (Synchronisation) bezeichnet den Prozess, bei dem der Originalton eines Films, Videos oder Audioclips durch eine neu eingesprochene Tonspur in einer anderen Sprache ersetzt wird. Dabei wird der √ºbersetzte Text so angepasst, dass er m√∂glichst gut zur Lippenbewegung und Stimmung der Originalaufnahme passt. Ziel ist es, dem Publikum ein m√∂glichst nat√ºrliches und immersives Seherlebnis in der eigenen Sprache zu bieten. Dubbing wird h√§ufig bei internationalen Filmproduktionen, Serien oder Videospielen eingesetzt und erfordert eine enge Zusammenarbeit von √úbersetzer:innen, Sprecher:innen, Toningenieur:innen und gegebenenfalls KI-gest√ºtzten Tools."],"metadata":{"id":"zuBirABpfiog"}},{"cell_type":"markdown","source":["| Modell / Anbieter      | API-Zugang m√∂glich? | Hinweise |\n","| :--------------------- | :------------------ | :------- |\n","| **ElevenLabs Dubbing** | Ja | Dubbing in 29 Sprachen, erh√§lt Emotion, Timing und Originalstimme. Plattformintegration (z.B. YouTube), <br>automatische Sprechererkennung, professionelle Human Review m√∂glich. |\n","| **Rask AI**            | Ja | √úber 130 Sprachen, Voice Cloning f√ºr 29 Sprachen, Multi-Speaker, Lip-Sync, Video-Editing, Integration mit Vimeo/Instagram. |\n","| **Heygen**             | Ja | 175+ Sprachen/Dialekte, Multi-Speaker, Custom Pronunciation, Brand Consistency, nat√ºrliche Intonation, Translate API verf√ºgbar. |\n","| **D-ID Video Translate** | Ja | Voice Cloning, Lip-Sync, Bulk-Rendering (bis 29 Sprachen), Drag-and-Drop-Interface, API ab $18/Monat. |\n","| **Murf Dub**           | Ja | 25+ Sprachen, pr√§zise √úbersetzung, Voice Cloning, Zeit- und Lip-Sync, Hintergrundmusik bleibt erhalten, API verf√ºgbar, SOC 2/GDPR/ISO-konform. |\n","| **Synthesys AI**       | Ja | 29 Sprachen, Voice Cloning, Echtzeit-Dubbing, AI-Lip-Sync, 300+ Stimmen, Video-Editing, virtuelle Schauspieler. |\n","| **Deepdub**            | Ja | 130+ Sprachen, 26+ Emotionen pro Sprecher (eTTS‚Ñ¢), Voice Cloning, API f√ºr Enterprise, menschliche Qualit√§tskontrolle, Preis auf Anfrage. |\n","| **Speechify Studio**   | Ja | 60+ Sprachen, 1.000+ Stimmen, Voice Cloning, Multi-Speaker, Auto-Subtitling, Skript- und Emotionsbearbeitung, Lip-Sync (Beta). |\n","| **Synthesia**          | Ja | 29 Sprachen, Multi-Speaker, Voice Cloning, Lip-Sync (in Entwicklung), AI-Avatare, einfache Bedienung, API f√ºr Unternehmen. |\n","| **InVideo**            | Ja | 50+ Sprachen, Team-Collaboration, gro√üe Voice-Bibliothek, Voice Cloning, Echtzeit-Editing, API verf√ºgbar. |\n","| **CAMB.AI**            | Ja | 140+ Sprachen, Echtzeit- und Postproduktion, Dubstream f√ºr Livestreams, Twitter-Integration, API. |\n","| **Maestra AI**         | Ja | 125+ Sprachen, Voice Cloning, Multi-Speaker, API, automatische Untertitel, Fokus auf professionelle Inhalte. |\n","| **Dubverse**           | Ja | Online-Video-Dubbing, TTS, Auto-Subtitles, API, Voice Cloning, Branding, Neo.One & Candy.Two Modelle, einfach integrierbar. |\n","| **VisualDub**          | Nein (Web-App)      | Studioqualit√§t-LipSync per Web-App, keine √∂ffentliche API dokumentiert. |\n","| **Dubbing-AI**         | Ja | Automatisiertes Dubbing f√ºr Video/Audio, Multi-Language Voiceover, API verf√ºgbar. |\n"],"metadata":{"id":"IQmMqbHJgBUm"}},{"cell_type":"markdown","source":["[ElevenLabs Dubbing](https://elevenlabs.io/app/dubbing)"],"metadata":{"id":"FUHqRG-DgH3p"}},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/under_construction_dall_e_klein.png)"],"metadata":{"id":"cjzDNlL6fxqp"}},{"cell_type":"markdown","source":["# 8 | H√§ufig verwendete Parameter\n","---"],"metadata":{"id":"-47CBLdZbLr0"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Zentrale Parameter\n","</font></p>\n","\n","\n","<p><font color='blue' size=\"4\">\n","Allgemeine Parameter\n","</font></p>\n","\n","1. **num_inference_steps**:\n","    \n","    - Bestimmt die Anzahl der Schritte im Diffusionsprozess\n","    - Mehr Schritte = bessere Qualit√§t, aber l√§ngere Berechnungszeit\n","    - Typische Werte: 20-50\n","2. **guidance_scale**:\n","    \n","    - Steuert, wie stark der Text-Prompt die Generierung beeinflusst\n","    - H√∂here Werte = st√§rkere √úbereinstimmung mit dem Prompt\n","    - Typische Werte: 7-9\n","3. **num_frames**:\n","    \n","    - Anzahl der zu generierenden Frames\n","    - Typische Werte: 16-60 (je nach Modell)\n","4. **fps** (Frames per Second):\n","    \n","    - Bestimmt die Abspielgeschwindigkeit des generierten Videos\n","    - Typische Werte: 7-30\n","\n","<p><font color='blue' size=\"4\">\n","Text-to-Video spezifische Parameter\n","</font></p>\n","\n","\n","5. **height/width**:\n","    - Aufl√∂sung des generierten Videos\n","    - Die optimalen Werte h√§ngen vom verwendeten Modell ab\n","\n","<p><font color='blue' size=\"4\">\n","Image-to-Video spezifische Parameter\n","</font></p>\n","\n","\n","6. **motion_bucket_id**:\n","    \n","    - Steuert die St√§rke der Bewegung im Video\n","    - Bereich: 0-999 (0=minimal, 999=maximal)\n","7. **noise_aug_strength**:\n","    \n","    - Bestimmt, wie stark vom Originalbild abgewichen wird\n","    - Bereich: 0-1 (0=nah am Original, 1=starke Abweichung)\n"],"metadata":{"id":"Si5CXZELbOmw"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Tipps f√ºr Einsteiger\n","</font></p>\n","\n","1. **Hardware-Anforderungen beachten:**\n","    \n","    - Video-Generierung ben√∂tigt viel Grafikspeicher (8GB+ VRAM empfohlen)\n","    - Google Colab kann eine gute Alternative sein\n","2. **Prompt-Engineering:**\n","    \n","    - Detaillierte und klare Prompts f√ºhren zu besseren Ergebnissen\n","    - Experimentiere mit verschiedenen Beschreibungen\n","3. **Parameter schrittweise anpassen:**\n","    \n","    - Beginne mit Standardwerten und passe sie dann an\n","    - Dokumentiere deine Experimente\n","4. **F√ºr Image-to-Video:**\n","    \n","    - Verwende hochwertige Ausgangsbilder\n","    - Die besten Ergebnisse erzielst du mit Bildern im Format 768x768 oder 1024x576"],"metadata":{"id":"fxBMalhDcZ4k"}},{"cell_type":"markdown","source":["# A | Aufgabe\n","---"],"metadata":{"id":"gRJbfUG9-ujC"}},{"cell_type":"markdown","source":["Bereit f√ºr eine neue Herausforderung? In diesen Aufgaben werden Sie die leistungsstarken Vision-Modelle von **MediaPipe** kennenlernen. Mit diesen Modellen k√∂nnen sie u.a. Objekterkennung, Handgesten und Gesichtsausdr√ºcke in Echtzeit analysieren. Entdecken Sie die Potenziale der GenAI. Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen.\n","\n","[MediaPipe](https://mediapipe-studio.webapps.google.com/home)\n","\n","[MediaPipe L√∂sungsleitfaden](https://ai.google.dev/edge/mediapipe/solutions/guide?hl=de)"],"metadata":{"id":"Cb6-1kyIx-RF"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Video-Objekterkennung\n","</font></p>"],"metadata":{"id":"gmigTCGFyI04"}},{"cell_type":"markdown","source":["* **Aufgabe:**\n","    * Installieren Sie die erforderlichen Bibliotheken (MediaPipe, OpenCV).\n","    * Laden Sie ein vortrainiertes Objekterkennungsmodell von MediaPipe herunter.\n","    * Schreiben Sie ein Python-Skript, das ein Bild oder einen Videostream einliest und die erkannten Objekte mit Begrenzungsrahmen und Labels visualisiert.\n","    * Experimentieren Sie mit verschiedenen Bildern und Videostreams und beobachten Sie die Ergebnisse.\n","* **Zusatzaufgabe:** Versuchen Sie die Genauigkeit der Objekterkennung zu verbessern, indem sie verschiedene Modelle oder Parameter ausprobieren.\n"],"metadata":{"id":"_DNE1RJmyNBu"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Handgestenerkennung\n","</font></p>"],"metadata":{"id":"eq_on7-3yZyE"}},{"cell_type":"markdown","source":["* **Aufgabe:**\n","    * Nutzen Sie das MediaPipe-Modell f√ºr die Handgestenerkennung.\n","    * Erstellen Sie ein Programm, das die Positionen der Handgelenke und Finger erkennt und visualisiert.\n","    * Implementieren Sie eine einfache Gestenerkennung, z. B. die Erkennung von \"Daumen hoch\" oder \"Daumen runter\".\n","    * Lassen Sie Ihr Programm auf erkannte Gesten mit einer Ausgabe im Terminal reagieren.\n","* **Zusatzaufgabe:** Die Teilnehmer k√∂nnen komplexere Gesten implementieren oder das Programm mit einer anderen Anwendung (z. B. einem Pr√§sentationsprogramm) verbinden, um Gesten zur Steuerung zu verwenden.\n","\n"],"metadata":{"id":"epm_6Q2Ny9gt"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Gesichtserkennung und -verfolgung\n","</font></p>"],"metadata":{"id":"PucCcraJzJ9i"}},{"cell_type":"markdown","source":["* **Aufgabe:**\n","    * Verwenden Sie das MediaPipe-Modell f√ºr die Gesichtserkennung und -verfolgung.\n","    * Erstellen Sie ein Programm, das Gesichter in einem Videostream erkennt und die Gesichtspunkte (z. B. Augen, Nase, Mund) verfolgt.\n","    * Implementieren Sie eine einfache Gesichtsausdrucksanalyse, z. B. die Erkennung von L√§cheln oder Stirnrunzeln.\n","    * Die Daten die erhoben werden k√∂nnen visuell in Echtzeit dargestellt werden.\n","* **Zusatzaufgabe:** Sie k√∂nnen versuchen, die Gesichtsausdrucksanalyse zu verbessern oder das Programm mit einer anderen Anwendung (z. B. einem Avatar-Programm) zu verbinden, um Gesichtsausdr√ºcke auf den Avatar zu √ºbertragen.\n"],"metadata":{"id":"AFG7kuUZzLvd"}}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["-ID_iebz9wyq","CSGjgTJz-cpi","83373f05","c477fff3","9d9cb086","gRJbfUG9-ujC"],"machine_shape":"hm","gpuType":"A100","toc_visible":true,"authorship_tag":"ABX9TyPv2UIEMGFett6sBUbjA8Ir"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["<p><font size=\"7\" color='grey'> <b>\n","Anwendung Generativer KI\n","</b></font> </br></p>"],"metadata":{"id":"Ih2CTVBnArVZ"}},{"cell_type":"markdown","source":["<p><font size=\"6\" color='grey'> <b>\n","Multimodal - Video\n","</b></font> </br></p>\n","\n","\n","---"],"metadata":{"id":"6jJZ7wbdArVc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfdhPIzcEYRG","cellView":"form","collapsed":true},"outputs":[],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  Colab-Umfeld</font> </br></p>\n","# Installierte Python Version\n","import sys\n","print(f\"Python Version: \",sys.version)\n","# Installierte LangChain Bibliotheken\n","print()\n","print(\"Installierte LangChain Bibliotheken:\")\n","!pip list | grep '^langchain'\n","# Unterdr√ºckt die \"DeprecationWarning\" von LangChain f√ºr die Memory-Funktionden\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")"]},{"cell_type":"code","source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  SetUp API-Keys (setup_api_keys)</font> </br></p>\n","def setup_api_keys():\n","    \"\"\"Konfiguriert alle ben√∂tigten API-Keys aus Google Colab userdata\"\"\"\n","    from google.colab import userdata\n","    import os\n","    from os import environ\n","\n","    # Dictionary der ben√∂tigten API-Keys\n","    keys = {\n","        'OPENAI_API_KEY': 'OPENAI_API_KEY',\n","        'HF_TOKEN': 'HF_TOKEN',\n","        # Weitere Keys bei Bedarf\n","    }\n","\n","    # Keys in Umgebungsvariablen setzen\n","    for env_var, key_name in keys.items():\n","        environ[env_var] = userdata.get(key_name)\n","\n","    return {k: environ[k] for k in keys.keys()}\n","\n","# Verwendung\n","all_keys = setup_api_keys()\n","# Bei Bedarf einzelne Keys direkt zugreifen\n","# WEATHER_API_KEY = all_keys['WEATHER_API_KEY']"],"metadata":{"cellView":"form","id":"WD3Wwr6sESX8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# **1 <font color='orange'>|</font> Einf√ºhrung**\n","\n","---"],"metadata":{"id":"-ID_iebz9wyq"}},{"cell_type":"markdown","source":["\n","\n","Video spielt eine zentrale Rolle in vielen Anwendungen, sei es in der Inhaltsanalyse, der automatischen Objekterkennung oder der Generierung von Inhalten. Durch moderne KI-Technologien k√∂nnen Videoinhalte effizient verarbeitet und ausgewertet werden. In diesem Abschnitt werden vier wesentliche Anwendungsbereiche der Videoverarbeitung vorgestellt:\n","\n","- **Video-zu-Text (Video-to-Text, VTT)**: Videoinhalte werden in Text umgewandelt, um visuelle Szenen, Handlungen oder Pr√§sentationen schriftlich festzuhalten. Dies erleichtert die Dokumentation und Weiterverarbeitung von visuellen Inhalten.\n","- **Videoanalyse**: Durch die Analyse von Bildinhalten, Bewegungen, Objekten und Szenen k√∂nnen relevante Informationen aus Videos extrahiert werden. Diese Methode wird h√§ufig in der Sicherheits√ºberwachung, im Marketing oder in der Forschung eingesetzt.\n","- **Video-Summary**: Wichtige Informationen aus langen Videodateien werden extrahiert und in einer kompakten Form dargestellt. Dies erm√∂glicht eine schnelle Erfassung der Kernaussage, ohne das gesamte Material ansehen zu m√ºssen.\n","- **Bild-zu-Video (Image-to-Video, ITV)**: Statische Bilder werden in bewegte Videosequenzen umgewandelt. Diese Technik wird f√ºr die Animation von Illustrationen, die Erstellung dynamischer Erkl√§rvideos oder die visuelle Inszenierung von Konzepten genutzt.\n","\n","Diese Techniken erleichtern die Verarbeitung und Nutzung gro√üer Mengen visueller Inhalte und er√∂ffnen neue M√∂glichkeiten f√ºr die Automatisierung und Analyse von Video-Daten.\n","\n","\n"],"metadata":{"id":"6hHK5izsH-cM"}},{"cell_type":"markdown","source":["# **2 <font color='orange'>|</font> Video-zu-Text (VTT)**\n","\n","---"],"metadata":{"id":"CSGjgTJz-cpi"}},{"cell_type":"markdown","source":["\n","\n","Die Umwandlung von Videoinhalten in Text, bekannt als Video-Transkription, erm√∂glicht die automatische Verschriftlichung von Videoszenen, Vortr√§gen oder Handlungen. Dies erleichtert die Dokumentation, Archivierung und Weiterverarbeitung von Videoinhalten. Moderne Bilderkennung und Spracherkennungstechnologien nutzen KI-Modelle, um visuelle und auditive Elemente pr√§zise zu erfassen und in lesbaren Text umzuwandeln.\n","\n","Videotranskriptionen sind in vielen Bereichen von Bedeutung ‚Äì von der Erstellung von Untertiteln und der Zug√§nglichkeit f√ºr Menschen mit Behinderungen √ºber die Erstellung durchsuchbarer Videoinhalte bis hin zur Analyse von Marktforschungsdaten. Durch den Einsatz automatisierter Systeme kann dieser Prozess erheblich beschleunigt werden, wodurch gro√üe Mengen an Videomaterial effizient verarbeitet werden k√∂nnen."],"metadata":{"id":"rZM_rb5ZmAYk"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Version auf Basis Transkript\n","</font></p>"],"metadata":{"id":"cVG8SXbzFa5u"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2192f76c"},"outputs":[],"source":["import openai\n","from google.colab import files\n","from IPython.display import display, Markdown\n","import os\n","\n","# Nutzer w√§hlt eine Datei aus und l√§dt sie hoch\n","uploaded = files.upload()\n","\n","# Den ersten Dateinamen extrahieren\n","video_file_path = list(uploaded.keys())[0]\n","\n","# Tempor√§ren Pfad f√ºr die Audio-Extraktion erstellen\n","audio_path = \"extracted_audio.mp3\"\n","\n","# FFmpeg verwenden, um Audio aus dem Video zu extrahieren\n","!ffmpeg -i \"{video_file_path}\" -q:a 0 -map a \"{audio_path}\" -y\n","\n","# Audiodatei √∂ffnen und Whisper nutzen\n","with open(audio_path, \"rb\") as audio_file:\n","    response = openai.audio.transcriptions.create(\n","        model=\"whisper-1\",\n","        file=audio_file\n","    )\n","\n","# Markdown-Formatierte Ausgabe\n","markdown_output = f\"\"\"\n","# üé• Transkript des Videos\n","\n","**üìÇ Datei:** `{video_file_path}`\n","**ü§ñ Modell:** `Whisper-1` (f√ºr Audiotranskription)\n","\n","---\n","\n","{response.text}\n","\"\"\"\n","\n","# Markdown in Google Colab anzeigen\n","display(Markdown(markdown_output))\n","\n","# Tempor√§re Audiodatei l√∂schen\n","# os.remove(audio_path)"]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Version auf Basis Bilder\n","</font></p>"],"metadata":{"id":"bxTaXF99FV6q"}},{"cell_type":"markdown","source":["In Anlehnung an: [Quelle](https://cookbook.openai.com/examples/gpt_with_vision_for_video_understanding)."],"metadata":{"id":"1e4QPJfzFt9s"}},{"cell_type":"code","source":["from IPython.display import display, Image, Markdown\n","from google.colab import files\n","import cv2\n","import base64\n","import time\n","from openai import OpenAI\n","import os\n","import requests\n","\n","client = OpenAI()"],"metadata":{"id":"t9ENnWPNC3KX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Datei-Upload erm√∂glichen\n","uploaded = files.upload()  # √ñffnet einen Dialog zum Datei-Upload\n","\n","# Name der hochgeladenen Datei ermitteln\n","filename = list(uploaded.keys())[0]\n","print(f\"Datei '{filename}' wurde hochgeladen.\")\n","\n","# Video mit OpenCV √∂ffnen\n","video = cv2.VideoCapture(filename)\n","\n","# Frames in base64 umwandeln\n","base64Frames = []\n","while video.isOpened():\n","    success, frame = video.read()\n","    if not success:\n","        break\n","    _, buffer = cv2.imencode(\".jpg\", frame)\n","    base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n","\n","# Video-Ressourcen freigeben\n","video.release()\n","print(len(base64Frames), \"Frames wurden eingelesen.\")"],"metadata":{"id":"jtcGOhf8DALD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display_handle = display(None, display_id=True)\n","for img in base64Frames:\n","    display_handle.update(Image(data=base64.b64decode(img.encode(\"utf-8\"))))\n","    time.sleep(0.025)"],"metadata":{"id":"eEB0YzHEDOhW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["PROMPT_MESSAGES = [\n","    {\n","        \"role\": \"user\",\n","        \"content\": [\n","            \"These are frames from a video that I want to upload. Generate a compelling description that I can upload along with the video. Please answer in German.\",\n","            *map(lambda x: {\"image\": x, \"resize\": 768}, base64Frames[0::50]),\n","        ],\n","    },\n","]\n","params = {\n","    \"model\": \"gpt-4o-mini\",\n","    \"messages\": PROMPT_MESSAGES,\n","    \"max_tokens\": 200,\n","}\n","\n","\n","# Markdown-Formatierte Ausgabe\n","markdown_output = f\"\"\"\n","# üé• Analyse des Videos auf Basis Frames\n","\n","**üìÇ Datei:** `{filename}`\n","**ü§ñ Modell:** gpt-4o-mini\n","\n","---\n","\"\"\"\n","\n","result = client.chat.completions.create(**params)\n","\n","display(Markdown(markdown_output))\n","display(Markdown(result.choices[0].message.content))"],"metadata":{"id":"U1xP9p6SDTcW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"83373f05"},"source":["# **3 <font color='orange'>|</font> Videoanalyse**\n","\n","---"]},{"cell_type":"markdown","source":["\n","\n","Durch die Analyse von Bildinhalten, Objekten, Bewegungen und Szenen k√∂nnen relevante Informationen aus Videos extrahiert werden. Die Fokus in diesem Modul ist die Videoanalyse mit Computer Vision, auch **Visual Analysis** genannt, sie ist eine Methode zur automatischen Erkennung und Klassifizierung von Objekten, Personen und Handlungen in Videosequenzen. Mithilfe maschinellen Sehens (Computer Vision) werden Videos auf verschiedene Elemente und Muster untersucht. Diese Technik wird h√§ufig in der Sicherheits√ºberwachung, in der Produktionslinie oder bei der Analyse von Social-Media-Inhalten eingesetzt.\n","\n","Moderne KI-Modelle analysieren nicht nur einzelne Frames, sondern ber√ºcksichtigen auch zeitliche Zusammenh√§nge, um Bewegungen und Handlungen pr√§ziser zu erfassen. Unternehmen nutzen diese Technologie, um Kundenverhalten zu verstehen, Sicherheitsrisiken zu erkennen oder Prozesse zu optimieren."],"metadata":{"id":"g3cVzmBZl-3y"}},{"cell_type":"code","source":["import openai\n","from google.colab import files\n","from IPython.display import display, Markdown, HTML\n","import cv2\n","import os\n","import numpy as np\n","import base64\n","import tempfile\n","\n","# Video hochladen und Frame-Extraction\n","uploaded = files.upload()\n","video_file_path = list(uploaded.keys())[0]\n","\n","def extract_frames(video_path, num_frames=5):\n","    \"\"\"Extrahiert Frames aus einem Video\"\"\"\n","    cap = cv2.VideoCapture(video_path)\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    frame_indices = np.linspace(0, total_frames-1, num_frames, dtype=int)\n","\n","    frames = []\n","    for idx in frame_indices:\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n","        ret, frame = cap.read()\n","        if ret:\n","            frames.append(frame)\n","    cap.release()\n","    return frames\n","\n","# Hilfsfunktion f√ºr Base64-Kodierung\n","def get_base64_encoded(image_path):\n","    \"\"\"Konvertiert ein Bild in Base64-String\"\"\"\n","    with open(image_path, \"rb\") as image_file:\n","        return base64.b64encode(image_file.read()).decode('utf-8')\n","\n","# Frames extrahieren und speichern\n","frames = extract_frames(video_file_path)\n","frame_paths = [f\"frame_{i}.jpg\" for i in range(len(frames))]\n","for i, frame in enumerate(frames):\n","    cv2.imwrite(frame_paths[i], frame)\n","\n","# Audio extrahieren und transkribieren\n","audio_path = \"extracted_audio.mp3\"\n","!ffmpeg -i \"{video_file_path}\" -q:a 0 -map a \"{audio_path}\" -y\n","\n","with open(audio_path, \"rb\") as audio_file:\n","    transcribed_text = openai.audio.transcriptions.create(\n","        model=\"whisper-1\",\n","        file=audio_file\n","    ).text\n","\n","# Frame-Analyse mit GPT-4o Vision\n","frame_descriptions = []\n","for frame_path in frame_paths:\n","    base64_img = get_base64_encoded(frame_path)\n","    response = openai.chat.completions.create(\n","        model=\"gpt-4o\",\n","        messages=[\n","            {\"role\": \"system\", \"content\": \"Du bist ein Experte f√ºr Videoanalyse. Beschreibe kurz und pr√§zise, was auf diesem Frame zu sehen ist.\"},\n","            {\"role\": \"user\", \"content\": [\n","                {\"type\": \"text\", \"text\": \"Beschreibe diesen Video-Frame mit einem kurzen Satz:\"},\n","                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img}\"}}\n","            ]}\n","        ],\n","        max_tokens=150\n","    )\n","    frame_descriptions.append(response.choices[0].message.content)\n","\n","# Gesamtanalyse mit GPT-4o-mini\n","analysis_prompt = f\"\"\"\n","Analysiere den folgenden Videoinhalt basierend auf dem Transkript und den Frame-Beschreibungen.\n","Gib eine detaillierte Zusammenfassung dessen, was im Video passiert.\n","\n","### Transkript:\n","{transcribed_text}\n","\n","### Frame-Beschreibungen:\n","{' | '.join([f\"Frame {i+1}: {desc}\" for i, desc in enumerate(frame_descriptions)])}\n","\"\"\"\n","\n","analysis_result = openai.chat.completions.create(\n","    model=\"gpt-4o-mini\",\n","    messages=[\n","        {\"role\": \"system\", \"content\": \"Du bist ein Experte f√ºr Videoanalyse.\"},\n","        {\"role\": \"user\", \"content\": analysis_prompt}\n","    ],\n","    temperature=0.2\n",").choices[0].message.content\n","\n","# Ergebnisse anzeigen\n","frames_html = \"\".join([\n","    f\"\"\"\n","    <div style=\"display: inline-block; margin: 10px; text-align: center;\">\n","        <img src=\"data:image/jpeg;base64,{get_base64_encoded(frame_path)}\" style=\"width: 200px; border: 1px solid #ddd;\">\n","        <p style=\"width: 200px; font-size: 12px;\">{frame_descriptions[i]}</p>\n","    </div>\n","    \"\"\" for i, frame_path in enumerate(frame_paths)\n","])\n","\n","# Markdown-Anzeige\n","display(Markdown(f\"\"\"\n","# üé• Videoanalyse\n","**üìÇ Datei:** `{video_file_path}`\n","**ü§ñ Modelle:** `Whisper-1` (Audio), `GPT-4o` (Frames), `GPT-4o-mini` (Gesamtanalyse)\n","\n","## üìù Transkript\n","{transcribed_text}\n","\n","## üñºÔ∏è Extrahierte Frames und Beschreibungen\n","\"\"\"))\n","display(HTML(frames_html))\n","display(Markdown(f\"\"\"\n","## üß† Videoanalyse-Ergebnis\n","{analysis_result}\n","\"\"\"))\n","\n","# Aufr√§umen\n","for path in frame_paths + [audio_path]:\n","    os.remove(path)"],"metadata":{"id":"cewmRLC-n1Y2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c477fff3"},"source":["# **4 <font color='orange'>|</font> Video-Objekterkennung**\n","\n","---"]},{"cell_type":"markdown","source":["Die Video-Objekterkennung ist ein wichtiger Bereich der Computer Vision, der sich mit der Identifikation und Klassifikation von Objekten in Videomaterial befasst. Im Gegensatz zur statischen Bilderkennung m√ºssen dabei nicht nur einzelne Frames analysiert, sondern auch zeitliche Zusammenh√§nge ber√ºcksichtigt werden.  \n","\n","Ziel dieses Kapitels ist es, ein grundlegendes Verst√§ndnis f√ºr die Herausforderungen und L√∂sungsans√§tze der Video-Objekterkennung zu vermitteln und praxisnahe Anwendungen zu demonstrieren. Zum Einsatz kommt **YOLO** (You Only Look Once), ein Echtzeit-Objekterkennungsmodell, das durch seine hohe Geschwindigkeit und Genauigkeit besonders f√ºr Anwendungen wie autonome Fahrzeuge, √úberwachungssysteme und interaktive KI-Systeme geeignet ist."],"metadata":{"id":"onWjUpoV0Vy8"}},{"cell_type":"code","source":["!uv pip install --system -q ultralytics opencv-python"],"metadata":{"id":"03nrbETSmmwu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import\n","import ultralytics\n","from ultralytics import YOLO\n","from google.colab import files\n","import cv2\n","from IPython.display import HTML\n","from base64 import b64encode\n","\n","# YOLO-Modell laden\n","model = YOLO(\"yolov8m.pt\")\n","\n","# Video-Datei hochladen\n","uploaded = files.upload()\n","\n","# Dateinamen abrufen (das erste hochgeladene Video)\n","video_path = list(uploaded.keys())[0]\n","\n","# YOLO-Vorhersage auf dem Video durchf√ºhren\n","# Hier werden Objekte erkannt und das Video mit Bounding Boxes gespeichert\n","results = model.predict(source=video_path, save=True, conf=0.25, save_txt=True, format='avi')\n","\n","# Das verarbeitete Video anzeigen\n","video_path = video_path.replace(\"mp4\", \"avi\")\n","processed_video_path = f\"{results[0].save_dir}/{video_path}\"\n","\n","# Video herunterladen\n","print(f\"Video mit Objekterkennung: {processed_video_path}\")\n","files.download(processed_video_path)"],"metadata":{"id":"P9ZQmS_OmzQB","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9d9cb086"},"source":["# **5 <font color='orange'>|</font> Bild-zu-Video (TTV)**\n","\n","---"]},{"cell_type":"markdown","source":["Der Schwerpunkt liegt auf der Anwendung von Bild-zu-Video (BTV) Large Language Models (LLMs), die mithilfe fortschrittlicher generativer KI-Technologien statische Bilder in realistische Videosequenzen umwandeln. Im Zentrum stehen Modelle wie Sora und Gen-2, leistungsstarke Systeme, die f√ºr die Transformation einzelner Bilder oder Bildfolgen in dynamische, koh√§rente Videoclips optimiert wurden. Diese Modelle wurden speziell entwickelt, um visuell ansprechende und detailreiche Animationen zu erzeugen, was sie besonders f√ºr kreative und professionelle Anwendungen attraktiv macht.\n","\n","Durch den Einsatz tief lernender Algorithmen erm√∂glichen diese Modelle eine hochqualitative Videosynthese, die nicht nur Bewegungsabl√§ufe realistisch nachbildet, sondern auch komplexe visuelle Effekte erzeugen kann. In diesem Modul werden die Funktionsweise dieser BTV-Modelle analysiert, ihre praktischen Anwendungsf√§lle untersucht und ihr Einfluss auf verschiedene Branchen bewertet ‚Äì von Filmproduktion und Werbung √ºber digitale Bildung bis hin zu virtuellen Simulationen und Produktvisualisierungen.\n","\n","F√ºhrende Modelle in diesem Bereich sind:\n","\n","- **Sora (OpenAI)** ‚Äì hochrealistische Videogenerierung mit beeindruckender Koh√§renz\n","- **Gen-2 (Runway)** ‚Äì vielseitiges Modell f√ºr kurze kreative Videos\n","- **Pika Labs** ‚Äì benutzerfreundliches Tool f√ºr einfache Animationen\n","- **Stable Video Diffusion** ‚Äì Open-Source-Ansatz f√ºr Videogenerierung\n","- **Lumiere (Google)** ‚Äì experimentelles Modell mit Fokus auf Bewegungsdynamik\n","\n","Jedes dieser Modelle hat eigene St√§rken und Einsatzgebiete, von der Erstellung von Marketingmaterial √ºber Bildungsvideos bis hin zu kreativen k√ºnstlerischen Projekten. Die Anzahl der Frames, die Aufl√∂sung und die visuelle Qualit√§t variieren je nach Modell. Die Nutzung dieser Technologie erfordert meist eine API-Verbindung zu den entsprechenden Diensten, da die lokale Ausf√ºhrung aufgrund der enormen Rechenanforderungen oft nicht praktikabel ist."],"metadata":{"id":"cw-XodgbBJ43"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6600bb55"},"outputs":[],"source":["import torch\n","from diffusers import StableVideoDiffusionPipeline\n","from diffusers.utils import export_to_video\n","from PIL import Image\n","from google.colab import files\n","from IPython.display import display, HTML, Markdown\n","\n","# Pipeline laden\n","pipe = StableVideoDiffusionPipeline.from_pretrained(\n","    \"stabilityai/stable-video-diffusion-img2vid-xt\",\n","    torch_dtype=torch.float16,\n","    variant=\"fp16\"\n",")\n","\n","# Pr√ºfen, ob eine GPU verf√ºgbar ist\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Verwendetes Ger√§t: {device}\")\n","\n","# Wenn eine CPU verwendet wird, sollte float32 statt float16 genutzt werden\n","if device == \"cpu\":\n","    pipe = pipe.to(torch.float32)\n","\n","pipe = pipe.to(device)\n","\n","# Pfad zu deinem Ausgangsbild und Bild laden\n","uploaded = files.upload()\n","\n","# Dateinamen abrufen (das erste hochgeladene Video)\n","image_path = list(uploaded.keys())[0]\n","# image_path = \"/content/avocado-g168eefcd0_1920.jpg\"\n","\n","# Bild mit PIL laden\n","image = Image.open(image_path)\n","image = image.resize((512, 512), Image.LANCZOS)\n","\n","# Parameter f√ºr die Videogenerierung\n","num_frames = 25  # 25 Frames f√ºr 5 Sekunden bei 5 FPS\n","fps = 5  # Frames pro Sekunde\n","motion_bucket_id = 180  # St√§rke der Bewegung (0-999)\n","\n","\n","# Video generieren\n","generator = torch.manual_seed(42)  # F√ºr Reproduzierbarkeit\n","frames = pipe(\n","    image,  # PIL-Bild-Objekt anstelle eines Strings\n","    decode_chunk_size=8,\n","    generator=generator,\n","    motion_bucket_id=motion_bucket_id,\n","    num_frames=num_frames,\n","    fps=fps\n",").frames[0]\n","\n","# Video speichern\n","output_path = \"output_video.mp4\"\n","export_to_video(frames, output_path, fps=fps)\n","files.download(output_path)\n","\n","# Ergebnisse zusammenfassen\n","result = {\n","    \"status\": \"success\",\n","    \"message\": \"Video wurde erfolgreich generiert\",\n","    \"video_url\": f\"Lokal gespeichert als '{output_path}'\"\n","}\n","\n","print(\"üìä Status-Informationen\")\n","print(\"Status:\", result[\"status\"])\n","print(\"Nachricht:\", result[\"message\"])\n","print(\"Video-URL:\", result[\"video_url\"])\n","\n","# üìù Markdown-Ausgabe mit Informationen zum generierten Video\n","video_info = f\"\"\"\n","## üé¨ Generiertes Video aus Bild\n","\n","**üñºÔ∏è Ausgangsbild:** {image_path}\n","**ü§ñ Modell:** Stable Video Diffusion (img2vid-xt)\n","**‚öôÔ∏è Parameter:**\n","- Frames: {num_frames}\n","- FPS: {fps}\n","- Motion Bucket ID: {motion_bucket_id}\n","\n","\"\"\"\n","\n","# Markdown-Ausgabe anzeigen\n","display(Markdown(video_info))"]},{"cell_type":"markdown","source":["# **A <font color='orange'>|</font> Aufgabe**\n","---"],"metadata":{"id":"gRJbfUG9-ujC"}},{"cell_type":"markdown","source":["Bereit f√ºr eine Herausforderung? In diesen Aufgaben werden Sie die leistungsstarken Vision-Modelle von **MediaPipe** kennenlernen. Mit diesen Modellen k√∂nnen sie u.a. Objekterkennung, Handgesten und Gesichtsausdr√ºcke in Echtzeit analysieren. Entdecken Sie die Potenziale der GenAI. Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen.\n","\n","[MediaPipe](https://mediapipe-studio.webapps.google.com/home)\n","\n","[MediaPipe L√∂sungsleitfaden](https://ai.google.dev/edge/mediapipe/solutions/guide?hl=de)"],"metadata":{"id":"Cb6-1kyIx-RF"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Video-Objekterkennung\n","</font></p>"],"metadata":{"id":"gmigTCGFyI04"}},{"cell_type":"markdown","source":["* **Aufgabe:**\n","    * Installieren Sie die erforderlichen Bibliotheken (MediaPipe, OpenCV).\n","    * Laden Sie ein vortrainiertes Objekterkennungsmodell von MediaPipe herunter.\n","    * Schreiben Sie ein Python-Skript, das ein Bild oder einen Videostream einliest und die erkannten Objekte mit Begrenzungsrahmen und Labels visualisiert.\n","    * Experimentieren Sie mit verschiedenen Bildern und Videostreams und beobachten Sie die Ergebnisse.\n","* **Zusatzaufgabe:** Versuchen Sie die Genauigkeit der Objekterkennung zu verbessern, indem sie verschiedene Modelle oder Parameter ausprobieren.\n"],"metadata":{"id":"_DNE1RJmyNBu"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Handgestenerkennung\n","</font></p>"],"metadata":{"id":"eq_on7-3yZyE"}},{"cell_type":"markdown","source":["* **Aufgabe:**\n","    * Nutzen Sie das MediaPipe-Modell f√ºr die Handgestenerkennung.\n","    * Erstellen Sie ein Programm, das die Positionen der Handgelenke und Finger erkennt und visualisiert.\n","    * Implementieren Sie eine einfache Gestenerkennung, z. B. die Erkennung von \"Daumen hoch\" oder \"Daumen runter\".\n","    * Lassen Sie Ihr Programm auf erkannte Gesten mit einer Ausgabe im Terminal reagieren.\n","* **Zusatzaufgabe:** Die Teilnehmer k√∂nnen komplexere Gesten implementieren oder das Programm mit einer anderen Anwendung (z. B. einem Pr√§sentationsprogramm) verbinden, um Gesten zur Steuerung zu verwenden.\n","\n"],"metadata":{"id":"epm_6Q2Ny9gt"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Gesichtserkennung und -verfolgung\n","</font></p>"],"metadata":{"id":"PucCcraJzJ9i"}},{"cell_type":"markdown","source":["* **Aufgabe:**\n","    * Verwenden Sie das MediaPipe-Modell f√ºr die Gesichtserkennung und -verfolgung.\n","    * Erstellen Sie ein Programm, das Gesichter in einem Videostream erkennt und die Gesichtspunkte (z. B. Augen, Nase, Mund) verfolgt.\n","    * Implementieren Sie eine einfache Gesichtsausdrucksanalyse, z. B. die Erkennung von L√§cheln oder Stirnrunzeln.\n","    * Die Daten die erhoben werden k√∂nnen visuell in Echtzeit dargestellt werden.\n","* **Zusatzaufgabe:** Sie k√∂nnen versuchen, die Gesichtsausdrucksanalyse zu verbessern oder das Programm mit einer anderen Anwendung (z. B. einem Avatar-Programm) zu verbinden, um Gesichtsausdr√ºcke auf den Avatar zu √ºbertragen.\n"],"metadata":{"id":"AFG7kuUZzLvd"}}]}
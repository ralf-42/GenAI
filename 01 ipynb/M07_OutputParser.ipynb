{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"collapsed_sections":["i0xOygLv6LaK","oYvUY6gMBKO1","MQFjKL7uDgfA","Q6PFBjUSDo_N","hzCTsn14DyU0","3TjZs_TRht1n"],"authorship_tag":"ABX9TyMTszkfGpx0wdpuHrgo7oo5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<p><font size=\"7\" color='grey'> <b>\n","Anwendung Generativer KI\n","</b></font> </br></p>"],"metadata":{"id":"Ih2CTVBnArVZ"}},{"cell_type":"markdown","source":["<p><font size=\"6\" color='grey'> <b>\n","OutputParser\n","</b></font> </br></p>\n","\n","\n","---"],"metadata":{"id":"6jJZ7wbdArVc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfdhPIzcEYRG","cellView":"form","collapsed":true},"outputs":[],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  Colab-Umfeld</font> </br></p>\n","# Installierte Python Version\n","import sys\n","print(f\"Python Version: \",sys.version)\n","# Installierte LangChain Bibliotheken\n","print()\n","print(\"Installierte LangChain Bibliotheken:\")\n","!pip list | grep '^langchain'\n","# Unterdrückt die \"DeprecationWarning\" von LangChain für die Memory-Funktionden\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"langchain\")"]},{"cell_type":"code","source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  SetUp API-Keys (setup_api_keys)</font> </br></p>\n","def setup_api_keys():\n","    \"\"\"Konfiguriert alle benötigten API-Keys aus Google Colab userdata\"\"\"\n","    from google.colab import userdata\n","    import os\n","    from os import environ\n","\n","    # Dictionary der benötigten API-Keys\n","    keys = {\n","        'OPENAI_API_KEY': 'OPENAI_API_KEY',\n","        'HF_TOKEN': 'HF_TOKEN',\n","        # Weitere Keys bei Bedarf\n","    }\n","\n","    # Keys in Umgebungsvariablen setzen\n","    for env_var, key_name in keys.items():\n","        environ[env_var] = userdata.get(key_name)\n","\n","    return {k: environ[k] for k in keys.keys()}\n","\n","# Verwendung\n","all_keys = setup_api_keys()\n","# Bei Bedarf einzelne Keys direkt zugreifen\n","# WEATHER_API_KEY = all_keys['WEATHER_API_KEY']"],"metadata":{"cellView":"form","id":"WD3Wwr6sESX8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **1 | OutputParser**\n","---"],"metadata":{"id":"i0xOygLv6LaK"}},{"cell_type":"markdown","metadata":{"id":"pC9A-LaYhsta"},"source":["LangChain stellt eine Vielzahl von Ausgabeparsern bereit, die speziell darauf ausgelegt sind, Informationen aus den Ergebnissen großer Sprachmodelle (LLMs) effizient zu extrahieren und zu strukturieren. Diese Parser sind essenzielle Bestandteile der LangChain-Architektur und häufig zentrale Elemente sogenannter LangChain-Ketten. Solche Ketten bestehen aus konfigurierbaren Abfolgen von Operationen, die Modellausgaben verarbeiten und für weiterführende Anwendungen nutzbar machen.\n","\n","Um die Erstellung und Verwaltung dieser Ketten zu vereinfachen, hat LangChain die LangChain Expression Language (LCEL) entwickelt. Im Folgenden wird erläutert, wie LCEL die erweiterte Konstruktion und Ausführung von LangChain-Ketten unterstützt. Anschließend wird untersucht, wie verschiedene Ausgabeparser innerhalb dieser Ketten eingesetzt werden können, um LLM-Ausgaben gezielt zu analysieren und in verwertbare Informationen zu überführen.\n","\n","**Warum sind OutputParser so wichtig?**\n","+ LLMs geben standardmäßig unstrukturierte Texte zurück.\n","+ OutputParser sind nötig, um das LLM-Output strukturiert weiterzuverarbeiten.\n","+ Besonders bei komplexen Anwendungen (z. B. Ketten mit mehreren Modellen, Agenten oder RAG-Systemen) müssen die Antworten klar definiert sein."]},{"cell_type":"markdown","source":["# **2 | Strukturierter Parser**\n","---"],"metadata":{"id":"oYvUY6gMBKO1"}},{"cell_type":"markdown","metadata":{"id":"HMLrKTqgSRkM"},"source":["\n","Der **Structured Output Parser** in LangChain ermöglicht die strukturierte Verarbeitung der Ausgaben großer Sprachmodelle (LLMs). Dies ist besonders hilfreich, wenn Informationen aus mehreren Feldern extrahiert und kategorisiert werden müssen. Durch die Segmentierung der Modellausgabe in definierte Bereiche wird die Interpretation und Handhabung der Daten vereinfacht.\n","\n","Während der **Pydantic/JSON-Parser** eine leistungsfähigere Lösung für komplexe Datenstrukturen darstellt, eignet sich der **Structured Output Parser** besonders für Umgebungen mit begrenzten Rechenressourcen oder für Modelle mit geringer Leistungsfähigkeit. Er bietet eine einfache und effiziente Möglichkeit, die Ausgabe zu strukturieren, ohne das System zu stark zu belasten.\n","\n","Der erste Schritt bei der Nutzung dieses Parsers besteht in der Erstellung eines **ResponseSchemas**, das definiert, welche Werte extrahiert werden sollen. Jeder Wert wird dabei detailliert beschrieben. Anschließend wird der **StructuredOutputParser** aus einer Liste dieser Schemata erstellt."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JvzHRtXd3hEd"},"outputs":[],"source":["# Abschnitt 0: Installation und API-Key\n","!uv pip install --system --upgrade --quiet langchain_community langchain_openai gradio"]},{"cell_type":"code","source":["# Abschnitt 1: Importe\n","from IPython.display import display, Markdown\n","\n","from langchain.output_parsers.structured import ResponseSchema, StructuredOutputParser\n","from langchain_core.prompts import PromptTemplate\n","from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","\n","# Abschnitt 2: Konstanten definieren\n","MODEL_NAME = \"gpt-4o-mini\"\n","TEMPERATURE = 0.0\n","\n","# Abschnitt 3: Schema und Parser definieren\n","response_schemas = [\n","    ResponseSchema(name=\"answer\", description=\"Antwort auf die Frage des Benutzers\"),\n","    ResponseSchema(name=\"source\", description=\"Verwendete Quelle (Website) für die Antwort\")\n","]\n","\n","parser = StructuredOutputParser.from_response_schemas(response_schemas)\n","\n","# Abschnitt 4: Prompt-Template erstellen\n","prompt = PromptTemplate(\n","    template=\"\"\"\n","    Beantworte die folgende Frage so präzise wie möglich.\n","    Gib auch die Quelle deiner Information an.\n","\n","    Frage: {question}\n","\n","    {format_instructions}\n","    \"\"\",\n","    input_variables=[\"question\"],\n","    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",")\n","\n","# Abschnitt 5: LLM-Kette mit LCEL erstellen\n","chain = (\n","    {\"question\": RunnablePassthrough()}\n","    | prompt\n","    | ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)\n","    | parser\n",")\n","\n","# Abschnitt 6: Hauptprogramm\n","def get_answer(question: str) -> dict:\n","    \"\"\" Verarbeitet eine Frage und gibt die strukturierte Antwort zurück. \"\"\"\n","    return chain.invoke(question)\n","\n","# Beispielnutzung\n","if __name__ == \"__main__\":\n","    test_question = \"Was ist Machine Learning?\"\n","    result = get_answer(test_question)\n","    display(Markdown(f\"**Antwort:** {result['answer']}\"))\n","    display(Markdown(f\"**Quelle:** {result['source']}\"))"],"metadata":{"id":"_m84Eigb4LDn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_dQTrDn8TuUk"},"source":["Hier ist eine Vorlage für eine Eingabeaufforderung (Prompt), die sowohl die gestellte Frage als auch die generierten Formatierungsanweisungen berücksichtigt. Diese kann flexibel an verschiedene Anwendungsfälle angepasst werden."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IX9IhDJpBDNN"},"outputs":[],"source":["question = \"Wann wurde Python eingeführt?\"\n","\n","result = get_answer(question)\n","display(Markdown(f\"**Antwort:** {result['answer']}\"))\n","display(Markdown(f\"**Quelle:** {result['source']}\"))"]},{"cell_type":"markdown","metadata":{"id":"sCC1bV0dUZgt"},"source":["Wie ersichtlich ist, nutzt LangChain Schemata, um das JSON-Format für die Antwortausgabe festzulegen. Durch dieses Format lassen sich die einzelnen Werte problemlos analysieren.  \n","\n","Nun wird eine Kette erstellt, die den StructuredOutputParser einbindet und das LLM abfragt."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CWjcI5zB9j9k"},"outputs":[],"source":["MODEL = \"gpt-4o-mini\"\n","TEMPERATURE = 0\n","\n","# Initialisieren des LLM\n","llm = ChatOpenAI(\n","    model=MODEL,\n","    temperature=TEMPERATURE,\n","    n=1\n",")\n","chain = prompt | llm | parser"]},{"cell_type":"markdown","metadata":{"id":"gux36efBgSR9"},"source":["Jetzt werden eine Frage, die dazugehörige Antwort sowie die Quelle der Frage dargestellt."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SxxA2nKN9j7B"},"outputs":[],"source":["question = \"Wann wurde Python eingeführt?\"\n","result = chain.invoke({\"question\": question})\n","display(Markdown(f\"**Antwort:** {result['answer']}\"))\n","display(Markdown(f\"**Quelle:** {result['source']}\"))"]},{"cell_type":"markdown","metadata":{"id":"wR5J9U9bg5Br"},"source":["<p><font color='black' size=\"5\">\n","Erkennen und Übersetzen\n","</font></p>"]},{"cell_type":"markdown","source":["Nun wird ein Beispiel mit einer größeren Anzahl an Werten getestet. Das folgende Programm nimmt Texte in beliebigen Sprachen entgegen und übersetzt sie ins Englische, Spanische und Chinesische."],"metadata":{"id":"nN1AWb1m9OK4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9snGgL4riemY"},"outputs":[],"source":["response_schemas = [\n","    ResponseSchema(name=\"detected\", description=\"Die Sprache ist Nutzer-Eingabe\"),\n","    ResponseSchema(name=\"english\", description=\"English translation\"),\n","    ResponseSchema(name=\"spanish\", description=\"Spanish translation\"),\n","    ResponseSchema(name=\"chinese\", description=\"Chinese translation\"),\n","]\n","output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n","\n","format_instructions = output_parser.get_format_instructions()\n","prompt = PromptTemplate(\n","    template=\"Übersetze in die angegebenen Sprachen.\\n{format_instructions}\\n{question}\",\n","    input_variables=[\"question\"],\n","    partial_variables={\"format_instructions\": format_instructions},\n",")\n","\n","chain = prompt | llm | output_parser"]},{"cell_type":"markdown","metadata":{"id":"sa6_dAk2ugre"},"source":["Zunächst wird ein deutscher Satz getestet, um zu beobachten, wie die Übersetzung in die drei Zielsprachen erfolgt."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c9wKUJR3tiYP"},"outputs":[],"source":["question = \"Wann wurde Python eingeführt?\"\n","chain.invoke({\"question\": question})"]},{"cell_type":"markdown","metadata":{"id":"MQFjKL7uDgfA"},"source":["# **3 | Datenformat Parser**\n","---"]},{"cell_type":"markdown","source":["LangChain bietet eine breite Palette an Parsern, die unterschiedliche Datenformate verarbeiten können, was seine Einsatzmöglichkeiten erheblich erweitert. Es unterstützt unter anderem die nahtlose Integration von Pandas-Datenrahmen, kommagetrennten Listen, JSON-Strukturen sowie Datums- und Zeitobjekten. Diese Flexibilität ermöglicht eine effiziente Anpassung an verschiedene Arten von Dateneingaben und macht LangChain zu einem leistungsstarken Werkzeug für die Analyse und Verarbeitung von Daten. Im Folgenden werden einige dieser Parser genauer betrachtet, ihre praktischen Anwendungen demonstriert und aufgezeigt, wie sie zur Optimierung von Prozessen und zur Gewinnung wertvoller Erkenntnisse beitragen können."],"metadata":{"id":"Q1oRFrLCx8eW"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","CSV\n","</font></p>"],"metadata":{"id":"EBERl78zyIzh"}},{"cell_type":"markdown","source":["Der **CommaSeparatedListOutputParser** ermöglicht die Umwandlung von LLM-Ausgaben in eine durch Kommas getrennte Liste und extrahiert diese als native Python-Liste. Dies ist besonders nützlich, wenn strukturierte Daten aus einem Sprachmodell extrahiert und für weitere Verarbeitungsschritte verwendet werden sollen. Indem der Parser die Ausgabe direkt in eine Listenstruktur überführt, erleichtert er die Handhabung und Weiterverarbeitung der generierten Daten in verschiedenen Anwendungen."],"metadata":{"id":"aptyNpJ0ynir"}},{"cell_type":"code","source":["# Abschnitt 1: Importe\n","from langchain.output_parsers import CommaSeparatedListOutputParser\n","from langchain_core.prompts import PromptTemplate\n","from langchain_openai import ChatOpenAI\n","\n","# Abschnitt 3: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATURE = 0\n","\n","# Abschnitt 4: Chat-Komponenten initialisieren\n","output_parser = CommaSeparatedListOutputParser()\n","\n","format_instructions = output_parser.get_format_instructions()\n","chat_prompt = PromptTemplate(\n","    template=\"List ten {subject}.\\n{format_instructions}\",\n","    input_variables=[\"subject\"],\n","    partial_variables={\"format_instructions\": format_instructions},\n",")\n","\n","chat_model = ChatOpenAI(\n","    model=MODEL,\n","    temperature=TEMPERATURE,\n","    n=1\n",")\n","\n","# Abschnitt 5: Funktionen definieren\n","def generate_list(subject: str):\n","    \"\"\"  Generiert eine Liste mit zehn Elementen zu einem gegebenen Thema.  \"\"\"\n","    chain = chat_prompt | chat_model | output_parser\n","    return chain.invoke({\"subject\": subject})\n","\n","# Abschnitt 6: Hauptprogramm\n","def main():\n","    \"\"\"Startet die Hauptkonversation und gibt eine Liste von Städten aus.\"\"\"\n","    cities = generate_list(\"Städte\")\n","    print(type(cities), cities)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"PT2ZjJv_2tBt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mE2avWGaysc3"},"source":["<p><font color='black' size=\"5\">\n","JSON\n","</font></p>"]},{"cell_type":"markdown","source":["Die Ausgabe des LLM kann im JSON-Format strukturiert werden. In diesem Beispiel wird ein Satz erkannt, der als Englisch identifiziert wird, und anschließend ins Spanische, Französische und Chinesische übersetzt."],"metadata":{"id":"S69wM83z-1Hh"}},{"cell_type":"code","source":["# Abschnitt 1: Importe\n","from langchain_core.output_parsers import JsonOutputParser\n","from langchain_core.prompts import PromptTemplate\n","from pydantic import BaseModel, Field\n","from langchain_openai import ChatOpenAI\n","\n","# Abschnitt 2: Konstanten und Datenmodell\n","class Translate(BaseModel):\n","    \"\"\"Datenmodell für mehrsprachige Übersetzungen\"\"\"\n","    detected: str = Field(description=\"Erkannte Ausgangssprache\")\n","    spanish: str = Field(description=\"Spanische Übersetzung\")\n","    french: str = Field(description=\"Französische Übersetzung\")\n","    chinese: str = Field(description=\"Chinesische Übersetzung\")\n","\n","# Abschnitt 3: Funktionen\n","def translate_text(text: str) -> dict:\n","    \"\"\"Übersetzt den Text in mehrere Sprachen\"\"\"\n","    # Parser und LLM konfigurieren\n","    parser = JsonOutputParser(pydantic_object=Translate)\n","    llm = ChatOpenAI(model=\"gpt-4\", temperature=0.0)\n","\n","    # Prompt erstellen und Kette ausführen\n","    prompt = PromptTemplate(\n","        template=\"Übersetze die Benutzereingabe.\\n{format_instructions}\\n{input}\\n\",\n","        input_variables=[\"input\"],\n","        partial_variables={\"format_instructions\": parser.get_format_instructions()}\n","    )\n","\n","    chain = prompt | llm | parser\n","    return chain.invoke({\"input\": text})\n","\n","# Abschnitt 4: Hauptprogramm\n","# Beispieltext übersetzen und Ergebnisse ausgeben\n","result = translate_text(\"What is your name?\")\n","print(type(result), result)\n","print()\n","print(f\"Erkannte Sprache: {result['detected']}\")\n","print(f\"Spanisch: {result['spanish']}\")\n","print(f\"Französisch: {result['french']}\")\n","print(f\"Chinesisch: {result['chinese']}\")"],"metadata":{"id":"Ixc32Np6_er_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y09-epHS5McO"},"source":["<p><font color='black' size=\"5\">\n","Datum/Uhrzeit\n","</font></p>"]},{"cell_type":"markdown","source":["Langchain bietet mit dem **DatetimeOutputParser** eine spezielle Funktion zur Interpretation von Datums- und Zeitangaben aus Texten. Sie erkennt unterschiedliche Formate und wandelt diese in ein standardisiertes Zeitformat um. Dies ist besonders hilfreich für Anwendungen wie Terminplanung, Datenanalyse oder andere Bereiche, in denen eine präzise Verarbeitung zeitlicher Informationen erforderlich ist. Der **DatetimeOutputParser** erleichtert die Verarbeitung von Datums- und Uhrzeitangaben und trägt dazu bei, dass Anwendungen zeitbezogene Daten effizient verwalten und nutzen können."],"metadata":{"id":"-zDlyRDUM4qS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRuzzhXF5Oa1"},"outputs":[],"source":["# Abschnitt 1: Importe\n","from langchain.output_parsers import DatetimeOutputParser\n","from langchain_core.prompts import PromptTemplate\n","from langchain_openai import OpenAI\n","\n","# Abschnitt 2: Konstanten\n","TEMPLATE = \"\"\"Beantworte die Frage zu genau wie möglich:\n","\n","{question}\n","\n","{format_instructions}\"\"\"\n","\n","# Abschnitt 3: Komponenten initialisieren\n","llm = ChatOpenAI(\n","    model=\"gpt-4o-mini\",\n","    temperature=0.2,\n","    n=1\n",")\n","\n","output_parser = DatetimeOutputParser()\n","prompt = PromptTemplate(\n","    template=TEMPLATE,\n","    input_variables=['question'],\n","    partial_variables={'format_instructions': output_parser.get_format_instructions()}\n",")"]},{"cell_type":"markdown","metadata":{"id":"roqInbqGFdUb"},"source":["Wir können die Eingabeaufforderung anzeigen, die wir zum Abrufen von Daten verwenden werden."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HecixPQl6kYk"},"outputs":[],"source":["print(prompt)"]},{"cell_type":"markdown","metadata":{"id":"kg5ZkOetFiPx"},"source":["Wir erstellen die Kette, die wir zum Parsen von Daten verwenden werden."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2w5gYty_5pKd"},"outputs":[],"source":["chain = prompt | llm | output_parser"]},{"cell_type":"markdown","metadata":{"id":"dD-gNvBHFooj"},"source":["Wir werden nach zwei Daten fragen, einem realen und einem fiktiven."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ymu48Ggg518q"},"outputs":[],"source":["output = chain.invoke({\"question\": \"Wann wurde die Progammiersprache Python eingeführt?\"})\n","print(output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nOPWYpxr6As1"},"outputs":[],"source":["output = chain.invoke({\"question\": \"An welchem Tag startet der Krieg im Videospiel Fallout?\"}) #  23. Oktober 2077\n","print(output)"]},{"cell_type":"markdown","metadata":{"id":"Q6PFBjUSDo_N"},"source":["\n","# **4 | Pydantic Parser**\n","---"]},{"cell_type":"markdown","source":["Pydantic ist eine Python-Bibliothek zur Datenvalidierung und Einstellungsverwaltung, die auf den Typanmerkungen von Python basiert. Sie ermöglicht eine effiziente und einfache Validierung sowie Umwandlung von Daten und nutzt dabei das standardisierte Typisierungssystem von Python.\n","\n","**Wichtige Funktionen von Pydantic:**\n","- **Datenvalidierung:** Überprüft, ob Daten den erwarteten Formaten entsprechen, und konvertiert sie bei Bedarf in die korrekten Typen.\n","- **Unterstützung für Code-Editoren:** Dank der Nutzung von Python-Typanmerkungen bieten moderne Editoren Funktionen wie Autovervollständigung und Typprüfung für Pydantic-Modelle.\n","- **Fehlermeldungen:** Detaillierte und verständliche Fehlermeldungen helfen, Probleme bei der Datenvalidierung gezielt zu identifizieren.\n","- **Einstellungsverwaltung:** Erleichtert das Laden und Verwalten von Konfigurationsparametern aus verschiedenen Quellen, darunter Umgebungsvariablen und JSON-Dateien.\n","- **Erweiterbarkeit:** Modelle lassen sich durch Methoden und Eigenschaften anpassen, und benutzerdefinierte Validierungen können mithilfe von Pydantic-Dekoratoren hinzugefügt werden.\n","- **Integration mit anderen Bibliotheken:** Besonders in Verbindung mit FastAPI verbessert Pydantic die Verwaltung und Validierung von API-Daten erheblich.\n","\n","Aufgrund seiner Flexibilität und Benutzerfreundlichkeit ist Pydantic besonders nützlich für Webentwicklung und datenintensive Anwendungen, bei denen eine zuverlässige Datenverarbeitung erforderlich ist."],"metadata":{"id":"ZB7QnNtDzaiz"}},{"cell_type":"markdown","source":["\n","\n","**Pydantic in LangChain:**    \n","LangChain, eine Bibliothek zur Entwicklung von Anwendungen mit Sprachmodellen, nutzt Pydantic für die strukturierte Verarbeitung von Modellantworten. Der **PydanticOutputParser** gewährleistet, dass die Ausgaben eines Sprachmodells einer vorgegebenen Struktur entsprechen. Dies ist besonders vorteilhaft für Anwendungen, bei denen konsistente Datenformate entscheidend sind – beispielsweise bei der Datenextraktion, API-Antworten oder der automatisierten Weiterverarbeitung von Modell-Ausgaben."],"metadata":{"id":"oxlZwhl-SfMX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"O_J3QNntH18p"},"outputs":[],"source":["from langchain.output_parsers import PydanticOutputParser\n","from langchain_core.prompts import PromptTemplate\n","from pydantic import BaseModel, Field, validator\n","from langchain_openai import ChatOpenAI\n","\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATURE = 0\n","\n","# Initialisieren des OpenAI LLM\n","llm = ChatOpenAI(\n","    model=MODEL,\n","    temperature=TEMPERATURE,\n","    n=1\n",")"]},{"cell_type":"markdown","metadata":{"id":"KUsS8qUHO2oc"},"source":["Hier ist ein Beispielcode, der ein Sprachmodell (LLM) verwendet, um einen Witz zu generieren, wobei der PydanticOutputParser sicherstellt, dass die Ausgabe mit einem Fragezeichen endet. Diese Validierung hilft dabei, eine konsistente Struktur zu gewährleisten, insbesondere da LLMs manchmal unvorhersehbare Antworten liefern können."]},{"cell_type":"markdown","source":["**Erklärung:**   \n","Das Programm ist ein einfacher Filmografie-Generator mit einer Geschlechtervalidierung. Hier die Hauptfunktionen:\n","\n","1. Es definiert eine Klasse `Schauspieler` mit drei Feldern:\n","   - Name des Schauspielers\n","   - Geschlecht (m/w)\n","   - Liste der Filme\n","\n","2. Es enthält einen Validator, der prüft ob das Geschlecht 'männlich' (m) ist. Falls ein weibliches Geschlecht (w) eingegeben wird, erzeugt es einen Fehler.\n","\n","3. Das Programm nutzt ChatGPT (über die OpenAI API) um:\n","   - Den Namen zu erkennen\n","   - Das Geschlecht zu bestimmen\n","   - Eine Liste von Filmen zu generieren\n","\n","4. Wenn man nach einer Schauspielerin fragt (wie Emma Stone), gibt das Programm einen Fehler aus, da der Validator nur männliche Schauspieler zulässt.\n","\n","Im konkreten Beispiel wird nach Emma Stone gefragt - das Programm erkennt sie als Schauspielerin (w), aber der Validator löst einen Fehler aus, da nur männliche Schauspieler (m) erlaubt sind."],"metadata":{"id":"GCyr_IUWS1iv"}},{"cell_type":"code","source":["from langchain.output_parsers import PydanticOutputParser\n","from langchain.prompts import PromptTemplate\n","from langchain_openai import ChatOpenAI\n","from pydantic import BaseModel, Field, field_validator\n","from langchain.schema.runnable import RunnablePassthrough\n","\n","class Schauspieler(BaseModel):\n","    name: str = Field(description=\"Name des Schauspielers/der Schauspielerin\")\n","    geschlecht: str = Field(description=\"Geschlecht (m/w)\")\n","    filmnamen: list[str] = Field(description=\"Liste der Filme\")\n","\n","    @field_validator(\"geschlecht\")\n","    def validiere_geschlecht(cls, v: str) -> str:\n","        if v.lower() != 'm': raise ValueError(\"Geschlecht muss 'männlich' sein\")\n","        return v.lower()\n","\n","# Chain Komponenten erstellen\n","parser = PydanticOutputParser(pydantic_object=Schauspieler)\n","llm = ChatOpenAI(temperature=0)\n","\n","# Prompt Template\n","prompt = PromptTemplate(\n","    template=\"Beantworte die Nutzeranfrage:\\n{formatierungsanweisungen}\\n{abfrage}\",\n","    input_variables=[\"abfrage\"],\n","    partial_variables={\"formatierungsanweisungen\": parser.get_format_instructions()},\n",")\n","\n","# LCEL Chain mit Pipe-Operator\n","chain = ({\"abfrage\": RunnablePassthrough()} | prompt | llm | parser)\n","\n","# Ausführung\n","try:\n","    frage = \"Erstelle eine Filmografie für Emma Stone.\"\n","    ergebnis = chain.invoke(frage)\n","    print(f\"Name: {ergebnis.name} ({ergebnis.geschlecht})\")\n","    print(f\"Filme: {', '.join(ergebnis.filmnamen)}\")\n","except ValueError as e:\n","    print(f\"Fehler: {e}\")"],"metadata":{"collapsed":true,"id":"O5gLruVieQlR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Kurze Fehlermeldung:"],"metadata":{"id":"zbCnHnUsdnYx"}},{"cell_type":"code","source":["try:\n","    frage = \"Erstelle eine Filmografie für Emma Stone.\"\n","    ergebnis = (prompt | llm | parser).invoke({\"abfrage\": frage})\n","    print(f\"Name: {ergebnis.name} ({ergebnis.geschlecht})\")\n","    print(f\"Filme: {', '.join(ergebnis.filmnamen)}\")\n","except ValueError as e:\n","    # Nur die wichtigste Fehlermeldung ausgeben\n","    error_msg = str(e)\n","    if \"Value error\" in error_msg:\n","        print(f\"Fehler: {error_msg.split('Value error,')[1].split('[')[0].strip()}\")\n","    else:\n","        print(f\"Fehler: {error_msg}\")"],"metadata":{"id":"6WGVdtBWdjXj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hzCTsn14DyU0"},"source":["# **5 | Eigene Parser**\n","---"]},{"cell_type":"markdown","source":["In bestimmten Szenarien kann es sinnvoll sein, einen benutzerdefinierten Parser zu erstellen, um die Modellausgabe eindeutig zu formatieren.  \n","\n","Dafür bietet sich die Verwendung von **RunnableLambda** oder **RunnableGenerator** in LCEL an, was für viele Fälle ein guten Ansatz ist.  "],"metadata":{"id":"DEXj9woZzzy0"}},{"cell_type":"markdown","metadata":{"id":"isGwPhF4GUM5"},"source":["<p><font color='black' size=\"5\">\n","Trennen Erläuterung/Code\n","</font></p>"]},{"cell_type":"markdown","source":["\n","\n","Large Language Models (LLMs) wie GPT-4 können Text generieren, der Code und erklärende Beschreibungen nahtlos vermischt. Dies kann zwar für Lern- und Dokumentationszwecke unglaublich nützlich sein, kann aber eine Herausforderung darstellen, wenn aus solchen Ausgaben mit gemischtem Inhalt nur der Code extrahiert und ausgeführt werden muss. Um dies zu beheben, implementieren wir eine einfache Funktion, die nicht-Python-Codezeilen aus einer gegebenen Textzeichenfolge entfernt.\n","\n","Bei diesem Ansatz werden reguläre Ausdrücke verwendet, um Zeilen zu identifizieren und beizubehalten, die der typischen Python-Syntax entsprechen, während Zeilen verworfen werden, die beschreibender Text zu sein scheinen. Aufgrund der inhärenten Komplexität und Variabilität sowohl von Python-Code als auch von natürlicher Sprache kann diese Methode jedoch nie perfekt sein. Sie basiert auf heuristischen Mustern, die Code manchmal fälschlicherweise als Text klassifizieren oder umgekehrt.\n","\n","Im nächsten Abschnitt werden wir untersuchen, wie ein anderes LLM beim Entfernen von Nicht-Python-Code helfen kann und möglicherweise eine ausgefeiltere und genauere Lösung bietet. Das folgende Beispiel enthält eine Mischung aus LLM-Kommentaren und generiertem Code.\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"sPq64ydYgJ42"}},{"cell_type":"code","source":["from langchain_core.runnables import RunnableLambda\n","from langchain_openai import ChatOpenAI\n","from IPython.display import display, Markdown\n","\n","\n","# 1. LLM-Modell definieren\n","llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n","\n","# 2. Funktion zur Trennung von Erläuterung und Code\n","def parse_ai_message(ai_message):\n","    \"\"\"Trennt die Erläuterung und den Code aus einer AIMessage und gibt sie separat zurück.\"\"\"\n","    text = ai_message.content  # Extrahiere den reinen Textinhalt der AIMessage\n","\n","    if \"```\" in text:\n","        # Trennen der Erläuterung und des Codes\n","        parts = text.split(\"```\")\n","        explanation = parts[0].strip()\n","        code = parts[1].strip() if len(parts) > 1 else \"\"\n","    else:\n","        # Falls kein Codeblock vorhanden ist, geben wir nur die Erläuterung zurück\n","        explanation = text.strip()\n","        code = \"\"\n","\n","    return explanation, code  # Rückgabe von zwei separaten Werten\n","\n","# 3. RunnableLambda für das Parsing erstellen\n","parser = RunnableLambda(parse_ai_message)\n","\n","# 4. LLM und Parser verketten (Pipeline)\n","pipeline = llm | parser\n","\n","# 5. Eingabe an die Pipeline senden und Ergebnis ausgeben\n","explanation, code = pipeline.invoke(\"Erkläre mir, wie man eine einfache Funktion in Python erstellt, und gib ein Beispiel.\")\n","\n","display(Markdown(\"## Erläuterung\"))\n","display(Markdown(explanation))\n","display(Markdown(\"## Code\"))\n","print(code)"],"metadata":{"id":"6wjQlnF7qeso"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3TjZs_TRht1n"},"source":["# **A | Aufgabe**\n","---"]},{"cell_type":"markdown","source":["Die Aufgabestellungen unten bieten Anregungen, Sie können aber auch gerne eine andere Herausforderung angehen."],"metadata":{"id":"smOJ1YD15z_Q"}},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","JSON-Parser mit LangChain\n","</font></p>"],"metadata":{"id":"L5IWdaBS0UZD"}},{"cell_type":"markdown","source":["\n","**Ziel:** Verständnis für den Einsatz von `JsonOutputParser` in LangChain.\n","\n","**Aufgabe:**  \n","1. Nutze den `JsonOutputParser` von LangChain, um eine KI-Antwort in JSON zu formatieren.  \n","2. Lasse ein Language Model (z. B. OpenAI GPT) eine Liste von drei zufälligen Städten in Deutschland generieren.  \n","3. Verwende den Parser, um die Ausgabe in ein JSON-Format umzuwandeln.\n","\n","**Erwartete Ausgabe (Beispiel):**\n","```json\n","{\n","  \"cities\": [\"Berlin\", \"Hamburg\", \"München\"]\n","}\n","```\n"],"metadata":{"id":"JTdhJeN8u99p"}},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Extraktion von Schlüsselwerten\n","</font></p>"],"metadata":{"id":"UCpw6-OHvHJJ"}},{"cell_type":"markdown","source":["\n","**Ziel:** Nutzung des `PydanticOutputParser`, um strukturierte Daten aus natürlicher Sprache zu extrahieren.\n","\n","**Aufgabe:**  \n","1. Definiere eine `Pydantic`-Datenklasse mit den Feldern: `name` (str), `alter` (int), `stadt` (str).  \n","2. Verwende ein Language Model, um aus einer gegebenen Beschreibung eine Person zu extrahieren.  \n","3. Parse die Antwort mit dem `PydanticOutputParser`.\n","\n","**Beispiel-Eingabe:**\n","> \"Max Mustermann ist 35 Jahre alt und lebt in Berlin.\"\n","\n","**Erwartete Ausgabe (Pydantic-Modell):**\n","```json\n","{\n","  \"name\": \"Max Mustermann\",\n","  \"alter\": 35,\n","  \"stadt\": \"Berlin\"\n","}\n","```"],"metadata":{"id":"geRHkOMCvKum"}},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Parser für Listenformate\n","</font></p>"],"metadata":{"id":"8-AzxHafvSE_"}},{"cell_type":"markdown","source":["\n","**Ziel:** Implementierung eines eigenen Parsers zur Umwandlung von KI-Ausgaben in Listen.\n","\n","**Aufgabe:**  \n","1. Erstelle eine eigene Parser-Klasse, die eine durch Kommas getrennte Liste in ein Listenformat umwandelt.  \n","2. Verwende diesen Parser, um eine Liste von fünf beliebten Büchern aus einer Language-Model-Antwort zu extrahieren.\n","\n","**Beispiel-Eingabe:**\n","> \"Die Verwandlung, Faust, Der Prozess, Die Blechtrommel, Der Vorleser\"\n","\n","**Erwartete Ausgabe:**\n","```python\n","[\"Die Verwandlung\", \"Faust\", \"Der Prozess\", \"Die Blechtrommel\", \"Der Vorleser\"]\n","```"],"metadata":{"id":"ytCh-C2uvUFw"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Kombination Parser & PromptTemplate\n","</font></p>"],"metadata":{"id":"I3LRv5YAviSF"}},{"cell_type":"markdown","source":["\n","**Ziel:** Verwendung von `StructuredOutputParser` in Kombination mit `PromptTemplate`.\n","\n","**Aufgabe:**  \n","1. Erstelle ein `PromptTemplate`, das eine strukturierte Antwort über ein Land ausgibt (Name, Hauptstadt, Einwohnerzahl).  \n","2. Verwende den `StructuredOutputParser`, um die Antwort in ein Dictionary zu konvertieren.\n","\n","**Beispiel-Prompt:**\n","> \"Gib mir Informationen zu Frankreich im folgenden JSON-Format: { 'name': '...', 'hauptstadt': '...', 'einwohner': ... }.\"\n","\n","**Erwartete Ausgabe:**\n","```json\n","{\n","  \"name\": \"Frankreich\",\n","  \"hauptstadt\": \"Paris\",\n","  \"einwohner\": 67000000\n","}\n","```\n"],"metadata":{"id":"bZqMjEWavk_i"}}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["i0xOygLv6LaK","oYvUY6gMBKO1","6ls-Rt2LwCfT","wR5J9U9bg5Br","MQFjKL7uDgfA","hzCTsn14DyU0","3TjZs_TRht1n"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)"],"metadata":{"id":"Ih2CTVBnArVZ"}},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b>\n","OutputParser\n","</b></font> </br></p>\n","\n","\n","---"],"metadata":{"id":"6jJZ7wbdArVc"}},{"cell_type":"code","source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>Umgebung einrichten</font> </br></p>\n","!uv pip install --system --prerelease allow -q git+https://github.com/ralf-42/genai_lib\n","from genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint\n","setup_api_keys(['OPENAI_API_KEY', 'HF_TOKEN'], create_globals=False)\n","print()\n","check_environment()\n","print()\n","get_ipinfo()"],"metadata":{"cellView":"form","id":"F27EoMohZg_X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1 | OutputParser\n","---"],"metadata":{"id":"i0xOygLv6LaK"}},{"cell_type":"markdown","metadata":{"id":"pC9A-LaYhsta"},"source":["LangChain stellt eine Vielzahl von Ausgabeparsern bereit, die speziell darauf ausgelegt sind, Informationen aus den Ergebnissen gro√üer Sprachmodelle (LLMs) effizient zu extrahieren und zu strukturieren. Diese Parser sind essenzielle Bestandteile der LangChain-Architektur und h√§ufig zentrale Elemente sogenannter LangChain-Ketten. Solche Ketten bestehen aus konfigurierbaren Abfolgen von Operationen, die Modellausgaben verarbeiten und f√ºr weiterf√ºhrende Anwendungen nutzbar machen.\n","\n","\n","**Warum sind OutputParser so wichtig?**\n","+ LLMs geben standardm√§√üig unstrukturierte Texte zur√ºck.\n","+ OutputParser sind n√∂tig, um das LLM-Output strukturiert weiterzuverarbeiten.\n","+ Besonders bei komplexen Anwendungen (z. B. Ketten mit mehreren Modellen, Agenten oder RAG-Systemen) m√ºssen die Antworten klar definiert sein."]},{"cell_type":"markdown","source":["# 2 | Strukturierter Parser (einfach)\n","---"],"metadata":{"id":"oYvUY6gMBKO1"}},{"cell_type":"markdown","metadata":{"id":"HMLrKTqgSRkM"},"source":["\n","Der **Structured Output Parser** in LangChain erm√∂glicht die strukturierte Verarbeitung der Ausgaben gro√üer Sprachmodelle (LLMs). Dies ist besonders hilfreich, wenn Informationen aus mehreren Feldern extrahiert und kategorisiert werden m√ºssen. Durch die Segmentierung der Modellausgabe in definierte Bereiche wird die Interpretation und Handhabung der Daten vereinfacht.\n","\n","W√§hrend der **Pydantic/JSON-Parser** eine leistungsf√§higere L√∂sung f√ºr komplexe Datenstrukturen darstellt, eignet sich der **Structured Output Parser** besonders f√ºr Umgebungen mit begrenzten Rechenressourcen oder f√ºr Modelle mit geringer Leistungsf√§higkeit. Er bietet eine einfache und effiziente M√∂glichkeit, die Ausgabe zu strukturieren, ohne das System zu stark zu belasten.\n","\n","Der erste Schritt bei der Nutzung dieses Parsers besteht in der Erstellung eines **ResponseSchemas**, das definiert, welche Werte extrahiert werden sollen. Jeder Wert wird dabei detailliert beschrieben. Anschlie√üend wird der **StructuredOutputParser** aus einer Liste dieser Schemata erstellt."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JvzHRtXd3hEd"},"outputs":[],"source":["# Abschnitt 0: Install\n","!uv pip install --system --prerelease allow -q langchain-core langchain_community langchain_openai gradio"]},{"cell_type":"code","source":["# Abschnitt 1: Importe\n","from IPython.display import display, Markdown\n","\n","# Neue strukturierte Imports f√ºr aktuelle LangChain-Versionen\n","from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n","from langchain_core.prompts import PromptTemplate\n","from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough"],"metadata":{"id":"_m84Eigb4LDn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='blue' size=\"4\">\n","Ohne Nachbearbeitung mit einem Parser\n","</font></p>"],"metadata":{"id":"jMvv2MQNrggb"}},{"cell_type":"code","source":["model_name = \"gpt-4o-mini\"\n","temperature = 0.0\n","\n","\n","llm = ChatOpenAI(model=model_name, temperature=temperature)"],"metadata":{"id":"78SUuguIrrSh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input = \"Was ist Machine Learning?\"\n","\n","response = llm.invoke(input)"],"metadata":{"id":"MWmROWbwrrPr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for r in response:\n","    print(r)"],"metadata":{"id":"ezuKTok2rrMb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='blue' size=\"4\">\n","Mit Nachbearbeitung mit einem Parser\n","</font></p>"],"metadata":{"id":"CovCEerkv8Eb"}},{"cell_type":"code","source":["parser = StrOutputParser()\n","\n","chain = llm | parser\n","\n","response = chain.invoke(input)"],"metadata":{"id":"9t7QNRv3rrJi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response"],"metadata":{"id":"6CkTZi6LusDo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3 | Strukturieter Parser (mittel)"],"metadata":{"id":"6ls-Rt2LwCfT"}},{"cell_type":"markdown","source":["<p><font color='blue' size=\"4\">\n","Structured Output Parser (mittel)\n","</font></p>"],"metadata":{"id":"n2igGVtTXE3I"}},{"cell_type":"markdown","source":["**Schritt 1:** Der **Parser** definiert ein Schema f√ºr die erwartete Ausgabe und verarbeitet die Antwort des LLMs in ein strukturiertes Format (z.B. ein Python-Dictionary)."],"metadata":{"id":"Dt428mOkTD3J"}},{"cell_type":"code","source":["# 1. Parser-Schema definieren\n","response_schema = [\n","    ResponseSchema(name=\"input\", description=\"Frage des Benutzers\"),\n","    ResponseSchema(name=\"answer\", description=\"Antwort auf die Frage des Benutzers\"),\n","    ResponseSchema(name=\"source\", description=\"Verwendete Quelle (Website) f√ºr die Antwort\")\n","]\n","\n","# 2. Parser erstellen\n","parser = StructuredOutputParser.from_response_schemas(response_schema)\n","\n","# 3. Formatierungs Instruktion zu diesem Parser f√ºr das Prompt erstellen\n","format_instructions = parser.get_format_instructions()"],"metadata":{"id":"Hc9NtLEdNnr_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3AD6gq-_jWew"},"source":["Wir k√∂nnen die einzelnen Elemente anzeigen, die wir zum Abrufen von Daten verwenden werden."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JCVMfLjvjWew"},"outputs":[],"source":["print(response_schema)"]},{"cell_type":"code","source":["print(parser)"],"metadata":{"id":"35ReNN3ukLdS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(format_instructions)"],"metadata":{"id":"tb3Xo6ZGkO8q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Schritt 2:** Verwendung des Schemas mit der Formatierungs Instruktion in einem Prompt."],"metadata":{"id":"aZWHfFm6TXir"}},{"cell_type":"code","source":["# 1. System-Prompt mit den Variablen (f-format) erstellen\n","system_prompt = \"\"\"\n","    Beantworte die folgende Frage so pr√§zise wie m√∂glich.\n","    Gib auch die Quelle deiner Information an.\n","\n","    Frage: {input}\n","\n","    {format_instructions}\n","\"\"\"\n","\n","# 2. Prompt-Template generieren\n","prompt = PromptTemplate(\n","    template=system_prompt,\n","    input_variables=[\"input\"],\n","    partial_variables={\"format_instructions\": format_instructions}\n",")"],"metadata":{"id":"VLGsxKXFTLbW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GtH3oH7vjRHL"},"source":["Wir k√∂nnen die Eingabeaufforderung anzeigen, die wir zum Abrufen von Daten verwenden werden."]},{"cell_type":"code","source":["print(prompt)"],"metadata":{"id":"uJq_paoikWgc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Funktion zur Aufbereitung der Druck-Ausgabe\n","def markdown_out(response):\n","    mprint(f\"### üßë Mensch\")\n","    mprint(f\"{response['input']}\")\n","    mprint(f\"### ü§ñ KI:\")\n","    mprint(f\"{response['answer']}\")\n","    mprint(f\"### Quelle:\\n {response['source']}\")"],"metadata":{"id":"uaWn2wPJGmus"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Einbindung des Parser in unterschiedliche Chain-Strukturen.\n","</font></p>"],"metadata":{"id":"XJcx6t4HXZbY"}},{"cell_type":"markdown","source":["**Variante 0:** Ohne Chain-Struktur"],"metadata":{"id":"CBQ90Au9Y4Ab"}},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(prompt.format(input=input))\n","\n","# Antwort parsen\n","parsed_response = parser.parse(response.content)\n","\n","markdown_out(parsed_response)"],"metadata":{"id":"fXB-iqLyTi18"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Variante 2:** Chain mit direkte Eingabe als Dictionary"],"metadata":{"id":"yhR3N3tPX5x9"}},{"cell_type":"code","source":["chain = (\n","    prompt\n","    | llm\n","    | parser\n",")\n","\n","# Aufruf mit Dictionary\n","response = chain.invoke({\"input\": \"Was ist Machine Learning?\"})\n","markdown_out(response)"],"metadata":{"id":"RIM9uWjhYjfd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Variante 3:** Chain mit Dictionary-Transformation `RunnablePassthrough`"],"metadata":{"id":"NWP7LkqoX50f"}},{"cell_type":"code","source":["# Chain mit Dictionary-Transformation\n","from langchain_core.runnables import RunnablePassthrough\n","\n","chain = (\n","    {\"input\": RunnablePassthrough()}  # Wandelt String in Dictionary um\n","    | prompt\n","    | llm\n","    | parser\n",")\n","\n","# Aufruf mit einfachem String\n","response = chain.invoke(input)\n","markdown_out(response)"],"metadata":{"id":"nUz7fytPYATz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wR5J9U9bg5Br"},"source":["# 4 | Strukturierter Parser (komplex)\n","---"]},{"cell_type":"markdown","source":["Nun wird ein Beispiel mit einer gr√∂√üeren Anzahl an Werten getestet. Das folgende Programm nimmt Texte in beliebigen Sprachen entgegen und √ºbersetzt sie ins Englische, Spanische und Chinesische."],"metadata":{"id":"nN1AWb1m9OK4"}},{"cell_type":"markdown","source":["**Schritt 1:** Der **Parser** definiert ein Schema f√ºr die erwartete Ausgabe und verarbeitet die Antwort des LLMs in ein strukturiertes Format (z.B. ein Python-Dictionary)."],"metadata":{"id":"lpn8vHKwcEui"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9snGgL4riemY"},"outputs":[],"source":["response_schema = [\n","    ResponseSchema(name=\"detected\", description=\"Die Sprache ist Nutzer-Eingabe\"),\n","    ResponseSchema(name=\"german\", description=\"German translation\"),\n","    ResponseSchema(name=\"english\", description=\"English translation\"),\n","    ResponseSchema(name=\"spanish\", description=\"Spanish translation\"),\n","    ResponseSchema(name=\"chinese\", description=\"Chinese translation\"),\n","]\n","parser = StructuredOutputParser.from_response_schemas(response_schema)\n","\n","format_instructions = parser.get_format_instructions()"]},{"cell_type":"markdown","source":["**Schritt 2:** Verwendung des Schemas mit der Formatierungs Instruktion in einem Prompt."],"metadata":{"id":"zyAeqALjcbFt"}},{"cell_type":"code","source":["# 1. System-Prompt mit den Variablen (f-format) erstellen\n","system_prompt = \"\"\"\n","    √úbersetze in die angegebenen Sprachen.\\n{format_instructions}\\n{input}\n","\"\"\"\n","\n","# 2. Prompt-Template generieren\n","prompt = PromptTemplate(\n","    template=system_prompt,\n","    input_variables=[\"input\"],\n","    partial_variables={\"format_instructions\": format_instructions}\n",")"],"metadata":{"id":"WXr_0z9PcbFt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Variante 2:** Chain mit direkte Eingabe als *Dictionary*"],"metadata":{"id":"8emVa-S1cuPI"}},{"cell_type":"markdown","metadata":{"id":"sa6_dAk2ugre"},"source":["Zun√§chst wird ein deutscher Satz getestet, um zu beobachten, wie die √úbersetzung in die drei Zielsprachen erfolgt."]},{"cell_type":"code","source":["chain = prompt | llm | parser\n","input = \"Wann wurde Python eingef√ºhrt?\"\n","chain.invoke({\"input\": input})"],"metadata":{"id":"01rQuqfdbv1o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dann wird ein franz√∂sischer Satz getestet, um zu beobachten, wie die √úbersetzung in die drei Zielsprachen erfolgt."],"metadata":{"id":"3lYL9wUcemex"}},{"cell_type":"code","source":["chain = prompt | llm | parser\n","question = \"Who rides so late through night and wind?\"\n","chain.invoke({\"input\": input})"],"metadata":{"id":"4qF-a2PbeNET"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MQFjKL7uDgfA"},"source":["# 5 | Datenformat Parser\n","---"]},{"cell_type":"markdown","source":["LangChain bietet eine breite Palette an Parsern, die unterschiedliche Datenformate verarbeiten k√∂nnen, was seine Einsatzm√∂glichkeiten erheblich erweitert. Es unterst√ºtzt unter anderem die nahtlose Integration von Pandas-Datenrahmen, kommagetrennten Listen, JSON-Strukturen sowie Datums- und Zeitobjekten. Diese Flexibilit√§t erm√∂glicht eine effiziente Anpassung an verschiedene Arten von Dateneingaben und macht LangChain zu einem leistungsstarken Werkzeug f√ºr die Analyse und Verarbeitung von Daten. Im Folgenden werden einige dieser Parser genauer betrachtet, ihre praktischen Anwendungen demonstriert und aufgezeigt, wie sie zur Optimierung von Prozessen und zur Gewinnung wertvoller Erkenntnisse beitragen k√∂nnen."],"metadata":{"id":"Q1oRFrLCx8eW"}},{"cell_type":"markdown","metadata":{"id":"mE2avWGaysc3"},"source":["<p><font color='black' size=\"5\">\n","JSON\n","</font></p>"]},{"cell_type":"markdown","source":["Die Ausgabe des LLM kann in einem JSON-Format strukturiert werden."],"metadata":{"id":"S69wM83z-1Hh"}},{"cell_type":"code","source":["#\n","# Variante: Prompt\n","#\n","from langchain_openai import ChatOpenAI\n","\n","prompt = (\n","    \"Bitte gib mir eine Person mit Name und Alter im JSON-Format zur√ºck, z.B. \"\n","    '{\"name\": \"Max\", \"age\": 30}'\n",")\n","\n","model_name = \"gpt-4o-mini\"\n","temperature = 0.0\n","\n","llm = ChatOpenAI(model=model_name, temperature=temperature)\n","\n","response = llm.invoke(prompt)\n","\n","mprint(\"### ü§ñ KI:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"KIYp-bKe5t3e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#\n","# Variante: JsonOutputParser\n","#\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import JsonOutputParser\n","\n","# Parser\n","parser = JsonOutputParser()\n","\n","# Prompt\n","prompt = ChatPromptTemplate.from_template(\n","    \"Give name and age of a person in JSON format.\"\n",")\n","\n","# Kette\n","chain = prompt | llm | parser\n","\n","# Ausf√ºhren\n","response = chain.invoke({})\n","\n","mprint(\"### ü§ñ KI:\")\n","mprint(\"---\")\n","print(response)"],"metadata":{"id":"0dJs83IJ5t6X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In diesem Beispiel wird ein Satz erkannt, der als Englisch identifiziert wird, und anschlie√üend ins Spanische, Franz√∂sische und Chinesische √ºbersetzt."],"metadata":{"id":"njnSr9msG424"}},{"cell_type":"code","source":["# Abschnitt 1: Importe\n","from langchain_core.output_parsers import JsonOutputParser\n","from langchain_core.prompts import PromptTemplate\n","from langchain_openai import ChatOpenAI\n","\n","# Abschnitt 2: Funktionen\n","def translate_text(text: str) -> dict:\n","    \"\"\"√úbersetzt den Text in mehrere Sprachen\"\"\"\n","    # Parser ohne Pydantic-Modell konfigurieren\n","    parser = JsonOutputParser()\n","    llm = ChatOpenAI(model=\"gpt-4\", temperature=0.0)\n","\n","    # Prompt erstellen mit direkten Anweisungen zum JSON-Format\n","    prompt_template = \"\"\"\n","    √úbersetze die Benutzereingabe in mehrere Sprachen und gib das Ergebnis als JSON zur√ºck.\n","\n","    Dein JSON sollte folgende Felder enthalten:\n","    - detected: Die erkannte Ausgangssprache\n","    - german: Die deutsche √úbersetzung\n","    - spanish: Die spanische √úbersetzung\n","    - french: Die franz√∂sische √úbersetzung\n","    - chinese: Die chinesische √úbersetzung\n","\n","    Benutzereingabe: {input}\n","    \"\"\"\n","\n","    prompt = PromptTemplate(\n","        template=prompt_template,\n","        input_variables=[\"input\"]\n","    )\n","\n","    # Kette ausf√ºhren\n","    chain = prompt | llm | parser\n","    return chain.invoke({\"input\": text})\n","\n","# Abschnitt 3: Hauptprogramm\n","# Beispieltext √ºbersetzen und Ergebnisse ausgeben\n","result = translate_text(\"What is your name?\")\n","\n","print()\n","mprint(f\"**Erkannte Sprache:** {result['detected']}\")\n","mprint(f\"**Deutsch:** {result['german']}\")\n","mprint(f\"**Spanisch:** {result['spanish']}\")\n","mprint(f\"**Franz√∂sisch:** {result['french']}\")\n","mprint(f\"**Chinesisch:** {result['chinese']}\")"],"metadata":{"id":"PYO2u9vKgyC8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y09-epHS5McO"},"source":["<p><font color='black' size=\"5\">\n","Datum/Uhrzeit\n","</font></p>"]},{"cell_type":"markdown","source":["Langchain bietet mit dem **DatetimeOutputParser** eine spezielle Funktion zur Interpretation von Datums- und Zeitangaben aus Texten. Sie erkennt unterschiedliche Formate und wandelt diese in ein standardisiertes Zeitformat um. Dies ist besonders hilfreich f√ºr Anwendungen wie Terminplanung, Datenanalyse oder andere Bereiche, in denen eine pr√§zise Verarbeitung zeitlicher Informationen erforderlich ist. Der **DatetimeOutputParser** erleichtert die Verarbeitung von Datums- und Uhrzeitangaben und tr√§gt dazu bei, dass Anwendungen zeitbezogene Daten effizient verwalten und nutzen k√∂nnen."],"metadata":{"id":"-zDlyRDUM4qS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRuzzhXF5Oa1"},"outputs":[],"source":["# Abschnitt 1: Importe\n","from langchain.output_parsers import DatetimeOutputParser\n","from langchain_core.prompts import PromptTemplate\n","from langchain_openai import OpenAI"]},{"cell_type":"code","source":["template = \"\"\"Beantworte die Frage zu genau wie m√∂glich.\n","Wenn Du die Frage nach dem Datum nicht beantworten kannst, dann geben mir das Datum 01.01.1111.\n","\n","{input}\n","\n","{format_instructions}\"\"\"\n","\n","# Abschnitt 3: Komponenten initialisieren\n","# model_name = \"gpt-4o-mini\"\n","# temperature = 0.0\n","\n","# llm = ChatOpenAI(model=\"gpt-4o-mini\",\n","#     temperature=0.2,\n","#     n=1\n","# )\n","\n","parser = DatetimeOutputParser()\n","\n","prompt = PromptTemplate(\n","    template=template,\n","    input_variables=['input'],\n","    partial_variables={'format_instructions': output_parser.get_format_instructions()}\n",")"],"metadata":{"id":"TBQjsTUvgSCb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"roqInbqGFdUb"},"source":["Wir k√∂nnen die Eingabeaufforderung anzeigen, die wir zum Abrufen von Daten verwenden werden."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HecixPQl6kYk"},"outputs":[],"source":["print(prompt)"]},{"cell_type":"markdown","metadata":{"id":"kg5ZkOetFiPx"},"source":["Wir erstellen die Kette, die wir zum Parsen von Daten verwenden werden."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2w5gYty_5pKd"},"outputs":[],"source":["chain = prompt | llm | parser"]},{"cell_type":"markdown","metadata":{"id":"dD-gNvBHFooj"},"source":["Wir werden nach zwei Daten fragen, einem realen und einem fiktiven."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ymu48Ggg518q"},"outputs":[],"source":["response = chain.invoke({\"input\": \"Wann wurde die Progammiersprache Python eingef√ºhrt?\"}) # 20.02.1991\n","print(response)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nOPWYpxr6As1"},"outputs":[],"source":["response = chain.invoke({\"input\": \"An welchem Tag startet der Krieg im Videospiel Fallout?\"}) #  23.10.2077\n","print(response)"]},{"cell_type":"markdown","metadata":{"id":"hzCTsn14DyU0"},"source":["# 6 | Eigene Parser\n","---"]},{"cell_type":"markdown","source":["In bestimmten Szenarien kann es sinnvoll sein, einen benutzerdefinierten Parser zu erstellen, um die Modellausgabe eindeutig zu formatieren.  \n","\n","Daf√ºr bietet sich die Verwendung von **RunnableLambda** oder **RunnableGenerator** in LCEL an, was f√ºr viele F√§lle ein guten Ansatz ist.  "],"metadata":{"id":"DEXj9woZzzy0"}},{"cell_type":"markdown","metadata":{"id":"isGwPhF4GUM5"},"source":["<p><font color='black' size=\"5\">\n","Anwendungsfall\n","</font></p>"]},{"cell_type":"markdown","source":["\n","\n","Large Language Models (LLMs) wie GPT-4 k√∂nnen Text generieren, der Code und erkl√§rende Beschreibungen nahtlos vermischt. Dies kann zwar f√ºr Lern- und Dokumentationszwecke unglaublich n√ºtzlich sein, kann aber eine Herausforderung darstellen, wenn aus solchen Ausgaben mit gemischtem Inhalt nur der Code extrahiert und ausgef√ºhrt werden muss. Um dies zu beheben, implementieren wir eine einfache Funktion, die nicht-Python-Codezeilen aus einer gegebenen Textzeichenfolge entfernt.\n","\n","Bei diesem Ansatz werden regul√§re Ausdr√ºcke verwendet, um Zeilen zu identifizieren und beizubehalten, die der typischen Python-Syntax entsprechen, w√§hrend Zeilen verworfen werden, die beschreibender Text zu sein scheinen. Aufgrund der inh√§renten Komplexit√§t und Variabilit√§t sowohl von Python-Code als auch von nat√ºrlicher Sprache kann diese Methode jedoch nie perfekt sein. Sie basiert auf heuristischen Mustern, die Code manchmal f√§lschlicherweise als Text klassifizieren oder umgekehrt.\n","\n","Im n√§chsten Abschnitt werden wir untersuchen, wie ein anderes LLM beim Entfernen von Nicht-Python-Code helfen kann und m√∂glicherweise eine ausgefeiltere und genauere L√∂sung bietet. Das folgende Beispiel enth√§lt eine Mischung aus LLM-Kommentaren und generiertem Code.\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"sPq64ydYgJ42"}},{"cell_type":"code","source":["from langchain_core.runnables import RunnableLambda\n","\n","# Benutzer-Funktion zur Trennung von Erl√§uterung und Code\n","def parse_ai_message(ai_message):\n","    \"\"\"Trennt die Erl√§uterung und den Code aus einer AIMessage und gibt sie separat zur√ºck.\"\"\"\n","    text = ai_message.content  # Extrahiere den reinen Textinhalt der AIMessage\n","\n","    if \"```\" in text:\n","        # Trennen der Erl√§uterung und des Codes\n","        parts = text.split(\"```\")\n","        explanation = parts[0].strip()\n","        code = parts[1].strip() if len(parts) > 1 else \"\"\n","    else:\n","        # Falls kein Codeblock vorhanden ist, geben wir nur die Erl√§uterung zur√ºck\n","        explanation = text.strip()\n","        code = \"\"\n","\n","    return explanation, code  # R√ºckgabe von zwei separaten Werten\n","\n","# 3. RunnableLambda f√ºr das Parsing erstellen\n","parser = RunnableLambda(parse_ai_message)\n","\n","# 4. LLM und Parser verketten\n","chain = llm | parser\n","\n","# 5. Eingabe an die Pipeline senden und Ergebnis ausgeben\n","prompt = \"\"\"\n","    Erkl√§re mir, wie man eine einfache Funktion in Python erstellt und gib ein Beispiel zur Berechnung von Primzahlen.\n","    Verwende bei Formeln das Format $ Formel $\n","\"\"\"\n","explanation, code = chain.invoke(prompt)\n","\n","mprint(\"### ü§ñ Erl√§uterung\")\n","mprint(explanation)\n","mprint(\"### ü§ñ Code\")\n","print(code)"],"metadata":{"id":"6wjQlnF7qeso"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3TjZs_TRht1n"},"source":["# A | Aufgabe\n","---"]},{"cell_type":"markdown","source":["Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen."],"metadata":{"id":"smOJ1YD15z_Q"}},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","JSON-Parser mit LangChain\n","</font></p>"],"metadata":{"id":"L5IWdaBS0UZD"}},{"cell_type":"markdown","source":["\n","**Ziel:** Verst√§ndnis f√ºr den Einsatz von `JsonOutputParser` in LangChain.\n","\n","**Aufgabe:**  \n","1. Nutze den `JsonOutputParser` von LangChain, um eine KI-Antwort in JSON zu formatieren.  \n","2. Lasse ein Language Model (z. B. OpenAI GPT) eine Liste von drei zuf√§lligen St√§dten in Deutschland generieren.  \n","3. Verwende den Parser, um die Ausgabe in ein JSON-Format umzuwandeln.\n","\n","**Erwartete Ausgabe (Beispiel):**\n","```json\n","{\n","  \"cities\": [\"Berlin\", \"Hamburg\", \"M√ºnchen\"]\n","}\n","```\n"],"metadata":{"id":"JTdhJeN8u99p"}},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Extraktion von Schl√ºsselwerten\n","</font></p>"],"metadata":{"id":"UCpw6-OHvHJJ"}},{"cell_type":"markdown","source":["\n","**Ziel:** Nutzung des `PydanticOutputParser`, um strukturierte Daten aus nat√ºrlicher Sprache zu extrahieren.\n","\n","**Aufgabe:**  \n","1. Definiere eine `Pydantic`-Datenklasse mit den Feldern: `name` (str), `alter` (int), `stadt` (str).  \n","2. Verwende ein Language Model, um aus einer gegebenen Beschreibung eine Person zu extrahieren.  \n","3. Parse die Antwort mit dem `PydanticOutputParser`.\n","\n","**Beispiel-Eingabe:**\n","> \"Max Mustermann ist 35 Jahre alt und lebt in Berlin.\"\n","\n","**Erwartete Ausgabe (Pydantic-Modell):**\n","```json\n","{\n","  \"name\": \"Max Mustermann\",\n","  \"alter\": 35,\n","  \"stadt\": \"Berlin\"\n","}\n","```"],"metadata":{"id":"geRHkOMCvKum"}},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Parser f√ºr Listenformate\n","</font></p>"],"metadata":{"id":"8-AzxHafvSE_"}},{"cell_type":"markdown","source":["\n","**Ziel:** Implementierung eines eigenen Parsers zur Umwandlung von KI-Ausgaben in Listen.\n","\n","**Aufgabe:**  \n","1. Erstelle eine eigene Parser-Klasse, die eine durch Kommas getrennte Liste in ein Listenformat umwandelt.  \n","2. Verwende diesen Parser, um eine Liste von f√ºnf beliebten B√ºchern aus einer Language-Model-Antwort zu extrahieren.\n","\n","**Beispiel-Eingabe:**\n","> \"Die Verwandlung, Faust, Der Prozess, Die Blechtrommel, Der Vorleser\"\n","\n","**Erwartete Ausgabe:**\n","```python\n","[\"Die Verwandlung\", \"Faust\", \"Der Prozess\", \"Die Blechtrommel\", \"Der Vorleser\"]\n","```"],"metadata":{"id":"ytCh-C2uvUFw"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Kombination Parser & PromptTemplate\n","</font></p>"],"metadata":{"id":"I3LRv5YAviSF"}},{"cell_type":"markdown","source":["\n","**Ziel:** Verwendung von `StructuredOutputParser` in Kombination mit `PromptTemplate`.\n","\n","**Aufgabe:**  \n","1. Erstelle ein `PromptTemplate`, das eine strukturierte Antwort √ºber ein Land ausgibt (Name, Hauptstadt, Einwohnerzahl).  \n","2. Verwende den `StructuredOutputParser`, um die Antwort in ein Dictionary zu konvertieren.\n","\n","**Beispiel-Prompt:**\n","> \"Gib mir Informationen zu Frankreich im folgenden JSON-Format: { 'name': '...', 'hauptstadt': '...', 'einwohner': ... }.\"\n","\n","**Erwartete Ausgabe:**\n","```json\n","{\n","  \"name\": \"Frankreich\",\n","  \"hauptstadt\": \"Paris\",\n","  \"einwohner\": 67000000\n","}\n","```\n"],"metadata":{"id":"bZqMjEWavk_i"}}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["i0xOygLv6LaK","oYvUY6gMBKO1","6ls-Rt2LwCfT","wR5J9U9bg5Br","MQFjKL7uDgfA","Q6PFBjUSDo_N","hzCTsn14DyU0","3TjZs_TRht1n"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)"],"metadata":{"id":"Ih2CTVBnArVZ"}},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b>\n","OutputParser\n","</b></font> </br></p>\n","\n","\n","---"],"metadata":{"id":"6jJZ7wbdArVc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfdhPIzcEYRG","cellView":"form","collapsed":true},"outputs":[],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  Colab-Umfeld</font> </br></p>\n","# Installierte Python Version\n","import sys\n","print(f\"Python Version: \",sys.version)\n","# Installierte LangChain Bibliotheken\n","print()\n","\n","print(\"Installierte LangChain Bibliotheken:\")\n","!pip list | grep '^langchain'\n","# Unterdrückt die \"DeprecationWarning\" von LangChain für die Memory-Funktionden\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"langchain\")"]},{"cell_type":"code","source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  SetUp API-Keys (setup_api_keys)</font> </br></p>\n","def setup_api_keys():\n","    \"\"\"Konfiguriert alle benötigten API-Keys aus Google Colab userdata\"\"\"\n","    from google.colab import userdata\n","    import os\n","    from os import environ\n","\n","    # Dictionary der benötigten API-Keys\n","    keys = {\n","        'OPENAI_API_KEY': 'OPENAI_API_KEY',\n","        'HF_TOKEN': 'HF_TOKEN',\n","        # Weitere Keys bei Bedarf\n","    }\n","\n","    # Keys in Umgebungsvariablen setzen\n","    for env_var, key_name in keys.items():\n","        environ[env_var] = userdata.get(key_name)\n","\n","    return {k: environ[k] for k in keys.keys()}\n","\n","# Verwendung\n","all_keys = setup_api_keys()\n","# Bei Bedarf einzelne Keys direkt zugreifen\n","# WEATHER_API_KEY = all_keys['WEATHER_API_KEY']"],"metadata":{"cellView":"form","id":"WD3Wwr6sESX8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1 | OutputParser\n","---"],"metadata":{"id":"i0xOygLv6LaK"}},{"cell_type":"markdown","metadata":{"id":"pC9A-LaYhsta"},"source":["LangChain stellt eine Vielzahl von Ausgabeparsern bereit, die speziell darauf ausgelegt sind, Informationen aus den Ergebnissen großer Sprachmodelle (LLMs) effizient zu extrahieren und zu strukturieren. Diese Parser sind essenzielle Bestandteile der LangChain-Architektur und häufig zentrale Elemente sogenannter LangChain-Ketten. Solche Ketten bestehen aus konfigurierbaren Abfolgen von Operationen, die Modellausgaben verarbeiten und für weiterführende Anwendungen nutzbar machen.\n","\n","\n","**Warum sind OutputParser so wichtig?**\n","+ LLMs geben standardmäßig unstrukturierte Texte zurück.\n","+ OutputParser sind nötig, um das LLM-Output strukturiert weiterzuverarbeiten.\n","+ Besonders bei komplexen Anwendungen (z. B. Ketten mit mehreren Modellen, Agenten oder RAG-Systemen) müssen die Antworten klar definiert sein."]},{"cell_type":"markdown","source":["**Übersicht LangChain-Parser:**\n","\n","<br>\n","\n","| **Name**                  | **Beschreibung**                                                                 | **Supports Streaming** | **Input-Typ**         | **Output-Typ**           |\n","|---------------------------|----------------------------------------------------------------------------------|-------------------------|-----------------------|--------------------------|\n","| **ListOutputParser**      | Wandelt eine durch Kommas getrennte Liste in eine Python-Liste um.               | Nein                    | `str`                | `list[str]`             |\n","| **DateTimeParser**        | Wandelt eine Datums- oder Zeitzeichenkette in ein Python-Datetime-Objekt um.     | Nein                    | `str`                | `datetime.datetime`     |\n","| **StructuredOutputParser**| Wandelt Text basierend auf einem Schema in ein Dictionary um.                    | Nein                    | `str`                | `Dict[str, str]`        |\n","| **JSON Parser**           | Gibt ein JSON-Objekt basierend auf einem Pydantic-Modell zurück.                 | Ja                      | `str`                | `Message` (JSON Objekt) |\n","| **XML Parser**            | Wandelt XML-Ausgaben in ein Dictionary von Tags um.                              | Ja                      | `str`                | `dict`                 |\n","| **CSV Parser**            | Gibt eine Liste von durch Kommas getrennten Werten zurück.                       | Ja                      | `str`                | `List[str]`            |\n","| **OutputFixingParser**    | Versucht, fehlerhafte Ausgaben zu korrigieren, indem es ein anderes LLM aufruft. | Ja                      | Wraps anderer Parser  | Abhängig vom Parser     |\n","| **RetryWithErrorParser**  | Wiederholt die Eingabe bei Fehlern und sendet ursprüngliche Anweisungen mit.     | Ja                      | Wraps anderer Parser  | Abhängig vom Parser     |\n","| **Pydantic Parser**       | Gibt Daten basierend auf einem benutzerdefinierten Pydantic-Modell zurück.       | Ja                      | `str`                | `pydantic.BaseModel`   |\n","| **YAML Parser**           | Wie der Pydantic Parser, aber verwendet YAML zur Kodierung.                      | Ja                      | `str`                | `pydantic.BaseModel`   |\n","| **PandasDataFrame Parser**| Gibt Daten zurück, die für Pandas DataFrames geeignet sind.                       | Ja                      | `str`                | `dict`                 |\n","| **Enum Parser**           | Wandelt die Antwort in einen der angegebenen Enum-Werte um.                     | Ja                      | `str`                | `Enum`                 |\n","\n","<br>\n","\n","Diese Parser ermöglichen es, die Ausgaben von Sprachmodellen (LLMs) in strukturierte und nutzbare Formate umzuwandeln, was besonders für Anwendungen mit spezifischen Anforderungen nützlich ist.\n"],"metadata":{"id":"DO2-q2e0a9sV"}},{"cell_type":"markdown","source":["# 2 | Strukturierter Parser (einfach)\n","---"],"metadata":{"id":"oYvUY6gMBKO1"}},{"cell_type":"markdown","metadata":{"id":"HMLrKTqgSRkM"},"source":["\n","Der **Structured Output Parser** in LangChain ermöglicht die strukturierte Verarbeitung der Ausgaben großer Sprachmodelle (LLMs). Dies ist besonders hilfreich, wenn Informationen aus mehreren Feldern extrahiert und kategorisiert werden müssen. Durch die Segmentierung der Modellausgabe in definierte Bereiche wird die Interpretation und Handhabung der Daten vereinfacht.\n","\n","Während der **Pydantic/JSON-Parser** eine leistungsfähigere Lösung für komplexe Datenstrukturen darstellt, eignet sich der **Structured Output Parser** besonders für Umgebungen mit begrenzten Rechenressourcen oder für Modelle mit geringer Leistungsfähigkeit. Er bietet eine einfache und effiziente Möglichkeit, die Ausgabe zu strukturieren, ohne das System zu stark zu belasten.\n","\n","Der erste Schritt bei der Nutzung dieses Parsers besteht in der Erstellung eines **ResponseSchemas**, das definiert, welche Werte extrahiert werden sollen. Jeder Wert wird dabei detailliert beschrieben. Anschließend wird der **StructuredOutputParser** aus einer Liste dieser Schemata erstellt."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JvzHRtXd3hEd"},"outputs":[],"source":["# Abschnitt 0: Install\n","!uv pip install --system --prerelease allow -q langchain-core langchain_community langchain_openai gradio"]},{"cell_type":"code","source":["# Abschnitt 1: Importe\n","from IPython.display import display, Markdown\n","\n","# Neue strukturierte Imports für aktuelle LangChain-Versionen\n","from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n","from langchain_core.prompts import PromptTemplate\n","from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough"],"metadata":{"id":"_m84Eigb4LDn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATURE = 0.0\n","\n","QUESTION = \"Was ist Machine Learning?\""],"metadata":{"id":"ZUJwhu1LNmMe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='blue' size=\"4\">\n","Ohne Nachbearbeitung mit einem Parser\n","</font></p>"],"metadata":{"id":"jMvv2MQNrggb"}},{"cell_type":"code","source":["llm = ChatOpenAI(model=MODEL, temperature=TEMPERATURE)"],"metadata":{"id":"78SUuguIrrSh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response = llm.invoke(QUESTION)"],"metadata":{"id":"MWmROWbwrrPr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for r in response:\n","    print(r)"],"metadata":{"id":"ezuKTok2rrMb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='blue' size=\"4\">\n","Mit Nachbearbeitung mit einem Parser\n","</font></p>"],"metadata":{"id":"CovCEerkv8Eb"}},{"cell_type":"code","source":["chain = llm | StrOutputParser()\n","response = chain.invoke(QUESTION)"],"metadata":{"id":"9t7QNRv3rrJi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response"],"metadata":{"id":"6CkTZi6LusDo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3 | Strukturieter Parser (mittel)"],"metadata":{"id":"6ls-Rt2LwCfT"}},{"cell_type":"markdown","source":["<p><font color='blue' size=\"4\">\n","Structured Output Parser (mittel)\n","</font></p>"],"metadata":{"id":"n2igGVtTXE3I"}},{"cell_type":"markdown","source":["**Schritt 1:** Der **Parser** definiert ein Schema für die erwartete Ausgabe und verarbeitet die Antwort des LLMs in ein strukturiertes Format (z.B. ein Python-Dictionary)."],"metadata":{"id":"Dt428mOkTD3J"}},{"cell_type":"code","source":["# 1. Parser-Schema definieren\n","RESPONSE_SCHEMAS = [\n","    ResponseSchema(name=\"question\", description=\"Frage des Benutzers\"),\n","    ResponseSchema(name=\"answer\", description=\"Antwort auf die Frage des Benutzers\"),\n","    ResponseSchema(name=\"source\", description=\"Verwendete Quelle (Website) für die Antwort\")\n","]\n","\n","# 2. Parser erstellen\n","parser = StructuredOutputParser.from_response_schemas(RESPONSE_SCHEMAS)\n","\n","# 3. Formatierungs Instruktion zu diesem Parser für das Prompt erstellen\n","FORMAT_INSTRUCTIONS = parser.get_format_instructions()"],"metadata":{"id":"Hc9NtLEdNnr_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3AD6gq-_jWew"},"source":["Wir können die einzelnen Elemente anzeigen, die wir zum Abrufen von Daten verwenden werden."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JCVMfLjvjWew"},"outputs":[],"source":["print(RESPONSE_SCHEMAS)"]},{"cell_type":"code","source":["print(parser)"],"metadata":{"id":"35ReNN3ukLdS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(FORMAT_INSTRUCTIONS)"],"metadata":{"id":"tb3Xo6ZGkO8q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Schritt 2:** Verwendung des Schemas mit der Formatierungs Instruktion in einem Prompt."],"metadata":{"id":"aZWHfFm6TXir"}},{"cell_type":"code","source":["# 1. System-Prompt mit den Variablen (f-format) erstellen\n","SYSTEM_PROMPT = \"\"\"\n","    Beantworte die folgende Frage so präzise wie möglich.\n","    Gib auch die Quelle deiner Information an.\n","\n","    Frage: {question}\n","\n","    {format_instructions}\n","\"\"\"\n","\n","# 2. Prompt-Template generieren\n","prompt = PromptTemplate(\n","    template=SYSTEM_PROMPT,\n","    input_variables=[\"question\"],\n","    partial_variables={\"format_instructions\": FORMAT_INSTRUCTIONS}\n",")"],"metadata":{"id":"VLGsxKXFTLbW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GtH3oH7vjRHL"},"source":["Wir können die Eingabeaufforderung anzeigen, die wir zum Abrufen von Daten verwenden werden."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffp8vPsjjRHM"},"outputs":[],"source":["print(SYSTEM_PROMPT)"]},{"cell_type":"code","source":["print(prompt)"],"metadata":{"id":"uJq_paoikWgc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Funktion zur Aufbereitung der Druck-Ausgabe\n","def markdown_out(result):\n","    display(Markdown(f\"**🧑 Mensch**\"))\n","    display(Markdown(f\"{result['question']}\"))\n","    display(Markdown(f\"**🤖 KI:**\"))\n","    display(Markdown(f\"{result['answer']}\"))\n","    display(Markdown(f\"**Quelle:** {result['source']}\"))"],"metadata":{"id":"uaWn2wPJGmus"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Einbindung des Parser in unterschiedliche Chain-Strukturen.\n","</font></p>"],"metadata":{"id":"XJcx6t4HXZbY"}},{"cell_type":"markdown","source":["**Variante 0:** Ohne Chain-Struktur"],"metadata":{"id":"CBQ90Au9Y4Ab"}},{"cell_type":"code","source":["# LLM definieren - ohne weitere Kette\n","llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n","\n","# Aufruf\n","llm_response = llm.invoke(prompt.format(question=QUESTION))\n","\n","# Antwort parsen\n","result = parser.parse(llm_response.content)\n","\n","markdown_out(parsed_response)"],"metadata":{"id":"fXB-iqLyTi18"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Variante 2:** Chain mit direkte Eingabe als Dictionary"],"metadata":{"id":"yhR3N3tPX5x9"}},{"cell_type":"code","source":["chain = (\n","    prompt\n","    | llm\n","    | parser\n",")\n","\n","# Aufruf mit Dictionary\n","result = chain.invoke({\"question\": \"Was ist Machine Learning?\"})\n","markdown_out(result)"],"metadata":{"id":"RIM9uWjhYjfd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Variante 3:** Chain mit Dictionary-Transformation `RunnablePassthrough`"],"metadata":{"id":"NWP7LkqoX50f"}},{"cell_type":"code","source":["# Chain mit Dictionary-Transformation\n","from langchain_core.runnables import RunnablePassthrough\n","\n","chain = (\n","    {\"question\": RunnablePassthrough()}  # Wandelt String in Dictionary um\n","    | prompt\n","    | llm\n","    | parser\n",")\n","\n","# Aufruf mit einfachem String\n","result = chain.invoke(QUESTION)\n","markdown_out(result)"],"metadata":{"id":"nUz7fytPYATz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wR5J9U9bg5Br"},"source":["# 4 | Strukturierter Parser (komplex)\n","---"]},{"cell_type":"markdown","source":["Nun wird ein Beispiel mit einer größeren Anzahl an Werten getestet. Das folgende Programm nimmt Texte in beliebigen Sprachen entgegen und übersetzt sie ins Englische, Spanische und Chinesische."],"metadata":{"id":"nN1AWb1m9OK4"}},{"cell_type":"markdown","source":["**Schritt 1:** Der **Parser** definiert ein Schema für die erwartete Ausgabe und verarbeitet die Antwort des LLMs in ein strukturiertes Format (z.B. ein Python-Dictionary)."],"metadata":{"id":"lpn8vHKwcEui"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9snGgL4riemY"},"outputs":[],"source":["RESPONSE_SCHEMAS = [\n","    ResponseSchema(name=\"detected\", description=\"Die Sprache ist Nutzer-Eingabe\"),\n","    ResponseSchema(name=\"german\", description=\"German translation\"),\n","    ResponseSchema(name=\"english\", description=\"English translation\"),\n","    ResponseSchema(name=\"spanish\", description=\"Spanish translation\"),\n","    ResponseSchema(name=\"chinese\", description=\"Chinese translation\"),\n","]\n","parser = StructuredOutputParser.from_response_schemas(RESPONSE_SCHEMAS)\n","\n","FORMAT_INSTRUCTIONS = parser.get_format_instructions()"]},{"cell_type":"markdown","source":["**Schritt 2:** Verwendung des Schemas mit der Formatierungs Instruktion in einem Prompt."],"metadata":{"id":"zyAeqALjcbFt"}},{"cell_type":"code","source":["# 1. System-Prompt mit den Variablen (f-format) erstellen\n","SYSTEM_PROMPT = \"\"\"\n","    Übersetze in die angegebenen Sprachen.\\n{format_instructions}\\n{question}\n","\"\"\"\n","\n","# 2. Prompt-Template generieren\n","prompt = PromptTemplate(\n","    template=SYSTEM_PROMPT,\n","    input_variables=[\"question\"],\n","    partial_variables={\"format_instructions\": FORMAT_INSTRUCTIONS}\n",")"],"metadata":{"id":"WXr_0z9PcbFt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Variante 2:** Chain mit direkte Eingabe als *Dictionary*"],"metadata":{"id":"8emVa-S1cuPI"}},{"cell_type":"markdown","metadata":{"id":"sa6_dAk2ugre"},"source":["Zunächst wird ein deutscher Satz getestet, um zu beobachten, wie die Übersetzung in die drei Zielsprachen erfolgt."]},{"cell_type":"code","source":["chain = prompt | llm | parser\n","question = \"Wann wurde Python eingeführt?\"\n","chain.invoke({\"question\": question})"],"metadata":{"id":"01rQuqfdbv1o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dann wird ein französischer Satz getestet, um zu beobachten, wie die Übersetzung in die drei Zielsprachen erfolgt."],"metadata":{"id":"3lYL9wUcemex"}},{"cell_type":"code","source":["chain = prompt | llm | parser\n","question = \"Who rides so late through night and wind?\"\n","chain.invoke({\"question\": question})"],"metadata":{"id":"4qF-a2PbeNET"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = chain.invoke({\"question\": question})"],"metadata":{"id":"_QZ8OvijgblY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MQFjKL7uDgfA"},"source":["# 5 | Datenformat Parser\n","---"]},{"cell_type":"markdown","source":["LangChain bietet eine breite Palette an Parsern, die unterschiedliche Datenformate verarbeiten können, was seine Einsatzmöglichkeiten erheblich erweitert. Es unterstützt unter anderem die nahtlose Integration von Pandas-Datenrahmen, kommagetrennten Listen, JSON-Strukturen sowie Datums- und Zeitobjekten. Diese Flexibilität ermöglicht eine effiziente Anpassung an verschiedene Arten von Dateneingaben und macht LangChain zu einem leistungsstarken Werkzeug für die Analyse und Verarbeitung von Daten. Im Folgenden werden einige dieser Parser genauer betrachtet, ihre praktischen Anwendungen demonstriert und aufgezeigt, wie sie zur Optimierung von Prozessen und zur Gewinnung wertvoller Erkenntnisse beitragen können."],"metadata":{"id":"Q1oRFrLCx8eW"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","CSV\n","</font></p>"],"metadata":{"id":"EBERl78zyIzh"}},{"cell_type":"markdown","source":["Der **CommaSeparatedListOutputParser** ermöglicht die Umwandlung von LLM-Ausgaben in eine durch Kommas getrennte Liste und extrahiert diese als native Python-Liste. Dies ist besonders nützlich, wenn strukturierte Daten aus einem Sprachmodell extrahiert und für weitere Verarbeitungsschritte verwendet werden sollen. Indem der Parser die Ausgabe direkt in eine Listenstruktur überführt, erleichtert er die Handhabung und Weiterverarbeitung der generierten Daten in verschiedenen Anwendungen."],"metadata":{"id":"aptyNpJ0ynir"}},{"cell_type":"code","source":["# Abschnitt 1: Importe\n","from langchain.output_parsers import CommaSeparatedListOutputParser\n","from langchain_core.prompts import PromptTemplate\n","from langchain_openai import ChatOpenAI\n","\n","# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATURE = 0\n","\n","# Abschnitt 3: Parser und Prompt\n","output_parser = CommaSeparatedListOutputParser()\n","\n","format_instructions = output_parser.get_format_instructions()\n","\n","chat_prompt = PromptTemplate(\n","    template=\"List ten {subject}.\\n{format_instructions}\",\n","    input_variables=[\"subject\"],\n","    partial_variables={\"format_instructions\": format_instructions},\n",")\n","\n","# Abschnitt 4: Chat-Komponenten initialisieren\n","chat_model = ChatOpenAI(\n","    model=MODEL,\n","    temperature=TEMPERATURE,\n","    n=1\n",")\n","\n","# Abschnitt 5: Funktionen definieren\n","def generate_list(subject: str):\n","    \"\"\"  Generiert eine Liste mit zehn Elementen zu einem gegebenen Thema.  \"\"\"\n","    chain = chat_prompt | chat_model | output_parser\n","    return chain.invoke({\"subject\": subject})\n","\n","# Abschnitt 6: Hauptprogramm\n","def main():\n","    \"\"\"Startet die Hauptkonversation und gibt eine Liste von Städten aus.\"\"\"\n","    cities = generate_list(\"Städte\")\n","    print(type(cities), cities)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"PT2ZjJv_2tBt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mE2avWGaysc3"},"source":["<p><font color='black' size=\"5\">\n","JSON\n","</font></p>"]},{"cell_type":"markdown","source":["Die Ausgabe des LLM kann in einem JSON-Format strukturiert werden."],"metadata":{"id":"S69wM83z-1Hh"}},{"cell_type":"code","source":["#\n","# Variante mit Prompt\n","#\n","from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n","\n","prompt = (\n","    \"Bitte gib mir eine Person mit Name und Alter im JSON-Format zurück, z.B. \"\n","    '{\"name\": \"Max\", \"age\": 30}'\n",")\n","\n","response = llm.invoke(prompt)\n","print(\"Antwort vom Modell:\")\n","print(response)"],"metadata":{"id":"KIYp-bKe5t3e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#\n","# Variante mit JsonOutputParser\n","#\n","from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers.json import JsonOutputParser\n","\n","# Einfaches JSON-Schema\n","schema = {\n","        \"name\": {\"type\": \"string\"},\n","        \"age\": {\"type\": \"integer\"}\n","}\n","\n","# JsonOutputParser mit dem Schema erstellen\n","parser = JsonOutputParser(schema=schema)\n","\n","# OpenAI LLM initialisieren\n","llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n","\n","# Modell fragen, Antwort soll JSON sein\n","prompt = (\n","    \"Please give as an answert name and age of a person in json-Format\"\n",")\n","\n","response = llm.invoke(prompt)\n","# print(\"Antwort vom Modell:\")\n","# print(response)\n","\n","# Antwort parsen\n","result = parser.invoke(response)\n","print(\"Geparstes Ergebnis:\")\n","print(result)"],"metadata":{"id":"0dJs83IJ5t6X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In diesem Beispiel wird ein Satz erkannt, der als Englisch identifiziert wird, und anschließend ins Spanische, Französische und Chinesische übersetzt."],"metadata":{"id":"njnSr9msG424"}},{"cell_type":"code","source":["# Abschnitt 1: Importe\n","from langchain_core.output_parsers import JsonOutputParser\n","from langchain_core.prompts import PromptTemplate\n","from langchain_openai import ChatOpenAI\n","\n","# Abschnitt 2: Funktionen\n","def translate_text(text: str) -> dict:\n","    \"\"\"Übersetzt den Text in mehrere Sprachen\"\"\"\n","    # Parser ohne Pydantic-Modell konfigurieren\n","    parser = JsonOutputParser()\n","    llm = ChatOpenAI(model=\"gpt-4\", temperature=0.0)\n","\n","    # Prompt erstellen mit direkten Anweisungen zum JSON-Format\n","    prompt_template = \"\"\"\n","    Übersetze die Benutzereingabe in mehrere Sprachen und gib das Ergebnis als JSON zurück.\n","\n","    Dein JSON sollte folgende Felder enthalten:\n","    - detected: Die erkannte Ausgangssprache\n","    - german: Die deutsche Übersetzung\n","    - spanish: Die spanische Übersetzung\n","    - french: Die französische Übersetzung\n","    - chinese: Die chinesische Übersetzung\n","\n","    Benutzereingabe: {input}\n","    \"\"\"\n","\n","    prompt = PromptTemplate(\n","        template=prompt_template,\n","        input_variables=[\"input\"]\n","    )\n","\n","    # Kette ausführen\n","    chain = prompt | llm | parser\n","    return chain.invoke({\"input\": text})\n","\n","# Abschnitt 3: Hauptprogramm\n","# Beispieltext übersetzen und Ergebnisse ausgeben\n","result = translate_text(\"What is your name?\")\n","\n","print()\n","display(Markdown(f\"**Erkannte Sprache:** {result['detected']}\"))\n","display(Markdown(f\"**Deutsch:** {result['german']}\"))\n","display(Markdown(f\"**Spanisch:** {result['spanish']}\"))\n","display(Markdown(f\"**Französisch:** {result['french']}\"))\n","display(Markdown(f\"**Chinesisch:** {result['chinese']}\"))"],"metadata":{"id":"PYO2u9vKgyC8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ausgabe json-Format\n","import json\n","print(json.dumps(result, indent=2, ensure_ascii=False))"],"metadata":{"id":"coWJgUUZlmxb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y09-epHS5McO"},"source":["<p><font color='black' size=\"5\">\n","Datum/Uhrzeit\n","</font></p>"]},{"cell_type":"markdown","source":["Langchain bietet mit dem **DatetimeOutputParser** eine spezielle Funktion zur Interpretation von Datums- und Zeitangaben aus Texten. Sie erkennt unterschiedliche Formate und wandelt diese in ein standardisiertes Zeitformat um. Dies ist besonders hilfreich für Anwendungen wie Terminplanung, Datenanalyse oder andere Bereiche, in denen eine präzise Verarbeitung zeitlicher Informationen erforderlich ist. Der **DatetimeOutputParser** erleichtert die Verarbeitung von Datums- und Uhrzeitangaben und trägt dazu bei, dass Anwendungen zeitbezogene Daten effizient verwalten und nutzen können."],"metadata":{"id":"-zDlyRDUM4qS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRuzzhXF5Oa1"},"outputs":[],"source":["# Abschnitt 1: Importe\n","from langchain.output_parsers import DatetimeOutputParser\n","from langchain_core.prompts import PromptTemplate\n","from langchain_openai import OpenAI\n","\n","# Abschnitt 2: Konstanten\n","TEMPLATE = \"\"\"Beantworte die Frage zu genau wie möglich.\n","Wenn Du die Frage nach dem Datum nicht beantworten kannst, dann geben mir das Datum 01.01.1111.\n","\n","{question}\n","\n","{format_instructions}\"\"\"\n","\n","# Abschnitt 3: Komponenten initialisieren\n","llm = ChatOpenAI(\n","    model=\"gpt-4o-mini\",\n","    temperature=0.2,\n","    n=1\n",")\n","\n","output_parser = DatetimeOutputParser()\n","prompt = PromptTemplate(\n","    template=TEMPLATE,\n","    input_variables=['question'],\n","    partial_variables={'format_instructions': output_parser.get_format_instructions()}\n",")"]},{"cell_type":"markdown","metadata":{"id":"roqInbqGFdUb"},"source":["Wir können die Eingabeaufforderung anzeigen, die wir zum Abrufen von Daten verwenden werden."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HecixPQl6kYk"},"outputs":[],"source":["print(prompt)"]},{"cell_type":"markdown","metadata":{"id":"kg5ZkOetFiPx"},"source":["Wir erstellen die Kette, die wir zum Parsen von Daten verwenden werden."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2w5gYty_5pKd"},"outputs":[],"source":["chain = prompt | llm | output_parser"]},{"cell_type":"markdown","metadata":{"id":"dD-gNvBHFooj"},"source":["Wir werden nach zwei Daten fragen, einem realen und einem fiktiven."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ymu48Ggg518q"},"outputs":[],"source":["output = chain.invoke({\"question\": \"Wann wurde die Progammiersprache Python eingeführt?\"}) # 20.02.1991\n","print(output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nOPWYpxr6As1"},"outputs":[],"source":["output = chain.invoke({\"question\": \"An welchem Tag startet der Krieg im Videospiel Fallout?\"}) #  23.10.2077\n","print(output)"]},{"cell_type":"markdown","metadata":{"id":"Q6PFBjUSDo_N"},"source":["\n","# 6 | Pydantic Parser\n","---"]},{"cell_type":"markdown","source":["Pydantic ist eine Python-Bibliothek zur Datenvalidierung und Einstellungsverwaltung, die auf den Typanmerkungen von Python basiert. Sie ermöglicht eine effiziente und einfache Validierung sowie Umwandlung von Daten und nutzt dabei das standardisierte Typisierungssystem von Python.\n","\n","**Wichtige Funktionen von Pydantic:**\n","- **Datenvalidierung:** Überprüft, ob Daten den erwarteten Formaten entsprechen, und konvertiert sie bei Bedarf in die korrekten Typen.\n","- **Unterstützung für Code-Editoren:** Dank der Nutzung von Python-Typanmerkungen bieten moderne Editoren Funktionen wie Autovervollständigung und Typprüfung für Pydantic-Modelle.\n","- **Fehlermeldungen:** Detaillierte und verständliche Fehlermeldungen helfen, Probleme bei der Datenvalidierung gezielt zu identifizieren.\n","- **Einstellungsverwaltung:** Erleichtert das Laden und Verwalten von Konfigurationsparametern aus verschiedenen Quellen, darunter Umgebungsvariablen und JSON-Dateien.\n","- **Erweiterbarkeit:** Modelle lassen sich durch Methoden und Eigenschaften anpassen, und benutzerdefinierte Validierungen können mithilfe von Pydantic-Dekoratoren hinzugefügt werden.\n","- **Integration mit anderen Bibliotheken:** Besonders in Verbindung mit FastAPI verbessert Pydantic die Verwaltung und Validierung von API-Daten erheblich.\n","\n","Aufgrund seiner Flexibilität und Benutzerfreundlichkeit ist Pydantic besonders nützlich für Webentwicklung und datenintensive Anwendungen, bei denen eine zuverlässige Datenverarbeitung erforderlich ist."],"metadata":{"id":"ZB7QnNtDzaiz"}},{"cell_type":"markdown","source":["\n","\n","**Pydantic in LangChain:**    \n","LangChain, eine Bibliothek zur Entwicklung von Anwendungen mit Sprachmodellen, nutzt Pydantic für die strukturierte Verarbeitung von Modellantworten. Der **PydanticOutputParser** gewährleistet, dass die Ausgaben eines Sprachmodells einer vorgegebenen Struktur entsprechen. Dies ist besonders vorteilhaft für Anwendungen, bei denen konsistente Datenformate entscheidend sind – beispielsweise bei der Datenextraktion, API-Antworten oder der automatisierten Weiterverarbeitung von Modell-Ausgaben."],"metadata":{"id":"oxlZwhl-SfMX"}},{"cell_type":"markdown","source":["Der Code nachfolgende demonstriert die Verwendung von Pydantic zur Validierung strukturierter Daten am Beispiel von Kochrezepten. Diese Implementierung zeigt, wie Datenstrukturen definiert, validiert und gegen spezifische Regeln geprüft werden können.\n","\n","**Anwendungsszenario:**   \n","Stellen Sie sich vor, Sie entwickeln eine Koch-App oder eine Rezeptdatenbank, bei der die Qualität und Konsistenz der eingegebenen Rezepte entscheidend ist. Der Code bildet das Herzstück einer Validierungsschicht, die sicherstellt, dass alle Rezepte den definierten Standards entsprechen:\n","\n","**Datenstruktur-Validierung:**\n","+ Jedes Rezept muss bestimmte Pflichtfelder enthalten (Name, Schwierigkeitsgrad, Zubereitungszeit, Zutaten).\n","+ Typprüfung: Die Felder müssen den korrekten Datentyp aufweisen (z.B. Zubereitungszeit als ganze Zahl).\n","+ Inhaltliche Validierung: Der Schwierigkeitsgrad darf nur bestimmte Werte annehmen, die Zubereitungszeit muss in einem realistischen Bereich liegen.\n"],"metadata":{"id":"D2TvCYBhqCp0"}},{"cell_type":"code","source":["# Import\n","from langchain.output_parsers import PydanticOutputParser\n","from langchain.prompts import PromptTemplate\n","from langchain_openai import ChatOpenAI\n","from pydantic import BaseModel, Field\n","from typing import List\n","\n","# 1. Datenstruktur definieren\n","class Rezept(BaseModel):\n","    name: str = Field(description=\"Name des Gerichts\")\n","    schwierigkeitsgrad: str = Field(description=\"Schwierigkeitsgrad (leicht/mittel/schwer)\")\n","    zubereitungszeit: int = Field(description=\"Zubereitungszeit in Minuten\")\n","    zutaten: List[str] = Field(description=\"Liste der benötigten Zutaten\")\n","\n","# 2. Parser erstellen\n","parser = PydanticOutputParser(pydantic_object=Rezept)\n","\n","# 3. LLM initialisieren\n","llm = ChatOpenAI(temperature=0.2)  # Niedrige Temperatur für konsistentere Ergebnisse\n","\n","# 4. Prompt-Template erstellen\n","prompt_text = \"\"\"\n","Erstelle ein strukturiertes Rezept für das folgende Gericht.\n","\n","{format_instructions}\n","\n","Gericht: {gericht}\n","\"\"\"\n","\n","prompt = PromptTemplate(\n","    template=prompt_text,\n","    input_variables=[\"gericht\"],\n","    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",")\n","\n","# 5. Funktion zum Abfragen des Rezepts\n","def rezept_abfragen(gericht_name):\n","    try:\n","        # Prompt formatieren\n","        formatted_prompt = prompt.format(gericht=gericht_name)\n","\n","        # LLM-Antwort abrufen und parsen\n","        antwort = llm.invoke(formatted_prompt)\n","        rezept_objekt = parser.parse(antwort.content)\n","        return rezept_objekt\n","    except Exception as e:\n","        print(f\"❌ Fehler bei der Rezeptabfrage: {e}\")\n","        return None\n","\n","# 6. Separate Funktion zur Überprüfung des Rezepts\n","def rezept_pruefen(rezept_objekt):\n","    try:\n","        if rezept_objekt is None:\n","            raise ValueError(\"Kein Rezeptobjekt vorhanden\")\n","\n","        # Überprüfung der Struktur und Datentypen\n","        assert isinstance(rezept_objekt.name, str), \"Name ist kein String\"\n","        assert isinstance(rezept_objekt.schwierigkeitsgrad, str), \"Schwierigkeitsgrad ist kein String\"\n","        assert isinstance(rezept_objekt.zubereitungszeit, int), \"Zubereitungszeit ist keine Zahl\"\n","        assert isinstance(rezept_objekt.zutaten, list), \"Zutaten ist keine Liste\"\n","        assert len(rezept_objekt.zutaten) > 0, \"Zutaten-Liste ist leer\"\n","\n","        # Inhaltliche Überprüfung\n","        assert rezept_objekt.schwierigkeitsgrad.lower() in [\"leicht\", \"mittel\", \"schwer\"], \\\n","            \"Schwierigkeitsgrad muss 'leicht', 'mittel' oder 'schwer' sein\"\n","        assert 5 <= rezept_objekt.zubereitungszeit <= 240, \\\n","            \"Zubereitungszeit sollte zwischen 5 und 240 Minuten liegen\"\n","\n","        print(\"✅ Überprüfung erfolgreich!\")\n","        return True\n","    except Exception as e:\n","        print(f\"❌ Fehler bei der Überprüfung: {e}\")\n","        return False"],"metadata":{"id":"lAp6ftC4nq1_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 7. Normale Ausführung mit einem echten Rezept\n","def test_mit_echtem_rezept():\n","    print(\"=== Test mit echtem Rezept ===\")\n","    gericht_name = \"Spaghetti Carbonara\"\n","    rezept = rezept_abfragen(gericht_name)\n","\n","    if rezept and rezept_pruefen(rezept):\n","        print(f\"\\nRezept: {rezept.name}\")\n","        print(f\"Schwierigkeitsgrad: {rezept.schwierigkeitsgrad}\")\n","        print(f\"Zubereitungszeit: {rezept.zubereitungszeit} Minuten\")\n","        print(\"Zutaten:\")\n","        for zutat in rezept.zutaten:\n","            print(f\"- {zutat}\")"],"metadata":{"id":"ECbX1FuBsT7g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_mit_echtem_rezept()"],"metadata":{"id":"P2YZn-Bxsawu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 8. Dummy-Case für Fehlererzeugung\n","def test_mit_fehler_rezept():\n","    print(\"\\n=== Test mit Dummy-Fehler-Rezept ===\")\n","    # Erstellen eines fehlerhaften Rezeptobjekts\n","    from pydantic import create_model\n","\n","    # Fehlerhafte Zubereitungszeit (als String statt int)\n","    fehlerhaftes_rezept = Rezept(\n","        name=\"Fehlerrezept\",\n","        schwierigkeitsgrad=\"unmöglich\",  # Ungültiger Schwierigkeitsgrad\n","        zubereitungszeit=300,  # Zu lange Zubereitungszeit\n","        zutaten=[]  # Leere Zutatenliste\n","    )\n","\n","    if rezept_pruefen(fehlerhaftes_rezept) == False:\n","        print(\"Der Fehlertest war erfolgreich: Die Validierung hat die Fehler erkannt!\")"],"metadata":{"id":"kt9ufk_qsVuG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_mit_fehler_rezept()"],"metadata":{"id":"0XAECoF8secr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hzCTsn14DyU0"},"source":["# 7 | Eigene Parser\n","---"]},{"cell_type":"markdown","source":["In bestimmten Szenarien kann es sinnvoll sein, einen benutzerdefinierten Parser zu erstellen, um die Modellausgabe eindeutig zu formatieren.  \n","\n","Dafür bietet sich die Verwendung von **RunnableLambda** oder **RunnableGenerator** in LCEL an, was für viele Fälle ein guten Ansatz ist.  "],"metadata":{"id":"DEXj9woZzzy0"}},{"cell_type":"markdown","metadata":{"id":"isGwPhF4GUM5"},"source":["<p><font color='black' size=\"5\">\n","Anwendungsfall\n","</font></p>"]},{"cell_type":"markdown","source":["\n","\n","Large Language Models (LLMs) wie GPT-4 können Text generieren, der Code und erklärende Beschreibungen nahtlos vermischt. Dies kann zwar für Lern- und Dokumentationszwecke unglaublich nützlich sein, kann aber eine Herausforderung darstellen, wenn aus solchen Ausgaben mit gemischtem Inhalt nur der Code extrahiert und ausgeführt werden muss. Um dies zu beheben, implementieren wir eine einfache Funktion, die nicht-Python-Codezeilen aus einer gegebenen Textzeichenfolge entfernt.\n","\n","Bei diesem Ansatz werden reguläre Ausdrücke verwendet, um Zeilen zu identifizieren und beizubehalten, die der typischen Python-Syntax entsprechen, während Zeilen verworfen werden, die beschreibender Text zu sein scheinen. Aufgrund der inhärenten Komplexität und Variabilität sowohl von Python-Code als auch von natürlicher Sprache kann diese Methode jedoch nie perfekt sein. Sie basiert auf heuristischen Mustern, die Code manchmal fälschlicherweise als Text klassifizieren oder umgekehrt.\n","\n","Im nächsten Abschnitt werden wir untersuchen, wie ein anderes LLM beim Entfernen von Nicht-Python-Code helfen kann und möglicherweise eine ausgefeiltere und genauere Lösung bietet. Das folgende Beispiel enthält eine Mischung aus LLM-Kommentaren und generiertem Code.\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"sPq64ydYgJ42"}},{"cell_type":"code","source":["from langchain_core.runnables import RunnableLambda\n","from langchain_openai import ChatOpenAI\n","from IPython.display import display, Markdown\n","\n","\n","# 1. LLM-Modell definieren\n","llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.9)\n","\n","# 2. Funktion zur Trennung von Erläuterung und Code\n","def parse_ai_message(ai_message):\n","    \"\"\"Trennt die Erläuterung und den Code aus einer AIMessage und gibt sie separat zurück.\"\"\"\n","    text = ai_message.content  # Extrahiere den reinen Textinhalt der AIMessage\n","\n","    if \"```\" in text:\n","        # Trennen der Erläuterung und des Codes\n","        parts = text.split(\"```\")\n","        explanation = parts[0].strip()\n","        code = parts[1].strip() if len(parts) > 1 else \"\"\n","    else:\n","        # Falls kein Codeblock vorhanden ist, geben wir nur die Erläuterung zurück\n","        explanation = text.strip()\n","        code = \"\"\n","\n","    return explanation, code  # Rückgabe von zwei separaten Werten\n","\n","# 3. RunnableLambda für das Parsing erstellen\n","parser = RunnableLambda(parse_ai_message)\n","\n","# 4. LLM und Parser verketten\n","chain = llm | parser\n","\n","# 5. Eingabe an die Pipeline senden und Ergebnis ausgeben\n","prompt = \"\"\"\n","    Erkläre mir, wie man eine einfache Funktion in Python erstellt und gib ein Beispiel.\n","    Verwende bei Formeln das Format $ Formel $\n","\"\"\"\n","explanation, code = chain.invoke(prompt)\n","\n","display(Markdown(\"## Erläuterung\"))\n","display(Markdown(explanation))\n","display(Markdown(\"## Code\"))\n","print(code)"],"metadata":{"id":"6wjQlnF7qeso"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3TjZs_TRht1n"},"source":["# A | Aufgabe\n","---"]},{"cell_type":"markdown","source":["Die Aufgabestellungen unten bieten Anregungen, Sie können aber auch gerne eine andere Herausforderung angehen."],"metadata":{"id":"smOJ1YD15z_Q"}},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","JSON-Parser mit LangChain\n","</font></p>"],"metadata":{"id":"L5IWdaBS0UZD"}},{"cell_type":"markdown","source":["\n","**Ziel:** Verständnis für den Einsatz von `JsonOutputParser` in LangChain.\n","\n","**Aufgabe:**  \n","1. Nutze den `JsonOutputParser` von LangChain, um eine KI-Antwort in JSON zu formatieren.  \n","2. Lasse ein Language Model (z. B. OpenAI GPT) eine Liste von drei zufälligen Städten in Deutschland generieren.  \n","3. Verwende den Parser, um die Ausgabe in ein JSON-Format umzuwandeln.\n","\n","**Erwartete Ausgabe (Beispiel):**\n","```json\n","{\n","  \"cities\": [\"Berlin\", \"Hamburg\", \"München\"]\n","}\n","```\n"],"metadata":{"id":"JTdhJeN8u99p"}},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Extraktion von Schlüsselwerten\n","</font></p>"],"metadata":{"id":"UCpw6-OHvHJJ"}},{"cell_type":"markdown","source":["\n","**Ziel:** Nutzung des `PydanticOutputParser`, um strukturierte Daten aus natürlicher Sprache zu extrahieren.\n","\n","**Aufgabe:**  \n","1. Definiere eine `Pydantic`-Datenklasse mit den Feldern: `name` (str), `alter` (int), `stadt` (str).  \n","2. Verwende ein Language Model, um aus einer gegebenen Beschreibung eine Person zu extrahieren.  \n","3. Parse die Antwort mit dem `PydanticOutputParser`.\n","\n","**Beispiel-Eingabe:**\n","> \"Max Mustermann ist 35 Jahre alt und lebt in Berlin.\"\n","\n","**Erwartete Ausgabe (Pydantic-Modell):**\n","```json\n","{\n","  \"name\": \"Max Mustermann\",\n","  \"alter\": 35,\n","  \"stadt\": \"Berlin\"\n","}\n","```"],"metadata":{"id":"geRHkOMCvKum"}},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Parser für Listenformate\n","</font></p>"],"metadata":{"id":"8-AzxHafvSE_"}},{"cell_type":"markdown","source":["\n","**Ziel:** Implementierung eines eigenen Parsers zur Umwandlung von KI-Ausgaben in Listen.\n","\n","**Aufgabe:**  \n","1. Erstelle eine eigene Parser-Klasse, die eine durch Kommas getrennte Liste in ein Listenformat umwandelt.  \n","2. Verwende diesen Parser, um eine Liste von fünf beliebten Büchern aus einer Language-Model-Antwort zu extrahieren.\n","\n","**Beispiel-Eingabe:**\n","> \"Die Verwandlung, Faust, Der Prozess, Die Blechtrommel, Der Vorleser\"\n","\n","**Erwartete Ausgabe:**\n","```python\n","[\"Die Verwandlung\", \"Faust\", \"Der Prozess\", \"Die Blechtrommel\", \"Der Vorleser\"]\n","```"],"metadata":{"id":"ytCh-C2uvUFw"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Kombination Parser & PromptTemplate\n","</font></p>"],"metadata":{"id":"I3LRv5YAviSF"}},{"cell_type":"markdown","source":["\n","**Ziel:** Verwendung von `StructuredOutputParser` in Kombination mit `PromptTemplate`.\n","\n","**Aufgabe:**  \n","1. Erstelle ein `PromptTemplate`, das eine strukturierte Antwort über ein Land ausgibt (Name, Hauptstadt, Einwohnerzahl).  \n","2. Verwende den `StructuredOutputParser`, um die Antwort in ein Dictionary zu konvertieren.\n","\n","**Beispiel-Prompt:**\n","> \"Gib mir Informationen zu Frankreich im folgenden JSON-Format: { 'name': '...', 'hauptstadt': '...', 'einwohner': ... }.\"\n","\n","**Erwartete Ausgabe:**\n","```json\n","{\n","  \"name\": \"Frankreich\",\n","  \"hauptstadt\": \"Paris\",\n","  \"einwohner\": 67000000\n","}\n","```\n"],"metadata":{"id":"bZqMjEWavk_i"}}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["IQm3u7mNZc6b","qSG-9nZaZj3q","8yUdGhEHadmI","DnLL7z-4ocSH","0hI3GKnNbJ99","23F6baGU4Cq2","3464d8Glm0yi","Hgn0xlXqDOyk","ccg_cmsN4chQ","Ku-TomzW498G","7KOoPcwW5H2i","0QVN1QOa5kNr","8gaiVjttbmno","K0TgYNpviMDn","fGd1iijaiSWo","ljyypqKlFt11","ayAtyWH0eH6Y","N9rNF-O1i0cT","bRQRxrNZw_7C","6AXueZa4xPgr","XWKhIzSIbs5M","9zv-Rce0fElT","BWt1r_ruguI0","yBUHN0ZOHKwZ","lWa5rMj3G2Wb","8wd5ZR15bxf7","Wou65uivk35y","og_moM7Qk-Vh","6NaQWfRFl_vz","R9u3hT1RIA6Z","iFllFW8wb080","Ej4ajzVLIf00","sox0Vtx-ItWK","8TaogQLhb62v","ymeJjPk-OMQS","EqHStIrLOeX4","oNfzqKrDOhaz","gqhIYi29Okn_","uFpRtFTmOnhv","VB7mOeCtOw8x","h_CPWZO5Swkl","i-YaukXicEYQ","qoaSA0q_b-yJ","z6a4og9mcJXd"],"authorship_tag":"ABX9TyNImuN7fSDEO2S/GTkFkH8T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)"],"metadata":{"id":"37rMyCQz2JNw"}},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b>\n","Snippets GenAI\n","</b></font> </br></p>\n","\n","---"],"metadata":{"id":"R5CfUEMJdvFQ"}},{"cell_type":"markdown","source":["# 0 | Setup & Konfiguration\n","---"],"metadata":{"id":"IQm3u7mNZc6b"}},{"cell_type":"markdown","source":["# 0.1 | Install Kursbibliothek üìå"],"metadata":{"id":"qSG-9nZaZj3q"}},{"cell_type":"code","source":["!uv pip install --system -q git+https://github.com/ralf-42/Python_Modules"],"metadata":{"id":"zYXfr5UlZnfk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 0.2 | Import Kursbibliothek üìå"],"metadata":{"id":"8yUdGhEHadmI"}},{"cell_type":"code","source":["from genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages\n","from genai_lib.chromadb_statistics import display_chromadb_statistics, export_statistics_to_json\n","from genai_lib.show_md import show_md"],"metadata":{"id":"YxBd5eXianVB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 0.3 | Kursbibliothek - Inhalt"],"metadata":{"id":"DnLL7z-4ocSH"}},{"cell_type":"markdown","source":["\n","\n","üõ†Ô∏è utilities.py"],"metadata":{"id":"F_sLhNUAROy5"}},{"cell_type":"markdown","source":["**Umgebungs- und Setup-Funktionen:**\n","\n","`check_environment()`\n","\n","- Zeigt die aktuelle Python-Version an\n","- Listet alle installierten LangChain-Bibliotheken auf\n","- Unterdr√ºckt st√∂rende Deprecation-Warnungen\n","\n","`install_packages(modules)`\n","\n","- Installiert Python-Module automatisch mit `uv pip install`\n","- Pr√ºft vorher, ob Module bereits verf√ºgbar sind\n","- Optimiert f√ºr Google Colab mit ruhiger Installation\n","\n","`get_ipinfo()`\n","\n","- Ruft Geoinformationen zur aktuellen √∂ffentlichen IP-Adresse ab\n","- Zeigt IP, Standort, Provider und weitere Netzwerkdaten an\n","\n","`setup_api_keys(key_names, create_globals=True)`\n","\n","- L√§dt API-Keys aus Google Colab userdata\n","- Setzt sie als Umgebungsvariablen und optional als globale Variablen\n","- Unterst√ºtzt mehrere Keys gleichzeitig (OpenAI, Anthropic, Hugging Face, etc.)\n","\n","`mprint(text)`\n","\n","- Gibt Text als formatiertes Markdown in Jupyter-Notebooks aus\n","- Nutzt IPython's `display()` und `Markdown()` f√ºr bessere Darstellung\n","\n","`process_response(response)`\n","\n","- Extrahiert strukturierte Informationen aus LLM-Antworten\n","- Parst Token-Nutzungsdaten (Prompt, Completion, Total)\n","- Gibt bereinigten Text und Metadaten als Dictionary zur√ºck\n","\n","\n","---"],"metadata":{"id":"mLJCYHpURRSo"}},{"cell_type":"markdown","source":["üîç **chromadb_statistics.py**    \n","Umfassendes Analysetool f√ºr ChromaDB-Vektor-Datenbanken:\n","* `analyze_collection(name, path)` - Analysiert einzelne Collection (Chunks, Dokumente, Quellen)\n","* `get_database_statistics(path)` - Erstellt Gesamt√ºbersicht aller Collections\n","* `get_collection_chunks(name, path, limit)` - Ruft Chunk-Daten mit Paginierung ab\n","* `analyze_chunk_sizes(name, path)` - Untersucht Gr√∂√üenverteilung der Text-Chunks\n","* `search_chunks_by_source(name, path, filter)` - Filtert Chunks nach Quelldatei\n","* `export_statistics_to_json(path, file)` - Exportiert Statistiken als JSON\n","* `display_chromadb_statistics(path)` - Formatierte Konsolen-Ausgabe aller Analysen\n","* `compare_collections(path, names)` - Vergleicht mehrere Collections miteinander\n","\n","**Anwendung**: Monitoring, Performance-Optimierung und Qualit√§tsanalyse von RAG-Systemen\n","\n","\n","---"],"metadata":{"id":"O0d8naMBtxRB"}},{"cell_type":"markdown","source":["\n","üé® show_md.py"],"metadata":{"id":"wqZvD_5YRJj8"}},{"cell_type":"markdown","source":["Markdown-Display-Funktionen f√ºr Jupyter Notebooks:\n","\n","- **`show_md(text, prefix)`** - Basis-Markdown-Anzeige mit optionalem Prefix\n","- **`show_title(text)`** - Zeigt Titel mit üí°-Emoji (`# Titel üí°`)\n","- **`show_subtitle(text)`** - Zeigt Untertitel (`## Untertitel`)\n","- **`show_info(text)`** - Info-Meldung mit ‚ÑπÔ∏è-Symbol\n","- **`show_warning(text)`** - Warnung mit ‚ö†Ô∏è-Symbol\n","- **`show_success(text)`** - Erfolgsmeldung mit ‚úÖ-Symbol\n","\n","**Anwendung**: Strukturierte und visuell ansprechende Notebook-Ausgaben\n","\n","---"],"metadata":{"id":"vcbrqq7LRGoy"}},{"cell_type":"markdown","source":["\n","\n","üéØ Typische Anwendung"],"metadata":{"id":"g_WI9yZkRgA5"}},{"cell_type":"markdown","source":["```python\n","# 1. Umgebung pr√ºfen und API-Keys laden\n","check_environment()\n","setup_api_keys([\"OPENAI_API_KEY\"])\n","\n","# 2. Strukturierte Notebook-Ausgaben\n","show_title(\"Mein KI-Projekt\")\n","show_info(\"Experiment gestartet\")\n","\n","# 3. PREPARE-Prompts erstellen\n","prompt = apply_prepare_framework(\n","    task=\"Erkl√§re maschinelles Lernen\",\n","    role=\"KI-Tutor\",\n","    tone=\"verst√§ndlich\"\n",")\n","\n","# 4. LLM-Antworten verarbeiten\n","result = process_response(llm_response)\n","show_success(f\"Antwort erhalten: {result['tokens_total']} Tokens\")\n","```\n","\n","**Ideal f√ºr**: KI-Kurse, Jupyter-Notebooks, LangChain-Projekte, strukturierte Prompt-Engineering"],"metadata":{"id":"XeJ4r80YRWFE"}},{"cell_type":"markdown","source":["# 0.4 | Umgebung einrichten üìå"],"metadata":{"id":"0hI3GKnNbJ99"}},{"cell_type":"code","source":["#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n","!uv pip install --system -q git+https://github.com/ralf-42/Python_Modules\n","from genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages\n","setup_api_keys(['OPENAI_API_KEY', 'HF_TOKEN'], create_globals=False)\n","print()\n","check_environment()\n","print()\n","get_ipinfo()\n","# ---\n","# Bei Bedarf: Trennen zwischen Installationsname () und Importname (f√ºr Python) beide Angaben in Klammern\n","# install_packages([('markitdown[all]', 'markitdown'), 'langchain_chroma', ]"],"metadata":{"id":"xlCNUpx_bOqq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 0.5 | Standard-Importe f√ºr GenAI"],"metadata":{"id":"23F6baGU4Cq2"}},{"cell_type":"code","source":["# Moderne LangChain-Imports (Version 0.2.0+)\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n","from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n","\n","# Standard-Python-Imports\n","import os\n","from IPython.display import Markdown, display"],"metadata":{"id":"aTr6UN0h4IG-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 0.6 | Namenskonventionen"],"metadata":{"id":"3464d8Glm0yi"}},{"cell_type":"markdown","source":["**Empfohlene Namenskonventionen (snake_case):**\n","\n","**Variablen:**\n","- `user_input` - Benutzereingabe\n","- `system_prompt` - Systemnachricht\n","- `chat_history` - Chatverlauf\n","- `model_name` - Modellname\n","- `temperature` - Temperatur-Parameter\n","- `response` - API-Antwort\n","- `result` - Verarbeitetes Ergebnis\n","\n","**LangChain-Objekte:**\n","- `prompt_template` - Prompt-Vorlage\n","- `prompt` - Fertiger Prompt\n","- `llm` - Sprachmodell\n","- `chain` - Verarbeitungskette\n","- `parser` - Output-Parser"],"metadata":{"id":"JHxMlnzWm4p8"}},{"cell_type":"markdown","source":["# 0.7 | Temperatur-Guidelines"],"metadata":{"id":"Hgn0xlXqDOyk"}},{"cell_type":"markdown","source":["**Temperatur-Richtlinien f√ºr KI-Modelle**\n","\n","| Temperatur | Kategorie | Wert | Anwendungsbereich | Beschreibung |\n","|------------|-----------|------|-------------------|--------------|\n","| **Faktisch** | `factual` | 0.0 | Mathematik, Code, Datenanalyse | Deterministische Ausgaben f√ºr pr√§zise, reproduzierbare Ergebnisse |\n","| **Ausgewogen** | `balanced` | 0.3 | Allgemeine Fragen, Erkl√§rungen | Leicht variierte Antworten bei gleichbleibender Genauigkeit |\n","| **Kreativ** | `creative` | 0.7 | Brainstorming, Texterstellung | Vielf√§ltige und innovative Antworten f√ºr kreative Aufgaben |\n","| **Sehr kreativ** | `very_creative` | 1.0 | Experimentelle Inhalte | Maximale Kreativit√§t und Unvorhersagbarkeit |\n","\n","**Verwendungshinweise**\n","\n","- **Niedrige Temperatur (0.0-0.3)**: F√ºr sachliche, pr√§zise Antworten\n","- **Mittlere Temperatur (0.4-0.6)**: F√ºr ausgewogene Kommunikation\n","- **Hohe Temperatur (0.7-1.0)**: F√ºr kreative und experimentelle Ausgaben\n","\n"],"metadata":{"id":"ce4MVxa6DXm-"}},{"cell_type":"markdown","source":["# 1 | Erste Schritte\n","---"],"metadata":{"id":"ccg_cmsN4chQ"}},{"cell_type":"markdown","source":["# 1.1 | Einfachster LLM-Aufruf"],"metadata":{"id":"Ku-TomzW498G"}},{"cell_type":"code","source":["# Einfachster LLM-Aufruf\n","llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n","user_input = \"Erkl√§re mir KI in einem Satz\"\n","\n","# Direkter Aufruf\n","response = llm.invoke([HumanMessage(content=user_input)])\n","print(response.content)"],"metadata":{"id":"i7cIWKYm5FD1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1.2 | Mit System-Prompt"],"metadata":{"id":"7KOoPcwW5H2i"}},{"cell_type":"code","source":["# System-Prompt mit modernem Ansatz\n","system_prompt = \"Du bist ein hilfreicher KI-Assistent. Antworte kurz und pr√§zise.\"\n","user_input = \"Was ist Machine Learning?\"\n","\n","messages = [\n","    SystemMessage(content=system_prompt),\n","    HumanMessage(content=user_input)\n","]\n","\n","response = llm.invoke(messages)\n","print(response.content)"],"metadata":{"id":"roPof5IR5NPZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1.3 | Streaming-Ausgabe"],"metadata":{"id":"0QVN1QOa5kNr"}},{"cell_type":"code","source":["# Streaming f√ºr Echtzeit-Ausgabe\n","print(\"ü§ñ Antwort: \", end=\"\")\n","for chunk in llm.stream([HumanMessage(content=\"Erz√§hle eine kurze Geschichte √ºber KI\")]):\n","    print(chunk.content, end=\"\", flush=True)\n","print(\"\\n\")"],"metadata":{"id":"EJVCuJD05pky"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2 | Prompts & Templates\n","---"],"metadata":{"id":"8gaiVjttbmno"}},{"cell_type":"markdown","source":["# 2.1 | Einfacher String-Prompt"],"metadata":{"id":"K0TgYNpviMDn"}},{"cell_type":"code","source":["# Einfacher String-Prompt\n","frage = \"Wer ist Albert Einstein?\"\n","user_input = f\"Du bist ein hilfreicher Assistent. Beantworte die Frage: {frage}\"\n","\n","# Als HumanMessage verwenden\n","response = llm.invoke([HumanMessage(content=user_input)])\n","print(f\"üéØ Antwort: {response.content}\")"],"metadata":{"id":"Crwgzo0XiRHt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.2 | PromptTemplate"],"metadata":{"id":"fGd1iijaiSWo"}},{"cell_type":"code","source":["from langchain_core.prompts import PromptTemplate\n","\n","# Prompt definieren\n","prompt = PromptTemplate.from_template(\n","    \"Systemanweisung: {system_prompt}\\n\"\n","    \"Benutzereingabe: {input}\\n\"\n","    \"Antwort:\"\n",")"],"metadata":{"id":"6NEV8D11iViQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.3 | ChatPromptTemplate"],"metadata":{"id":"ljyypqKlFt11"}},{"cell_type":"code","source":["# Modernes ChatPromptTemplate (einfache Version)\n","prompt_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent.\"),\n","    (\"human\", \"{user_input}\")\n","])"],"metadata":{"id":"tdd5vchMFzml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Direkt mit Chain verwenden (moderne LCEL-Syntax)\n","chain = prompt_template | llm | StrOutputParser()\n","result = chain.invoke({\"user_input\": \"Was ist Machine Learning?\"})\n","\n","print(f\"üéØ Ergebnis: {result}\")"],"metadata":{"id":"OsYo1K4TF2VT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.4 | ChatPromptTemplate - History üìå"],"metadata":{"id":"ayAtyWH0eH6Y"}},{"cell_type":"code","source":["from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","\n","# Erweitert mit Chat-History\n","prompt_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"human\", \"{user_input}\")\n","])"],"metadata":{"id":"x_VtyM8CeL9W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Moderne Chain-Syntax\n","chain = prompt_template | llm | StrOutputParser()\n","\n","# Ausf√ºhrung mit Input-Dictionary\n","result = chain.invoke({\n","    \"system_prompt\": \"Du bist ein Experte f√ºr Wissenschaft.\",\n","    \"chat_history\": [],  # Leer f√ºr erstes Gespr√§ch\n","    \"user_input\": \"Erkl√§re mir die Relativit√§tstheorie\"\n","})\n","\n","print(f\"üìã Ergebnis: {result}\")"],"metadata":{"id":"jM72ewbNFc2H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.5 | FewShotPromptTemplate"],"metadata":{"id":"N9rNF-O1i0cT"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, FewShotChatMessagePromptTemplate\n","from langchain_core.messages import HumanMessage, AIMessage\n","\n","# Beispiele direkt als Liste von (input, output)\n","examples = [\n","    {\"input\": \"Was ist die Hauptstadt von Frankreich?\", \"output\": \"Die Hauptstadt von Frankreich ist Paris.\"},\n","    {\"input\": \"Wie viele Kontinente gibt es?\", \"output\": \"Es gibt sieben Kontinente.\"}\n","]"],"metadata":{"id":"mFxG58QEi8jf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Few-Shot Prompt als Nachrichtenblock\n","few_shot_template = FewShotChatMessagePromptTemplate.from_examples(\n","    examples=examples,\n","    example_prompt=ChatPromptTemplate.from_messages([\n","        (\"human\", \"{user_input}\"),\n","        (\"ai\", \"{output}\")\n","    ])\n",")"],"metadata":{"id":"xcWLkftvvPIx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Finales PromptTemplate mit Systemrolle, Few-Shot-Beispielen, Chat-Verlauf und neuer Eingabe\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    *few_shot.messages,\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"human\", \"{user_input}\")\n","])"],"metadata":{"id":"OsO6aQ7cvQcv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.6 | ChatMessagePromptTemplate"],"metadata":{"id":"bRQRxrNZw_7C"}},{"cell_type":"code","source":["from langchain_core.prompts.chat import ChatMessagePromptTemplate\n","\n","moderator_message_template = ChatMessagePromptTemplate(\n","    role=\"moderator\",\n","    prompt=\"Bitte bleib respektvoll und h√∂flich.\"\n",")"],"metadata":{"id":"GLrSc2XXxDSx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent.\"),\n","    moderator_prompt,  # eigene Rolle\n","    (\"human\", \"{user_input}\")\n","])"],"metadata":{"id":"q5APfPbDxHh2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.7 | partial_variables"],"metadata":{"id":"6AXueZa4xPgr"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","# Prompt mit zwei Variablen\n","template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    (\"human\", \"{user_input}\")\n","])\n","\n","# system_prompt wird hier \"fest verdrahtet\"\n","prompt_with_fixed_system = template.partial(system_prompt=\"Du bist ein KI-Experte f√ºr Geschichte.\")"],"metadata":{"id":"NVs62-IzxWFU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Jetzt muss man beim .invoke() nur noch input √ºbergeben\n","response = (prompt_with_fixed_system | llm | parser).invoke({\n","    \"user_input\": \"Was war die Ursache des Drei√üigj√§hrigen Krieges?\"\n","})"],"metadata":{"id":"zpZX7bAlxa3u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3 | Model & Chains\n","---"],"metadata":{"id":"XWKhIzSIbs5M"}},{"cell_type":"markdown","source":["# 3.1 | Model - simple"],"metadata":{"id":"9zv-Rce0fElT"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","model = \"gpt-4o-mini\"\n","temperature = 0.0\n","llm = ChatOpenAI(model=model, temperatur=temperatur)\n","\n","# Modell aufrufen\n","response = llm.invoke(prompt)"],"metadata":{"id":"0eQCRJ9JfKxU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3.3 | Chain üìå"],"metadata":{"id":"BWt1r_ruguI0"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","\n","model = \"gpt-4o-mini\"\n","temperature = 0.0\n","llm = ChatOpenAI(model=model, temperature=temperature)\n","\n","parser = StrOutputParser()  # Wenn noch kein eigener Parser existiert\n","\n","# LCEL-Chain\n","chain = prompt | llm | parser"],"metadata":{"id":"NpvGhi7fgx8I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Beispielhafte Eingabewerte\n","input_variables = {\n","    \"system_prompt\": system_prompt,\n","    \"chat_history\": history,\n","    \"input\": user_input\n","}\n","\n","# Aufruf der Kette mit Eingabewerten\n","response = chain.invoke(input_variables)"],"metadata":{"id":"uV6TfLiLuDMp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Historie aktualisieren\n","history.append(HumanMessage(content=user_input))\n","history.append(AIMessage(content=response.content))"],"metadata":{"id":"3IqqDB-1gR8d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3.4 | Konfigurationsfunktion"],"metadata":{"id":"yBUHN0ZOHKwZ"}},{"cell_type":"code","source":["# Verschiedene LLM-Konfigurationen (moderne Syntax)\n","def create_llm(model_name: str = \"gpt-4o-mini\", temp: float = 0.0):\n","    \"\"\"\n","    Erstellt ein LLM mit modernen Parametern\n","    \"\"\"\n","    return ChatOpenAI(\n","        model=model_name,\n","        temperature=temp,\n","        max_tokens=1000,\n","        timeout=30,\n","        max_retries=2\n","    )\n","\n","# Verschiedene Modell-Varianten\n","llm_creative = create_llm(\"gpt-4o-mini\", 0.8)  # Kreativ\n","llm_factual = create_llm(\"gpt-4o-mini\", 0.0)   # Faktisch\n","llm_balanced = create_llm(\"gpt-4o-mini\", 0.3)  # Ausgewogen\n","\n","print(\"ü§ñ LLM-Instanzen erstellt\")"],"metadata":{"id":"B7JMtR4HHO4h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3.5 | Runnables-Methoden"],"metadata":{"id":"lWa5rMj3G2Wb"}},{"cell_type":"markdown","source":["Jedes Runnable unterst√ºtzt standardm√§√üig folgende Methoden:\n","\n","1. **invoke()**: F√ºr einzelne, synchrone Anfragen\n","2. **batch()**: Verarbeitet mehrere Eingaben parallel\n","3. **stream()**: Gibt Teilergebnisse zur√ºck, sobald sie verf√ºgbar sind\n","4. **ainvoke()**: Asynchrone Version von invoke\n","5. **abatch()**: Asynchrone Version von batch\n","6. **astream()**: Asynchrone Version von stream"],"metadata":{"id":"hUrE01P8G58H"}},{"cell_type":"markdown","source":["# 4 | Output Parser\n","---"],"metadata":{"id":"8wd5ZR15bxf7"}},{"cell_type":"markdown","source":["# 4.1 | Simple"],"metadata":{"id":"Wou65uivk35y"}},{"cell_type":"code","source":["result = {\n","    \"text\": response.content,\n","    \"tokens_used\": response.response_metadata[\"token_usage\"][\"total_tokens\"]\n","}"],"metadata":{"id":"UNWKS2VNk6hT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4.2 | StrOutputParser üìå"],"metadata":{"id":"og_moM7Qk-Vh"}},{"cell_type":"code","source":["from langchain_core.output_parsers.string import StrOutputParser\n","\n","parser = StrOutputParser()"],"metadata":{"id":"iJB4yap0lRq3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chain = prompt| llm | parser\n","response = chain.invoke()"],"metadata":{"id":"wM_MqJzYHiJn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4.3 | SimpleJsonOutputParser"],"metadata":{"id":"6NaQWfRFl_vz"}},{"cell_type":"code","source":["from langchain.output_parsers import SimpleJsonOutputParser\n","\n","# Nur wenn keine spezifische Struktur ben√∂tigt wird\n","simple_parser = SimpleJsonOutputParser()"],"metadata":{"id":"XEYphtWRl___"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4.4 | Strukturierter Output mit Pydantic"],"metadata":{"id":"R9u3hT1RIA6Z"}},{"cell_type":"code","source":["from pydantic import BaseModel, Field\n","from langchain_core.output_parsers import PydanticOutputParser\n","\n","# Pydantic-Model definieren\n","class QuestionAnswer(BaseModel):\n","    question: str = Field(description=\"Die gestellte Frage\")\n","    answer: str = Field(description=\"Die Antwort auf die Frage\")\n","    confidence: int = Field(description=\"Vertrauen in die Antwort (0-100)\")\n","    category: str = Field(description=\"Kategorie der Frage\")\n","\n","# Parser erstellen\n","pydantic_parser = PydanticOutputParser(pydantic_object=QuestionAnswer)\n","\n","# Chain mit strukturiertem Output\n","structured_chain = (\n","    ChatPromptTemplate.from_messages([\n","        (\"system\", \"Du bist ein Experte. {format_instructions}\"),\n","        (\"user\", \"{question}\")\n","    ]).partial(format_instructions=pydantic_parser.get_format_instructions())\n","    | llm\n","    | pydantic_parser\n",")\n","\n","# Testen\n","try:\n","    structured_result = structured_chain.invoke({\"question\": \"Was ist Photosynthese?\"})\n","    print(f\"üì¶ Strukturiertes Ergebnis: {structured_result}\")\n","    print(f\"üéØ Antwort: {structured_result.answer}\")\n","    print(f\"üìä Vertrauen: {structured_result.confidence}%\")\n","except Exception as e:\n","    print(f\"‚ùå Pydantic-Parser Fehler: {e}\")"],"metadata":{"id":"97ih8v9QIHp9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5 | Chat-History & Memory\n","---"],"metadata":{"id":"iFllFW8wb080"}},{"cell_type":"markdown","source":["# 5.1 | Simple History üìå"],"metadata":{"id":"Ej4ajzVLIf00"}},{"cell_type":"code","source":["# Moderne Chat-History-Verwaltung\n","chat_history = []\n","\n","def add_to_history(user_msg: str, ai_msg: str):\n","    \"\"\" F√ºgt Nachrichten zur Chat-History hinzu \"\"\"\n","    chat_history.extend([\n","        HumanMessage(content=user_msg),\n","        AIMessage(content=ai_msg)\n","    ])\n","\n","# Chat-Chain mit History\n","history_chain = (\n","    ChatPromptTemplate.from_messages([\n","        (\"system\", \"Du bist ein hilfreicher Assistent.\"),\n","        MessagesPlaceholder(variable_name=\"history\"),\n","        (\"user\", \"{input}\")\n","    ])\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","# Erste Nachricht\n","user_input = \"Mein Name ist Max\"\n","response = history_chain.invoke({\"history\": chat_history, \"input\": user_input})\n","add_to_history(user_input, response)\n","\n","print(f\"ü§ñ Antwort 1: {response}\")\n","print(f\"üí¨ History-L√§nge: {len(chat_history)}\")"],"metadata":{"id":"7L7eEwMrIjcx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5.2 | Simple History (Max=10)"],"metadata":{"id":"sox0Vtx-ItWK"}},{"cell_type":"code","source":["# Chat-History mit maximal 10 Eintr√§gen\n","chat_history = []\n","\n","def add_to_history(user_msg: str, ai_msg: str):\n","    \"\"\" F√ºgt Nachrichten zur Chat-History hinzu und begrenzt auf 10 Eintr√§ge \"\"\"\n","    # Neue Nachrichten hinzuf√ºgen\n","    chat_history.extend([\n","        HumanMessage(content=user_msg),\n","        AIMessage(content=ai_msg)\n","    ])\n","\n","    # Auf maximal 10 Eintr√§ge begrenzen\n","    if len(chat_history) > 10:\n","        chat_history.pop(0)  # √Ñltesten Eintrag entfernen\n","        chat_history.pop(0)  # N√§chsten Eintrag entfernen (damit Paare erhalten bleiben)\n","\n","# Chat-Chain mit History\n","history_chain = (\n","    ChatPromptTemplate.from_messages([\n","        (\"system\", \"Du bist ein hilfreicher Assistent.\"),\n","        MessagesPlaceholder(variable_name=\"history\"),\n","        (\"user\", \"{input}\")\n","    ])\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","# Erste Nachricht\n","user_input = \"Mein Name ist Max\"\n","response = history_chain.invoke({\"history\": chat_history, \"input\": user_input})\n","add_to_history(user_input, response)\n","\n","print(f\"ü§ñ Antwort 1: {response}\")\n","print(f\"üí¨ History-L√§nge: {len(chat_history)}\")\n","\n","# Zweite Nachricht\n","user_input = \"Wie ist mein  Name?\"\n","response = history_chain.invoke({\"history\": chat_history, \"input\": user_input})\n","add_to_history(user_input, response)\n","\n","print(f\"ü§ñ Antwort 1: {response}\")\n","print(f\"üí¨ History-L√§nge: {len(chat_history)}\")"],"metadata":{"id":"EBr8CBP9I2bK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6 | RAG\n","---"],"metadata":{"id":"8TaogQLhb62v"}},{"cell_type":"markdown","source":["# 6.1 | Load Documents"],"metadata":{"id":"ymeJjPk-OMQS"}},{"cell_type":"code","source":["# Loader-Konfiguration\n","loader_mapping = {\n","    \"*.md\": UnstructuredMarkdownLoader,\n","    \"*.docx\": UnstructuredWordDocumentLoader,\n","    \"*.pdf\": PyPDFLoader,\n","    \"*.txt\": UnstructuredFileLoader,  # Loader f√ºr .txt Dateien\n","}\n","\n","# Funktion zum Laden der Dokumente\n","def load_documents_from_directory(directory_path):\n","    \"\"\"L√§dt Dokumente aus dem angegebenen Verzeichnis basierend auf den unterst√ºtzten Dateitypen.\"\"\"\n","    documents = []\n","    for file_pattern, loader_cls in loader_mapping.items():\n","        loader = DirectoryLoader(directory_path, glob=file_pattern, loader_cls=loader_cls)\n","        documents.extend(loader.load())\n","    return documents\n","\n","# Dokumente laden\n","directory_path = \"/content/files\"\n","\n","documents = load_documents_from_directory(directory_path)"],"metadata":{"id":"HuDNq1kYPAE9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6.2 | Chunking"],"metadata":{"id":"EqHStIrLOeX4"}},{"cell_type":"code","source":["# Text-Splitter konfigurieren und Dokumente aufteilen\n","chunk_size = 900\n","chunk_overlap = 300\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_overlap, chunk_overlap=chunk_overlap)\n","docs = text_splitter.split_documents(documents)"],"metadata":{"id":"i1R5aO6cPBcC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6.3 | Embedding"],"metadata":{"id":"oNfzqKrDOhaz"}},{"cell_type":"code","source":["# Embeddingsmodell festlegen\n","embedding_model = \"text-embedding-3-small\"\n","embeddings = OpenAIEmbeddings(model=embedding_model)"],"metadata":{"id":"aMQiuE53PHNh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6.4 | Vectorstore"],"metadata":{"id":"gqhIYi29Okn_"}},{"cell_type":"code","source":["# Vektordatenbank erstellen und speichern\n","persistent_directory = \"/content/chroma_db\"\n","vectorstore = Chroma.from_documents(docs, embeddings, persist_directory=persistent_directory)"],"metadata":{"id":"Uwhx1HJWPLZy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6.5 | Retriever"],"metadata":{"id":"uFpRtFTmOnhv"}},{"cell_type":"code","source":["# Festlegen LLM und Retriever\n","model_name = 'gpt-4o-mini'\n","temperature = 0\n","llm = ChatOpenAI(model=model_name, temperature=temperature)\n","\n","retriever = vectorstore.as_retriever()"],"metadata":{"id":"HI_k6vY3PQA-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6.6 | Inference"],"metadata":{"id":"VB7mOeCtOw8x"}},{"cell_type":"code","source":["rag_prompt = hub.pull(\"rlm/rag-prompt\")"],"metadata":{"id":"fFrz2tKzPvSD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.    \n","Question: {question}      \n","Context: {context}         \n","Answer:"],"metadata":{"id":"zBWHYOg-QTdS"}},{"cell_type":"code","source":["# Aufbereitung Ergebnis Retriever\n","def format_documents(documents):\n","    return \"\\n\\n\".join(doc.page_content for doc in documents)"],"metadata":{"id":"MjxuS6K8P5ho"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Chat-Verlauf initialisieren\n","chat_history = []\n","\n","chain = (\n","    {\"context\": retriever | format_documents, \"question\": RunnablePassthrough()}\n","    | rag_prompt\n","    | llm\n","    | StrOutputParser()\n",")"],"metadata":{"id":"OmVfIDeePbzT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aufruf\n","input = \"Was macht Tariq Hassan?\"\n","response = chain.invoke(input)"],"metadata":{"id":"iyEdX5mFQD32"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6.7 | SQL RAG"],"metadata":{"id":"h_CPWZO5Swkl"}},{"cell_type":"code","source":["# SQL-Datenbank initialisieren\n","DB_PATH = \"/content/northwind.db\"\n","DB_URI = f\"sqlite:///{DB_PATH}\"\n","\n","db = SQLDatabase.from_uri(DB_URI)"],"metadata":{"id":"7i71YL5THyox"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Erweiterten Prompt f√ºr SQL-Abfragen erstellen\n","prompt_template = \"\"\"\n","Du bist ein SQL-Experte. Deine Aufgabe ist es, Benutzeranfragen in SQL-Abfragen zu √ºbersetzen.\n","Verwende die SQLite-Syntax und nur die Tabellen und Spalten aus dem bereitgestellten Schema.\n","Schreibe NUR die SQL-Abfrage ohne Pr√§fixe oder Kommentare.\n","Gebe neben den Id auch den Namen von Produkten, Kunden, etc. mit aus.\n","Gebe maximal 10 Zeilen einer Liste aus.\n","\n","Wichtig: Bei Ja/Nein-Fragen oder Fragen, die eine Analyse erfordern (z.B. \"Sind alle Artikel auf Lager?\"),\n","erstelle eine SQL-Abfrage, die ALLE relevanten Daten zur√ºckgibt, damit eine fundierte Antwort gegeben werden kann.\n","F√ºr komplexe Fragen mit Bedingungen wie \"vom 1998-05-06\" oder einem bestimmten Kundennamen,\n","stelle sicher, dass diese Bedingungen in der WHERE-Klausel korrekt ber√ºcksichtigt werden.\n","Achte darauf, ob bei der Frage nach einer Id oder dem Namen von Produkten, Kunden, Unternehmen, etc. gefragt wird.\n","\n","Datenbank-Schema:\n","{schema}\n","\n","Benutzeranfrage: {query}\n","\n","SQL-Abfrage:\n","\"\"\"\n","\n","# Template f√ºr die Ergebnisinterpretation\n","analysis_template = \"\"\"\n","Du bist ein Business-Analyst, der SQL-Abfrageergebnisse interpretiert und verst√§ndliche Antworten gibt.\n","Beantworte die Benutzeranfrage basierend auf den SQL-Ergebnissen.\n","\n","Bei Ja/Nein-Fragen gib eine klare Antwort und erkl√§re die Gr√ºnde.\n","Bei Fragen nach Empfehlungen oder notwendigen Anpassungen, analysiere die Daten und gib konkrete Vorschl√§ge.\n","\n","Benutzeranfrage: {query}\n","SQL-Abfrage: {sql_query}\n","Abfrageergebnisse:\n","{results}\n","\n","Deine Analyse und Antwort:\n","\"\"\""],"metadata":{"id":"QN7uJUGyTDi-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SQL erstellen\n","def get_schema(_):\n","    return db.get_table_info()\n","\n","sql_generator = (\n","    RunnablePassthrough.assign(schema=get_schema)\n","    | PromptTemplate.from_template(prompt_template)\n","    | llm\n","    | StrOutputParser()\n",")"],"metadata":{"id":"BhYJbvlVTq_I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Datenbank abfragen\n","def execute_query(sql_query: str) -> str:\n","    \"\"\"F√ºhrt eine SQL-Abfrage aus und formatiert die Ergebnisse als String.\"\"\"\n","    try:\n","        # Bereinige die Abfrage von eventuellen Formatierungen\n","        cleaned_query = sql_query.strip()\n","\n","        conn = sqlite3.connect(DB_PATH)\n","        cursor = conn.cursor()\n","        cursor.execute(cleaned_query)\n","\n","        # Spalten√ºberschriften abrufen\n","        column_names = [description[0] for description in cursor.description]\n","\n","        # Ergebnisse abrufen\n","        results = cursor.fetchall()\n","\n","        # Ergebnisse formatieren\n","        output = \"| \" + \" | \".join(column_names) + \" |\\n\"\n","        output += \"| \" + \" | \".join([\"---\" for _ in column_names]) + \" |\\n\"\n","\n","        for row in results:\n","            output += \"| \" + \" | \".join([str(cell) for cell in row]) + \" |\\n\"\n","\n","        conn.close()\n","\n","        # Keine Ergebnisse gefunden\n","        if len(results) == 0:\n","            return \"Keine Ergebnisse gefunden.\"\n","\n","        return output\n","\n","    except Exception as e:\n","        return f\"Fehler bei der Ausf√ºhrung der Abfrage: {str(e)}\\nAbfrage: {cleaned_query}\""],"metadata":{"id":"sEBK76iATu05"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Analyse des Ergebnisses der Datenbank-Abfrage durch ein LLM\n","def analyze_results(query, sql_query, results):\n","    \"\"\"Analysiert die Ergebnisse und gibt eine nat√ºrlichsprachliche Antwort zur√ºck.\"\"\"\n","    analysis_prompt = PromptTemplate.from_template(analysis_template)\n","    analysis_chain = analysis_prompt | llm | StrOutputParser()\n","\n","    return analysis_chain.invoke({\n","        \"query\": query,\n","        \"sql_query\": sql_query,\n","        \"results\": results\n","    })"],"metadata":{"id":"bXSgvVGaT2DG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Funktion f√ºr Aufruf der Kette aus Gradio\n","def chatbot_response(mesSQL, history):\n","    \"\"\"Verarbeitet Benutzeranfragen, erstellt SQL und gibt formatierte Ergebnisse mit Analyse zur√ºck.\"\"\"\n","    try:\n","        # SQL-Abfrage mit LLM generieren\n","        sql_query = sql_generator.invoke({\"query\": mesSQL})\n","\n","        # Bereinige eventuelles Markdown-Markup\n","        sql_query = re.sub(r'```sql\\s*(.*?)\\s*```', r'\\1', sql_query, flags=re.DOTALL)\n","        sql_query = sql_query.replace(\"```\", \"\").strip()\n","\n","        # Debug-Ausgabe\n","        print(f\"Generierte SQL: {sql_query}\")\n","\n","        # F√ºhre die Abfrage aus\n","        results = execute_query(sql_query)\n","\n","        # Analysiere die Ergebnisse f√ºr komplexe Fragen\n","        analysis = analyze_results(mesSQL, sql_query, results)\n","\n","        # Antwort formatieren\n","        response = f\"### Deine Anfrage\\n{mesSQL}\\n\\n### SQL-Abfrage\\n```sql\\n{sql_query}\\n```\\n\\n### Ergebnisse\\n{results}\\n\\n### Analyse\\n{analysis}\"\n","\n","        return response\n","\n","    except Exception as e:\n","        return f\"Ein Fehler ist aufgetreten: {str(e)}\""],"metadata":{"id":"7fsjmBe_T90o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 7 | Image, Audio, Video\n","---"],"metadata":{"id":"i-YaukXicEYQ"}},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/under_construction_dall_e_klein.png)"],"metadata":{"id":"paQPeXd-usUX"}},{"cell_type":"markdown","source":["# 8 | Agent\n","---"],"metadata":{"id":"qoaSA0q_b-yJ"}},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/under_construction_dall_e_klein.png)"],"metadata":{"id":"VHI2iJ3DulT4"}},{"cell_type":"markdown","source":["# 9 | Fine-Tuning & Local Models\n","---"],"metadata":{"id":"z6a4og9mcJXd"}},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/under_construction_dall_e_klein.png)"],"metadata":{"id":"kx8kz41Su6W9"}}]}
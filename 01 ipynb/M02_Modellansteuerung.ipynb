{"cells":[{"cell_type":"markdown","source":["<p><font size=\"7\" color='grey'> <b>\n","Anwendung Generativer KI\n","</b></font> </br></p>"],"metadata":{"id":"8x1hrQqQ27a3"}},{"cell_type":"markdown","source":["<p><font size=\"6\" color='grey'> <b>\n","Grundlagen Modellansteuerung\n","</b></font> </br></p>\n","\n","---"],"metadata":{"id":"R5CfUEMJdvFQ"}},{"cell_type":"markdown","source":["<a target=\"_blank\" href=\"https://colab.research.google.com/github/ralf-42/GenAI/blob/main/01%20ipynb/M00_Prolog.ipynb\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>"],"metadata":{"id":"j6hrMKRRG0pu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfdhPIzcEYRG","cellView":"form","collapsed":true},"outputs":[],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  Colab-Umfeld</font> </br></p>\n","# Installierte Python Version\n","import sys\n","print(f\"Python Version: \",sys.version)\n","# Installierte LangChain Bibliotheken\n","print()\n","print(\"Installierte LangChain Bibliotheken:\")\n","!pip list | grep '^langchain'\n","# Unterdrückt die \"DeprecationWarning\" von LangChain für die Memory-Funktionden\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")"]},{"cell_type":"code","source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  SetUp API-Keys (setup_api_keys)</font> </br></p>\n","def setup_api_keys():\n","    \"\"\"Konfiguriert alle benötigten API-Keys aus Google Colab userdata\"\"\"\n","    from google.colab import userdata\n","    import os\n","    from os import environ\n","\n","    # Dictionary der benötigten API-Keys\n","    keys = {\n","        'OPENAI_API_KEY': 'OPENAI_API_KEY',\n","        'HF_TOKEN': 'HF_TOKEN',\n","        # Weitere Keys bei Bedarf\n","    }\n","\n","    # Keys in Umgebungsvariablen setzen\n","    for env_var, key_name in keys.items():\n","        environ[env_var] = userdata.get(key_name)\n","\n","    return {k: environ[k] for k in keys.keys()}\n","\n","# Verwendung\n","all_keys = setup_api_keys()\n","# Bei Bedarf einzelne Keys direkt zugreifen\n","# WEATHER_API_KEY = all_keys['WEATHER_API_KEY']"],"metadata":{"cellView":"form","id":"WD3Wwr6sESX8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **1 <font color='orange'>|</font> Überblick**\n","---\n"],"metadata":{"id":"PHDCKRbtgVXy"}},{"cell_type":"markdown","source":["Die rasante Entwicklung im Bereich der künstlichen Intelligenz (KI) hat zu beeindruckenden Fortschritten bei großen Sprachmodellen (LLMs) geführt. Diese Modelle können menschenähnlichen Text generieren, Sprachen übersetzen und komplexe Fragen beantworten. Doch wie genau steuert man diese Modelle, um die gewünschten Ergebnisse zu erzielen? In diesem Modul werden wir drei gängige Methoden der Modellansteuerung untersuchen: Prompting, Retrieval-Augmented Generation (RAG) und Fine-Tuning.\n","\n"],"metadata":{"id":"lSg1eGf95Vz1"}},{"cell_type":"markdown","source":["# **2 <font color='orange'>|</font> Prompting**\n","---"],"metadata":{"id":"owVDrCRD5hoI"}},{"cell_type":"markdown","source":["Prompting ist die einfachste und direkteste Methode zur Steuerung von LLMs. Dabei wird dem Modell eine Textaufforderung, der sogenannte \"Prompt\", gegeben, der die gewünschte Ausgabe beschreibt. Die Qualität der Ausgabe hängt stark von der Qualität des Prompts ab. Ein gut formulierter Prompt sollte klar, prägnant und spezifisch sein.\n"],"metadata":{"id":"o7lp262M4u0Z"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Prompt-Elemente\n","</font></p>"],"metadata":{"id":"DlkRGwcR3rBp"}},{"cell_type":"markdown","source":["Eingabeaufforderungen bestehen in der Regel aus vier wesentlichen Elementen, die je nach Aufgabe variieren können. Die Hauptbestandteile sind:\n","\n","* **Anweisungen** – Definieren die Aufgabe des Modells, entweder als Aufgabenbeschreibung oder als konkrete Handlungsanweisung.  \n","* **Kontext** – Liefert ergänzende Informationen, um das Modell bei der Bearbeitung zu unterstützen.  \n","* **Eingabedaten** – Enthalten die spezifischen Informationen, zu denen eine Antwort generiert werden soll.  \n","* **Ausgabeindikator** – Signalisiert den Übergang zur erwarteten Modellantwort.  \n","\n","Weitere Beispiele zur Anwendung dieser Struktur folgen später."],"metadata":{"id":"nUJCfQ_-71Ti"}},{"cell_type":"markdown","source":["Nachfolgend ein Beispiel für eine Eingabeaufforderung zur Erstellung einer Produktzusammenfassung.\n","\n","```\n","Fassen Sie die folgende Produktbewertung in einem Satz zusammen:\n","Produkt: Intelligente digitale Personenwaage für Körpergewicht, Fett, BMI, Muskelmassezusammensetzung\n","Produktbewertung: Diese Waage ist fantastisch! Der Einrichtungsvorgang war schnell und einfach und dauerte nur wenige Minuten. Ich konnte die App kostenlos auf mein etwas veraltetes iPhone 8 herunterladen. Die Funktion, bei der Ihr Gewicht sofort mit der App synchronisiert wird, macht die Nachverfolgung mühelos. Die Funktionen zur Essensplanung und Kalorienzählung der App sind unglaublich benutzerfreundlich. Ich bin absolut begeistert! Außerdem hat die Waage ein elegantes, modernes Erscheinungsbild, das wirklich attraktiv ist.\n","Zusammenfassung: ...\n","```\n","\n","Diese Eingabeaufforderung erzeugt möglicherweise die folgende Ausgabe:\n","\n","```\n","Die intelligente digitale Personenwaage wird für ihre schnelle Einrichtung, die einfache Synchronisierung mit ihrer App auf älteren Telefonen zur mühelosen Nachverfolgung, die benutzerfreundlichen Funktionen zur Essensplanung und Kalorienzählung sowie ihr schlankes, modernes Design hoch gelobt.\n","```"],"metadata":{"id":"rj5-ap_p4Htg"}},{"cell_type":"markdown","source":["Nachfolgend ein Beispiel für eine Eingabeaufforderung für die Zusammenfassung einer Produktbewertung.\n","\n","```\n","Aufgabe: Erstellen Sie eine prägnante Zusammenfassung der folgenden Produktbewertung in einem Satz.\n","Produkt: Smarte digitale Personenwaage zur Messung von Körpergewicht, Körperfett, BMI und Muskelmasse.\n","Bewertung: Diese Waage ist hervorragend! Die Einrichtung war unkompliziert und innerhalb weniger Minuten erledigt. Die zugehörige App ließ sich problemlos auf mein älteres iPhone 8 installieren. Besonders praktisch ist die automatische Synchronisation des Gewichts mit der App, wodurch die Nachverfolgung erleichtert wird. Zudem bietet die App intuitive Funktionen zur Essensplanung und Kalorienkontrolle. Ich bin absolut begeistert! Das moderne, stilvolle Design der Waage ist ebenfalls ein Highlight.\n","Zusammenfassung: ...\n","```"],"metadata":{"id":"x9aWyG4I5Ek8"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Code Beispiel\n","</font></p>"],"metadata":{"id":"yD8yi1td35er"}},{"cell_type":"code","source":["!uv pip install --system -q langchain_openai"],"metadata":{"id":"dXE534fDMNHP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TMF-rtxgRAea","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1741342371866,"user_tz":-60,"elapsed":3110,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"82015a12-20cb-4918-bf39-af041c1377cc"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## ✨ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Die Reaktion von Schwefelsäure (H₂SO₄) mit Natriumchlorid (NaCl) ergibt Natriumsulfat (Na₂SO₄) und Chlorwasserstoff (HCl). \n\nDaher sind:\n\n- **chemical1**: Na₂SO₄\n- **chemical2**: HCl\n\nDie Reaktionsgleichung lautet:\n\n$$\n\\text{H}_2\\text{SO}_4 + 2 \\text{NaCl} \\rightarrow \\text{Na}_2\\text{SO}_4 + 2 \\text{HCl}\n$$"},"metadata":{}}],"source":["from langchain_core.messages import HumanMessage, SystemMessage\n","from langchain_openai import ChatOpenAI\n","from IPython.display import Markdown, display, Latex\n","\n","messages = [\n","    SystemMessage(\n","        content=\"Du bist ein hilfreicher KI-Assistent. Erstelle die Antwort im Markdown-Format. Setze die Formel zwischen zwei doppelte Dollarzeichen.\"\n","    ),\n","    HumanMessage(\n","        content=\\\n","\"\"\"\n","Human:\n","Schwefelsäure reagiert mit Natriumchlorid und ergibt <chemical1>_____</chemical1> und <chemical2>_____</chemical2>:\n","Assistent: chemical1 und chemical2 sind:\n","\"\"\"\n","    ),\n","]\n","\n","MODEL = 'gpt-4o-mini'\n","TEMPERATURE = 0.0\n","\n","# Initialisieren Sie das OpenAI LLM mit Ihrem API-Schlüssel\n","llm = ChatOpenAI(\n","  model=MODEL,\n","  temperature= TEMPERATURE,\n",")\n","\n","output = llm.invoke(messages)\n","display(Markdown(\"## ✨ Model response:\"))\n","display(Markdown(output.content))"]},{"cell_type":"markdown","source":["# **3 <font color='orange'>|</font> Retrieval-Augmented Generation**\n","---"],"metadata":{"id":"L0grUpA56y3K"}},{"cell_type":"markdown","source":["RAG (Retrieval-Augmented Generation) ist eine hybride Methode, die die Stärken von LLMs mit denen von Information-Retrieval-Systemen kombiniert. Dabei greift das LLM auf externe Wissensdatenbanken zu, um seine Antworten zu verbessern und die Probleme von Halluzinationen und veralteten Informationen zu mindern.\n","\n","**Funktionsweise:**\n","\n","+ Der Benutzer gibt eine Anfrage in das RAG-System ein.\n","+ Das System sucht mithilfe von Embedding Language Models und Vektor Datenbanken in externen Wissensdatenbanken nach relevanten Informationen.\n","+ Die relevanten Informationen werden dem LLM als Kontext bereitgestellt.\n","+ Das LLM generiert eine Antwort basierend auf der Anfrage und dem Kontext.\n","\n","\n","**Einsatzszenarien:**\n","\n","RAG eignet sich besonders für Aufgaben, die aktuelle oder domänenspezifische Informationen erfordern, wie z. B.:\n","\n","+ Kundensupport mit Zugriff auf Produktdatenbanken\n","+ Medizinische Diagnose mit Zugriff auf aktuelle Forschungsergebnisse\n","+ Finanzplanung mit Zugriff auf Marktdaten\n","+ Beantwortung von Fragen zu Unternehmensinformationen\n","\n"],"metadata":{"id":"EbmfDWKg4xmC"}},{"cell_type":"markdown","source":["# **4 <font color='orange'>|</font> Fine-Tuning**\n","---"],"metadata":{"id":"iQpWHr88612f"}},{"cell_type":"markdown","source":["Fine-Tuning ist eine Methode, bei der ein bereits trainiertes LLM auf einer kleineren, aufgabenspezifischen Datenmenge weiter trainiert wird. Dadurch kann das Modell an spezifische Anforderungen angepasst und seine Leistung für diese Aufgaben verbessert werden.\n","\n","**Funktionsweise:**\n","\n","+ Ein vortrainiertes LLM wird ausgewählt.\n","+ Eine aufgabenspezifische Datenmenge wird vorbereitet.\n","+ Das LLM wird auf dieser Datenmenge weiter trainiert.\n","+ Die Parameter des LLM werden angepasst, um die Leistung für die spezifische Aufgabe zu optimieren.\n","\n","**Methoden des Fine-Tunings:**\n","\n","Es gibt verschiedene Methoden, um ein LLM zu fine-tunen:\n","\n","+ Instruction Fine-Tuning: Das Modell wird mit Beispielen trainiert, die zeigen, wie es auf bestimmte Anfragen reagieren soll. Zum Beispiel könnte man dem Modell Anweisungen wie \"Fasse diesen Text zusammen\" geben, gefolgt von dem eigentlichen Text.\n","+ Weitere Methoden: Es gibt noch weitere Fine-Tuning-Methoden, die je nach Anwendungsfall und Zielsetzung eingesetzt werden können.\n","\n","\n","**Einsatzszenarien:**\n","\n","Fine-Tuning eignet sich besonders für Aufgaben, die eine hohe Genauigkeit und Domänenspezialisierung erfordern, wie z. B.:\n","\n","+ Sentimentanalyse\n","+ Textklassifizierung\n","+ Spam-Erkennung\n","+ Personalisierte Kundeninteraktionen\n","\n"],"metadata":{"id":"WEyWOZtP40s0"}},{"cell_type":"markdown","source":["\n","# **5 <font color='orange'>|</font> Entscheidungskriterien**\n","---"],"metadata":{"id":"EU_aZQuK5kfc"}},{"cell_type":"markdown","source":["Die Wahl der geeigneten Methode hängt von verschiedenen Faktoren ab, darunter:"],"metadata":{"id":"w4I1UPS97apC"}},{"cell_type":"markdown","source":["\n","| Kriterium | Prompting | RAG | Fine-Tuning |\n","|-----------|-----------|-----|-------------|\n","| **Komplexität** | Niedrig<br><br>Die Implementierung ist <br>einfach und erfordert keine <br>Änderungen am Modell. | Mittel<br><br>RAG erfordert die Integration<br>von Retrieval-Mechanismen<br>und einer Wissensdatenbank. | Hoch<br><br>Fine-Tuning beinhaltet<br>einen komplexeren Prozess<br>des weiteren Trainings. |\n","| **Effizienz** | Hoch<br><br>Prompting ermöglicht eine<br>schnelle und flexible<br>Interaktion mit dem Modell. | Mittel<br><br>RAG benötigt zusätzliche<br>Schritte für den Datenabruf,<br>was die Effizienz mindert. | Niedrig<br><br>Fine-Tuning ist rechen-<br>intensiv und zeitaufwendig. |\n","| **Genauigkeit** | Niedrig<br><br>Die Genauigkeit hängt stark<br>von der Qualität des<br>Prompts ab. | Mittel<br><br>RAG bietet höhere<br>Genauigkeit durch externe<br>Informationen. | Hoch<br><br>Führt zu höherer<br>Genauigkeit bei<br>spezifischen Aufgaben. |\n","| **Flexibilität** | Hoch<br><br>Sehr flexibel und für<br>verschiedene Aufgaben<br>einsetzbar. | Mittel<br><br>Flexibler als Fine-Tuning,<br>aber weniger flexibel als<br>Prompting. | Niedrig<br><br>Ist auf bestimmte Aufgaben<br>spezialisiert und weniger<br>anpassungsfähig. |\n","| **Ressourcen-<br>bedarf** | Niedrig<br><br>Benötigt nur minimale<br>Ressourcen für die<br>Ausführung. | Mittel<br><br>Ressourcen für Datenbank-<br>verwaltung und Retrieval<br>erforderlich. | Hoch<br><br>Erfordert erhebliche<br>Rechenleistung und Zeit<br>für Training. |\n","| **Datenbedarf** | Niedrig<br><br>Keine zusätzlichen<br>Trainingsdaten<br>erforderlich. | Mittel<br><br>Benötigt Wissensdatenbank<br>mit relevanten<br>Informationen. | Hoch<br><br>Benötigt große Mengen an<br>spezifischen<br>Trainingsdaten. |\n","| **Beschreibung** | Einfache Methode mit<br>Textanweisungen für das<br>Modell. | Kombination von LLMs mit<br>Information-Retrieval-<br>Systemen. | Weiteres Training des<br>Modells auf spezifischen<br>Datensätzen. |\n","\n"],"metadata":{"id":"rSAuP0cW54O1"}},{"cell_type":"markdown","source":["\n","\n","Bei der Wahl zwischen den verschiedenen Modellansteuerungstechniken gibt es immer Trade-offs, die berücksichtigt werden müssen:\n","\n","- **Kosteneffizienz**: Fine-Tuning ist kostspieliger, da es nicht nur erhebliche Rechenressourcen erfordert, sondern auch Zeit, um ein Modell zu trainieren. RAG bietet einen Mittelweg, da es externe Daten nutzt, ohne dass ein vollständiges Modell neu trainiert werden muss. Prompting ist hingegen am kostengünstigsten, da es keine zusätzlichen Trainingsphasen erfordert.\n","\n","- **Flexibilität**: Fine-Tuning bietet die höchste Flexibilität, da es das Modell vollständig an die speziellen Anforderungen anpassen kann. RAG bietet ebenfalls Flexibilität, allerdings hängt die Qualität der Ergebnisse stark von der Qualität der externen Daten ab. Prompting ist flexibler in der Formulierung der Anfragen, hat jedoch in komplexen Szenarien oft Einschränkungen in der Präzision und Tiefe der Antworten.\n","\n","- **Generalität vs. Spezifität**: Modelle, die nur mit Prompting betrieben werden, sind in der Regel allgemeiner und weniger präzise in der Bearbeitung spezifischer Anfragen. Fine-Tuning ermöglicht die Anpassung an spezifische Anforderungen, reduziert jedoch die Generalisierbarkeit auf andere Domänen. RAG bietet eine Balance, da es externe Daten zur Verbesserung der Antwortqualität verwendet, ohne die Generalität des zugrundeliegenden Modells zu stark zu beeinflussen.\n","\n"],"metadata":{"id":"1vLsxmbW6EBG"}},{"cell_type":"markdown","source":["\n","[Zoom](https://raw.githubusercontent.com/ralf-42/Image/main/GenAI/rag.jpg)"],"metadata":{"id":"csSxOSAzTXVt"}}],"metadata":{"colab":{"provenance":[{"file_id":"/v2/external/notebooks/empty.ipynb","timestamp":1736181733357}],"collapsed_sections":["PHDCKRbtgVXy","owVDrCRD5hoI","L0grUpA56y3K","iQpWHr88612f","EU_aZQuK5kfc"]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
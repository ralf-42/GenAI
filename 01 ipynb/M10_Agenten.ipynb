{"cells":[{"cell_type":"markdown","id":"0684551f","metadata":{"id":"0684551f"},"source":["![GenAI Banner](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)\n"]},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b>\n","Agenten\n","</b></font> </br></p>\n","\n","\n","---"],"metadata":{"id":"ogH-Fzpmbueo"},"id":"ogH-Fzpmbueo"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfdhPIzcEYRG","cellView":"form","collapsed":true},"outputs":[],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  Colab-Umfeld</font> </br></p>\n","# Installierte Python Version\n","import sys\n","print(f\"Python Version: \",sys.version)\n","\n","# Installierte LangChain Bibliotheken\n","print()\n","print(\"Installierte LangChain Bibliotheken:\")\n","!pip list | grep '^langchain'\n","# Unterdr√ºckt die \"DeprecationWarning\" von LangChain f√ºr die Memory-Funktionden\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")"],"id":"dfdhPIzcEYRG"},{"cell_type":"code","source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  SetUp API-Keys (setup_api_keys)</font> </br></p>\n","\n","def setup_api_keys():\n","    \"\"\"Konfiguriert alle ben√∂tigten API-Keys aus Google Colab userdata\"\"\"\n","    from google.colab import userdata\n","    import os\n","    from os import environ\n","\n","    # Dictionary der ben√∂tigten API-Keys\n","    keys = {\n","        'OPENAI_API_KEY': 'OPENAI_API_KEY',\n","        'SERPAPI_API_KEY': 'SERPAPI_API_KEY',\n","        'WEATHER_API_KEY': 'WEATHER_API_KEY',\n","        'HF_TOKEN': 'HF_TOKEN',\n","        'GOOGLE_API_KEY': 'GOOGLE_API_KEY',\n","        # Weitere Keys bei Bedarf\n","    }\n","\n","    # Keys in Umgebungsvariablen setzen\n","    for env_var, key_name in keys.items():\n","        environ[env_var] = userdata.get(key_name)\n","\n","    return {k: environ[k] for k in keys.keys()}\n","\n","# Verwendung\n","all_keys = setup_api_keys()\n","# Bei Bedarf einzelne Keys direkt zugreifen\n","WEATHER_API_KEY = all_keys['WEATHER_API_KEY']"],"metadata":{"cellView":"form","id":"WD3Wwr6sESX8"},"execution_count":null,"outputs":[],"id":"WD3Wwr6sESX8"},{"cell_type":"markdown","id":"bf088ba7","metadata":{"id":"bf088ba7"},"source":["# 1 | Grundlagen von KI-Agenten"]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Was sind KI-Agenten?\n","</font></p>\n","\n","\n","**Definition und Abgrenzung zu anderen KI-Systemen:**    \n","KI-Agenten sind autonome Systeme, die ihre Umgebung wahrnehmen, Entscheidungen treffen und Aktionen ausf√ºhren k√∂nnen, um bestimmte Ziele zu erreichen. Im Gegensatz zu einfachen KI-Modellen, die nur auf Eingabe reagieren und Ausgabe produzieren, k√∂nnen Agenten proaktiv handeln und aus ihren Erfahrungen lernen.\n","\n","**Autonomie, Reaktivit√§t und zielgerichtetes Handeln:**    \n","Ein KI-Agent kann selbstst√§ndig (autonom) Entscheidungen treffen, auf Ver√§nderungen in seiner Umgebung reagieren (Reaktivit√§t) und hat ein oder mehrere Ziele, die sein Handeln leiten. Dabei verfolgt er langfristige Strategien statt nur auf unmittelbare Reize zu reagieren.\n","\n","**Unterschied zwischen einfachen LLM-Anwendungen und Agenten:**    \n","Einfache LLM-Anwendungen (Large Language Models) wie ein Chatbot antworten lediglich auf Anfragen, w√§hrend Agenten dar√ºber hinaus eigene Entscheidungen treffen, Tools nutzen und zielgerichtet handeln k√∂nnen. Ein Agent kann beispielsweise entscheiden, wann er eine Suchanfrage starten sollte, um eine Frage besser zu beantworten.\n","\n"],"metadata":{"id":"D3XA-ajxbwq_"},"id":"D3XA-ajxbwq_"},{"cell_type":"markdown","source":["# 2 | Agententypen\n","---\n"],"metadata":{"id":"xNBlsRoIb5T8"},"id":"xNBlsRoIb5T8"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Traditionelle Agententypen\n","</font></p>\n"],"metadata":{"id":"ye_O8MlLb6vv"},"id":"ye_O8MlLb6vv"},{"cell_type":"markdown","source":["Haupttypen von Agenten nach Anthropic:\n","\n","- **Workflows**: Systeme mit vordefinierten, festen Abl√§ufen (**Codepfaden**), bei denen LLMs und Tools nach einem **festen Plan** orchestriert werden. Die Entscheidungspfade sind vorab festgelegt und √§ndern sich nicht dynamisch w√§hrend der Ausf√ºhrung[.\n","- **Agenten**: Systeme, in denen LLMs ihre Prozesse und Tool-Nutzung **dynamisch** steuern, **flexibel** auf Situationen reagieren und eigenst√§ndig Aufgaben erf√ºllen k√∂nnen. Sie k√∂nnen **Entscheidungen treffen, planen, lernen** und sich an neue Situationen anpassen."],"metadata":{"id":"fDa7wNwhvvxY"},"id":"fDa7wNwhvvxY"},{"cell_type":"markdown","source":["| Agententyp                       | Zuordnung Haupttyp (Anthropic) | Beschreibung                                                                            |\n","| :------------------------------- | :----------------------------- | :-------------------------------------------------------------------------------------- |\n","| Einfache Reflexionsagenten       | Workflow                       | Folgen starren Wenn-Dann-Regeln, keine Flexibilit√§t oder dynamische Steuerung.          |\n","| Modellbasierte Reflexionsagenten | Workflow                       | Haben internes Modell, aber Abl√§ufe sind weiterhin vorgegeben und nicht dynamisch.      |\n","| Zielbasierte Agenten             | √úbergang Workflow ‚Üí Agent      | Treffen Entscheidungen basierend auf Zielen, k√∂nnen aber noch festen Abl√§ufen folgen.   |\n","| Nutzungsbasierte Agenten         | Agent                          | Bewerten Optionen, treffen differenzierte Entscheidungen, zeigen bereits adaptive Z√ºge. |\n","| Lernende Agenten                 | Agent                          | Lernen aus Erfahrungen, passen Verhalten an, sind maximal flexibel und autonom.         |"],"metadata":{"id":"ZicZ97AjvmY2"},"id":"ZicZ97AjvmY2"},{"cell_type":"markdown","source":["- **Workflows** eignen sich f√ºr einfache bis m√§√üig komplexe Aufgaben, bei denen der Ablauf vorhersehbar und festgelegt ist. Das entspricht den einfachen und modellbasierten Reflexionsagenten, die keine oder nur sehr eingeschr√§nkte Anpassungsf√§higkeit besitzen[^3][^5].\n","- **Agenten** im Sinne von Anthropic sind Systeme, die flexibel, adaptiv und oft lernf√§hig sind. Sie treffen eigenst√§ndig Entscheidungen, planen und k√∂nnen ihr Verhalten anpassen. Das entspricht den nutzungsbasierten und lernenden Agenten aus der KI-Literatur[^2][^5].\n","- **Zielbasierte Agenten** liegen an der Schnittstelle: Sie k√∂nnen noch als Workflows umgesetzt werden, wenn die Zielerreichung einem festen Plan folgt. Sobald sie aber flexibel zwischen Zielen abw√§gen oder den Ablauf dynamisch anpassen, z√§hlen sie zu den Agenten.\n","\n","\n","Diese Einteilung entspricht den klassischen Definitionen nach Russell & Norvig sowie Wooldridge & Jennings und wird in der Fachliteratur als Standard angesehen.¬†Sie beschreibt die zunehmende Komplexit√§t und Autonomie der Agenten und ist besonders geeignet, wenn die interne Funktionsweise, die Entscheidungsfindung und die F√§higkeit zur Anpassung im Vordergrund stehen.\n"],"metadata":{"id":"8rCvGZctv_mE"},"id":"8rCvGZctv_mE"},{"cell_type":"markdown","id":"8968dbad","metadata":{"id":"8968dbad"},"source":["<p><font color='black' size=\"5\">\n","Aufgabenorientierte Agententypen\n","</font></p>\n"]},{"cell_type":"markdown","source":["\n","In der Praxis ist es oft sinnvoller, Agenten nach ihrem Anwendungszweck und ihrer funktionalen Ausrichtung zu klassifizieren:\n","\n","| Agentenklasse | Prim√§rer Zweck | Typische Anwendungen | Schl√ºsselmerkmale |\n","|---------------|----------------|----------------------|-------------------|\n","| **Problem-Solving Agent** | Komplexe Probleme l√∂sen | Recherchen, analytische Aufgaben, mehrstufige Entscheidungen | - Kombiniert Reasoning mit Tool-Nutzung<br>- Explizites Denken und strukturierte Schritte<br>- Flexibilit√§t bei verschiedenen Problemtypen |\n","| **Verbesserungs-Agent** | Inhaltsqualit√§t optimieren | Content-Erstellung, Code-Review, Textkorrekturen | - Selbstbewertung und -kritik<br>- Iterative Verbesserung<br>- Vergleich von Versionen |\n","| **Service-Agent** | Strukturierte Dienste ausf√ºhren | API-Integrationen, Datenabfragen, automatisierte Aktionen | - Standardisierte Schnittstellen<br>- Klare Ein- und Ausgaben<br>- Zuverl√§ssige Ausf√ºhrung definierter Funktionen |\n","| **Spezialisierter Agent** | Dom√§nenspezifische Aufgaben | Kundenservice, medizinische Beratung, rechtliche Informationen | - Tiefes Dom√§nenwissen<br>- Angepasste Tools f√ºr spezifische Zwecke<br>- Optimiert f√ºr einen Anwendungsbereich |"],"metadata":{"id":"RfuqNgdScFy3"},"id":"RfuqNgdScFy3"},{"cell_type":"markdown","source":["\n","\n","**Wichtiger Hinweis zur Implementierung:**\n","\n","Die in diesem Kurs gezeigten Implementierungen verwenden eine flexible Toolset-Architektur, die f√ºr verschiedene Agentenklassen genutzt werden kann. Beachten Sie, dass der Unterschied zwischen den Agentenklassen weniger in der technischen Implementierung liegt als in:\n","\n","1. Der Aufgabenstellung und dem Anwendungsfall\n","2. Dem Verhaltensmuster des Agenten\n","3. Der Art, wie verschiedene Werkzeuge kombiniert werden\n","\n","In der Praxis kombinieren moderne Agenten oft Elemente aus verschiedenen konzeptionellen Architekturen, um optimale Ergebnisse zu erzielen.\n","\n"],"metadata":{"id":"EH3iRN38wJbA"},"id":"EH3iRN38wJbA"},{"cell_type":"markdown","source":["# 3 | Aufgabenorientierte Agenten\n","---\n"],"metadata":{"id":"40c0n4k_cLvP"},"id":"40c0n4k_cLvP"},{"cell_type":"markdown","source":["\n","\n","**Setup der Entwicklungsumgebung:** Installation und Konfiguration der notwendigen Bibliotheken, insbesondere LangChain und die Anbindung an ein LLM wie OpenAI.\n","\n","**ToolSet definieren:** Definition der durch die Agents benutzten Tools.\n","\n","**Implementierung eines einfachen Agenten:** Ein Beispiel f√ºr einen einfachen Agenten, der Fragen beantwortet und Berechnungen durchf√ºhrt.\n","\n"],"metadata":{"id":"Nm15DiHAcNOI"},"id":"Nm15DiHAcNOI"},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","3.1 Zentrale Tool-Bibliothek\n","</font></p>"],"metadata":{"id":"QqTZZMB10aot"},"id":"QqTZZMB10aot"},{"cell_type":"code","source":["# Install\n","!uv pip install --system --prerelease allow -q langchain-openai langchain-community google-search-results wikipedia"],"metadata":{"id":"4edTQIC1c6eT"},"id":"4edTQIC1c6eT","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hier eine kurze √úbersicht der nachfolgend definierten Tools:"],"metadata":{"id":"X2FTwy3TsBgQ"},"id":"X2FTwy3TsBgQ"},{"cell_type":"markdown","source":["| **Tool-/Funktion-Name**   | **Funktion**                                                                                |\n","| ------------------------- | ------------------------------------------------------------------------------------------- |\n","| `search`                  | Sucht nach Informationen im Internet √ºber SerpAPI.                                          |\n","| `wiki`                    | Holt den ersten Wikipedia-Abschnitt zu einem Thema.                                         |\n","| `calculator`              | F√ºhrt einfache mathematische Berechnungen aus (z. B. 2+2 oder (3\\*4)/2).                    |\n","| `weather`                 | Gibt das aktuelle Wetter f√ºr einen angegebenen Ort zur√ºck (via OpenWeather).                |\n","| `read_file`               | Liest den Inhalt einer Datei aus (Pfadangabe erforderlich).                                 |\n","| `write_file`              | Schreibt Inhalt in eine Datei im Format `pfad//inhalt`.                                     |\n","| `days_between_dates`      | Berechnet die Anzahl der Tage zwischen zwei Datumsangaben im Format `DD.MM.YYYY`.           |\n","| `extract_math_expression` | Extrahiert einen mathematischen Ausdruck aus einem Text (z. B. aus ‚ÄûWas ist 3+5?\" ‚Üí `3+5`). |\n","\n"],"metadata":{"id":"stuD1BECr_Do"},"id":"stuD1BECr_Do"},{"cell_type":"code","source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  Definition der Tools</font> </br></p>\n","# Import\n","import os\n","import re\n","import requests\n","import json\n","from datetime import datetime\n","from langchain_core.tools import Tool\n","from langchain_community.utilities.serpapi import SerpAPIWrapper\n","from langchain_community.utilities.wikipedia import WikipediaAPIWrapper\n","\n","# Tool-Funktionen definieren\n","def get_weather(location):\n","    \"\"\"Gibt das aktuelle Wetter f√ºr einen bestimmten Ort zur√ºck.\"\"\"\n","    url = f\"https://api.openweathermap.org/data/2.5/weather?q={location}&appid={WEATHER_API_KEY}&units=metric\"\n","    response = requests.get(url)\n","    data = response.json()\n","    return f\"Aktuelle Temperatur in {location}: {data['main']['temp']}¬∞C, {data['weather'][0]['description']}\"\n","\n","def read_file(file_path):\n","    \"\"\"Liest den Inhalt einer Datei.\"\"\"\n","    if os.path.exists(file_path):\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            return file.read()\n","    return f\"Fehler: Datei {file_path} nicht gefunden.\"\n","\n","def write_file(args):\n","    \"\"\"Schreibt Inhalt in eine Datei. Format: 'pfad//inhalt'\"\"\"\n","    try:\n","        file_path, content = args.split(\"//\", 1)\n","        with open(file_path, 'w', encoding='utf-8') as file:\n","            file.write(content)\n","        return f\"Erfolgreich in {file_path} geschrieben.\"\n","    except Exception as e:\n","        return f\"Fehler beim Schreiben: {str(e)}\"\n","\n","def days_between_dates(date_string):\n","    \"\"\"Berechnet Tage zwischen zwei Datumsangaben im Format DD.MM.YYYY\"\"\"\n","    try:\n","        date1_str, date2_str = date_string.split(\",\")\n","        date1_str = date1_str.strip()\n","        date2_str = date2_str.strip()\n","        date1 = datetime.strptime(date1_str, \"%d.%m.%Y\").date()\n","        date2 = datetime.strptime(date2_str, \"%d.%m.%Y\").date()\n","        return str((date2 - date1).days)\n","    except ValueError:\n","        return \"Ung√ºltiges Datumsformat. Bitte verwende DD.MM.YYYY.\"\n","\n","def extract_math_expression(text):\n","    \"\"\"Extrahiert mathematischen Ausdruck aus Text.\"\"\"\n","    match = re.search(r'([\\d+\\-*/().]+)', text)\n","    return match.group(0) if match else None\n","\n","def calculate(expression):\n","    \"\"\"F√ºhrt mathematische Berechnungen durch.\"\"\"\n","    try:\n","        math_expr = extract_math_expression(expression)\n","        if math_expr:\n","            return str(eval(math_expr))\n","        return \"Keine g√ºltige Berechnung gefunden.\"\n","    except:\n","        return \"Fehler bei der Berechnung.\"\n","\n","# Externe APIs\n","serpapi = SerpAPIWrapper()\n","wiki = WikipediaAPIWrapper()\n","\n","# Tools-Liste, die in allen Beispielen verwendet werden kann\n","tools = [\n","    Tool(name=\"search\", func=serpapi.run, description=\"Infos aus dem Internet\"),\n","    Tool(name=\"wiki\", func=lambda x: wiki.run(x).split(\"\\n\")[0] if wiki.run(x) else \"Keine relevanten Infos gefunden.\",\n","         description=\"Fakten aus Wikipedia\"),\n","    Tool(name=\"calculator\", func=calculate, description=\"Mathematische Berechnungen\"),\n","    Tool(name=\"weather\", func=get_weather, description=\"Gibt das aktuelle Wetter f√ºr einen bestimmten Ort zur√ºck\"),\n","    Tool(name=\"read_file\", func=read_file, description=\"Liest den Inhalt einer Datei. Input sollte der Dateipfad sein.\"),\n","    Tool(name=\"write_file\", func=write_file, description=\"√úberschreibt eine Datei mit neuem Inhalt. Input sollte im Format 'pfad//inhalt' sein.\"),\n","    Tool(name=\"days_between_dates\", func=days_between_dates, description=\"Berechnet die Anzahl der Tage zwischen zwei Datumsangaben im Format DD.MM.YYYY.\")\n","]"],"metadata":{"cellView":"form","id":"js4Z7ChoD65M"},"id":"js4Z7ChoD65M","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"38d28189","metadata":{"id":"38d28189"},"source":["\n","<p><font color='black' size=\"5\">\n","3.2 Aufbau eines Agenten\n","</font></p>"]},{"cell_type":"markdown","source":["\n","\n","Erl√§uterung der einzelnen Funktionen:\n","\n","* `ChatOpenAI` erm√∂glicht die Nutzung von OpenAI's Chat-Modellen.\n","* `create_tool_calling_agent` und `AgentExecutor` erschaffen und f√ºhren einen Agenten aus, der Werkzeuge nutzen kann.\n","* `ChatPromptTemplate` und `MessagesPlaceholder` definieren die Struktur f√ºr Nachrichten und Vorlagen im Chat.\n","* `IPython.display display, Markdown` erm√∂glichen die Darstellung von formatiertem Text in einer Jupyter-Umgebung."],"metadata":{"id":"u2kzqAYkdN3n"},"id":"u2kzqAYkdN3n"},{"cell_type":"code","execution_count":null,"id":"2fe7f7d3","metadata":{"id":"2fe7f7d3"},"outputs":[],"source":["# Import\n","from langchain_openai import ChatOpenAI\n","from langchain.agents import create_tool_calling_agent, AgentExecutor\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from IPython.display import display, Markdown\n","\n","# Konstaten\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATUR = 0.0\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model_name=MODEL, temperature=TEMPERATUR)\n","\n","# Prompt erstellen\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent mit Zugang zu Tools. Bitte Formeln in $ Formel $ formatieren.\"),\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"human\", \"{input}\"),\n","    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n","])"]},{"cell_type":"markdown","id":"851b3358","metadata":{"id":"851b3358"},"source":["Das `agent_scratchpad` ist ein entscheidender Bestandteil des Agenten-Frameworks, insbesondere bei der Verwendung von Werkzeugen (Tools):\n","\n","* **Arbeitsbereich f√ºr den Agenten:**\n","    * Das `agent_scratchpad` dient als tempor√§rer Arbeitsbereich, in dem der Agent seine √úberlegungen, Aktionen und Beobachtungen w√§hrend der Ausf√ºhrung speichert.\n","    * Es ist der Ort, an dem der Agent seine \"Denkschritte\" festh√§lt, bevor er eine endg√ºltige Antwort gibt.\n","* **Kommunikation mit dem Sprachmodell:**\n","    * Wenn der Agent ein Werkzeug aufruft, werden die Details des Aufrufs (z. B. der Name des Werkzeugs und die Eingabeparameter) in den `agent_scratchpad` geschrieben.\n","    * Nachdem das Werkzeug ausgef√ºhrt wurde, wird das Ergebnis (die \"Beobachtung\") ebenfalls in den `agent_scratchpad` geschrieben.\n","    * Das Sprachmodell verwendet den Inhalt des `agent_scratchpad`, um zu entscheiden, welche n√§chsten Schritte es unternehmen soll (z. B. ein weiteres Werkzeug aufrufen oder eine endg√ºltige Antwort geben).\n","* **Debugging und Transparenz:**\n","    * Das `agent_scratchpad` kann auch f√ºr Debugging-Zwecke n√ºtzlich sein, da er Einblicke in die Denkprozesse des Agenten bietet.\n","    * Es macht die Aktionskette des Agenten transparenter.\n","\n","Im Kontext des Prompts:\n","\n","* `MessagesPlaceholder(variable_name=\"agent_scratchpad\")` f√ºgt den Inhalt des `agent_scratchpad` in den Prompt ein, der an das Sprachmodell gesendet wird.\n","* Dadurch kann das Sprachmodell die vorherigen Aktionen und Beobachtungen des Agenten ber√ºcksichtigen, wenn es seine n√§chste Antwort generiert.\n","\n","Zusammengefasst ist das `agent_scratchpad` ein dynamischer Bereich, der die Kommunikation und das Ged√§chtnis des Agenten w√§hrend der Ausf√ºhrung von Werkzeugen erm√∂glicht."]},{"cell_type":"code","execution_count":null,"id":"50b590a7","metadata":{"id":"50b590a7"},"outputs":[],"source":["# Agent erstellen und ausf√ºhren\n","agent = create_tool_calling_agent(llm, tools, prompt)\n","agent_executor = AgentExecutor(\n","    agent=agent,\n","    tools=tools,\n","    verbose=True,\n","    return_intermediate_steps=True\n",")"]},{"cell_type":"code","source":["# Anfrage stellen\n","response = agent_executor.invoke({\n","    \"input\": \"Welche Tools stehen zu Deiner Verf√ºgung?\",\n","    \"chat_history\": []\n","})"],"metadata":{"id":"6ngiCqYJPONy"},"id":"6ngiCqYJPONy","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ergebnis anzeigen\n","display(Markdown(\"## üïµÔ∏è‚Äç‚ôÄÔ∏è Agent\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response[\"output\"]))"],"metadata":{"id":"8uRcUghPPpsK"},"id":"8uRcUghPPpsK","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Weitere Fragen an den Agenten\n","</font></p>"],"metadata":{"id":"xwYyRL27Pgkd"},"id":"xwYyRL27Pgkd"},{"cell_type":"code","source":["# Anfrage stellen\n","response = agent_executor.invoke({\n","    \"input\": \"Wie viel ist 12 * 12? Und wer ist Taylor Swift?\",\n","    \"chat_history\": []\n","})"],"metadata":{"id":"x5NXER6tdp-j"},"id":"x5NXER6tdp-j","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ergebnis anzeigen\n","display(Markdown(\"## üïµÔ∏è‚Äç‚ôÄÔ∏è Agent\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response[\"output\"]))"],"metadata":{"id":"-A3dB9z8JUEp"},"id":"-A3dB9z8JUEp","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Anfrage stellen\n","response = agent_executor.invoke({\n","    \"input\": \"Schreibe eine Notiz zum Wetter in Essen in eine Datei mit dem Namen wetter.txt\",\n","    \"chat_history\": []\n","})"],"metadata":{"id":"ATtStR4QPT-n"},"id":"ATtStR4QPT-n","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ergebnis anzeigen\n","display(Markdown(\"## üïµÔ∏è‚Äç‚ôÄÔ∏è Agent\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response[\"output\"]))"],"metadata":{"id":"-k6W-njJPsNX"},"id":"-k6W-njJPsNX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Anfrage stellen\n","response = agent_executor.invoke({\n","    \"input\": \"Lese den Inhalt der Datei wetter.txt\",\n","    \"chat_history\": []\n","})"],"metadata":{"id":"vc-kX_CjPW3u"},"id":"vc-kX_CjPW3u","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ergebnis anzeigen\n","display(Markdown(\"## üïµÔ∏è‚Äç‚ôÄÔ∏è Agent\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response[\"output\"]))"],"metadata":{"id":"HwBIq9y3PzsP"},"id":"HwBIq9y3PzsP","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"3ab5305c","metadata":{"id":"3ab5305c"},"source":["\n","<p><font color='black' size=\"5\">\n","3.3 Problem-Solving Agent\n","</font></p>\n"]},{"cell_type":"markdown","source":["**Kernprinzip:** Zyklischer Ablauf von Denken, Handeln und Beobachten mit explizitem Reasoning.\n","\n","**Charakteristika:**\n","\n","- Explizite Gedankenformulierung vor jeder Aktion\n","- Beobachtung der Ergebnisse nach jeder Aktion\n","- Iterative Anpassung basierend auf Beobachtungen\n","\n","**Historische Einordnung:**\n","Diese Implementierung folgt dem ReAct-Paradigma (Reasoning + Acting) und steht beispielhaft f√ºr die breitere Kategorie der Problem-Solving Agents."],"metadata":{"id":"OlTZj-0Td4mX"},"id":"OlTZj-0Td4mX"},{"cell_type":"code","execution_count":null,"id":"c87487a0","metadata":{"id":"c87487a0"},"outputs":[],"source":["# Dieses Beispiel demonstriert einen Problem-Solving Agent, der eine komplexe\n","# Recherche-Aufgabe bew√§ltigt. Beachten Sie, wie der Agent explizite\n","# Gedankeng√§nge formuliert und sein Reasoning offenlegt. Diese Implementierung\n","# folgt historisch dem ReAct-Paradigma (Reasoning + Acting), ist aber ein\n","# Beispiel f√ºr die breitere Kategorie der Problem-Solving Agents."]},{"cell_type":"code","source":["# Import\n","from langchain import hub\n","from langchain.agents import create_react_agent, AgentExecutor\n","from langchain_openai import ChatOpenAI\n","from datetime import datetime\n","from IPython.display import display, Markdown\n","\n","# Konstaten\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATUR = 0.0"],"metadata":{"id":"x5x00bp1wfT7"},"id":"x5x00bp1wfT7","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LLM initialisieren\n","llm = ChatOpenAI(model_name=MODEL, temperature=TEMPERATUR)"],"metadata":{"id":"LjEL1LrOEP13"},"id":"LjEL1LrOEP13","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Spezifischer Prompt aus LangChain Hub\n","prompt = hub.pull(\"hwchase17/react\")\n","for p in prompt:\n","    print(p)"],"metadata":{"id":"c_N4oLFuEUMf"},"id":"c_N4oLFuEUMf","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Agent erstellen\n","agent = create_react_agent(llm, tools, prompt)\n","\n","# Agent Executor zur Ausf√ºhrung verwenden\n","agent_executor = AgentExecutor(\n","    agent=agent,\n","    tools=tools,\n","    verbose=True\n",")"],"metadata":{"id":"EPo_zJItEVy9"},"id":"EPo_zJItEVy9","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Beispiel f√ºr einen Problem-Solving Agent bei komplexen Recherche-Aufgaben\n","query = \"\"\"\n","Welcher Schauspieler hat sowohl mit Christopher Nolan als auch mit Steven Spielberg gearbeitet?\n","Wie ist der Name des Schaupielers?\n","Wenn es mehrere Schauspieler gibt, dann nehme den ersten in Deiner Liste.\n","Welche Filme waren das und\n","In welchem Jahr wurden sie jeweils ver√∂ffentlicht?\n","Beschr√§nke Dich auf meine Fragen.\n","\"\"\"\n","\n","result = agent_executor.invoke({\"input\": query})"],"metadata":{"id":"Dgm8e9ykEW-u"},"id":"Dgm8e9ykEW-u","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ergebnis anzeigen\n","display(Markdown(\"## üîç Problem-Solving Agent (ReAct) Ergebnis\"))\n","display(Markdown(\"---\"))\n","display(Markdown(result[\"output\"]))"],"metadata":{"id":"y0rMhQaTJQqS"},"id":"y0rMhQaTJQqS","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"1a896193","metadata":{"id":"1a896193"},"source":["\n","<p><font color='black' size=\"5\">\n","3.4 Spezialisierter Agent\n","</font></p>"]},{"cell_type":"markdown","source":["\n","\n","**Kernprinzip:** Kombination eines zentralen Sprachmodells mit spezialisierten Modulen f√ºr verschiedene Dom√§nen.\n","\n","**Charakteristika:**\n","\n","- Modularit√§t: Verschiedene Tools f√ºr spezialisierte Aufgaben\n","- Routing: Weiterleitung von Teilaufgaben an die passenden Tools\n","- Integration: Zusammenf√ºhrung der Ergebnisse in eine koh√§rente Antwort\n","\n","**Historische Einordnung:**\n","Diese Implementierung folgt dem MRKL-Paradigma (Modular Reasoning, Knowledge and Language) und repr√§sentiert die breitere Kategorie der Spezialisierten Agents."],"metadata":{"id":"LKNWBkd9fKJP"},"id":"LKNWBkd9fKJP"},{"cell_type":"code","execution_count":null,"id":"6a95f5d1","metadata":{"id":"6a95f5d1"},"outputs":[],"source":["# Dieses Beispiel demonstriert einen Spezialisierten Agent, der verschiedene\n","# Aspekte einer Reiseplanung bew√§ltigt. Die Implementierung zeigt, wie ein\n","# Agent verschiedene Tools f√ºr unterschiedliche Teilaufgaben einsetzen kann.\n","# Historisch folgt dieser Ansatz dem MRKL-Paradigma (Modular Reasoning, Knowledge\n","# and Language), repr√§sentiert aber die breitere Kategorie der Spezialisierten Agents.\n","# Wir k√∂nnen hier denselben AgentExecutor weiterverwenden, aber mit einer anderen Aufgabe"]},{"cell_type":"code","execution_count":null,"id":"2207afda","metadata":{"id":"2207afda"},"outputs":[],"source":["# Beispiel f√ºr einen Spezialisierten Dom√§nen-Agent (basierend auf MRKL)\n","query = \"\"\"\n","Ich plane eine Reise nach Rom f√ºr die erste Juniwoche.\n","Wie wird das Wetter dort sein?\n","Welche historischen St√§tten sollte ich besuchen?\n","Wie hoch sind die Eintrittsgelder in EURO?\n","Welche lokalen Gerichte sind empfehlenswert?\n","Wieviele Tage sind bis zur Abreise? Heute ist der 03.05.2025.\n","Beschr√§nke Dich auf meine Fragen.\n","\"\"\"\n","\n","# Agent ausf√ºhren\n","response = agent_executor.invoke({\"input\": query})"]},{"cell_type":"code","source":["# Ergebnis anzeigen\n","display(Markdown(\"## üß© Spezialisierter Agent Ergebnis\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response[\"output\"]))"],"metadata":{"id":"9KgMzcACHeMa"},"id":"9KgMzcACHeMa","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"0c10d657","metadata":{"id":"0c10d657"},"source":["\n","<p><font color='black' size=\"5\">\n","3.5 Verbesserungs-Agent\n","</font></p>"]},{"cell_type":"markdown","source":["\n","\n","**Kernprinzip:** Selbstbewertung und -verbesserung durch kritische Analyse der eigenen Outputs.\n","\n","**Charakteristika:**\n","\n","- Self-criticism: Kritische Bewertung der eigenen Ausgaben\n","- Verbesserungszyklen: Mehrfache Iteration zur Qualit√§tsverbesserung\n","- Meta-Kognition: \"Denken √ºber das Denken\"\n","\n","**Historische Einordnung:**\n","Diese Implementierung basiert auf dem Konzept der Reflective Agents und repr√§sentiert die breitere Kategorie der Verbesserungs-Agents."],"metadata":{"id":"Z9vj7rZwgKBx"},"id":"Z9vj7rZwgKBx"},{"cell_type":"code","execution_count":null,"id":"7190f5ac","metadata":{"id":"7190f5ac"},"outputs":[],"source":["# Import\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import PromptTemplate\n","from langchain.schema.output_parser import StrOutputParser\n","from IPython.display import display, Markdown"]},{"cell_type":"code","source":["# Prompts f√ºr die drei Phasen des Reflection-Prozesses\n","answer_prompt = PromptTemplate.from_template(\"Beantworte: {question}\")\n","reflection_prompt = PromptTemplate.from_template(\"\"\"\n","Bewerte:\n","Frage: {question}\n","Antwort: {answer}\n","\n","Fehler/Verbesserungen?\"\"\")\n","improvement_prompt = PromptTemplate.from_template(\"\"\"\n","Frage: {question}\n","Antwort: {answer}\n","Reflexion: {reflection}\n","\n","Verbesserte Antwort:\"\"\")"],"metadata":{"id":"_gZdqHRxhj20"},"id":"_gZdqHRxhj20","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LCEL-Chains f√ºr die drei Phasen\n","\n","# LLMs mit unterschiedlichen Einstellungen\n","answer_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)  # Kreativere Erstantwort\n","reflection_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)  # Kritische Bewertung\n","\n","# Chains answer, reflection, improvement\n","answer_chain = answer_prompt | answer_llm | StrOutputParser()\n","\n","reflection_chain = {\n","    \"question\": lambda x: x[\"question\"],\n","    \"answer\": lambda x: x[\"answer\"]\n","} | reflection_prompt | reflection_llm | StrOutputParser()\n","\n","improvement_chain = {\n","    \"question\": lambda x: x[\"question\"],\n","    \"answer\": lambda x: x[\"initial_answer\"],\n","    \"reflection\": lambda x: x[\"reflection\"]\n","} | improvement_prompt | answer_llm | StrOutputParser()"],"metadata":{"id":"qj2oqncUhl0N"},"id":"qj2oqncUhl0N","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"38e329c9","metadata":{"id":"38e329c9"},"outputs":[],"source":["# Ausf√ºhrung der Chains / des Verbesserungs-Agents\n","question = \"Was sind die Hauptursachen des Klimawandels?\"\n","\n","initial_answer = answer_chain.invoke({\"question\": question})\n","\n","reflection = reflection_chain.invoke({\"question\": question, \"answer\": initial_answer})\n","\n","improved_answer = improvement_chain.invoke({\n","    \"question\": question,\n","    \"initial_answer\": initial_answer,\n","    \"reflection\": reflection\n","})"]},{"cell_type":"code","source":["# Ergebnisse anzeigen\n","display(Markdown(\"## üîç Verbesserungs-Agent (Reflective) Prozess\"))\n","display(Markdown(\"---\"))\n","\n","display(Markdown(\"### Initiale Antwort:\"))\n","display(Markdown(initial_answer))\n","display(Markdown(\"---\"))\n","\n","display(Markdown(\"### Selbstreflexion:\"))\n","display(Markdown(reflection))\n","display(Markdown(\"---\"))\n","\n","display(Markdown(\"### Verbesserte Antwort:\"))\n","display(Markdown(improved_answer))\n","display(Markdown(\"---\"))"],"metadata":{"id":"1kPASGDrhTcH"},"id":"1kPASGDrhTcH","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"aab8597f","metadata":{"id":"aab8597f"},"source":["\n","<p><font color='black' size=\"5\">\n","3.6 Service-Agent\n","</font></p>\n"]},{"cell_type":"markdown","source":["\n","\n","**Kernprinzip:** Standardisierte Schnittstellen f√ºr den Aufruf externer Funktionen und Dienste.\n","\n","**Charakteristika:**\n","- Direkte Integration: Nahtlose Verbindung zu externen Funktionen\n","- Standardisierte Schnittstellen: Klare Definitionen f√ºr Ein- und Ausgaben\n","- Spezialisierte Aktionen: Fokus auf spezifische Funktionalit√§ten\n","\n","**Historische Einordnung:**\n","Diese Implementierung folgt dem Function-Calling Paradigma und repr√§sentiert die breitere Kategorie der Service-Agents."],"metadata":{"id":"ZecabM9ShuJq"},"id":"ZecabM9ShuJq"},{"cell_type":"code","execution_count":null,"id":"3ac21b5e","metadata":{"id":"3ac21b5e"},"outputs":[],"source":["# Import\n","from langchain_openai import ChatOpenAI\n","from langchain.agents import AgentExecutor, create_openai_functions_agent\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from IPython.display import display, Markdown\n","\n","# Konstaten\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATUR = 0.0\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model=MODEL, temperature=TEMPERATUR)"]},{"cell_type":"code","source":["# Prompt f√ºr den Service-Agent\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent, der Informationen √ºber das Wetter und andere Fakten liefert.\"),\n","    (\"human\", \"{input}\"),\n","    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n","])"],"metadata":{"id":"ASLC1AWviBdw"},"id":"ASLC1AWviBdw","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function-Calling Agent (fokussiert auf direkten Funktionsaufruf)\n","function_agent = create_openai_functions_agent(llm, tools, prompt)\n","\n","service_agent_executor = AgentExecutor.from_agent_and_tools(\n","    agent=function_agent,\n","    tools=tools,\n","    verbose=True\n",")"],"metadata":{"id":"mMuupfUJiJVc"},"id":"mMuupfUJiJVc","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"e490212e","metadata":{"id":"e490212e"},"outputs":[],"source":["# Typisches Service-Agent Beispiel\n","location = \"Berlin\"\n","response = service_agent_executor.invoke({\n","    \"input\": f\"Wie ist das Wetter in {location}? Und was gibt es f√ºr allgemeine Informationen zu {location}?\",\n","})\n","\n","# Ergebnis anzeigen\n","display(Markdown(\"## üå°Ô∏è Service-Agent (Function-Calling) Ergebnis\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response[\"output\"]))"]},{"cell_type":"markdown","id":"038b1632","metadata":{"id":"038b1632"},"source":["\n","<p><font color='black' size=\"5\">\n","3.7 Vergleich der Agenten\n","</font></p>"]},{"cell_type":"markdown","id":"de011993","metadata":{"id":"de011993"},"source":["|Aspekt|Problem-Solving Agent|Verbesserungs-Agent|Service-Agent|Spezialisierter Agent|\n","|---|---|---|---|---|\n","|Hauptfokus|Komplexe Probleme l√∂sen|Inhaltsqualit√§t optimieren|Dienste ausf√ºhren|Dom√§nenexpertise|\n","|Arbeitsweise|Strukturierte Schritte mit Reasoning|Iterative Selbstbewertung|Standardisierte Schnittstellen|Spezialisierte Tools f√ºr ein Thema|\n","|Typischer Anwendungsfall|Recherche, Analyse, Entscheidungsfindung|Content-Erstellung, Review, QA|API-Interaktionen, Datenabfragen|Spezialisierte Beratung, Expertensysteme|\n","|Flexibilit√§t|Hoch f√ºr verschiedene Probleme|Mittel, fokussiert auf Verbesserung|Begrenzt auf definierte Funktionen|Hoch innerhalb der Dom√§ne|\n","|Transparenz|Hoch durch explizite Gedanken|Sehr hoch durch Selbsterkl√§rung|Mittel|Variiert je nach Implementierung|\n","|Komplexit√§tsbew√§ltigung|Vielseitig f√ºr komplexe Probleme|Gut f√ºr Qualit√§tsverbesserung|Gut f√ºr definierte Aufgaben|Hervorragend in der Fachdom√§ne|\n","|Implementation|Strukturierte Reasoning-Prozesse|Feedback-Schleifen|API-Definitionen|Dom√§nenspezifische Tools|\n","\n","Diese Tabelle bietet einen vollst√§ndigen Vergleich der aufgabenorientierten Agententypen in allen relevanten Aspekten und hilft bei der Auswahl des geeigneten Agententyps f√ºr bestimmte Anwendungsf√§lle.\n"]},{"cell_type":"markdown","id":"448fb547","metadata":{"id":"448fb547"},"source":["\n","<p><font color='black' size=\"5\">\n","3.8 Kombinationsm√∂glichkeiten\n","</font></p>"]},{"cell_type":"markdown","source":["\n","\n","Die St√§rke moderner KI-Agenten liegt oft in der Kombination verschiedener Ans√§tze:\n","\n","- **Problem-Solving + Service**: Verbindet strukturierte API-Aufrufe mit transparentem Reasoning\n","- **Spezialisiert + Verbesserung**: Dom√§nenspezifische Systeme mit Selbstverbesserungsf√§higkeit\n","- **Service + Spezialisiert**: Standardisierte Schnittstellen f√ºr modulare Systeme\n","- **Problem-Solving + Verbesserung**: Transparentes Reasoning mit iterativer Qualit√§tsverbesserung\n","\n","In der Praxis implementieren die meisten Agenten eine Mischung dieser Ans√§tze, angepasst an spezifische Anwendungsf√§lle.\n"],"metadata":{"id":"8RQefWTGinK6"},"id":"8RQefWTGinK6"},{"cell_type":"markdown","source":["# 4 | Erweiterte Konzepte\n","---\n","\n","Nach der Darstellung der grundlegenden Architekturen folgen nun erweiterte Konzepte, die auf diesen Architekturen aufbauen oder sie kombinieren.\n"],"metadata":{"id":"QO-PsEtTioiG"},"id":"QO-PsEtTioiG"},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","4.1 Einfache Planung\n","</font></p>"],"metadata":{"id":"iLAXNTM5iq-P"},"id":"iLAXNTM5iq-P"},{"cell_type":"markdown","source":["\n","\n","Planung und Zielverfolgung ist kein eigenst√§ndiger Architekturtyp, sondern ein Anwendungskonzept, das auf den grundlegenden Architekturen aufbaut.\n","\n","**Kombination:** Meist basierend auf Problem-Solving oder Verbesserungs-Agents"],"metadata":{"id":"4bc63yCTitXi"},"id":"4bc63yCTitXi"},{"cell_type":"code","execution_count":null,"id":"80952963","metadata":{"id":"80952963"},"outputs":[],"source":["# Import\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import PromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from IPython.display import display, Markdown\n","\n","# Konstaten\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATUR = 0.0\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model_name=MODEL, temperature=TEMPERATUR)"]},{"cell_type":"code","source":["# Chain-of-Thought Prompt erstellen\n","cot_prompt = PromptTemplate(\n","    input_variables=[\"problem\"],\n","    template=\"\"\"\n","    Zerlege das folgende Problem in kleinere Teilschritte und l√∂se es Schritt f√ºr Schritt:\n","\n","    Problem: {problem}\n","\n","    Denke schrittweise:\n","    1. Welche Teilprobleme m√ºssen gel√∂st werden?\n","    2. Wie l√∂se ich jeden Teilschritt?\n","    3. Wie kombiniere ich die Teill√∂sungen zur Gesamtl√∂sung?\n","    \"\"\"\n",")"],"metadata":{"id":"t0T7KmNXi6B6"},"id":"t0T7KmNXi6B6","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LCEL-Pipeline erstellen und ausf√ºhren\n","planning_chain = (\n","    {\"problem\": RunnablePassthrough()}\n","    | cot_prompt\n","    | llm\n","    | StrOutputParser()\n",")"],"metadata":{"id":"Ge12gfRGjFZu"},"id":"Ge12gfRGjFZu","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Beispielanwendung\n","complex_problem = \"\"\"\n","Berechne die Gesamtkosten f√ºr einen Einkauf von 3 B√ºchern zu je 12,99 ‚Ç¨,\n","2 Notizb√ºchern zu je 4,50 ‚Ç¨ und einem Stift f√ºr 2,25 ‚Ç¨,\n","wenn auf B√ºcher 7% Mehrwertsteuer und auf andere Artikel 19% Mehrwertsteuer anf√§llt.\n","Verwende f√ºr Formeln das Format $ Formel $.\n","\"\"\"\n","\n","solution = planning_chain.invoke(complex_problem)"],"metadata":{"id":"D4nx3S7SjIID"},"id":"D4nx3S7SjIID","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ergebnis anzeigen\n","display(Markdown(\"## üìù Einfache Planung mit Chain-of-Thought\"))\n","display(Markdown(\"---\"))\n","display(Markdown(solution))"],"metadata":{"id":"Nu_33eAijOsF"},"id":"Nu_33eAijOsF","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"0f6e66d7","metadata":{"id":"0f6e66d7"},"source":["\n","<p><font color='black' size=\"5\">\n","4.2 Fortgeschrittene Planung\n","</font></p>"]},{"cell_type":"markdown","source":["\n","\n","**Kombination:** Oft Spezialisierte + Verbesserungs-Agents f√ºr hierarchische Planung"],"metadata":{"id":"h-sXqpZojnju"},"id":"h-sXqpZojnju"},{"cell_type":"code","execution_count":null,"id":"ea2d5596","metadata":{"id":"ea2d5596"},"outputs":[],"source":["# Import\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import PromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from IPython.display import display, Markdown\n","\n","# Konstaten\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATUR = 0.0\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model_name=MODEL, temperature=TEMPERATUR)"]},{"cell_type":"code","source":["# Funktionen\n","def create_planner_chain():\n","    \"\"\"Erstellt eine Chain f√ºr die hierarchische Planung.\"\"\"\n","    planer_prompt = PromptTemplate.from_template(\n","        \"\"\"\n","        Erstelle einen hierarchischen Plan, um das folgende Ziel zu erreichen:\n","\n","        Ziel: {objective}\n","\n","        Teile den Plan in:\n","        1. Strategische Ziele (hohe Ebene)\n","        2. Taktische Schritte (mittlere Ebene)\n","        3. Konkrete Aktionen (detaillierte Ebene)\n","\n","        F√ºr jede konkrete Aktion gib an, welche Tools oder Ressourcen ben√∂tigt werden.\n","        \"\"\"\n","    )\n","    return ({\"objective\": RunnablePassthrough()} | planer_prompt | llm | StrOutputParser())\n","\n","def create_execution_chain():\n","    \"\"\"Erstellt eine Chain f√ºr die Anpassung des Plans basierend auf Feedback.\"\"\"\n","    execution_prompt = PromptTemplate.from_template(\n","        \"\"\"\n","        Hier ist der aktuelle Plan:\n","\n","        {plan}\n","\n","        Basierend auf dem folgenden Feedback, passe den Plan an:\n","\n","        Feedback: {feedback}\n","\n","        √úberarbeiteter Plan:\n","        \"\"\"\n","    )\n","    return ({\"plan\": RunnablePassthrough(), \"feedback\": RunnablePassthrough()}\n","            | execution_prompt | llm | StrOutputParser())"],"metadata":{"id":"iIX6zAdpjxCz"},"id":"iIX6zAdpjxCz","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Planungsketten erstellen\n","planner_chain = create_planner_chain()\n","execution_chain = create_execution_chain()"],"metadata":{"id":"EkLsaa5lj-VN"},"id":"EkLsaa5lj-VN","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"619c9e45","metadata":{"id":"619c9e45"},"outputs":[],"source":["# Beispiel ausf√ºhren\n","objective = \"Organisiere eine zweit√§gige Konferenz f√ºr 100 Teilnehmer zum Thema KI-Ethik\"\n","initial_plan = planner_chain.invoke(objective)"]},{"cell_type":"code","source":["# Feedback geben und Plan anpassen\n","feedback = \"Der Budgetrahmen wurde auf 15.000 ‚Ç¨ reduziert, und wir m√ºssen die Konferenz auf einen Tag verk√ºrzen.\"\n","revised_plan = execution_chain.invoke({\"plan\": initial_plan, \"feedback\": feedback})"],"metadata":{"id":"sY1rJ1JqkHct"},"id":"sY1rJ1JqkHct","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ergebnisse anzeigen\n","display(Markdown(\"## üìî Hierarchische Planung\"))\n","display(Markdown(\"---\"))\n","\n","display(Markdown(\"### Initialer Plan\"))\n","display(Markdown(initial_plan))\n","display(Markdown(\"---\"))\n","\n","display(Markdown(\"### √úberarbeiteter Plan\"))\n","display(Markdown(revised_plan))\n","display(Markdown(\"---\"))"],"metadata":{"id":"m9Y2twmZkMXW"},"id":"m9Y2twmZkMXW","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"79b04997","metadata":{"id":"79b04997"},"source":["\n","<p><font color='black' size=\"5\">\n","4.3 Zielorientierter Agent\n","</font></p>"]},{"cell_type":"markdown","source":["\n","\n","**Kombination:** Service-Agent + Problem-Solving Agent f√ºr zielgerichtete Aktionen"],"metadata":{"id":"lX0PHQyC1EpE"},"id":"lX0PHQyC1EpE"},{"cell_type":"code","source":["# Import\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.messages import AIMessage, HumanMessage\n","from langchain.agents import AgentExecutor, create_openai_functions_agent\n","from langchain.agents.format_scratchpad import format_to_openai_functions\n","\n","# Konstaten\n","MODEL = \"gpt-4o\"\n","TEMPERATUR = 0.0\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model_name=MODEL, temperature=TEMPERATUR)\n","\n","# Chat-Verlauf initialisieren\n","chat_history = []"],"metadata":{"id":"Tbyc_g3Tmk_V"},"id":"Tbyc_g3Tmk_V","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prompt erstellen\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher KI-Assistent. Antworte auf Deutsch.\"),\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"human\", \"{input}\"),\n","    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n","])"],"metadata":{"id":"AHR5vRnFpAa1"},"id":"AHR5vRnFpAa1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Agent erstellen\n","agent = create_openai_functions_agent(llm, tools, prompt)\n","\n","# Agent-Executor konfigurieren\n","agent_executor = AgentExecutor(\n","    agent=agent,\n","    tools=tools,\n","    verbose=True,\n","    handle_parsing_errors=True\n",")"],"metadata":{"id":"EfWPk4HRpCc4"},"id":"EfWPk4HRpCc4","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Funktion f√ºr die Ausf√ºhrung einer Konversation\n","def run_conversation(input_text):\n","    # Agent ausf√ºhren mit Zugriff auf den Chat-Verlauf\n","    result = agent_executor.invoke({\n","        \"input\": input_text,\n","        \"chat_history\": chat_history\n","    })\n","\n","    # Chat-Verlauf aktualisieren\n","    chat_history.extend([\n","        HumanMessage(content=input_text),\n","        AIMessage(content=result[\"output\"])\n","    ])\n","\n","    return result[\"output\"]"],"metadata":{"id":"fizbyEaopEox"},"id":"fizbyEaopEox","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Komplexe Anfrage\n","complex_goal = \"\"\"\n","Finde heraus, wer der aktuelle Bundeskanzler von Deutschland ist,\n","wann er geboren wurde und erstelle eine kurze Zusammenfassung\n","seiner politischen Karriere.\n","\"\"\""],"metadata":{"id":"kgE_cCnipIup"},"id":"kgE_cCnipIup","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Agent mit komplexer Anfrage aufrufen\n","result = run_conversation(complex_goal)"],"metadata":{"collapsed":true,"id":"s922eEBVpNGP"},"id":"s922eEBVpNGP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ergebnis anzeigen\n","display(Markdown(\"## üîé Zielorientierter Agent\"))\n","display(Markdown(\"---\"))\n","display(Markdown(result))"],"metadata":{"id":"usDCcj2RpLUl"},"id":"usDCcj2RpLUl","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Komplexe Anfrage\n","complex_goal = \"\"\"\n","Finde heraus, ob er verheiratet ist und mit wem?\n","Stelle Informationen aus ihrer Biografie zusammen.\n","\"\"\""],"metadata":{"id":"wG_ZEchDpuA6"},"id":"wG_ZEchDpuA6","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Agent mit komplexer Anfrage aufrufen\n","result = run_conversation(complex_goal)"],"metadata":{"id":"ifwUc1P4p1Kn"},"id":"ifwUc1P4p1Kn","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ergebnis anzeigen\n","display(Markdown(\"## üîé Zielorientierter Agent\"))\n","display(Markdown(\"---\"))\n","display(Markdown(result))"],"metadata":{"id":"rRECiAJ5p5BR"},"id":"rRECiAJ5p5BR","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"2f064c9f","metadata":{"id":"2f064c9f"},"source":["\n","<p><font color='black' size=\"5\">\n","4.4 Kollaborative Analyse\n","</font></p>"]},{"cell_type":"markdown","source":["\n","\n","**Kombination:** Spezialisierte Agenten mit verschiedenen Schwerpunkten"],"metadata":{"id":"s6pdGMFMqUJE"},"id":"s6pdGMFMqUJE"},{"cell_type":"code","execution_count":null,"id":"6d518413","metadata":{"id":"6d518413"},"outputs":[],"source":["# Import\n","from IPython.display import display, Markdown\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import PromptTemplate\n","from langchain_core.output_parsers import StrOutputParser"]},{"cell_type":"code","source":["# Pipelines mit optimierten Modellen und Temperaturen\n","\n","research_pipeline = PromptTemplate.from_template(\n","    \"\"\"Du bist ein Recherche-Spezialist. Sammle Informationen zum Thema: {topic}\"\"\"\n",") | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)  # Gute Balance zwischen Kreativit√§t und Fakten\n","\n","\n","critique_pipeline = PromptTemplate.from_template(\n","    \"\"\"Du bist ein kritischer Analyst. Bewerte folgende Recherche: {research}\"\"\"\n",") | ChatOpenAI(model=\"gpt-4o\", temperature=0)  # Maximale Pr√§zision f√ºr Analyse\n","\n","\n","synthesis_pipeline = PromptTemplate.from_template(\n","    \"\"\"Du bist ein Informationssynthetisierer. Erstelle eine Zusammenfassung:\n","    Recherche: {research}\n","    Kritik: {critique}\"\"\"\n",") | ChatOpenAI(model=\"gpt-4o\", temperature=0.2)  # Verbesserte Qualit√§t f√ºr komplexe Zusammenfassung\n","\n","\n","decision_pipeline = PromptTemplate.from_template(\n","    \"\"\"Du bist ein Entscheidungstr√§ger. Basierend auf: {synthesis}\n","    Beantworte: {question}\"\"\"\n",") | ChatOpenAI(model=\"gpt-4o\", temperature=0)  # Konsistente Entscheidungsfindung"],"metadata":{"id":"NSPsC8jMqZN4"},"id":"NSPsC8jMqZN4","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Schrittweise Analyse definieren\n","def collaborative_analysis(topic, question):\n","    \"\"\"Kollaborative Analyse mit LCEL-Pipelines.\"\"\"\n","    # Prozess ausf√ºhren\n","    research = research_pipeline.invoke({\"topic\": topic})\n","    display(Markdown(\"### ‚úÖ Recherche abgeschlossen\"))\n","\n","    critique = critique_pipeline.invoke({\"research\": research})\n","    display(Markdown(\"### ‚úÖ Kritische Analyse abgeschlossen\"))\n","\n","    synthesis = synthesis_pipeline.invoke({\"research\": research, \"critique\": critique})\n","    display(Markdown(\"### ‚úÖ Synthese abgeschlossen\"))\n","\n","    decision = decision_pipeline.invoke({\"synthesis\": synthesis, \"question\": question})\n","    display(Markdown(\"### ‚úÖ Entscheidung getroffen\"))\n","\n","    return {\n","        \"research\": research,\n","        \"critique\": critique,\n","        \"synthesis\": synthesis,\n","        \"decision\": decision\n","    }"],"metadata":{"id":"_nMydwn-rEwp"},"id":"_nMydwn-rEwp","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"2591decd","metadata":{"id":"2591decd"},"outputs":[],"source":["# Beispielanwendung\n","topic = \"K√ºnstliche Intelligenz in der Gesundheitsversorgung\"\n","question = \"Sollten Krankenh√§user KI-Systeme zur Diagnoseunterst√ºtzung einsetzen?\""]},{"cell_type":"code","source":["# Multi-Agenten-Analyse ausf√ºhren\n","display(Markdown(\"## üë• Multi-Agenten-System: Kollaborative Analyse\"))\n","display(Markdown(\"---\"))\n","results = collaborative_analysis(topic, question)"],"metadata":{"id":"X7jTRqujrZ7L"},"id":"X7jTRqujrZ7L","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Finale Entscheidung anzeigen\n","display(Markdown(f\"## üìë Finale Entscheidung\"))\n","display(Markdown(\"---\"))\n","\n","for step, txt  in results.items():\n","    display(Markdown(f\"### {step.upper()}\"))\n","    display(Markdown(f\"{txt.content}\"))\n","    display(Markdown(\"---\"))"],"metadata":{"id":"mpQP8H_7rcg7"},"id":"mpQP8H_7rcg7","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"518d7d5a","metadata":{"id":"518d7d5a"},"source":["# 5 | Model Context Protocol (MCP)\n","---\n"]},{"cell_type":"markdown","source":["\n","**Wichtige Ressourcen:**\n","\n","[Anthropic MCP](https://www.anthropic.com/news/model-context-protocol)\n","\n","[OpenAI MCP](https://openai.github.io/openai-agents-python/mcp/)\n","\n","[MCPServer](https://github.com/modelcontextprotocol/servers)\n","\n"],"metadata":{"id":"w6nFlzHDwuJy"},"id":"w6nFlzHDwuJy"},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","5.1 Was ist MCP?\n","</font></p>"],"metadata":{"id":"zs4LhIqxwqMP"},"id":"zs4LhIqxwqMP"},{"cell_type":"markdown","source":["\n","Ein Protokoll ist ein Regelwerk, das bestimmt, wie zwei Systeme miteinander kommunizieren. Protokolle regeln die Daten√ºbertragung in Computernetzwerken, bei der Internetkommunikation und zwischen Softwaresystemen.\n","\n","**Zum Beispiel:**\n","\n","+ HTTP (Hypertext Transfer Protocol): Erm√∂glicht Websites die Kommunikation mit Browsern.\n","+ TCP/IP (Transmission Control Protocol/Internetprotokoll): Definiert, wie Datenpakete im Internet geroutet werden.\n","+ JSON-RPC (Aufrufen von Remoteprozeduren): Ein Protokoll, das den Datenaustausch im JSON-Format erm√∂glicht.    \n","\n","Das **Model Context Protocol (MCP)** ist ein offenes Protokoll, das es gro√üen Sprachmodellen (LLMs) erm√∂glicht, sich auf standardisierte Weise in externe Datenquellen und Tools zu integrieren. Dieses von Anthropic entwickelte Protokoll macht es KI-Modellen leicht, nahtlos mit einer Vielzahl von Tools und Datenquellen zusammenzuarbeiten.\n","\n","Es soll die Interaktion von KI-Modellen, insbesondere Large Language Models (LLMs) und autonomen Agenten, mit externen Datenquellen und Tools zu **standardisieren und zu vereinfachen**. Ziel ist es, einen **einheitlichen Rahmen** zu schaffen, der es KI-Agenten erm√∂glicht, auf strukturierte Daten aus verschiedenen Quellen wie Datenbanken, APIs, Cloud-Speicher und Unternehmensanwendungen auf standardisierte Weise zuzugreifen, diese zu verarbeiten und darauf zu reagieren, **ohne dass f√ºr jede Quelle spezifische API-Integrationen erforderlich sind**.\n","\n"],"metadata":{"id":"55pocnEnww4g"},"id":"55pocnEnww4g"},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","5.2 Warum wurde MCP entwickelt?\n","</font></p>"],"metadata":{"id":"xQnjvh7HwyuL"},"id":"xQnjvh7HwyuL"},{"cell_type":"markdown","source":["\n","\n","Die Notwendigkeit von MCP ergibt sich aus den **Ineffizienzen und Herausforderungen** aktueller KI-API-Interaktionen. Derzeit ist der Aufbau von KI-Agenten, die Daten aus verschiedenen Quellen abrufen, **fragmentiert, repetitiv und schwer zu skalieren**. Jedes Tool spricht seine eigene Sprache und erfordert **individuelle Integrationen**. MCP zielt darauf ab, diese Komplexit√§t zu reduzieren und den **Entwicklungsaufwand zu minimieren**.\n","\n"],"metadata":{"id":"NevFl-6xw0lH"},"id":"NevFl-6xw0lH"},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","5.3 Wie funktioniert MCP?\n","</font></p>"],"metadata":{"id":"dsTGqA4Zw3io"},"id":"dsTGqA4Zw3io"},{"cell_type":"markdown","source":["\n","\n","MCP basiert auf einer **Client-Server-Architektur**.\n","\n","*   **MCP-Clients** sind typischerweise KI-Agenten, Anwendungen oder Systeme, die strukturierte Daten ben√∂tigen. Beispiele hierf√ºr sind Claude Desktop, Cursor, Windsurf und Frameworks wie Langchain und Pydantic AI.\n","*   **MCP-Server** fungieren als **Vermittler**, die Daten von verschiedenen APIs, Datenbanken oder Unternehmenssystemen abrufen und diese in einem **einheitlichen Format** an die Clients zur√ºckgeben.\n","\n","Der Interaktionsprozess folgt einem strukturierten **Anfrage-Antwort-Zyklus**:\n","\n","1.  Der KI-Agent (Client) sendet eine **strukturierte Anfrage** an den MCP-Server, in der die ben√∂tigten Daten oder die auszuf√ºhrende Aktion in einem standardisierten Format definiert sind.\n","2.  Der MCP-Server **verarbeitet** diese Anfrage, authentifiziert sie, pr√ºft die Berechtigungen und ermittelt, welche externen Systeme abgefragt werden m√ºssen.\n","3.  Die eigentlichen **Datenabfragen** an die verschiedenen Quellen k√∂nnen parallel erfolgen.\n","4.  Die **Antworten** der verschiedenen Quellen werden vom MCP-Server in einem **einheitlichen, strukturierten Format standardisiert**, das f√ºr KI-Modelle leicht zu verarbeiten ist.\n","\n","Ein wesentliches Konzept von MCP ist die **Reflection**. Dies bedeutet, dass ein MCP-Client einen MCP-Server nach seinen **verf√ºgbaren Tools und Ressourcen** fragen kann, ohne vorherige Kenntnisse dieser Schnittstellen zu ben√∂tigen.\n","\n","[Interaktive Visualisierung des MCP-Prozesses](https://claude.site/artifacts/1ba26344-3819-427d-9080-98381785f8df)\n","\n"],"metadata":{"id":"XI9msU-Dw5i1"},"id":"XI9msU-Dw5i1"},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","5.4 Kernkonzepte\n","</font></p>\n"],"metadata":{"id":"sfl6-F7xw8OR"},"id":"sfl6-F7xw8OR"},{"cell_type":"markdown","source":["\n","*   **Tools:** Funktionen, die das Modell nutzen kann, um Aktionen durchzuf√ºhren (z. B. Websuchen, Datenbankabfragen).\n","*   **Ressourcen:** Anh√§nge oder Daten, die dem Modell zur Verf√ºgung gestellt werden (z. B. Dateien, Datenbankinhalte).\n","*   **Prompts:** Vorlagen, die Clients f√ºr Anfragen an das Modell verwenden k√∂nnen.\n","\n"],"metadata":{"id":"CwJNrxKtw_rE"},"id":"CwJNrxKtw_rE"},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","5.5 Vorteile von MCP\n","</font></p>"],"metadata":{"id":"bYE_S93Ow-at"},"id":"bYE_S93Ow-at"},{"cell_type":"markdown","source":["\n","*   **Vereinfachte Integrationen** und **reduzierte Komplexit√§t**.\n","*   **Verbesserte Skalierbarkeit** und **Wiederverwendbarkeit** von Integrationen.\n","*   **Erh√∂hte Interoperabilit√§t** zwischen KI-Modellen und externen Systemen.\n","*   **Zeitersparnis f√ºr Entwickler** durch weniger benutzerdefinierte API-Implementierungen.\n","*   Potenzial f√ºr eine **zentrale Authentifizierung** (zuk√ºnftig)."],"metadata":{"id":"_ONCUz4txA0z"},"id":"_ONCUz4txA0z"},{"cell_type":"markdown","source":["[Interaktive Visualisierung](https://claude.site/artifacts/1ba26344-3819-427d-9080-98381785f8df)"],"metadata":{"id":"OeTr_m93LVEB"},"id":"OeTr_m93LVEB"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Code-Beispiel zur Interaktion\n","</font></p>"],"metadata":{"id":"k6reCVSRO5WY"},"id":"k6reCVSRO5WY"},{"cell_type":"code","execution_count":null,"id":"3c002ac7","metadata":{"lines_to_next_cell":0,"id":"3c002ac7"},"outputs":[],"source":["import json\n","import uuid\n","from typing import Dict, Any\n","\n","class MCPClient:\n","    \"\"\"Einfacher MCP-Client f√ºr die Kommunikation mit MCP-Servern.\"\"\"\n","\n","    def __init__(self, client_id: str, api_key: str):\n","        self.client_id = client_id\n","        self.api_key = api_key\n","        self.headers = {\n","            \"Authorization\": f\"Bearer {api_key}\",\n","            \"Client-ID\": client_id\n","        }\n","\n","    def create_request(self, tool_name: str, parameters: Dict[str, Any]) -> Dict[str, Any]:\n","        \"\"\"Erstellt eine strukturierte MCP-Anfrage.\"\"\"\n","        request_data = {\n","            \"protocol_version\": \"1.0\",\n","            \"request_id\": str(uuid.uuid4()),\n","            \"tool\": {\n","                \"name\": tool_name,\n","                \"parameters\": parameters\n","            }\n","        }\n","        return request_data\n","\n","    def send_request(self, request_data: Dict[str, Any]) -> Dict[str, Any]:\n","        \"\"\"Sendet eine MCP-Anfrage an den MCP-Server.\"\"\"\n","        # In einer realen Implementierung w√ºrde hier ein HTTP-Request erfolgen\n","        return {\"status\": \"success\", \"result\": {}}  # Platzhalter\n","\n","\n","class MCPServer:\n","    \"\"\"Simulierter MCP-Server zum Testen der Interaktion.\"\"\"\n","\n","    def __init__(self):\n","        self.registered_tools = {\n","            \"weather_info\": self._get_weather_data,\n","            \"database_query\": self._execute_database_query,\n","            \"text_translation\": self._translate_text\n","        }\n","\n","    def validate_request(self, request: Dict[str, Any]) -> bool:\n","        \"\"\"√úberpr√ºft, ob die Anfrage dem MCP-Protokoll entspricht.\"\"\"\n","        required_fields = [\"protocol_version\", \"request_id\", \"tool\"]\n","        if not all(field in request for field in required_fields):\n","            return False\n","\n","        if \"name\" not in request[\"tool\"] or request[\"tool\"][\"name\"] not in self.registered_tools:\n","            return False\n","\n","        return True\n","\n","    def process_request(self, request: Dict[str, Any]) -> Dict[str, Any]:\n","        \"\"\"Verarbeitet eine validierte MCP-Anfrage.\"\"\"\n","        if not self.validate_request(request):\n","            return {\"error\": \"Ung√ºltige Anfrage\", \"status\": \"error\"}\n","\n","        tool_name = request[\"tool\"][\"name\"]\n","        parameters = request[\"tool\"].get(\"parameters\", {})\n","\n","        # Tool ausf√ºhren\n","        tool_func = self.registered_tools[tool_name]\n","        result = tool_func(parameters)\n","\n","        # Antwort standardisieren\n","        response = {\n","            \"request_id\": request[\"request_id\"],\n","            \"status\": \"success\",\n","            \"result\": result\n","        }\n","        return response\n","\n","    def _get_weather_data(self, params: Dict[str, Any]) -> Dict[str, Any]:\n","        \"\"\"Simuliert den Abruf von Wetterdaten von einem externen Dienst.\"\"\"\n","        return {\n","            \"city\": params.get(\"city\", \"Berlin\"),\n","            \"temperature\": 22,\n","            \"condition\": \"sonnig\",\n","            \"humidity\": 65\n","        }\n","\n","    def _execute_database_query(self, params: Dict[str, Any]) -> Dict[str, Any]:\n","        \"\"\"Simuliert eine Datenbankabfrage.\"\"\"\n","        return {\n","            \"rows\": [\n","                {\"id\": 1, \"name\": \"Produkt A\", \"price\": 29.99},\n","                {\"id\": 2, \"name\": \"Produkt B\", \"price\": 49.99}\n","            ],\n","            \"total_rows\": 2\n","        }\n","\n","    def _translate_text(self, params: Dict[str, Any]) -> Dict[str, Any]:\n","        \"\"\"Simuliert einen √úbersetzungsdienst.\"\"\"\n","        target_lang = params.get(\"target_language\", \"en\")\n","\n","        translations = {\n","            \"en\": \"Hello world\",\n","            \"fr\": \"Bonjour le monde\",\n","            \"de\": \"Hallo Welt\"\n","        }\n","\n","        return {\n","            \"original_text\": params.get(\"text\", \"\"),\n","            \"translated_text\": translations.get(target_lang, \"√úbersetzung nicht verf√ºgbar\"),\n","            \"target_language\": target_lang\n","        }\n","\n","\n","class LLMApplication:\n","    \"\"\"LLM-Anwendung, die den MCP-Client verwendet.\"\"\"\n","\n","    def __init__(self, mcp_client: MCPClient):\n","        self.mcp_client = mcp_client\n","\n","    def process_user_query(self, query: str) -> str:\n","        \"\"\"Verarbeitet eine Nutzeranfrage mit MCP f√ºr zus√§tzliche Daten.\"\"\"\n","        if \"wetter\" in query.lower():\n","            # Extrahiere Stadt (vereinfacht)\n","            city = \"Berlin\"\n","\n","            # Erstelle und sende MCP-Anfrage\n","            request = self.mcp_client.create_request(\"weather_info\", {\"city\": city})\n","            response = self.mcp_client.send_request(request)\n","\n","            if response.get(\"status\") == \"success\":\n","                weather_data = response.get(\"result\", {})\n","                return self._format_weather_response(weather_data)\n","            return \"Entschuldigung, ich konnte keine Wetterdaten abrufen.\"\n","\n","        elif \"√ºbersetze\" in query.lower():\n","            text = \"Hallo, wie geht es dir?\"\n","            target_lang = \"en\"\n","\n","            request = self.mcp_client.create_request(\"text_translation\", {\n","                \"text\": text,\n","                \"target_language\": target_lang\n","            })\n","            response = self.mcp_client.send_request(request)\n","\n","            if response.get(\"status\") == \"success\":\n","                translation_data = response.get(\"result\", {})\n","                return self._format_translation_response(translation_data)\n","            return \"Entschuldigung, ich konnte den Text nicht √ºbersetzen.\"\n","\n","        return \"Ich verstehe Ihre Anfrage. Wie kann ich Ihnen helfen?\"\n","\n","    def _format_weather_response(self, weather_data: Dict[str, Any]) -> str:\n","        \"\"\"Formatiert Wetterdaten f√ºr die Anzeige.\"\"\"\n","        return (f\"Das aktuelle Wetter in {weather_data.get('city')}: \"\n","                f\"{weather_data.get('temperature')}¬∞C, {weather_data.get('condition')}.\")\n","\n","    def _format_translation_response(self, translation_data: Dict[str, Any]) -> str:\n","        \"\"\"Formatiert √úbersetzungsdaten f√ºr die Anzeige.\"\"\"\n","        return (f\"√úbersetzung ({translation_data.get('target_language')}): \"\n","                f\"{translation_data.get('translated_text')}\")\n","\n","\n","def run_example():\n","    \"\"\"F√ºhrt ein Beispiel der MCP-Interaktion aus.\"\"\"\n","    print(\"=== MCP-Interaktionsbeispiel ===\\n\")\n","\n","    # MCP-Server initialisieren\n","    server = MCPServer()\n","\n","    # MCP-Client initialisieren\n","    client = MCPClient(\n","        client_id=\"llm_app_123\",\n","        api_key=\"sk_test_12345\"\n","    )\n","\n","    # LLM-Anwendung initialisieren\n","    llm_app = LLMApplication(client)\n","\n","    # Mock f√ºr send_request\n","    def mock_send_request(request_data):\n","        print(\"\\n[1] LLM-Anwendung erstellt MCP-Anfrage:\")\n","        print(json.dumps(request_data, indent=2))\n","\n","        print(\"\\n[2] MCP-Client sendet Anfrage an MCP-Server\")\n","\n","        print(\"\\n[3] MCP-Server validiert die Anfrage\")\n","        valid = server.validate_request(request_data)\n","        print(f\"Anfrage ist g√ºltig: {valid}\")\n","\n","        print(\"\\n[4] MCP-Server verarbeitet die Anfrage und kommuniziert mit externen Systemen\")\n","        response = server.process_request(request_data)\n","\n","        print(\"\\n[5] Externe Systeme liefern Daten an MCP-Server\")\n","\n","        print(\"\\n[6] MCP-Server standardisiert die Antwort:\")\n","        print(json.dumps(response, indent=2))\n","\n","        print(\"\\n[7] MCP-Server sendet Antwort an MCP-Client\")\n","\n","        print(\"\\n[8] MCP-Client stellt Kontext der LLM-Anwendung zur Verf√ºgung\")\n","\n","        return response\n","\n","    # √úberschreiben der Methode\n","    client.send_request = mock_send_request\n","\n","    # Beispiel-Nutzeranfrage\n","    query = \"Wie ist das Wetter in Berlin?\"\n","\n","    print(f\"\\nNutzer fragt: '{query}'\")\n","    print(\"\\n[0] LLM-Anwendung analysiert die Nutzeranfrage\")\n","\n","    # LLM verarbeitet die Anfrage und nutzt MCP\n","    response = llm_app.process_user_query(query)\n","\n","    print(\"\\n[9] LLM-Anwendung nutzt den Kontext zur Beantwortung\")\n","    print(f\"\\nAntwort an den Nutzer: '{response}'\")\n","\n","\n","if __name__ == \"__main__\":\n","    run_example()"]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Fazit\n","</font></p>\n"],"metadata":{"id":"ioHO4YiNKxDE"},"id":"ioHO4YiNKxDE"},{"cell_type":"markdown","source":["\n","\n","\n","Das Model Context Protocol (MCP) stellt einen vielversprechenden Ansatz dar, um die Integration von KI-Modellen mit der Au√üenwelt zu **standardisieren und zu vereinfachen**. Durch die Definition eines gemeinsamen Protokolls und die Bereitstellung von Konzepten wie Tools und Ressourcen erm√∂glicht MCP die Entwicklung **flexiblerer, skalierbarer und wiederverwendbarer** KI-Anwendungen. Obwohl es noch Herausforderungen zu bew√§ltigen gibt, hat MCP das Potenzial, die Art und Weise, wie KI-Agenten mit Daten und Tools interagieren, **grundlegend zu ver√§ndern**."],"metadata":{"id":"79FfrbNOLf5o"},"id":"79FfrbNOLf5o"},{"cell_type":"markdown","source":["# A | Aufgabe\n","---"],"metadata":{"id":"Pzc1rzQlNV8J"},"id":"Pzc1rzQlNV8J"},{"cell_type":"markdown","source":["Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen."],"metadata":{"id":"QQUImb-86GUw"},"id":"QQUImb-86GUw"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Kalkulation\n","</font></p>"],"metadata":{"id":"gkF5wVxdx_iA"},"id":"gkF5wVxdx_iA"},{"cell_type":"markdown","source":["Gegeben ist eine Datei, die eine Reihe von Gleichungen enth√§lt.\n","Der Dateiname ist GenAI/02 data/gleichungen.txt\n","\n","**Gleichung:**    \n","41748459 - 87226336    \n","92995162 * 46769739    \n","61530438 * 56074589    \n","95329602 + 45418854    \n","412907 + 3731910    \n","...\n","\n","Verwenden Sie einen LangChain-Agenten mit einem Tool, um jede dieser Gleichungen zu berechnen, und erstellen Sie eine Datei √§hnlich dieser:\n","\n","**Ergebnisse:**  \n","41748459 - 87226336 = 45477877   \n","92995162 * 46769739 = 4349359455002718   \n","61530438 * 56074589 = 3450294021839982   \n","95329602 + 45418854 = 140748456   \n","412907 + 3731910 = 4144817   \n","... ...\n","\n"],"metadata":{"id":"Y5y-OP9OemcW"},"id":"Y5y-OP9OemcW"}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"colab":{"provenance":[],"collapsed_sections":["bf088ba7","xNBlsRoIb5T8","40c0n4k_cLvP","QO-PsEtTioiG","518d7d5a","Pzc1rzQlNV8J"]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}
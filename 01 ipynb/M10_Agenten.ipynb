{"cells":[{"cell_type":"markdown","id":"0684551f","metadata":{"id":"0684551f"},"source":["![GenAI Banner](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)\n"]},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b>\n","Agenten\n","</b></font> </br></p>\n","\n","\n","---"],"metadata":{"id":"ogH-Fzpmbueo"},"id":"ogH-Fzpmbueo"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfdhPIzcEYRG","cellView":"form","collapsed":true},"outputs":[],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  Colab-Umfeld</font> </br></p>\n","# Installierte Python Version\n","import sys\n","print(f\"Python Version: \",sys.version)\n","\n","# Installierte LangChain Bibliotheken\n","print()\n","print(\"Installierte LangChain Bibliotheken:\")\n","!pip list | grep '^langchain'\n","# Unterdrückt die \"DeprecationWarning\" von LangChain für die Memory-Funktionden\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")"],"id":"dfdhPIzcEYRG"},{"cell_type":"code","source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  SetUp API-Keys (setup_api_keys)</font> </br></p>\n","\n","def setup_api_keys():\n","    \"\"\"Konfiguriert alle benötigten API-Keys aus Google Colab userdata\"\"\"\n","    from google.colab import userdata\n","    import os\n","    from os import environ\n","\n","    # Dictionary der benötigten API-Keys\n","    keys = {\n","        'OPENAI_API_KEY': 'OPENAI_API_KEY',\n","        'SERPAPI_API_KEY': 'SERPAPI_API_KEY',\n","        'WEATHER_API_KEY': 'WEATHER_API_KEY',\n","        'HF_TOKEN': 'HF_TOKEN',\n","        'GOOGLE_API_KEY': 'GOOGLE_API_KEY',\n","        # Weitere Keys bei Bedarf\n","    }\n","\n","    # Keys in Umgebungsvariablen setzen\n","    for env_var, key_name in keys.items():\n","        environ[env_var] = userdata.get(key_name)\n","\n","    return {k: environ[k] for k in keys.keys()}\n","\n","# Verwendung\n","all_keys = setup_api_keys()\n","# Bei Bedarf einzelne Keys direkt zugreifen\n","WEATHER_API_KEY = all_keys['WEATHER_API_KEY']"],"metadata":{"cellView":"form","id":"WD3Wwr6sESX8"},"execution_count":null,"outputs":[],"id":"WD3Wwr6sESX8"},{"cell_type":"markdown","id":"bf088ba7","metadata":{"id":"bf088ba7"},"source":["# 1 | Grundlagen von KI-Agenten"]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Was sind KI-Agenten?\n","</font></p>\n","\n","\n","**Definition und Abgrenzung zu anderen KI-Systemen:**    \n","KI-Agenten sind autonome Systeme, die ihre Umgebung wahrnehmen, Entscheidungen treffen und Aktionen ausführen können, um bestimmte Ziele zu erreichen. Im Gegensatz zu einfachen KI-Modellen, die nur auf Eingabe reagieren und Ausgabe produzieren, können Agenten proaktiv handeln und aus ihren Erfahrungen lernen.\n","\n","**Autonomie, Reaktivität und zielgerichtetes Handeln:**    \n","Ein KI-Agent kann selbstständig (autonom) Entscheidungen treffen, auf Veränderungen in seiner Umgebung reagieren (Reaktivität) und hat ein oder mehrere Ziele, die sein Handeln leiten. Dabei verfolgt er langfristige Strategien statt nur auf unmittelbare Reize zu reagieren.\n","\n","**Unterschied zwischen einfachen LLM-Anwendungen und Agenten:**    \n","Einfache LLM-Anwendungen (Large Language Models) wie ein Chatbot antworten lediglich auf Anfragen, während Agenten darüber hinaus eigene Entscheidungen treffen, Tools nutzen und zielgerichtet handeln können. Ein Agent kann beispielsweise entscheiden, wann er eine Suchanfrage starten sollte, um eine Frage besser zu beantworten.\n","\n"],"metadata":{"id":"D3XA-ajxbwq_"},"id":"D3XA-ajxbwq_"},{"cell_type":"markdown","source":["# 2 | Agententypen\n","---\n"],"metadata":{"id":"xNBlsRoIb5T8"},"id":"xNBlsRoIb5T8"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Traditionelle Agententypen\n","</font></p>\n"],"metadata":{"id":"ye_O8MlLb6vv"},"id":"ye_O8MlLb6vv"},{"cell_type":"markdown","source":["Haupttypen von Agenten nach Anthropic:\n","\n","- **Workflows**: Systeme mit vordefinierten, festen Abläufen (**Codepfaden**), bei denen LLMs und Tools nach einem **festen Plan** orchestriert werden. Die Entscheidungspfade sind vorab festgelegt und ändern sich nicht dynamisch während der Ausführung[.\n","- **Agenten**: Systeme, in denen LLMs ihre Prozesse und Tool-Nutzung **dynamisch** steuern, **flexibel** auf Situationen reagieren und eigenständig Aufgaben erfüllen können. Sie können **Entscheidungen treffen, planen, lernen** und sich an neue Situationen anpassen."],"metadata":{"id":"fDa7wNwhvvxY"},"id":"fDa7wNwhvvxY"},{"cell_type":"markdown","source":["| Agententyp                       | Zuordnung Haupttyp (Anthropic) | Beschreibung                                                                            |\n","| :------------------------------- | :----------------------------- | :-------------------------------------------------------------------------------------- |\n","| Einfache Reflexionsagenten       | Workflow                       | Folgen starren Wenn-Dann-Regeln, keine Flexibilität oder dynamische Steuerung.          |\n","| Modellbasierte Reflexionsagenten | Workflow                       | Haben internes Modell, aber Abläufe sind weiterhin vorgegeben und nicht dynamisch.      |\n","| Zielbasierte Agenten             | Übergang Workflow → Agent      | Treffen Entscheidungen basierend auf Zielen, können aber noch festen Abläufen folgen.   |\n","| Nutzungsbasierte Agenten         | Agent                          | Bewerten Optionen, treffen differenzierte Entscheidungen, zeigen bereits adaptive Züge. |\n","| Lernende Agenten                 | Agent                          | Lernen aus Erfahrungen, passen Verhalten an, sind maximal flexibel und autonom.         |"],"metadata":{"id":"ZicZ97AjvmY2"},"id":"ZicZ97AjvmY2"},{"cell_type":"markdown","source":["- **Workflows** eignen sich für einfache bis mäßig komplexe Aufgaben, bei denen der Ablauf vorhersehbar und festgelegt ist. Das entspricht den einfachen und modellbasierten Reflexionsagenten, die keine oder nur sehr eingeschränkte Anpassungsfähigkeit besitzen[^3][^5].\n","- **Agenten** im Sinne von Anthropic sind Systeme, die flexibel, adaptiv und oft lernfähig sind. Sie treffen eigenständig Entscheidungen, planen und können ihr Verhalten anpassen. Das entspricht den nutzungsbasierten und lernenden Agenten aus der KI-Literatur[^2][^5].\n","- **Zielbasierte Agenten** liegen an der Schnittstelle: Sie können noch als Workflows umgesetzt werden, wenn die Zielerreichung einem festen Plan folgt. Sobald sie aber flexibel zwischen Zielen abwägen oder den Ablauf dynamisch anpassen, zählen sie zu den Agenten.\n","\n","\n","Diese Einteilung entspricht den klassischen Definitionen nach Russell & Norvig sowie Wooldridge & Jennings und wird in der Fachliteratur als Standard angesehen. Sie beschreibt die zunehmende Komplexität und Autonomie der Agenten und ist besonders geeignet, wenn die interne Funktionsweise, die Entscheidungsfindung und die Fähigkeit zur Anpassung im Vordergrund stehen.\n"],"metadata":{"id":"8rCvGZctv_mE"},"id":"8rCvGZctv_mE"},{"cell_type":"markdown","id":"8968dbad","metadata":{"id":"8968dbad"},"source":["<p><font color='black' size=\"5\">\n","Aufgabenorientierte Agententypen\n","</font></p>\n"]},{"cell_type":"markdown","source":["\n","In der Praxis ist es oft sinnvoller, Agenten nach ihrem Anwendungszweck und ihrer funktionalen Ausrichtung zu klassifizieren:\n","\n","| Agentenklasse | Primärer Zweck | Typische Anwendungen | Schlüsselmerkmale |\n","|---------------|----------------|----------------------|-------------------|\n","| **Problem-Solving Agent** | Komplexe Probleme lösen | Recherchen, analytische Aufgaben, mehrstufige Entscheidungen | - Kombiniert Reasoning mit Tool-Nutzung<br>- Explizites Denken und strukturierte Schritte<br>- Flexibilität bei verschiedenen Problemtypen |\n","| **Verbesserungs-Agent** | Inhaltsqualität optimieren | Content-Erstellung, Code-Review, Textkorrekturen | - Selbstbewertung und -kritik<br>- Iterative Verbesserung<br>- Vergleich von Versionen |\n","| **Service-Agent** | Strukturierte Dienste ausführen | API-Integrationen, Datenabfragen, automatisierte Aktionen | - Standardisierte Schnittstellen<br>- Klare Ein- und Ausgaben<br>- Zuverlässige Ausführung definierter Funktionen |\n","| **Spezialisierter Agent** | Domänenspezifische Aufgaben | Kundenservice, medizinische Beratung, rechtliche Informationen | - Tiefes Domänenwissen<br>- Angepasste Tools für spezifische Zwecke<br>- Optimiert für einen Anwendungsbereich |"],"metadata":{"id":"RfuqNgdScFy3"},"id":"RfuqNgdScFy3"},{"cell_type":"markdown","source":["\n","\n","**Wichtiger Hinweis zur Implementierung:**\n","\n","Die in diesem Kurs gezeigten Implementierungen verwenden eine flexible Toolset-Architektur, die für verschiedene Agentenklassen genutzt werden kann. Beachten Sie, dass der Unterschied zwischen den Agentenklassen weniger in der technischen Implementierung liegt als in:\n","\n","1. Der Aufgabenstellung und dem Anwendungsfall\n","2. Dem Verhaltensmuster des Agenten\n","3. Der Art, wie verschiedene Werkzeuge kombiniert werden\n","\n","In der Praxis kombinieren moderne Agenten oft Elemente aus verschiedenen konzeptionellen Architekturen, um optimale Ergebnisse zu erzielen.\n","\n"],"metadata":{"id":"EH3iRN38wJbA"},"id":"EH3iRN38wJbA"},{"cell_type":"markdown","source":["# 3 | Aufgabenorientierte Agenten\n","---\n"],"metadata":{"id":"40c0n4k_cLvP"},"id":"40c0n4k_cLvP"},{"cell_type":"markdown","source":["\n","\n","**Setup der Entwicklungsumgebung:** Installation und Konfiguration der notwendigen Bibliotheken, insbesondere LangChain und die Anbindung an ein LLM wie OpenAI.\n","\n","**ToolSet definieren:** Definition der durch die Agents benutzten Tools.\n","\n","**Implementierung eines einfachen Agenten:** Ein Beispiel für einen einfachen Agenten, der Fragen beantwortet und Berechnungen durchführt.\n","\n"],"metadata":{"id":"Nm15DiHAcNOI"},"id":"Nm15DiHAcNOI"},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","3.1 Zentrale Tool-Bibliothek\n","</font></p>"],"metadata":{"id":"QqTZZMB10aot"},"id":"QqTZZMB10aot"},{"cell_type":"code","source":["# Install\n","!uv pip install --system --prerelease allow -q langchain-openai langchain-community google-search-results wikipedia"],"metadata":{"id":"4edTQIC1c6eT"},"id":"4edTQIC1c6eT","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hier eine kurze Übersicht der nachfolgend definierten Tools:"],"metadata":{"id":"X2FTwy3TsBgQ"},"id":"X2FTwy3TsBgQ"},{"cell_type":"markdown","source":["| **Tool-/Funktion-Name**   | **Funktion**                                                                                |\n","| ------------------------- | ------------------------------------------------------------------------------------------- |\n","| `search`                  | Sucht nach Informationen im Internet über SerpAPI.                                          |\n","| `wiki`                    | Holt den ersten Wikipedia-Abschnitt zu einem Thema.                                         |\n","| `calculator`              | Führt einfache mathematische Berechnungen aus (z. B. 2+2 oder (3\\*4)/2).                    |\n","| `weather`                 | Gibt das aktuelle Wetter für einen angegebenen Ort zurück (via OpenWeather).                |\n","| `read_file`               | Liest den Inhalt einer Datei aus (Pfadangabe erforderlich).                                 |\n","| `write_file`              | Schreibt Inhalt in eine Datei im Format `pfad//inhalt`.                                     |\n","| `days_between_dates`      | Berechnet die Anzahl der Tage zwischen zwei Datumsangaben im Format `DD.MM.YYYY`.           |\n","| `extract_math_expression` | Extrahiert einen mathematischen Ausdruck aus einem Text (z. B. aus „Was ist 3+5?\" → `3+5`). |\n","\n"],"metadata":{"id":"stuD1BECr_Do"},"id":"stuD1BECr_Do"},{"cell_type":"code","source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  Definition der Tools</font> </br></p>\n","# Import\n","import os\n","import re\n","import requests\n","import json\n","from datetime import datetime\n","from langchain_core.tools import Tool\n","from langchain_community.utilities.serpapi import SerpAPIWrapper\n","from langchain_community.utilities.wikipedia import WikipediaAPIWrapper\n","\n","# Tool-Funktionen definieren\n","def get_weather(location):\n","    \"\"\"Gibt das aktuelle Wetter für einen bestimmten Ort zurück.\"\"\"\n","    url = f\"https://api.openweathermap.org/data/2.5/weather?q={location}&appid={WEATHER_API_KEY}&units=metric\"\n","    response = requests.get(url)\n","    data = response.json()\n","    return f\"Aktuelle Temperatur in {location}: {data['main']['temp']}°C, {data['weather'][0]['description']}\"\n","\n","def read_file(file_path):\n","    \"\"\"Liest den Inhalt einer Datei.\"\"\"\n","    if os.path.exists(file_path):\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            return file.read()\n","    return f\"Fehler: Datei {file_path} nicht gefunden.\"\n","\n","def write_file(args):\n","    \"\"\"Schreibt Inhalt in eine Datei. Format: 'pfad//inhalt'\"\"\"\n","    try:\n","        file_path, content = args.split(\"//\", 1)\n","        with open(file_path, 'w', encoding='utf-8') as file:\n","            file.write(content)\n","        return f\"Erfolgreich in {file_path} geschrieben.\"\n","    except Exception as e:\n","        return f\"Fehler beim Schreiben: {str(e)}\"\n","\n","def days_between_dates(date_string):\n","    \"\"\"Berechnet Tage zwischen zwei Datumsangaben im Format DD.MM.YYYY\"\"\"\n","    try:\n","        date1_str, date2_str = date_string.split(\",\")\n","        date1_str = date1_str.strip()\n","        date2_str = date2_str.strip()\n","        date1 = datetime.strptime(date1_str, \"%d.%m.%Y\").date()\n","        date2 = datetime.strptime(date2_str, \"%d.%m.%Y\").date()\n","        return str((date2 - date1).days)\n","    except ValueError:\n","        return \"Ungültiges Datumsformat. Bitte verwende DD.MM.YYYY.\"\n","\n","def extract_math_expression(text):\n","    \"\"\"Extrahiert mathematischen Ausdruck aus Text.\"\"\"\n","    match = re.search(r'([\\d+\\-*/().]+)', text)\n","    return match.group(0) if match else None\n","\n","def calculate(expression):\n","    \"\"\"Führt mathematische Berechnungen durch.\"\"\"\n","    try:\n","        math_expr = extract_math_expression(expression)\n","        if math_expr:\n","            return str(eval(math_expr))\n","        return \"Keine gültige Berechnung gefunden.\"\n","    except:\n","        return \"Fehler bei der Berechnung.\"\n","\n","# Externe APIs\n","serpapi = SerpAPIWrapper()\n","wiki = WikipediaAPIWrapper()\n","\n","# Tools-Liste, die in allen Beispielen verwendet werden kann\n","tools = [\n","    Tool(name=\"search\", func=serpapi.run, description=\"Infos aus dem Internet\"),\n","    Tool(name=\"wiki\", func=lambda x: wiki.run(x).split(\"\\n\")[0] if wiki.run(x) else \"Keine relevanten Infos gefunden.\",\n","         description=\"Fakten aus Wikipedia\"),\n","    Tool(name=\"calculator\", func=calculate, description=\"Mathematische Berechnungen\"),\n","    Tool(name=\"weather\", func=get_weather, description=\"Gibt das aktuelle Wetter für einen bestimmten Ort zurück\"),\n","    Tool(name=\"read_file\", func=read_file, description=\"Liest den Inhalt einer Datei. Input sollte der Dateipfad sein.\"),\n","    Tool(name=\"write_file\", func=write_file, description=\"Überschreibt eine Datei mit neuem Inhalt. Input sollte im Format 'pfad//inhalt' sein.\"),\n","    Tool(name=\"days_between_dates\", func=days_between_dates, description=\"Berechnet die Anzahl der Tage zwischen zwei Datumsangaben im Format DD.MM.YYYY.\")\n","]"],"metadata":{"cellView":"form","id":"js4Z7ChoD65M"},"id":"js4Z7ChoD65M","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"38d28189","metadata":{"id":"38d28189"},"source":["\n","<p><font color='black' size=\"5\">\n","3.2 Aufbau eines Agenten\n","</font></p>"]},{"cell_type":"markdown","source":["\n","\n","Erläuterung der einzelnen Funktionen:\n","\n","* `ChatOpenAI` ermöglicht die Nutzung von OpenAI's Chat-Modellen.\n","* `create_tool_calling_agent` und `AgentExecutor` erschaffen und führen einen Agenten aus, der Werkzeuge nutzen kann.\n","* `ChatPromptTemplate` und `MessagesPlaceholder` definieren die Struktur für Nachrichten und Vorlagen im Chat.\n","* `IPython.display display, Markdown` ermöglichen die Darstellung von formatiertem Text in einer Jupyter-Umgebung."],"metadata":{"id":"u2kzqAYkdN3n"},"id":"u2kzqAYkdN3n"},{"cell_type":"code","execution_count":null,"id":"2fe7f7d3","metadata":{"id":"2fe7f7d3"},"outputs":[],"source":["# Import\n","from langchain_openai import ChatOpenAI\n","from langchain.agents import create_tool_calling_agent, AgentExecutor\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from IPython.display import display, Markdown\n","\n","# Konstaten\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATUR = 0.0\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model_name=MODEL, temperature=TEMPERATUR)\n","\n","# Prompt erstellen\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent mit Zugang zu Tools. Bitte Formeln in $ Formel $ formatieren.\"),\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"human\", \"{input}\"),\n","    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n","])"]},{"cell_type":"markdown","id":"851b3358","metadata":{"id":"851b3358"},"source":["Das `agent_scratchpad` ist ein entscheidender Bestandteil des Agenten-Frameworks, insbesondere bei der Verwendung von Werkzeugen (Tools):\n","\n","* **Arbeitsbereich für den Agenten:**\n","    * Das `agent_scratchpad` dient als temporärer Arbeitsbereich, in dem der Agent seine Überlegungen, Aktionen und Beobachtungen während der Ausführung speichert.\n","    * Es ist der Ort, an dem der Agent seine \"Denkschritte\" festhält, bevor er eine endgültige Antwort gibt.\n","* **Kommunikation mit dem Sprachmodell:**\n","    * Wenn der Agent ein Werkzeug aufruft, werden die Details des Aufrufs (z. B. der Name des Werkzeugs und die Eingabeparameter) in den `agent_scratchpad` geschrieben.\n","    * Nachdem das Werkzeug ausgeführt wurde, wird das Ergebnis (die \"Beobachtung\") ebenfalls in den `agent_scratchpad` geschrieben.\n","    * Das Sprachmodell verwendet den Inhalt des `agent_scratchpad`, um zu entscheiden, welche nächsten Schritte es unternehmen soll (z. B. ein weiteres Werkzeug aufrufen oder eine endgültige Antwort geben).\n","* **Debugging und Transparenz:**\n","    * Das `agent_scratchpad` kann auch für Debugging-Zwecke nützlich sein, da er Einblicke in die Denkprozesse des Agenten bietet.\n","    * Es macht die Aktionskette des Agenten transparenter.\n","\n","Im Kontext des Prompts:\n","\n","* `MessagesPlaceholder(variable_name=\"agent_scratchpad\")` fügt den Inhalt des `agent_scratchpad` in den Prompt ein, der an das Sprachmodell gesendet wird.\n","* Dadurch kann das Sprachmodell die vorherigen Aktionen und Beobachtungen des Agenten berücksichtigen, wenn es seine nächste Antwort generiert.\n","\n","Zusammengefasst ist das `agent_scratchpad` ein dynamischer Bereich, der die Kommunikation und das Gedächtnis des Agenten während der Ausführung von Werkzeugen ermöglicht."]},{"cell_type":"code","execution_count":null,"id":"50b590a7","metadata":{"id":"50b590a7"},"outputs":[],"source":["# Agent erstellen und ausführen\n","agent = create_tool_calling_agent(llm, tools, prompt)\n","agent_executor = AgentExecutor(\n","    agent=agent,\n","    tools=tools,\n","    verbose=True,\n","    return_intermediate_steps=True\n",")"]},{"cell_type":"code","source":["# Anfrage stellen\n","response = agent_executor.invoke({\n","    \"input\": \"Welche Tools stehen zu Deiner Verfügung?\",\n","    \"chat_history\": []\n","})"],"metadata":{"id":"6ngiCqYJPONy"},"id":"6ngiCqYJPONy","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ergebnis anzeigen\n","display(Markdown(\"## 🕵️‍♀️ Agent\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response[\"output\"]))"],"metadata":{"id":"8uRcUghPPpsK"},"id":"8uRcUghPPpsK","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Weitere Fragen an den Agenten\n","</font></p>"],"metadata":{"id":"xwYyRL27Pgkd"},"id":"xwYyRL27Pgkd"},{"cell_type":"code","source":["# Anfrage stellen\n","response = agent_executor.invoke({\n","    \"input\": \"Wie viel ist 12 * 12? Und wer ist Taylor Swift?\",\n","    \"chat_history\": []\n","})"],"metadata":{"id":"x5NXER6tdp-j"},"id":"x5NXER6tdp-j","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ergebnis anzeigen\n","display(Markdown(\"## 🕵️‍♀️ Agent\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response[\"output\"]))"],"metadata":{"id":"-A3dB9z8JUEp"},"id":"-A3dB9z8JUEp","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Anfrage stellen\n","response = agent_executor.invoke({\n","    \"input\": \"Schreibe eine Notiz zum Wetter in Essen in eine Datei mit dem Namen wetter.txt\",\n","    \"chat_history\": []\n","})"],"metadata":{"id":"ATtStR4QPT-n"},"id":"ATtStR4QPT-n","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ergebnis anzeigen\n","display(Markdown(\"## 🕵️‍♀️ Agent\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response[\"output\"]))"],"metadata":{"id":"-k6W-njJPsNX"},"id":"-k6W-njJPsNX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Anfrage stellen\n","response = agent_executor.invoke({\n","    \"input\": \"Lese den Inhalt der Datei wetter.txt\",\n","    \"chat_history\": []\n","})"],"metadata":{"id":"vc-kX_CjPW3u"},"id":"vc-kX_CjPW3u","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ergebnis anzeigen\n","display(Markdown(\"## 🕵️‍♀️ Agent\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response[\"output\"]))"],"metadata":{"id":"HwBIq9y3PzsP"},"id":"HwBIq9y3PzsP","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"3ab5305c","metadata":{"id":"3ab5305c"},"source":["\n","<p><font color='black' size=\"5\">\n","3.3 Problem-Solving Agent\n","</font></p>\n"]},{"cell_type":"markdown","source":["**Kernprinzip:** Zyklischer Ablauf von Denken, Handeln und Beobachten mit explizitem Reasoning.\n","\n","**Charakteristika:**\n","\n","- Explizite Gedankenformulierung vor jeder Aktion\n","- Beobachtung der Ergebnisse nach jeder Aktion\n","- Iterative Anpassung basierend auf Beobachtungen\n","\n","**Historische Einordnung:**\n","Diese Implementierung folgt dem ReAct-Paradigma (Reasoning + Acting) und steht beispielhaft für die breitere Kategorie der Problem-Solving Agents."],"metadata":{"id":"OlTZj-0Td4mX"},"id":"OlTZj-0Td4mX"},{"cell_type":"code","execution_count":null,"id":"c87487a0","metadata":{"id":"c87487a0"},"outputs":[],"source":["# Dieses Beispiel demonstriert einen Problem-Solving Agent, der eine komplexe\n","# Recherche-Aufgabe bewältigt. Beachten Sie, wie der Agent explizite\n","# Gedankengänge formuliert und sein Reasoning offenlegt. Diese Implementierung\n","# folgt historisch dem ReAct-Paradigma (Reasoning + Acting), ist aber ein\n","# Beispiel für die breitere Kategorie der Problem-Solving Agents."]},{"cell_type":"code","source":["# Import\n","from langchain import hub\n","from langchain.agents import create_react_agent, AgentExecutor\n","from langchain_openai import ChatOpenAI\n","from datetime import datetime\n","from IPython.display import display, Markdown\n","\n","# Konstaten\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATUR = 0.0"],"metadata":{"id":"x5x00bp1wfT7"},"id":"x5x00bp1wfT7","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LLM initialisieren\n","llm = ChatOpenAI(model_name=MODEL, temperature=TEMPERATUR)"],"metadata":{"id":"LjEL1LrOEP13"},"id":"LjEL1LrOEP13","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Spezifischer Prompt aus LangChain Hub\n","prompt = hub.pull(\"hwchase17/react\")\n","for p in prompt:\n","    print(p)"],"metadata":{"id":"c_N4oLFuEUMf"},"id":"c_N4oLFuEUMf","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Agent erstellen\n","agent = create_react_agent(llm, tools, prompt)\n","\n","# Agent Executor zur Ausführung verwenden\n","agent_executor = AgentExecutor(\n","    agent=agent,\n","    tools=tools,\n","    verbose=True\n",")"],"metadata":{"id":"EPo_zJItEVy9"},"id":"EPo_zJItEVy9","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Beispiel für einen Problem-Solving Agent bei komplexen Recherche-Aufgaben\n","query = \"\"\"\n","Welcher Schauspieler hat sowohl mit Christopher Nolan als auch mit Steven Spielberg gearbeitet?\n","Wie ist der Name des Schaupielers?\n","Wenn es mehrere Schauspieler gibt, dann nehme den ersten in Deiner Liste.\n","Welche Filme waren das und\n","In welchem Jahr wurden sie jeweils veröffentlicht?\n","Beschränke Dich auf meine Fragen.\n","\"\"\"\n","\n","result = agent_executor.invoke({\"input\": query})"],"metadata":{"id":"Dgm8e9ykEW-u"},"id":"Dgm8e9ykEW-u","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ergebnis anzeigen\n","display(Markdown(\"## 🔍 Problem-Solving Agent (ReAct) Ergebnis\"))\n","display(Markdown(\"---\"))\n","display(Markdown(result[\"output\"]))"],"metadata":{"id":"y0rMhQaTJQqS"},"id":"y0rMhQaTJQqS","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"1a896193","metadata":{"id":"1a896193"},"source":["\n","<p><font color='black' size=\"5\">\n","3.4 Spezialisierter Agent\n","</font></p>"]},{"cell_type":"markdown","source":["\n","\n","**Kernprinzip:** Kombination eines zentralen Sprachmodells mit spezialisierten Modulen für verschiedene Domänen.\n","\n","**Charakteristika:**\n","\n","- Modularität: Verschiedene Tools für spezialisierte Aufgaben\n","- Routing: Weiterleitung von Teilaufgaben an die passenden Tools\n","- Integration: Zusammenführung der Ergebnisse in eine kohärente Antwort\n","\n","**Historische Einordnung:**\n","Diese Implementierung folgt dem MRKL-Paradigma (Modular Reasoning, Knowledge and Language) und repräsentiert die breitere Kategorie der Spezialisierten Agents."],"metadata":{"id":"LKNWBkd9fKJP"},"id":"LKNWBkd9fKJP"},{"cell_type":"code","execution_count":null,"id":"6a95f5d1","metadata":{"id":"6a95f5d1"},"outputs":[],"source":["# Dieses Beispiel demonstriert einen Spezialisierten Agent, der verschiedene\n","# Aspekte einer Reiseplanung bewältigt. Die Implementierung zeigt, wie ein\n","# Agent verschiedene Tools für unterschiedliche Teilaufgaben einsetzen kann.\n","# Historisch folgt dieser Ansatz dem MRKL-Paradigma (Modular Reasoning, Knowledge\n","# and Language), repräsentiert aber die breitere Kategorie der Spezialisierten Agents.\n","# Wir können hier denselben AgentExecutor weiterverwenden, aber mit einer anderen Aufgabe"]},{"cell_type":"code","execution_count":null,"id":"2207afda","metadata":{"id":"2207afda"},"outputs":[],"source":["# Beispiel für einen Spezialisierten Domänen-Agent (basierend auf MRKL)\n","query = \"\"\"\n","Ich plane eine Reise nach Rom für die erste Juniwoche.\n","Wie wird das Wetter dort sein?\n","Welche historischen Stätten sollte ich besuchen?\n","Wie hoch sind die Eintrittsgelder in EURO?\n","Welche lokalen Gerichte sind empfehlenswert?\n","Wieviele Tage sind bis zur Abreise? Heute ist der 03.05.2025.\n","Beschränke Dich auf meine Fragen.\n","\"\"\"\n","\n","# Agent ausführen\n","response = agent_executor.invoke({\"input\": query})"]},{"cell_type":"code","source":["# Ergebnis anzeigen\n","display(Markdown(\"## 🧩 Spezialisierter Agent Ergebnis\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response[\"output\"]))"],"metadata":{"id":"9KgMzcACHeMa"},"id":"9KgMzcACHeMa","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"0c10d657","metadata":{"id":"0c10d657"},"source":["\n","<p><font color='black' size=\"5\">\n","3.5 Verbesserungs-Agent\n","</font></p>"]},{"cell_type":"markdown","source":["\n","\n","**Kernprinzip:** Selbstbewertung und -verbesserung durch kritische Analyse der eigenen Outputs.\n","\n","**Charakteristika:**\n","\n","- Self-criticism: Kritische Bewertung der eigenen Ausgaben\n","- Verbesserungszyklen: Mehrfache Iteration zur Qualitätsverbesserung\n","- Meta-Kognition: \"Denken über das Denken\"\n","\n","**Historische Einordnung:**\n","Diese Implementierung basiert auf dem Konzept der Reflective Agents und repräsentiert die breitere Kategorie der Verbesserungs-Agents."],"metadata":{"id":"Z9vj7rZwgKBx"},"id":"Z9vj7rZwgKBx"},{"cell_type":"code","execution_count":null,"id":"7190f5ac","metadata":{"id":"7190f5ac"},"outputs":[],"source":["# Import\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import PromptTemplate\n","from langchain.schema.output_parser import StrOutputParser\n","from IPython.display import display, Markdown"]},{"cell_type":"code","source":["# Prompts für die drei Phasen des Reflection-Prozesses\n","answer_prompt = PromptTemplate.from_template(\"Beantworte: {question}\")\n","reflection_prompt = PromptTemplate.from_template(\"\"\"\n","Bewerte:\n","Frage: {question}\n","Antwort: {answer}\n","\n","Fehler/Verbesserungen?\"\"\")\n","improvement_prompt = PromptTemplate.from_template(\"\"\"\n","Frage: {question}\n","Antwort: {answer}\n","Reflexion: {reflection}\n","\n","Verbesserte Antwort:\"\"\")"],"metadata":{"id":"_gZdqHRxhj20"},"id":"_gZdqHRxhj20","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LCEL-Chains für die drei Phasen\n","\n","# LLMs mit unterschiedlichen Einstellungen\n","answer_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)  # Kreativere Erstantwort\n","reflection_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)  # Kritische Bewertung\n","\n","# Chains answer, reflection, improvement\n","answer_chain = answer_prompt | answer_llm | StrOutputParser()\n","\n","reflection_chain = {\n","    \"question\": lambda x: x[\"question\"],\n","    \"answer\": lambda x: x[\"answer\"]\n","} | reflection_prompt | reflection_llm | StrOutputParser()\n","\n","improvement_chain = {\n","    \"question\": lambda x: x[\"question\"],\n","    \"answer\": lambda x: x[\"initial_answer\"],\n","    \"reflection\": lambda x: x[\"reflection\"]\n","} | improvement_prompt | answer_llm | StrOutputParser()"],"metadata":{"id":"qj2oqncUhl0N"},"id":"qj2oqncUhl0N","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"38e329c9","metadata":{"id":"38e329c9"},"outputs":[],"source":["# Ausführung der Chains / des Verbesserungs-Agents\n","question = \"Was sind die Hauptursachen des Klimawandels?\"\n","\n","initial_answer = answer_chain.invoke({\"question\": question})\n","\n","reflection = reflection_chain.invoke({\"question\": question, \"answer\": initial_answer})\n","\n","improved_answer = improvement_chain.invoke({\n","    \"question\": question,\n","    \"initial_answer\": initial_answer,\n","    \"reflection\": reflection\n","})"]},{"cell_type":"code","source":["# Ergebnisse anzeigen\n","display(Markdown(\"## 🔍 Verbesserungs-Agent (Reflective) Prozess\"))\n","display(Markdown(\"---\"))\n","\n","display(Markdown(\"### Initiale Antwort:\"))\n","display(Markdown(initial_answer))\n","display(Markdown(\"---\"))\n","\n","display(Markdown(\"### Selbstreflexion:\"))\n","display(Markdown(reflection))\n","display(Markdown(\"---\"))\n","\n","display(Markdown(\"### Verbesserte Antwort:\"))\n","display(Markdown(improved_answer))\n","display(Markdown(\"---\"))"],"metadata":{"id":"1kPASGDrhTcH"},"id":"1kPASGDrhTcH","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"aab8597f","metadata":{"id":"aab8597f"},"source":["\n","<p><font color='black' size=\"5\">\n","3.6 Service-Agent\n","</font></p>\n"]},{"cell_type":"markdown","source":["\n","\n","**Kernprinzip:** Standardisierte Schnittstellen für den Aufruf externer Funktionen und Dienste.\n","\n","**Charakteristika:**\n","- Direkte Integration: Nahtlose Verbindung zu externen Funktionen\n","- Standardisierte Schnittstellen: Klare Definitionen für Ein- und Ausgaben\n","- Spezialisierte Aktionen: Fokus auf spezifische Funktionalitäten\n","\n","**Historische Einordnung:**\n","Diese Implementierung folgt dem Function-Calling Paradigma und repräsentiert die breitere Kategorie der Service-Agents."],"metadata":{"id":"ZecabM9ShuJq"},"id":"ZecabM9ShuJq"},{"cell_type":"code","execution_count":null,"id":"3ac21b5e","metadata":{"id":"3ac21b5e"},"outputs":[],"source":["# Import\n","from langchain_openai import ChatOpenAI\n","from langchain.agents import AgentExecutor, create_openai_functions_agent\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from IPython.display import display, Markdown\n","\n","# Konstaten\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATUR = 0.0\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model=MODEL, temperature=TEMPERATUR)"]},{"cell_type":"code","source":["# Prompt für den Service-Agent\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent, der Informationen über das Wetter und andere Fakten liefert.\"),\n","    (\"human\", \"{input}\"),\n","    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n","])"],"metadata":{"id":"ASLC1AWviBdw"},"id":"ASLC1AWviBdw","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function-Calling Agent (fokussiert auf direkten Funktionsaufruf)\n","function_agent = create_openai_functions_agent(llm, tools, prompt)\n","\n","service_agent_executor = AgentExecutor.from_agent_and_tools(\n","    agent=function_agent,\n","    tools=tools,\n","    verbose=True\n",")"],"metadata":{"id":"mMuupfUJiJVc"},"id":"mMuupfUJiJVc","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"e490212e","metadata":{"id":"e490212e"},"outputs":[],"source":["# Typisches Service-Agent Beispiel\n","location = \"Berlin\"\n","response = service_agent_executor.invoke({\n","    \"input\": f\"Wie ist das Wetter in {location}? Und was gibt es für allgemeine Informationen zu {location}?\",\n","})\n","\n","# Ergebnis anzeigen\n","display(Markdown(\"## 🌡️ Service-Agent (Function-Calling) Ergebnis\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response[\"output\"]))"]},{"cell_type":"markdown","id":"038b1632","metadata":{"id":"038b1632"},"source":["\n","<p><font color='black' size=\"5\">\n","3.7 Vergleich der Agenten\n","</font></p>"]},{"cell_type":"markdown","id":"de011993","metadata":{"id":"de011993"},"source":["|Aspekt|Problem-Solving Agent|Verbesserungs-Agent|Service-Agent|Spezialisierter Agent|\n","|---|---|---|---|---|\n","|Hauptfokus|Komplexe Probleme lösen|Inhaltsqualität optimieren|Dienste ausführen|Domänenexpertise|\n","|Arbeitsweise|Strukturierte Schritte mit Reasoning|Iterative Selbstbewertung|Standardisierte Schnittstellen|Spezialisierte Tools für ein Thema|\n","|Typischer Anwendungsfall|Recherche, Analyse, Entscheidungsfindung|Content-Erstellung, Review, QA|API-Interaktionen, Datenabfragen|Spezialisierte Beratung, Expertensysteme|\n","|Flexibilität|Hoch für verschiedene Probleme|Mittel, fokussiert auf Verbesserung|Begrenzt auf definierte Funktionen|Hoch innerhalb der Domäne|\n","|Transparenz|Hoch durch explizite Gedanken|Sehr hoch durch Selbsterklärung|Mittel|Variiert je nach Implementierung|\n","|Komplexitätsbewältigung|Vielseitig für komplexe Probleme|Gut für Qualitätsverbesserung|Gut für definierte Aufgaben|Hervorragend in der Fachdomäne|\n","|Implementation|Strukturierte Reasoning-Prozesse|Feedback-Schleifen|API-Definitionen|Domänenspezifische Tools|\n","\n","Diese Tabelle bietet einen vollständigen Vergleich der aufgabenorientierten Agententypen in allen relevanten Aspekten und hilft bei der Auswahl des geeigneten Agententyps für bestimmte Anwendungsfälle.\n"]},{"cell_type":"markdown","id":"448fb547","metadata":{"id":"448fb547"},"source":["\n","<p><font color='black' size=\"5\">\n","3.8 Kombinationsmöglichkeiten\n","</font></p>"]},{"cell_type":"markdown","source":["\n","\n","Die Stärke moderner KI-Agenten liegt oft in der Kombination verschiedener Ansätze:\n","\n","- **Problem-Solving + Service**: Verbindet strukturierte API-Aufrufe mit transparentem Reasoning\n","- **Spezialisiert + Verbesserung**: Domänenspezifische Systeme mit Selbstverbesserungsfähigkeit\n","- **Service + Spezialisiert**: Standardisierte Schnittstellen für modulare Systeme\n","- **Problem-Solving + Verbesserung**: Transparentes Reasoning mit iterativer Qualitätsverbesserung\n","\n","In der Praxis implementieren die meisten Agenten eine Mischung dieser Ansätze, angepasst an spezifische Anwendungsfälle.\n"],"metadata":{"id":"8RQefWTGinK6"},"id":"8RQefWTGinK6"},{"cell_type":"markdown","source":["# 4 | Erweiterte Konzepte\n","---\n","\n","Nach der Darstellung der grundlegenden Architekturen folgen nun erweiterte Konzepte, die auf diesen Architekturen aufbauen oder sie kombinieren.\n"],"metadata":{"id":"QO-PsEtTioiG"},"id":"QO-PsEtTioiG"},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","4.1 Einfache Planung\n","</font></p>"],"metadata":{"id":"iLAXNTM5iq-P"},"id":"iLAXNTM5iq-P"},{"cell_type":"markdown","source":["\n","\n","Planung und Zielverfolgung ist kein eigenständiger Architekturtyp, sondern ein Anwendungskonzept, das auf den grundlegenden Architekturen aufbaut.\n","\n","**Kombination:** Meist basierend auf Problem-Solving oder Verbesserungs-Agents"],"metadata":{"id":"4bc63yCTitXi"},"id":"4bc63yCTitXi"},{"cell_type":"code","execution_count":null,"id":"80952963","metadata":{"id":"80952963"},"outputs":[],"source":["# Import\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import PromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from IPython.display import display, Markdown\n","\n","# Konstaten\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATUR = 0.0\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model_name=MODEL, temperature=TEMPERATUR)"]},{"cell_type":"code","source":["# Chain-of-Thought Prompt erstellen\n","cot_prompt = PromptTemplate(\n","    input_variables=[\"problem\"],\n","    template=\"\"\"\n","    Zerlege das folgende Problem in kleinere Teilschritte und löse es Schritt für Schritt:\n","\n","    Problem: {problem}\n","\n","    Denke schrittweise:\n","    1. Welche Teilprobleme müssen gelöst werden?\n","    2. Wie löse ich jeden Teilschritt?\n","    3. Wie kombiniere ich die Teillösungen zur Gesamtlösung?\n","    \"\"\"\n",")"],"metadata":{"id":"t0T7KmNXi6B6"},"id":"t0T7KmNXi6B6","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LCEL-Pipeline erstellen und ausführen\n","planning_chain = (\n","    {\"problem\": RunnablePassthrough()}\n","    | cot_prompt\n","    | llm\n","    | StrOutputParser()\n",")"],"metadata":{"id":"Ge12gfRGjFZu"},"id":"Ge12gfRGjFZu","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Beispielanwendung\n","complex_problem = \"\"\"\n","Berechne die Gesamtkosten für einen Einkauf von 3 Büchern zu je 12,99 €,\n","2 Notizbüchern zu je 4,50 € und einem Stift für 2,25 €,\n","wenn auf Bücher 7% Mehrwertsteuer und auf andere Artikel 19% Mehrwertsteuer anfällt.\n","Verwende für Formeln das Format $ Formel $.\n","\"\"\"\n","\n","solution = planning_chain.invoke(complex_problem)"],"metadata":{"id":"D4nx3S7SjIID"},"id":"D4nx3S7SjIID","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ergebnis anzeigen\n","display(Markdown(\"## 📝 Einfache Planung mit Chain-of-Thought\"))\n","display(Markdown(\"---\"))\n","display(Markdown(solution))"],"metadata":{"id":"Nu_33eAijOsF"},"id":"Nu_33eAijOsF","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"0f6e66d7","metadata":{"id":"0f6e66d7"},"source":["\n","<p><font color='black' size=\"5\">\n","4.2 Fortgeschrittene Planung\n","</font></p>"]},{"cell_type":"markdown","source":["\n","\n","**Kombination:** Oft Spezialisierte + Verbesserungs-Agents für hierarchische Planung"],"metadata":{"id":"h-sXqpZojnju"},"id":"h-sXqpZojnju"},{"cell_type":"code","execution_count":null,"id":"ea2d5596","metadata":{"id":"ea2d5596"},"outputs":[],"source":["# Import\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import PromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from IPython.display import display, Markdown\n","\n","# Konstaten\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATUR = 0.0\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model_name=MODEL, temperature=TEMPERATUR)"]},{"cell_type":"code","source":["# Funktionen\n","def create_planner_chain():\n","    \"\"\"Erstellt eine Chain für die hierarchische Planung.\"\"\"\n","    planer_prompt = PromptTemplate.from_template(\n","        \"\"\"\n","        Erstelle einen hierarchischen Plan, um das folgende Ziel zu erreichen:\n","\n","        Ziel: {objective}\n","\n","        Teile den Plan in:\n","        1. Strategische Ziele (hohe Ebene)\n","        2. Taktische Schritte (mittlere Ebene)\n","        3. Konkrete Aktionen (detaillierte Ebene)\n","\n","        Für jede konkrete Aktion gib an, welche Tools oder Ressourcen benötigt werden.\n","        \"\"\"\n","    )\n","    return ({\"objective\": RunnablePassthrough()} | planer_prompt | llm | StrOutputParser())\n","\n","def create_execution_chain():\n","    \"\"\"Erstellt eine Chain für die Anpassung des Plans basierend auf Feedback.\"\"\"\n","    execution_prompt = PromptTemplate.from_template(\n","        \"\"\"\n","        Hier ist der aktuelle Plan:\n","\n","        {plan}\n","\n","        Basierend auf dem folgenden Feedback, passe den Plan an:\n","\n","        Feedback: {feedback}\n","\n","        Überarbeiteter Plan:\n","        \"\"\"\n","    )\n","    return ({\"plan\": RunnablePassthrough(), \"feedback\": RunnablePassthrough()}\n","            | execution_prompt | llm | StrOutputParser())"],"metadata":{"id":"iIX6zAdpjxCz"},"id":"iIX6zAdpjxCz","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Planungsketten erstellen\n","planner_chain = create_planner_chain()\n","execution_chain = create_execution_chain()"],"metadata":{"id":"EkLsaa5lj-VN"},"id":"EkLsaa5lj-VN","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"619c9e45","metadata":{"id":"619c9e45"},"outputs":[],"source":["# Beispiel ausführen\n","objective = \"Organisiere eine zweitägige Konferenz für 100 Teilnehmer zum Thema KI-Ethik\"\n","initial_plan = planner_chain.invoke(objective)"]},{"cell_type":"code","source":["# Feedback geben und Plan anpassen\n","feedback = \"Der Budgetrahmen wurde auf 15.000 € reduziert, und wir müssen die Konferenz auf einen Tag verkürzen.\"\n","revised_plan = execution_chain.invoke({\"plan\": initial_plan, \"feedback\": feedback})"],"metadata":{"id":"sY1rJ1JqkHct"},"id":"sY1rJ1JqkHct","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ergebnisse anzeigen\n","display(Markdown(\"## 📔 Hierarchische Planung\"))\n","display(Markdown(\"---\"))\n","\n","display(Markdown(\"### Initialer Plan\"))\n","display(Markdown(initial_plan))\n","display(Markdown(\"---\"))\n","\n","display(Markdown(\"### Überarbeiteter Plan\"))\n","display(Markdown(revised_plan))\n","display(Markdown(\"---\"))"],"metadata":{"id":"m9Y2twmZkMXW"},"id":"m9Y2twmZkMXW","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"79b04997","metadata":{"id":"79b04997"},"source":["\n","<p><font color='black' size=\"5\">\n","4.3 Zielorientierter Agent\n","</font></p>"]},{"cell_type":"markdown","source":["\n","\n","**Kombination:** Service-Agent + Problem-Solving Agent für zielgerichtete Aktionen"],"metadata":{"id":"lX0PHQyC1EpE"},"id":"lX0PHQyC1EpE"},{"cell_type":"code","source":["# Import\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.messages import AIMessage, HumanMessage\n","from langchain.agents import AgentExecutor, create_openai_functions_agent\n","from langchain.agents.format_scratchpad import format_to_openai_functions\n","\n","# Konstaten\n","MODEL = \"gpt-4o\"\n","TEMPERATUR = 0.0\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model_name=MODEL, temperature=TEMPERATUR)\n","\n","# Chat-Verlauf initialisieren\n","chat_history = []"],"metadata":{"id":"Tbyc_g3Tmk_V"},"id":"Tbyc_g3Tmk_V","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prompt erstellen\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher KI-Assistent. Antworte auf Deutsch.\"),\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"human\", \"{input}\"),\n","    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n","])"],"metadata":{"id":"AHR5vRnFpAa1"},"id":"AHR5vRnFpAa1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Agent erstellen\n","agent = create_openai_functions_agent(llm, tools, prompt)\n","\n","# Agent-Executor konfigurieren\n","agent_executor = AgentExecutor(\n","    agent=agent,\n","    tools=tools,\n","    verbose=True,\n","    handle_parsing_errors=True\n",")"],"metadata":{"id":"EfWPk4HRpCc4"},"id":"EfWPk4HRpCc4","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Funktion für die Ausführung einer Konversation\n","def run_conversation(input_text):\n","    # Agent ausführen mit Zugriff auf den Chat-Verlauf\n","    result = agent_executor.invoke({\n","        \"input\": input_text,\n","        \"chat_history\": chat_history\n","    })\n","\n","    # Chat-Verlauf aktualisieren\n","    chat_history.extend([\n","        HumanMessage(content=input_text),\n","        AIMessage(content=result[\"output\"])\n","    ])\n","\n","    return result[\"output\"]"],"metadata":{"id":"fizbyEaopEox"},"id":"fizbyEaopEox","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Komplexe Anfrage\n","complex_goal = \"\"\"\n","Finde heraus, wer der aktuelle Bundeskanzler von Deutschland ist,\n","wann er geboren wurde und erstelle eine kurze Zusammenfassung\n","seiner politischen Karriere.\n","\"\"\""],"metadata":{"id":"kgE_cCnipIup"},"id":"kgE_cCnipIup","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Agent mit komplexer Anfrage aufrufen\n","result = run_conversation(complex_goal)"],"metadata":{"collapsed":true,"id":"s922eEBVpNGP"},"id":"s922eEBVpNGP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ergebnis anzeigen\n","display(Markdown(\"## 🔎 Zielorientierter Agent\"))\n","display(Markdown(\"---\"))\n","display(Markdown(result))"],"metadata":{"id":"usDCcj2RpLUl"},"id":"usDCcj2RpLUl","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Komplexe Anfrage\n","complex_goal = \"\"\"\n","Finde heraus, ob er verheiratet ist und mit wem?\n","Stelle Informationen aus ihrer Biografie zusammen.\n","\"\"\""],"metadata":{"id":"wG_ZEchDpuA6"},"id":"wG_ZEchDpuA6","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Agent mit komplexer Anfrage aufrufen\n","result = run_conversation(complex_goal)"],"metadata":{"id":"ifwUc1P4p1Kn"},"id":"ifwUc1P4p1Kn","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ergebnis anzeigen\n","display(Markdown(\"## 🔎 Zielorientierter Agent\"))\n","display(Markdown(\"---\"))\n","display(Markdown(result))"],"metadata":{"id":"rRECiAJ5p5BR"},"id":"rRECiAJ5p5BR","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"2f064c9f","metadata":{"id":"2f064c9f"},"source":["\n","<p><font color='black' size=\"5\">\n","4.4 Kollaborative Analyse\n","</font></p>"]},{"cell_type":"markdown","source":["\n","\n","**Kombination:** Spezialisierte Agenten mit verschiedenen Schwerpunkten"],"metadata":{"id":"s6pdGMFMqUJE"},"id":"s6pdGMFMqUJE"},{"cell_type":"code","execution_count":null,"id":"6d518413","metadata":{"id":"6d518413"},"outputs":[],"source":["# Import\n","from IPython.display import display, Markdown\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import PromptTemplate\n","from langchain_core.output_parsers import StrOutputParser"]},{"cell_type":"code","source":["# Pipelines mit optimierten Modellen und Temperaturen\n","\n","research_pipeline = PromptTemplate.from_template(\n","    \"\"\"Du bist ein Recherche-Spezialist. Sammle Informationen zum Thema: {topic}\"\"\"\n",") | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)  # Gute Balance zwischen Kreativität und Fakten\n","\n","\n","critique_pipeline = PromptTemplate.from_template(\n","    \"\"\"Du bist ein kritischer Analyst. Bewerte folgende Recherche: {research}\"\"\"\n",") | ChatOpenAI(model=\"gpt-4o\", temperature=0)  # Maximale Präzision für Analyse\n","\n","\n","synthesis_pipeline = PromptTemplate.from_template(\n","    \"\"\"Du bist ein Informationssynthetisierer. Erstelle eine Zusammenfassung:\n","    Recherche: {research}\n","    Kritik: {critique}\"\"\"\n",") | ChatOpenAI(model=\"gpt-4o\", temperature=0.2)  # Verbesserte Qualität für komplexe Zusammenfassung\n","\n","\n","decision_pipeline = PromptTemplate.from_template(\n","    \"\"\"Du bist ein Entscheidungsträger. Basierend auf: {synthesis}\n","    Beantworte: {question}\"\"\"\n",") | ChatOpenAI(model=\"gpt-4o\", temperature=0)  # Konsistente Entscheidungsfindung"],"metadata":{"id":"NSPsC8jMqZN4"},"id":"NSPsC8jMqZN4","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Schrittweise Analyse definieren\n","def collaborative_analysis(topic, question):\n","    \"\"\"Kollaborative Analyse mit LCEL-Pipelines.\"\"\"\n","    # Prozess ausführen\n","    research = research_pipeline.invoke({\"topic\": topic})\n","    display(Markdown(\"### ✅ Recherche abgeschlossen\"))\n","\n","    critique = critique_pipeline.invoke({\"research\": research})\n","    display(Markdown(\"### ✅ Kritische Analyse abgeschlossen\"))\n","\n","    synthesis = synthesis_pipeline.invoke({\"research\": research, \"critique\": critique})\n","    display(Markdown(\"### ✅ Synthese abgeschlossen\"))\n","\n","    decision = decision_pipeline.invoke({\"synthesis\": synthesis, \"question\": question})\n","    display(Markdown(\"### ✅ Entscheidung getroffen\"))\n","\n","    return {\n","        \"research\": research,\n","        \"critique\": critique,\n","        \"synthesis\": synthesis,\n","        \"decision\": decision\n","    }"],"metadata":{"id":"_nMydwn-rEwp"},"id":"_nMydwn-rEwp","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"2591decd","metadata":{"id":"2591decd"},"outputs":[],"source":["# Beispielanwendung\n","topic = \"Künstliche Intelligenz in der Gesundheitsversorgung\"\n","question = \"Sollten Krankenhäuser KI-Systeme zur Diagnoseunterstützung einsetzen?\""]},{"cell_type":"code","source":["# Multi-Agenten-Analyse ausführen\n","display(Markdown(\"## 👥 Multi-Agenten-System: Kollaborative Analyse\"))\n","display(Markdown(\"---\"))\n","results = collaborative_analysis(topic, question)"],"metadata":{"id":"X7jTRqujrZ7L"},"id":"X7jTRqujrZ7L","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Finale Entscheidung anzeigen\n","display(Markdown(f\"## 📑 Finale Entscheidung\"))\n","display(Markdown(\"---\"))\n","\n","for step, txt  in results.items():\n","    display(Markdown(f\"### {step.upper()}\"))\n","    display(Markdown(f\"{txt.content}\"))\n","    display(Markdown(\"---\"))"],"metadata":{"id":"mpQP8H_7rcg7"},"id":"mpQP8H_7rcg7","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"518d7d5a","metadata":{"id":"518d7d5a"},"source":["# 5 | Model Context Protocol (MCP)\n","---\n"]},{"cell_type":"markdown","source":["\n","**Wichtige Ressourcen:**\n","\n","[Anthropic MCP](https://www.anthropic.com/news/model-context-protocol)\n","\n","[OpenAI MCP](https://openai.github.io/openai-agents-python/mcp/)\n","\n","[MCPServer](https://github.com/modelcontextprotocol/servers)\n","\n"],"metadata":{"id":"w6nFlzHDwuJy"},"id":"w6nFlzHDwuJy"},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","5.1 Was ist MCP?\n","</font></p>"],"metadata":{"id":"zs4LhIqxwqMP"},"id":"zs4LhIqxwqMP"},{"cell_type":"markdown","source":["\n","Ein Protokoll ist ein Regelwerk, das bestimmt, wie zwei Systeme miteinander kommunizieren. Protokolle regeln die Datenübertragung in Computernetzwerken, bei der Internetkommunikation und zwischen Softwaresystemen.\n","\n","**Zum Beispiel:**\n","\n","+ HTTP (Hypertext Transfer Protocol): Ermöglicht Websites die Kommunikation mit Browsern.\n","+ TCP/IP (Transmission Control Protocol/Internetprotokoll): Definiert, wie Datenpakete im Internet geroutet werden.\n","+ JSON-RPC (Aufrufen von Remoteprozeduren): Ein Protokoll, das den Datenaustausch im JSON-Format ermöglicht.    \n","\n","Das **Model Context Protocol (MCP)** ist ein offenes Protokoll, das es großen Sprachmodellen (LLMs) ermöglicht, sich auf standardisierte Weise in externe Datenquellen und Tools zu integrieren. Dieses von Anthropic entwickelte Protokoll macht es KI-Modellen leicht, nahtlos mit einer Vielzahl von Tools und Datenquellen zusammenzuarbeiten.\n","\n","Es soll die Interaktion von KI-Modellen, insbesondere Large Language Models (LLMs) und autonomen Agenten, mit externen Datenquellen und Tools zu **standardisieren und zu vereinfachen**. Ziel ist es, einen **einheitlichen Rahmen** zu schaffen, der es KI-Agenten ermöglicht, auf strukturierte Daten aus verschiedenen Quellen wie Datenbanken, APIs, Cloud-Speicher und Unternehmensanwendungen auf standardisierte Weise zuzugreifen, diese zu verarbeiten und darauf zu reagieren, **ohne dass für jede Quelle spezifische API-Integrationen erforderlich sind**.\n","\n"],"metadata":{"id":"55pocnEnww4g"},"id":"55pocnEnww4g"},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","5.2 Warum wurde MCP entwickelt?\n","</font></p>"],"metadata":{"id":"xQnjvh7HwyuL"},"id":"xQnjvh7HwyuL"},{"cell_type":"markdown","source":["\n","\n","Die Notwendigkeit von MCP ergibt sich aus den **Ineffizienzen und Herausforderungen** aktueller KI-API-Interaktionen. Derzeit ist der Aufbau von KI-Agenten, die Daten aus verschiedenen Quellen abrufen, **fragmentiert, repetitiv und schwer zu skalieren**. Jedes Tool spricht seine eigene Sprache und erfordert **individuelle Integrationen**. MCP zielt darauf ab, diese Komplexität zu reduzieren und den **Entwicklungsaufwand zu minimieren**.\n","\n"],"metadata":{"id":"NevFl-6xw0lH"},"id":"NevFl-6xw0lH"},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","5.3 Wie funktioniert MCP?\n","</font></p>"],"metadata":{"id":"dsTGqA4Zw3io"},"id":"dsTGqA4Zw3io"},{"cell_type":"markdown","source":["\n","\n","MCP basiert auf einer **Client-Server-Architektur**.\n","\n","*   **MCP-Clients** sind typischerweise KI-Agenten, Anwendungen oder Systeme, die strukturierte Daten benötigen. Beispiele hierfür sind Claude Desktop, Cursor, Windsurf und Frameworks wie Langchain und Pydantic AI.\n","*   **MCP-Server** fungieren als **Vermittler**, die Daten von verschiedenen APIs, Datenbanken oder Unternehmenssystemen abrufen und diese in einem **einheitlichen Format** an die Clients zurückgeben.\n","\n","Der Interaktionsprozess folgt einem strukturierten **Anfrage-Antwort-Zyklus**:\n","\n","1.  Der KI-Agent (Client) sendet eine **strukturierte Anfrage** an den MCP-Server, in der die benötigten Daten oder die auszuführende Aktion in einem standardisierten Format definiert sind.\n","2.  Der MCP-Server **verarbeitet** diese Anfrage, authentifiziert sie, prüft die Berechtigungen und ermittelt, welche externen Systeme abgefragt werden müssen.\n","3.  Die eigentlichen **Datenabfragen** an die verschiedenen Quellen können parallel erfolgen.\n","4.  Die **Antworten** der verschiedenen Quellen werden vom MCP-Server in einem **einheitlichen, strukturierten Format standardisiert**, das für KI-Modelle leicht zu verarbeiten ist.\n","\n","Ein wesentliches Konzept von MCP ist die **Reflection**. Dies bedeutet, dass ein MCP-Client einen MCP-Server nach seinen **verfügbaren Tools und Ressourcen** fragen kann, ohne vorherige Kenntnisse dieser Schnittstellen zu benötigen.\n","\n","[Interaktive Visualisierung des MCP-Prozesses](https://claude.site/artifacts/1ba26344-3819-427d-9080-98381785f8df)\n","\n"],"metadata":{"id":"XI9msU-Dw5i1"},"id":"XI9msU-Dw5i1"},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","5.4 Kernkonzepte\n","</font></p>\n"],"metadata":{"id":"sfl6-F7xw8OR"},"id":"sfl6-F7xw8OR"},{"cell_type":"markdown","source":["\n","*   **Tools:** Funktionen, die das Modell nutzen kann, um Aktionen durchzuführen (z. B. Websuchen, Datenbankabfragen).\n","*   **Ressourcen:** Anhänge oder Daten, die dem Modell zur Verfügung gestellt werden (z. B. Dateien, Datenbankinhalte).\n","*   **Prompts:** Vorlagen, die Clients für Anfragen an das Modell verwenden können.\n","\n"],"metadata":{"id":"CwJNrxKtw_rE"},"id":"CwJNrxKtw_rE"},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","5.5 Vorteile von MCP\n","</font></p>"],"metadata":{"id":"bYE_S93Ow-at"},"id":"bYE_S93Ow-at"},{"cell_type":"markdown","source":["\n","*   **Vereinfachte Integrationen** und **reduzierte Komplexität**.\n","*   **Verbesserte Skalierbarkeit** und **Wiederverwendbarkeit** von Integrationen.\n","*   **Erhöhte Interoperabilität** zwischen KI-Modellen und externen Systemen.\n","*   **Zeitersparnis für Entwickler** durch weniger benutzerdefinierte API-Implementierungen.\n","*   Potenzial für eine **zentrale Authentifizierung** (zukünftig)."],"metadata":{"id":"_ONCUz4txA0z"},"id":"_ONCUz4txA0z"},{"cell_type":"markdown","source":["[Interaktive Visualisierung](https://claude.site/artifacts/1ba26344-3819-427d-9080-98381785f8df)"],"metadata":{"id":"OeTr_m93LVEB"},"id":"OeTr_m93LVEB"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Code-Beispiel zur Interaktion\n","</font></p>"],"metadata":{"id":"k6reCVSRO5WY"},"id":"k6reCVSRO5WY"},{"cell_type":"code","execution_count":null,"id":"3c002ac7","metadata":{"lines_to_next_cell":0,"id":"3c002ac7"},"outputs":[],"source":["import json\n","import uuid\n","from typing import Dict, Any\n","\n","class MCPClient:\n","    \"\"\"Einfacher MCP-Client für die Kommunikation mit MCP-Servern.\"\"\"\n","\n","    def __init__(self, client_id: str, api_key: str):\n","        self.client_id = client_id\n","        self.api_key = api_key\n","        self.headers = {\n","            \"Authorization\": f\"Bearer {api_key}\",\n","            \"Client-ID\": client_id\n","        }\n","\n","    def create_request(self, tool_name: str, parameters: Dict[str, Any]) -> Dict[str, Any]:\n","        \"\"\"Erstellt eine strukturierte MCP-Anfrage.\"\"\"\n","        request_data = {\n","            \"protocol_version\": \"1.0\",\n","            \"request_id\": str(uuid.uuid4()),\n","            \"tool\": {\n","                \"name\": tool_name,\n","                \"parameters\": parameters\n","            }\n","        }\n","        return request_data\n","\n","    def send_request(self, request_data: Dict[str, Any]) -> Dict[str, Any]:\n","        \"\"\"Sendet eine MCP-Anfrage an den MCP-Server.\"\"\"\n","        # In einer realen Implementierung würde hier ein HTTP-Request erfolgen\n","        return {\"status\": \"success\", \"result\": {}}  # Platzhalter\n","\n","\n","class MCPServer:\n","    \"\"\"Simulierter MCP-Server zum Testen der Interaktion.\"\"\"\n","\n","    def __init__(self):\n","        self.registered_tools = {\n","            \"weather_info\": self._get_weather_data,\n","            \"database_query\": self._execute_database_query,\n","            \"text_translation\": self._translate_text\n","        }\n","\n","    def validate_request(self, request: Dict[str, Any]) -> bool:\n","        \"\"\"Überprüft, ob die Anfrage dem MCP-Protokoll entspricht.\"\"\"\n","        required_fields = [\"protocol_version\", \"request_id\", \"tool\"]\n","        if not all(field in request for field in required_fields):\n","            return False\n","\n","        if \"name\" not in request[\"tool\"] or request[\"tool\"][\"name\"] not in self.registered_tools:\n","            return False\n","\n","        return True\n","\n","    def process_request(self, request: Dict[str, Any]) -> Dict[str, Any]:\n","        \"\"\"Verarbeitet eine validierte MCP-Anfrage.\"\"\"\n","        if not self.validate_request(request):\n","            return {\"error\": \"Ungültige Anfrage\", \"status\": \"error\"}\n","\n","        tool_name = request[\"tool\"][\"name\"]\n","        parameters = request[\"tool\"].get(\"parameters\", {})\n","\n","        # Tool ausführen\n","        tool_func = self.registered_tools[tool_name]\n","        result = tool_func(parameters)\n","\n","        # Antwort standardisieren\n","        response = {\n","            \"request_id\": request[\"request_id\"],\n","            \"status\": \"success\",\n","            \"result\": result\n","        }\n","        return response\n","\n","    def _get_weather_data(self, params: Dict[str, Any]) -> Dict[str, Any]:\n","        \"\"\"Simuliert den Abruf von Wetterdaten von einem externen Dienst.\"\"\"\n","        return {\n","            \"city\": params.get(\"city\", \"Berlin\"),\n","            \"temperature\": 22,\n","            \"condition\": \"sonnig\",\n","            \"humidity\": 65\n","        }\n","\n","    def _execute_database_query(self, params: Dict[str, Any]) -> Dict[str, Any]:\n","        \"\"\"Simuliert eine Datenbankabfrage.\"\"\"\n","        return {\n","            \"rows\": [\n","                {\"id\": 1, \"name\": \"Produkt A\", \"price\": 29.99},\n","                {\"id\": 2, \"name\": \"Produkt B\", \"price\": 49.99}\n","            ],\n","            \"total_rows\": 2\n","        }\n","\n","    def _translate_text(self, params: Dict[str, Any]) -> Dict[str, Any]:\n","        \"\"\"Simuliert einen Übersetzungsdienst.\"\"\"\n","        target_lang = params.get(\"target_language\", \"en\")\n","\n","        translations = {\n","            \"en\": \"Hello world\",\n","            \"fr\": \"Bonjour le monde\",\n","            \"de\": \"Hallo Welt\"\n","        }\n","\n","        return {\n","            \"original_text\": params.get(\"text\", \"\"),\n","            \"translated_text\": translations.get(target_lang, \"Übersetzung nicht verfügbar\"),\n","            \"target_language\": target_lang\n","        }\n","\n","\n","class LLMApplication:\n","    \"\"\"LLM-Anwendung, die den MCP-Client verwendet.\"\"\"\n","\n","    def __init__(self, mcp_client: MCPClient):\n","        self.mcp_client = mcp_client\n","\n","    def process_user_query(self, query: str) -> str:\n","        \"\"\"Verarbeitet eine Nutzeranfrage mit MCP für zusätzliche Daten.\"\"\"\n","        if \"wetter\" in query.lower():\n","            # Extrahiere Stadt (vereinfacht)\n","            city = \"Berlin\"\n","\n","            # Erstelle und sende MCP-Anfrage\n","            request = self.mcp_client.create_request(\"weather_info\", {\"city\": city})\n","            response = self.mcp_client.send_request(request)\n","\n","            if response.get(\"status\") == \"success\":\n","                weather_data = response.get(\"result\", {})\n","                return self._format_weather_response(weather_data)\n","            return \"Entschuldigung, ich konnte keine Wetterdaten abrufen.\"\n","\n","        elif \"übersetze\" in query.lower():\n","            text = \"Hallo, wie geht es dir?\"\n","            target_lang = \"en\"\n","\n","            request = self.mcp_client.create_request(\"text_translation\", {\n","                \"text\": text,\n","                \"target_language\": target_lang\n","            })\n","            response = self.mcp_client.send_request(request)\n","\n","            if response.get(\"status\") == \"success\":\n","                translation_data = response.get(\"result\", {})\n","                return self._format_translation_response(translation_data)\n","            return \"Entschuldigung, ich konnte den Text nicht übersetzen.\"\n","\n","        return \"Ich verstehe Ihre Anfrage. Wie kann ich Ihnen helfen?\"\n","\n","    def _format_weather_response(self, weather_data: Dict[str, Any]) -> str:\n","        \"\"\"Formatiert Wetterdaten für die Anzeige.\"\"\"\n","        return (f\"Das aktuelle Wetter in {weather_data.get('city')}: \"\n","                f\"{weather_data.get('temperature')}°C, {weather_data.get('condition')}.\")\n","\n","    def _format_translation_response(self, translation_data: Dict[str, Any]) -> str:\n","        \"\"\"Formatiert Übersetzungsdaten für die Anzeige.\"\"\"\n","        return (f\"Übersetzung ({translation_data.get('target_language')}): \"\n","                f\"{translation_data.get('translated_text')}\")\n","\n","\n","def run_example():\n","    \"\"\"Führt ein Beispiel der MCP-Interaktion aus.\"\"\"\n","    print(\"=== MCP-Interaktionsbeispiel ===\\n\")\n","\n","    # MCP-Server initialisieren\n","    server = MCPServer()\n","\n","    # MCP-Client initialisieren\n","    client = MCPClient(\n","        client_id=\"llm_app_123\",\n","        api_key=\"sk_test_12345\"\n","    )\n","\n","    # LLM-Anwendung initialisieren\n","    llm_app = LLMApplication(client)\n","\n","    # Mock für send_request\n","    def mock_send_request(request_data):\n","        print(\"\\n[1] LLM-Anwendung erstellt MCP-Anfrage:\")\n","        print(json.dumps(request_data, indent=2))\n","\n","        print(\"\\n[2] MCP-Client sendet Anfrage an MCP-Server\")\n","\n","        print(\"\\n[3] MCP-Server validiert die Anfrage\")\n","        valid = server.validate_request(request_data)\n","        print(f\"Anfrage ist gültig: {valid}\")\n","\n","        print(\"\\n[4] MCP-Server verarbeitet die Anfrage und kommuniziert mit externen Systemen\")\n","        response = server.process_request(request_data)\n","\n","        print(\"\\n[5] Externe Systeme liefern Daten an MCP-Server\")\n","\n","        print(\"\\n[6] MCP-Server standardisiert die Antwort:\")\n","        print(json.dumps(response, indent=2))\n","\n","        print(\"\\n[7] MCP-Server sendet Antwort an MCP-Client\")\n","\n","        print(\"\\n[8] MCP-Client stellt Kontext der LLM-Anwendung zur Verfügung\")\n","\n","        return response\n","\n","    # Überschreiben der Methode\n","    client.send_request = mock_send_request\n","\n","    # Beispiel-Nutzeranfrage\n","    query = \"Wie ist das Wetter in Berlin?\"\n","\n","    print(f\"\\nNutzer fragt: '{query}'\")\n","    print(\"\\n[0] LLM-Anwendung analysiert die Nutzeranfrage\")\n","\n","    # LLM verarbeitet die Anfrage und nutzt MCP\n","    response = llm_app.process_user_query(query)\n","\n","    print(\"\\n[9] LLM-Anwendung nutzt den Kontext zur Beantwortung\")\n","    print(f\"\\nAntwort an den Nutzer: '{response}'\")\n","\n","\n","if __name__ == \"__main__\":\n","    run_example()"]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Fazit\n","</font></p>\n"],"metadata":{"id":"ioHO4YiNKxDE"},"id":"ioHO4YiNKxDE"},{"cell_type":"markdown","source":["\n","\n","\n","Das Model Context Protocol (MCP) stellt einen vielversprechenden Ansatz dar, um die Integration von KI-Modellen mit der Außenwelt zu **standardisieren und zu vereinfachen**. Durch die Definition eines gemeinsamen Protokolls und die Bereitstellung von Konzepten wie Tools und Ressourcen ermöglicht MCP die Entwicklung **flexiblerer, skalierbarer und wiederverwendbarer** KI-Anwendungen. Obwohl es noch Herausforderungen zu bewältigen gibt, hat MCP das Potenzial, die Art und Weise, wie KI-Agenten mit Daten und Tools interagieren, **grundlegend zu verändern**."],"metadata":{"id":"79FfrbNOLf5o"},"id":"79FfrbNOLf5o"},{"cell_type":"markdown","source":["# A | Aufgabe\n","---"],"metadata":{"id":"Pzc1rzQlNV8J"},"id":"Pzc1rzQlNV8J"},{"cell_type":"markdown","source":["Die Aufgabestellungen unten bieten Anregungen, Sie können aber auch gerne eine andere Herausforderung angehen."],"metadata":{"id":"QQUImb-86GUw"},"id":"QQUImb-86GUw"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Kalkulation\n","</font></p>"],"metadata":{"id":"gkF5wVxdx_iA"},"id":"gkF5wVxdx_iA"},{"cell_type":"markdown","source":["Gegeben ist eine Datei, die eine Reihe von Gleichungen enthält.\n","Der Dateiname ist GenAI/02 data/gleichungen.txt\n","\n","**Gleichung:**    \n","41748459 - 87226336    \n","92995162 * 46769739    \n","61530438 * 56074589    \n","95329602 + 45418854    \n","412907 + 3731910    \n","...\n","\n","Verwenden Sie einen LangChain-Agenten mit einem Tool, um jede dieser Gleichungen zu berechnen, und erstellen Sie eine Datei ähnlich dieser:\n","\n","**Ergebnisse:**  \n","41748459 - 87226336 = 45477877   \n","92995162 * 46769739 = 4349359455002718   \n","61530438 * 56074589 = 3450294021839982   \n","95329602 + 45418854 = 140748456   \n","412907 + 3731910 = 4144817   \n","... ...\n","\n"],"metadata":{"id":"Y5y-OP9OemcW"},"id":"Y5y-OP9OemcW"}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"colab":{"provenance":[],"collapsed_sections":["bf088ba7","xNBlsRoIb5T8","40c0n4k_cLvP","QO-PsEtTioiG","518d7d5a","Pzc1rzQlNV8J"]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}
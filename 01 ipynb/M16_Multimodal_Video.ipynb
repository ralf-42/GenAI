{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["-ID_iebz9wyq","CSGjgTJz-cpi","QH9WDuH8_e8y","9d9cb086","c477fff3","x0S24SQRPc-o","HvLa5GWZfgVH","-47CBLdZbLr0","gRJbfUG9-ujC"],"machine_shape":"hm","gpuType":"L4","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)"],"metadata":{"id":"Ih2CTVBnArVZ"}},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b>\n","Multimodal - Video\n","</b></font> </br></p>\n","\n","\n","---"],"metadata":{"id":"6jJZ7wbdArVc"}},{"cell_type":"code","source":["#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n","!uv pip install --system -q git+https://github.com/ralf-42/genai_lib\n","from genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages\n","setup_api_keys(['OPENAI_API_KEY', 'HF_TOKEN'], create_globals=False)\n","print()\n","check_environment()\n","print()\n","get_ipinfo()\n","# Bei Bedarf: Trennen zwischen Installationsname () und Importname (f√ºr Python) beide Angaben in Klammern\n","# install_packages([('markitdown[all]', 'markitdown'), 'langchain_chroma', ]"],"metadata":{"id":"nHQeXa9AFygf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title üõ†Ô∏è Installationen { display-mode: \"form\" }\n","install_packages(['ultralytics', ])"],"metadata":{"id":"wCUcLdwz8iW3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# 1 | Einf√ºhrung & Aufgaben\n","\n","---"],"metadata":{"id":"-ID_iebz9wyq"}},{"cell_type":"markdown","source":["In der modernen Videotechnologie spielen KI-gest√ºtzte Verfahren eine zentrale Rolle bei der Analyse, Verarbeitung und Generierung visueller Inhalte. Die folgende √úbersicht kategorisiert typische Aufgabenfelder im Bereich der Videoanalyse und -verarbeitung mit K√ºnstlicher Intelligenz. Sie zeigt praxisrelevante Anwendungsgebiete ‚Äì von der Klassifikation und Erkennung √ºber die Erzeugung neuer Inhalte bis hin zur Strukturierung und Sicherung von Videodaten. Die Kategorien helfen dabei, Potenziale und Einsatzm√∂glichkeiten in unterschiedlichen Szenarien schnell zu erfassen.\n","\n","\n","\n"],"metadata":{"id":"6hHK5izsH-cM"}},{"cell_type":"markdown","source":["| **Kategorie**                        | **Aufgabe**                          | **Beschreibung**                                                                     |\n","| ------------------------------------ | ------------------------------------ | ------------------------------------------------------------------------------------ |\n","| üß† **Analyse & Klassifikation**      | **Video-Klassifikation**             | Einordnung ganzer Videos in Kategorien (z.‚ÄØB. Sport, Nachrichten, Animation).        |\n","|                                      | *Objekterkennung im Video*             | Detektion und Verfolgung von Objekten √ºber mehrere Frames hinweg.                    |\n","|                                      | Aktivit√§tserkennung                  | Identifikation bestimmter Handlungen oder Bewegungsmuster (z.‚ÄØB. Laufen, Winken).    |\n","|                                      | **Gesichtserkennung**                    | Erkennung und Verfolgung von Gesichtern in Videosequenzen.                           |\n","|                                      | Szenenerkennung                      | Identifikation und Trennung inhaltlich zusammenh√§ngender Szenen.                     |\n","|                                      | Anomalieerkennung                    | Erkennung ungew√∂hnlicher Bewegungen oder Ereignisse (z.‚ÄØB. √úberf√§lle, Unf√§lle).      |\n","| üìù **Generierung & Transformation**  | **Videoerzeugung (Video Synthesis)** | Erzeugung synthetischer Videos aus Text oder Einzelbildern.                          |\n","|                                      | Videostiltransfer                    | √úbertragung eines visuellen Stils auf ein Video (z.‚ÄØB. Comicstil).                   |\n","|                                      | Zeitlupen-/Zeitraffer-Erzeugung      | Anpassung der Zeitachse durch Zwischenbildsynthese oder Beschleunigung.              |\n","|                                      | Video-Super-Resolution               | Verbesserung der Aufl√∂sung von Videos durch KI-gest√ºtzte Hochskalierung.             |\n","|                                      | Videorestauration                    | Entfernung von Rauschen, Kratzern oder Artefakten in alten oder besch√§digten Videos. |\n","|                                      | Hintergrundaustausch                 | Dynamisches Ersetzen des Hintergrunds in einem Video (z.‚ÄØB. Green Screen).           |\n","| üß© **Erg√§nzung & Vervollst√§ndigung** | Videoinpainting                      | Rekonstruktion fehlender oder besch√§digter Bildbereiche √ºber mehrere Frames.         |\n","|                                      | Video-Vervollst√§ndigung          | Erg√§nzung fehlender Frames oder Szenen, z.‚ÄØB. durch KI-generierte √úberg√§nge.         |\n","|                                      | Szenenerweiterung                    | Fortsetzung oder Erg√§nzung einer Videosequenz mit synthetischen Inhalten.            |\n","| üìö **Informationsgewinnung**         | Textextraktion (OCR im Video)        | Erkennung und Extraktion eingeblendeter Texte (z.‚ÄØB. Untertitel, Nummernschilder).   |\n","|                                      | **Audiotranskription im Video**          | Umwandlung gesprochener Sprache im Video in Text.                                    |\n","|                                      | Untertitelgenerierung                | Automatische Erstellung zeitlich synchronisierter Untertitel.                        |\n","|                                      | Inhaltsbasierte Suche                | Durchsuchbarkeit von Videos nach Objekten, Szenen oder gesprochenen Inhalten.        |\n","| üõ°Ô∏è **Sicherheit & Strukturierung**  | Deepfake-Erkennung im Video          | Aufdeckung manipulierten Videomaterials (z.‚ÄØB. durch KI generierte Gesichter).       |\n","|                                      | Gesichtsverpixelung                  | Anonymisierung durch Unkenntlichmachung von Gesichtern im Video.                     |\n","|                                      | Wasserzeichen-Erkennung              | Nachweis sichtbarer oder unsichtbarer Wasserzeichen.                                 |\n","|                                      | Formatkonvertierung                  | Umwandlung zwischen Videoformaten (z.‚ÄØB. MP4 ‚Üî AVI).                                 |\n"],"metadata":{"id":"KhWsBjeulKFe"}},{"cell_type":"markdown","source":["# 2 | Video-to-Text (VTT)\n","\n","---"],"metadata":{"id":"CSGjgTJz-cpi"}},{"cell_type":"markdown","source":["\n","\n","Die Umwandlung von Videoinhalten in Text, bekannt als Video-Transkription, erm√∂glicht die automatische Verschriftlichung von Videoszenen, Vortr√§gen oder Handlungen. Dies erleichtert die Dokumentation, Archivierung und Weiterverarbeitung von Videoinhalten. Moderne Bilderkennung und Spracherkennungstechnologien nutzen KI-Modelle, um visuelle und auditive Elemente pr√§zise zu erfassen und in lesbaren Text umzuwandeln.\n","\n","Videotranskriptionen sind in vielen Bereichen von Bedeutung ‚Äì von der Erstellung von Untertiteln und der Zug√§nglichkeit f√ºr Menschen mit Behinderungen √ºber die Erstellung durchsuchbarer Videoinhalte bis hin zur Analyse von Marktforschungsdaten. Durch den Einsatz automatisierter Systeme kann dieser Prozess erheblich beschleunigt werden, wodurch gro√üe Mengen an Videomaterial effizient verarbeitet werden k√∂nnen."],"metadata":{"id":"rZM_rb5ZmAYk"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Version auf Basis Transkript\n","</font></p>"],"metadata":{"id":"cVG8SXbzFa5u"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2192f76c"},"outputs":[],"source":["# Import\n","import openai\n","from google.colab import files\n","from IPython.display import display, Markdown\n","import os\n","\n","# Nutzer w√§hlt eine Datei aus und l√§dt sie hoch\n","uploaded = files.upload()\n","\n","# Den ersten Dateinamen extrahieren\n","video_file_path = list(uploaded.keys())[0]\n","\n","# Tempor√§ren Pfad f√ºr die Audio-Extraktion erstellen\n","audio_path = \"extracted_audio.mp3\"\n","\n","# FFmpeg verwenden, um Audio aus dem Video zu extrahieren\n","!ffmpeg -i \"{video_file_path}\" -q:a 0 -map a \"{audio_path}\" -y\n","\n","# Audiodatei √∂ffnen und Whisper nutzen\n","with open(audio_path, \"rb\") as audio_file:\n","    response = openai.audio.transcriptions.create(\n","        model=\"whisper-1\",\n","        file=audio_file\n","    )"]},{"cell_type":"code","source":["# Markdown-Formatierte Ausgabe\n","markdown_output = f\"\"\"\n","## üé• Transkript des Videos\n","\n","**üìÇ Datei:** `{video_file_path}`\n","**ü§ñ Modell:** `Whisper-1` (f√ºr Audiotranskription)\n","\n","---\n","\n","{response.text}\n","\"\"\"\n","\n","# Markdown in Google Colab anzeigen\n","display(Markdown(markdown_output))\n","\n","# Tempor√§re Audiodatei l√∂schen\n","# os.remove(audio_path)"],"metadata":{"id":"9Ui5P84-SFJF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Version auf Basis Bilder\n","</font></p>"],"metadata":{"id":"bxTaXF99FV6q"}},{"cell_type":"markdown","source":["In Anlehnung an: [Quelle](https://cookbook.openai.com/examples/gpt_with_vision_for_video_understanding)."],"metadata":{"id":"1e4QPJfzFt9s"}},{"cell_type":"code","source":["from IPython.display import display, Image, Markdown\n","from google.colab import files\n","import cv2\n","import base64\n","import time\n","from openai import OpenAI\n","import os\n","import requests\n","\n","client = OpenAI()"],"metadata":{"id":"t9ENnWPNC3KX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Datei-Upload erm√∂glichen\n","uploaded = files.upload()  # √ñffnet einen Dialog zum Datei-Upload\n","\n","# Name der hochgeladenen Datei ermitteln\n","filename = list(uploaded.keys())[0]\n","print(f\"Datei '{filename}' wurde hochgeladen.\")\n","\n","# Video mit OpenCV √∂ffnen\n","video = cv2.VideoCapture(filename)\n","\n","# Frames in base64 umwandeln\n","base64Frames = []\n","while video.isOpened():\n","    success, frame = video.read()\n","    if not success:\n","        break\n","    _, buffer = cv2.imencode(\".jpg\", frame)\n","    base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n","\n","# Video-Ressourcen freigeben\n","video.release()\n","print(len(base64Frames), \"Frames wurden eingelesen.\")"],"metadata":{"id":"jtcGOhf8DALD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prompt und Chat-Aufruf\n","PROMPT_MESSAGES = [\n","    {\n","        \"role\": \"user\",\n","        \"content\": [\n","            \"These are frames from a video that I want to upload. Generate a compelling description that I can upload along with the video. Please answer in German.\",\n","            *map(lambda x: {\"image\": x, \"resize\": 768}, base64Frames[0::50]),\n","        ],\n","    },\n","]\n","params = {\n","    \"model\": \"gpt-4o-mini\",\n","    \"messages\": PROMPT_MESSAGES,\n","    \"max_tokens\": 200,\n","}\n","\n","result = client.chat.completions.create(**params)"],"metadata":{"id":"U1xP9p6SDTcW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Markdown-Formatierte Ausgabe\n","markdown_output = f\"\"\"\n","## üé• Analyse des Videos auf Basis Frames\n","\n","**üìÇ Datei:** `{filename}`\n","**ü§ñ Modell:** gpt-4o-mini\n","\n","---\n","\"\"\"\n","\n","display(Markdown(markdown_output))\n","display(Markdown(result.choices[0].message.content))"],"metadata":{"id":"1_BD4Al9SsLf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3 | Image-to-Video (I2V)\n","\n","---"],"metadata":{"id":"QH9WDuH8_e8y"}},{"cell_type":"markdown","source":["Image-zu-Video (I2V) mit generativer KI erm√∂glicht die Erstellung realistischer Videos auf Basis einzelner Bilder oder Bildsequenzen. Modelle wie Sora (OpenAI), Gen-2 (Runway) und Lumiere (Google) verwandeln statische Bilder in dynamische, visuell konsistente Videoclips ‚Äì h√§ufig erg√§nzt durch passende Audiospuren oder die M√∂glichkeit, eigenen Ton hinzuzuf√ºgen.\n","\n","Diese KI-Systeme nutzen multimodale Architekturen und Deep Learning, um Bewegungen, Kamerafahrten und visuelle Effekte glaubw√ºrdig zu simulieren. Anwendungsfelder sind u.‚ÄØa. Marketing, Bildung, digitale Kunst und Simulation.\n","\n","F√ºhrende I2V-Modelle:\n","\n","+ Sora (OpenAI) ‚Äì Erzeugt aus Bildern und Bildfolgen realistische Szenen mit hoher Koh√§renz.\n","\n","+ Gen-2 (Runway) ‚Äì Unterst√ºtzt Bild-, Text- und Hybridprompts f√ºr vielseitige Anwendungen.\n","\n","+ Pika Labs ‚Äì Ideal f√ºr einfache Animationen aus Einzelbildern.\n","\n","+ Stable Video Diffusion ‚Äì Quelloffen, mit wachsender Community.\n","\n","+ Lumiere (Google) ‚Äì Experimentell, mit Fokus auf Bewegungsdynamik und Stil√ºbertragung.\n","\n","+ Hedra ‚Äì Erm√∂glicht die Erstellung von Videos aus statischen Bildern mit synchronisiertem Audio, ideal f√ºr die Generierung sprechender oder singender Avatare.\n","\n","+ HeyGen ‚Äì Spezialisiert auf die schnelle Erstellung personalisierter, sprechender Avatare aus Fotos oder Videos. Bietet pr√§zise Lippensynchronisation, Voice-Cloning, Text-zu-Video und Video√ºbersetzung in √ºber 40 Sprachen. Besonders geeignet f√ºr Marketing, E-Learning und Social Media; erm√∂glicht Multi-Avatar-Videos und individuelle Anpassungen bis hin zu 4K-Qualit√§t.\n","\n","\n","Viele Tools bieten zus√§tzlich Funktionen zur Audiointegration, etwa durch automatische Tonunterlegung oder individuelle Sprachkommentare, um das Ergebnis noch lebendiger zu gestalten."],"metadata":{"id":"cw-XodgbBJ43"}},{"cell_type":"markdown","source":["Hier ist eine √úbersicht, welche der genannten Modelle mit/ohne **API-Zugang** nutzbar sind:\n","\n","| Modell                   | API-Zugang m√∂glich?               | Hinweise |\n","| :--                      | :--                              | :-- |\n","| **Sora (OpenAI)**        | Eingeschr√§nkt, nur nach Bewerbung | Sora ist nicht √∂ffentlich per API verf√ºgbar. Zugang gibt es nur nach individueller Bewerbung und Freischaltung durch OpenAI. |\n","| **Gen-2 (Runway)**       | Ja                               | Runway bietet eine API f√ºr Gen-3 und Gen-4 Modelle, die f√ºr Entwickler offen zug√§nglich ist. |\n","| **Pika Labs**            | Ja                               | Die Pika API ist verf√ºgbar, allerdings mit bestimmten Einschr√§nkungen und nach Registrierung. |\n","| **Stable Video Diffusion** | Ja                             | √úber die Developer Platform von Stability AI ist eine API f√ºr Stable Video Diffusion verf√ºgbar. |\n","| **Lumiere (Google)**     | Nein, aktuell nicht              | Lumiere befindet sich noch in der Entwicklung und ist nicht √∂ffentlich oder per API nutzbar. |\n","| **Hedra**                | Nein, aktuell nicht              | Hedra ist als Web-App nutzbar, aber es gibt derzeit keine √∂ffentlich dokumentierte API. |\n","| **HeyGen**               | Ja                               | HeyGen bietet einen dokumentierten API-Zugang f√ºr Video- und Avatar-Erstellung. Authentifizierung erfolgt per API-Key oder OAuth2. Die API ist f√ºr Entwickler offen zug√§nglich, Details in der offiziellen Doku[1][3][8]. |\n","\n","\n","**Zusammengefasst:**  \n","Per API lassen sich aktuell **Gen-2 (Runway)**, **Pika Labs**, **Stable Video Diffusion** und **HeyGen** direkt nutzen.  \n","**Sora** ist nur nach individueller Bewerbung und Freischaltung per API zug√§nglich.  \n","**Lumiere** und **Hedra** bieten derzeit keinen √∂ffentlichen API-Zugang.\n","\n","\n","Stand: 05.2025"],"metadata":{"id":"-bG3ZNZSFmhj"}},{"cell_type":"markdown","source":["[Hedra](https://www.hedra.com/app/video)\n","\n","[HeyGen](https://app.heygen.com/get-started)"],"metadata":{"id":"cuANbDNpc4p0"}},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/under_construction_dall_e_klein.png)"],"metadata":{"id":"uv-Nursv_mqg"}},{"cell_type":"markdown","source":["[Diffuser](https://huggingface.co/docs/diffusers/v0.33.1/en/index)\n","\n","\n","[Videogeneration](https://huggingface.co/docs/diffusers/using-diffusers/text-img2vid?popular-models=CogVideoX)"],"metadata":{"id":"ydCJj0xgP-u7"}},{"cell_type":"markdown","metadata":{"id":"9d9cb086"},"source":["# 4 | Text-to-Video (T2V)\n","\n","---"]},{"cell_type":"markdown","source":["Text-zu-Video (T2V) mit generativer KI erm√∂glicht die Erstellung realistischer Videos allein auf Basis von Texteingaben. Modelle wie Sora (OpenAI), Gen-2 (Runway) und Lumiere (Google) wandeln sprachliche Beschreibungen in dynamische, visuell konsistente Videoclips um ‚Äì oft erg√§nzt durch passende Audiospuren oder die M√∂glichkeit, eigenen Ton hinzuzuf√ºgen.\n","\n","Diese KI-Systeme nutzen multimodale Architekturen und tiefes Lernen, um Bewegungen, Perspektiven und visuelle Effekte realit√§tsnah zu generieren. Sie finden Anwendung in Bereichen wie Marketing, Bildung, Storytelling und Simulation.\n","\n","F√ºhrende T2V-Modelle:\n","\n","\n","+ Sora (OpenAI) ‚Äì Erzeugt aus Bildern und Bildfolgen realistische Szenen mit hoher Koh√§renz.\n","\n","+ Gen-2 (Runway) ‚Äì Unterst√ºtzt Bild-, Text- und Hybridprompts f√ºr vielseitige Anwendungen.\n","\n","+ Pika Labs ‚Äì Ideal f√ºr einfache Animationen aus Einzelbildern.\n","\n","+ Stable Video Diffusion ‚Äì Quelloffen, mit wachsender Community.\n","\n","+ Lumiere (Google) ‚Äì Experimentell, mit Fokus auf Bewegungsdynamik und Stil√ºbertragung.\n","\n","+ Hedra ‚Äì Erm√∂glicht die Erstellung von Videos aus statischen Bildern mit synchronisiertem Audio, ideal f√ºr die Generierung sprechender oder singender Avatare.\n","\n","+ HeyGen ‚Äì Spezialisiert auf die schnelle Erstellung personalisierter, sprechender Avatare aus Fotos oder Videos. Bietet pr√§zise Lippensynchronisation, Voice-Cloning, Text-zu-Video und Video√ºbersetzung in √ºber 40 Sprachen. Besonders geeignet f√ºr Marketing, E-Learning und Social Media; erm√∂glicht Multi-Avatar-Videos und individuelle Anpassungen bis hin zu 4K-Qualit√§t.\n","\n","Viele Tools bieten zus√§tzlich Funktionen zur Audiointegration, etwa durch automatische Tonunterlegung oder individuelle Sprachkommentare, um das Ergebnis noch lebendiger zu gestalten.\n","\n"],"metadata":{"id":"ya_B4h3PA6I6"}},{"cell_type":"markdown","source":["Hier ist eine √úbersicht, welche der genannten Modelle mit/ohne **API-Zugang** nutzbar sind:\n","\n","| Modell                   | API-Zugang m√∂glich?               | Hinweise |\n","| :--                      | :--                              | :-- |\n","| **Sora (OpenAI)**        | Eingeschr√§nkt, nur nach Bewerbung | Sora ist nicht √∂ffentlich per API verf√ºgbar. Zugang gibt es nur nach individueller Bewerbung und Freischaltung durch OpenAI. |\n","| **Gen-2 (Runway)**       | Ja                               | Runway bietet eine API f√ºr Gen-3 und Gen-4 Modelle, die f√ºr Entwickler offen zug√§nglich ist. |\n","| **Pika Labs**            | Ja                               | Die Pika API ist verf√ºgbar, allerdings mit bestimmten Einschr√§nkungen und nach Registrierung. |\n","| **Stable Video Diffusion** | Ja                             | √úber die Developer Platform von Stability AI ist eine API f√ºr Stable Video Diffusion verf√ºgbar. |\n","| **Lumiere (Google)**     | Nein, aktuell nicht              | Lumiere befindet sich noch in der Entwicklung und ist nicht √∂ffentlich oder per API nutzbar. |\n","| **Hedra**                | Nein, aktuell nicht              | Hedra ist als Web-App nutzbar, aber es gibt derzeit keine √∂ffentlich dokumentierte API. |\n","| **HeyGen**               | Ja                               | HeyGen bietet einen dokumentierten API-Zugang f√ºr Video- und Avatar-Erstellung. Authentifizierung erfolgt per API-Key oder OAuth2. Die API ist f√ºr Entwickler offen zug√§nglich, Details in der offiziellen Doku[1][3][8]. |\n","\n","\n","**Zusammengefasst:**  \n","Per API lassen sich aktuell **Gen-2 (Runway)**, **Pika Labs**, **Stable Video Diffusion** und **HeyGen** direkt nutzen.  \n","**Sora** ist nur nach individueller Bewerbung und Freischaltung per API zug√§nglich.  \n","**Lumiere** und **Hedra** bieten derzeit keinen √∂ffentlichen API-Zugang.\n","\n","\n","Stand: 05.2025"],"metadata":{"id":"uiP1vkgsI0IS"}},{"cell_type":"markdown","source":["[Hedra](https://www.hedra.com/app/video)\n","\n","[HeyGen](https://app.heygen.com/get-started)"],"metadata":{"id":"DlWW-HxTI0IU"}},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/under_construction_dall_e_klein.png)"],"metadata":{"id":"Hwrf9njg9gSH"}},{"cell_type":"markdown","metadata":{"id":"c477fff3"},"source":["# 5 | Objekterkennung\n","\n","---"]},{"cell_type":"markdown","source":["Die Video-Objekterkennung ist ein wichtiger Bereich der Computer Vision, der sich mit der Identifikation und Klassifikation von Objekten in Videomaterial befasst. Im Gegensatz zur statischen Bilderkennung m√ºssen dabei nicht nur einzelne Frames analysiert, sondern auch zeitliche Zusammenh√§nge ber√ºcksichtigt werden.  \n","\n","Ziel dieses Kapitels ist es, ein grundlegendes Verst√§ndnis f√ºr die Herausforderungen und L√∂sungsans√§tze der Video-Objekterkennung zu vermitteln und praxisnahe Anwendungen zu demonstrieren. Zum Einsatz kommt **YOLO** (You Only Look Once), ein Echtzeit-Objekterkennungsmodell, das durch seine hohe Geschwindigkeit und Genauigkeit besonders f√ºr Anwendungen wie autonome Fahrzeuge, √úberwachungssysteme und interaktive KI-Systeme geeignet ist."],"metadata":{"id":"onWjUpoV0Vy8"}},{"cell_type":"markdown","source":["[Ultralytics | Yolo](https://www.ultralytics.com/de)"],"metadata":{"id":"2zBB6_P9hEkF"}},{"cell_type":"code","source":["# Import\n","from ultralytics import YOLO\n","from google.colab import files\n","import cv2\n","from IPython.display import HTML\n","from base64 import b64encode\n","\n","# YOLO-Modell laden\n","model = YOLO(\"yolo11s.pt\")\n","\n","# Video-Datei hochladen\n","uploaded = files.upload()\n","\n","# Dateinamen abrufen (das erste hochgeladene Video)\n","video_path = list(uploaded.keys())[0]\n","\n","# YOLO-Vorhersage auf dem Video durchf√ºhren\n","# Hier werden Objekte erkannt und das Video mit Bounding Boxes gespeichert\n","results = model.predict(source=video_path, save=True, conf=0.25, save_txt=True, format='avi')\n","\n","# Das verarbeitete Video anzeigen\n","video_path = video_path.replace(\"mp4\", \"avi\")\n","processed_video_path = f\"{results[0].save_dir}/{video_path}\"\n","\n","# Video herunterladen\n","print(f\"Video mit Objekterkennung: {processed_video_path}\")\n","files.download(processed_video_path)"],"metadata":{"id":"P9ZQmS_OmzQB","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x0S24SQRPc-o"},"source":["# 6 | Video-/Bild-/Sprachanalyse\n","\n","---"]},{"cell_type":"markdown","source":["\n","\n","Durch die Analyse von Bildinhalten, Objekten, Bewegungen und Szenen k√∂nnen relevante Informationen aus Videos extrahiert werden. Die Fokus in diesem Modul ist die Videoanalyse mit Computer Vision, auch **Visual Analysis** genannt, sie ist eine Methode zur automatischen Erkennung und Klassifizierung von Objekten, Personen und Handlungen in Videosequenzen. Mithilfe maschinellen Sehens (Computer Vision) werden Videos auf verschiedene Elemente und Muster untersucht. Diese Technik wird h√§ufig in der Sicherheits√ºberwachung, in der Produktionslinie oder bei der Analyse von Social-Media-Inhalten eingesetzt.\n","\n","Moderne KI-Modelle analysieren nicht nur einzelne Frames, sondern ber√ºcksichtigen auch zeitliche Zusammenh√§nge, um Bewegungen und Handlungen pr√§ziser zu erfassen. Unternehmen nutzen diese Technologie, um Kundenverhalten zu verstehen, Sicherheitsrisiken zu erkennen oder Prozesse zu optimieren."],"metadata":{"id":"kLrmSculPc-o"}},{"cell_type":"code","source":["import openai\n","from google.colab import files\n","from IPython.display import display, Markdown, HTML\n","import cv2\n","import os\n","import numpy as np\n","import base64\n","import tempfile"],"metadata":{"id":"FBBH8k4bPc-p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Video hochladen und Frame-Extraction\n","uploaded = files.upload()\n","video_file_path = list(uploaded.keys())[0]"],"metadata":{"id":"H6RafyZYaCtF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Frame-Extraction\n","def extract_frames(video_path, num_frames=5):\n","    \"\"\"Extrahiert Frames aus einem Video\"\"\"\n","    cap = cv2.VideoCapture(video_path)\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    frame_indices = np.linspace(0, total_frames-1, num_frames, dtype=int)\n","\n","    frames = []\n","    for idx in frame_indices:\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n","        ret, frame = cap.read()\n","        if ret:\n","            frames.append(frame)\n","    cap.release()\n","    return frames\n","\n","# Hilfsfunktion f√ºr Base64-Kodierung\n","def get_base64_encoded(image_path):\n","    \"\"\"Konvertiert ein Bild in Base64-String\"\"\"\n","    with open(image_path, \"rb\") as image_file:\n","        return base64.b64encode(image_file.read()).decode('utf-8')\n","\n","# Frames extrahieren und speichern\n","frames = extract_frames(video_file_path)\n","frame_paths = [f\"frame_{i}.jpg\" for i in range(len(frames))]\n","for i, frame in enumerate(frames):\n","    cv2.imwrite(frame_paths[i], frame)\n","\n","# Audio extrahieren und transkribieren\n","audio_path = \"extracted_audio.mp3\"\n","!ffmpeg -i \"{video_file_path}\" -q:a 0 -map a \"{audio_path}\" -y\n","\n","with open(audio_path, \"rb\") as audio_file:\n","    transcribed_text = openai.audio.transcriptions.create(\n","        model=\"whisper-1\",\n","        file=audio_file\n","    ).text\n","\n","# Frame-Analyse mit GPT-4o\n","frame_descriptions = []\n","for frame_path in frame_paths:\n","    base64_img = get_base64_encoded(frame_path)\n","    response = openai.chat.completions.create(\n","        model=\"gpt-4o\",\n","        messages=[\n","            {\"role\": \"system\", \"content\": \"Du bist ein Experte f√ºr Videoanalyse. Beschreibe kurz und pr√§zise, was auf diesem Frame zu sehen ist.\"},\n","            {\"role\": \"user\", \"content\": [\n","                {\"type\": \"text\", \"text\": \"Beschreibe diesen Video-Frame mit einem kurzen Satz:\"},\n","                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img}\"}}\n","            ]}\n","        ],\n","        max_tokens=150\n","    )\n","    frame_descriptions.append(response.choices[0].message.content)"],"metadata":{"id":"OsiwldHYaSR3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Gesamtanalyse\n","analysis_prompt = f\"\"\"\n","Analysiere den folgenden Videoinhalt basierend auf dem Transkript und den Frame-Beschreibungen.\n","Gib eine detaillierte Zusammenfassung dessen, was im Video passiert.\n","\n","Hauptpunkt: Inhaltsanalyse:\n","    Themen und Argumentationsstruktur: Was wird gesagt? Wie ist das Gesagte aufgebaut?\n","    Sprachstil: Alltagssprache vs. Fachsprache, formell vs. informell\n","    Rhetorische Mittel: Metaphern, Wiederholungen, Fragen, Ironie, Pausen\n","    Satzstruktur & Wortwahl: Einfach oder komplex? Viele F√ºllw√∂rter?\n","    Erz√§hlperspektive: Ich-Form, Du-Ansprache, distanziert?\n","    Tonfall und Haltung: Neutral, ironisch, kritisch, emotional?\n","\n","Hauptpunkt: Parasprachliche Merkmale (Stimme, Ton, Sprechweise):\n","    Sprechtempo: Schnell, langsam, variierend\n","    Intonation: Monoton oder lebendig? Betonung gezielt eingesetzt?\n","    Lautst√§rke: Konstant, wechselnd, passend zur Situation?\n","    Stimmqualit√§t: Klar, nasal, heiser, angespannt?\n","    Pausen: Nat√ºrlich oder unnat√ºrlich? Strategisch gesetzt?\n","    Versprecher oder Selbstkorrekturen: H√§ufig? Sympathisch oder irritierend?\n","\n","Hauptpunkt: Visuelle Elemente:\n","    Bildkomposition: Einstellungsgr√∂√üen, Perspektiven, Bildausschnitte\n","    Kameraf√ºhrung: Statisch, dynamisch, Schwenks, Zooms\n","    Schnittrhythmus: Schnell, langsam, variierend, passend zum Inhalt?\n","    Lichtgestaltung: Nat√ºrliches vs. k√ºnstliches Licht, Farbstimmung\n","    Setting/Location: Bedeutung und Wirkung des Hintergrunds\n","    Kleidung/Styling: Formell, informell, thematisch passend?\n","\n","Hauptpunkt: Nonverbale Kommunikation:\n","    K√∂rperhaltung: Offen, geschlossen, angespannt, entspannt?\n","    Gestik: H√§ufigkeit, Nat√ºrlichkeit, unterst√ºtzend zum Gesagten?\n","    Mimik: Ausdrucksstark, zur√ºckhaltend, authentisch?\n","    Blickkontakt: Direkt in die Kamera oder ablenkend?\n","    Bewegung im Raum: Statisch oder dynamisch?\n","\n","Hauptpunkt: Zusammenspiel von Bild und Ton:\n","    Synchronit√§t: Passen Bild und Ton zusammen?\n","    Erg√§nzung: Unterst√ºtzen visuelle Elemente das Gesagte?\n","    Musik/Soundeffekte: Einsatz und Wirkung auf die Gesamtwahrnehmung\n","    Grafiken/Einblendungen: Sinnvoll integriert und informativ?\n","\n","Hauptpunkt: Wirkung auf die Zuschauer:innen\n","    Verst√§ndlichkeit: Wird der Inhalt klar transportiert?\n","    Authentizit√§t: Wirkt die Person/Szene glaubw√ºrdig und echt?\n","    Emotionale Ansprache: Ber√ºhrt der Inhalt? Motiviert? Regt zum Denken an?\n","    Zielgruppenorientierung: Passt Sprache, Ton und Bild zum Publikum?\n","\n","Hauptpunkt: Stimmungsanalyse:\n","    Stimmung: Positiv üòä, Neutral üòê, Negativ üòû\n","    Begr√ºnde Deine Einsch√§tzung unter Ber√ºcksichtigung visueller und auditiver Elemente\n","\n","Hauptpunkt: Technische Aspekte\n","    Audioqualit√§t: Rauschfrei, Hall, Hintergrundger√§usche?\n","    Mikrofonierung: Richtiger Abstand, Pop-Ger√§usche, Aussteuerung?\n","    Bildqualit√§t: Sch√§rfe, Aufl√∂sung, Kontrast, Farbwiedergabe\n","    Schnitt & Nachbearbeitung: Erkennbar geschnitten? √úberg√§nge sauber?\n","    Spezialeffekte/CGI: Sinnvoll eingesetzt und qualitativ hochwertig?\n","\n","Hauptpunkt: Weitere Aspekte:\n","    Gibt es ethische Aspekte die angesprochen werden m√ºssten?\n","    Gibt es versteckte Botschaften in Bild oder Ton?\n","    Wie ist die Konsistenz zwischen verbaler und visueller Kommunikation?\n","    Welche Punkte k√∂nnten noch interessant sein, an die ich nicht gedacht habe?\n","\n","Vermeide unscharfe Formulierungen  wie wahrscheinlich, m√∂glicherweise, scheint, k√∂nnte oder sollte.\n","Nimm konkret Stellung.\n","\n","Stelle bei der Ausgabe der Ergebnisse nur bei den Hauptpunkten zur besseren Unterscheidung ein \"üîπ\" voran.\n","Zeige die Unterpunkte als dot-Aufz√§hlung an.\n","\n","### Video-Transkript:\n","{transcribed_text}\n","\n","### Frame-Beschreibungen:\n","{' | '.join([f\"Frame {i+1}: {desc}\" for i, desc in enumerate(frame_descriptions)])}\n","\"\"\""],"metadata":{"id":"fDXzlAwrQbQQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["analysis_result = openai.chat.completions.create(\n","    model=\"gpt-4o-mini\",\n","    messages=[\n","        {\"role\": \"system\", \"content\": \"Du bist ein Experte f√ºr Videoanalyse.\"},\n","        {\"role\": \"user\", \"content\": analysis_prompt}\n","    ],\n","    temperature=0.2\n",").choices[0].message.content"],"metadata":{"id":"egq3jA03QgXW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# HTML f√ºr Ergebnisse-Anzeige definieren\n","\n","frames_html = \"\".join([\n","    f\"\"\"\n","    <div style=\"display: inline-block; margin: 10px; text-align: center;\">\n","        <img src=\"data:image/jpeg;base64,{get_base64_encoded(frame_path)}\" style=\"width: 200px; border: 1px solid #ddd;\">\n","        <p style=\"width: 200px; font-size: 12px;\">{frame_descriptions[i]}</p>\n","    </div>\n","    \"\"\" for i, frame_path in enumerate(frame_paths)\n","    ])\n","\n","# Markdown-Anzeige\n","display(Markdown(f\"\"\"\n","## üé• Videoanalyse\n","---\n","\n","**üìÇ Datei:** `{video_file_path}`\n","**ü§ñ Modelle:** `Whisper-1` (Audio), `GPT-4o` (Frames), `GPT-4o-mini` (Gesamtanalyse)\n","\n","<br>\n","\n","### üìù Transkript\n","---\n","{transcribed_text}\n","\n","<br>\n","\n","### üñºÔ∏è Extrahierte Frames und Beschreibungen\n","---\n","\"\"\"))\n","\n","display(HTML(frames_html))\n","display(Markdown(f\"\"\"\n","### üß† Videoanalyse-Ergebnis\n","{analysis_result}\n","\"\"\"))"],"metadata":{"id":"55TU4u1QRD8r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aufr√§umen - Dateien l√∂schen\n","for path in frame_paths + [audio_path]:\n","    os.remove(path)"],"metadata":{"id":"dPOMymT1aaEc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 7 | Dubbing\n","---\n"],"metadata":{"id":"HvLa5GWZfgVH"}},{"cell_type":"markdown","source":["Dubbing (Synchronisation) bezeichnet den Prozess, bei dem der Originalton eines Films, Videos oder Audioclips durch eine neu eingesprochene Tonspur in einer anderen Sprache ersetzt wird. Dabei wird der √ºbersetzte Text so angepasst, dass er m√∂glichst gut zur Lippenbewegung und Stimmung der Originalaufnahme passt. Ziel ist es, dem Publikum ein m√∂glichst nat√ºrliches und immersives Seherlebnis in der eigenen Sprache zu bieten. Dubbing wird h√§ufig bei internationalen Filmproduktionen, Serien oder Videospielen eingesetzt und erfordert eine enge Zusammenarbeit von √úbersetzer:innen, Sprecher:innen, Toningenieur:innen und gegebenenfalls KI-gest√ºtzten Tools."],"metadata":{"id":"zuBirABpfiog"}},{"cell_type":"markdown","source":["| Modell / Anbieter      | API-Zugang m√∂glich? | Hinweise |\n","| :--------------------- | :------------------ | :------- |\n","| **ElevenLabs Dubbing** | Ja | Dubbing in 29 Sprachen, erh√§lt Emotion, Timing und Originalstimme. Plattformintegration (z.B. YouTube), <br>automatische Sprechererkennung, professionelle Human Review m√∂glich. |\n","| **Rask AI**            | Ja | √úber 130 Sprachen, Voice Cloning f√ºr 29 Sprachen, Multi-Speaker, Lip-Sync, Video-Editing, Integration mit Vimeo/Instagram. |\n","| **Heygen**             | Ja | 175+ Sprachen/Dialekte, Multi-Speaker, Custom Pronunciation, Brand Consistency, nat√ºrliche Intonation, Translate API verf√ºgbar. |\n","| **D-ID Video Translate** | Ja | Voice Cloning, Lip-Sync, Bulk-Rendering (bis 29 Sprachen), Drag-and-Drop-Interface, API ab $18/Monat. |\n","| **Murf Dub**           | Ja | 25+ Sprachen, pr√§zise √úbersetzung, Voice Cloning, Zeit- und Lip-Sync, Hintergrundmusik bleibt erhalten, API verf√ºgbar, SOC 2/GDPR/ISO-konform. |\n","| **Synthesys AI**       | Ja | 29 Sprachen, Voice Cloning, Echtzeit-Dubbing, AI-Lip-Sync, 300+ Stimmen, Video-Editing, virtuelle Schauspieler. |\n","| **Deepdub**            | Ja | 130+ Sprachen, 26+ Emotionen pro Sprecher (eTTS‚Ñ¢), Voice Cloning, API f√ºr Enterprise, menschliche Qualit√§tskontrolle, Preis auf Anfrage. |\n","| **Speechify Studio**   | Ja | 60+ Sprachen, 1.000+ Stimmen, Voice Cloning, Multi-Speaker, Auto-Subtitling, Skript- und Emotionsbearbeitung, Lip-Sync (Beta). |\n","| **Synthesia**          | Ja | 29 Sprachen, Multi-Speaker, Voice Cloning, Lip-Sync (in Entwicklung), AI-Avatare, einfache Bedienung, API f√ºr Unternehmen. |\n","| **InVideo**            | Ja | 50+ Sprachen, Team-Collaboration, gro√üe Voice-Bibliothek, Voice Cloning, Echtzeit-Editing, API verf√ºgbar. |\n","| **CAMB.AI**            | Ja | 140+ Sprachen, Echtzeit- und Postproduktion, Dubstream f√ºr Livestreams, Twitter-Integration, API. |\n","| **Maestra AI**         | Ja | 125+ Sprachen, Voice Cloning, Multi-Speaker, API, automatische Untertitel, Fokus auf professionelle Inhalte. |\n","| **Dubverse**           | Ja | Online-Video-Dubbing, TTS, Auto-Subtitles, API, Voice Cloning, Branding, Neo.One & Candy.Two Modelle, einfach integrierbar. |\n","| **VisualDub**          | Nein (Web-App)      | Studioqualit√§t-LipSync per Web-App, keine √∂ffentliche API dokumentiert. |\n","| **Dubbing-AI**         | Ja | Automatisiertes Dubbing f√ºr Video/Audio, Multi-Language Voiceover, API verf√ºgbar. |\n"],"metadata":{"id":"IQmMqbHJgBUm"}},{"cell_type":"markdown","source":["[ElevenLabs Dubbing](https://elevenlabs.io/app/dubbing)"],"metadata":{"id":"FUHqRG-DgH3p"}},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/under_construction_dall_e_klein.png)"],"metadata":{"id":"cjzDNlL6fxqp"}},{"cell_type":"markdown","source":["# 8 | H√§ufig verwendete Parameter\n","---"],"metadata":{"id":"-47CBLdZbLr0"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Zentrale Parameter\n","</font></p>\n","\n","\n","<p><font color='blue' size=\"4\">\n","Allgemeine Parameter\n","</font></p>\n","\n","1. **num_inference_steps**:\n","    \n","    - Bestimmt die Anzahl der Schritte im Diffusionsprozess\n","    - Mehr Schritte = bessere Qualit√§t, aber l√§ngere Berechnungszeit\n","    - Typische Werte: 20-50\n","2. **guidance_scale**:\n","    \n","    - Steuert, wie stark der Text-Prompt die Generierung beeinflusst\n","    - H√∂here Werte = st√§rkere √úbereinstimmung mit dem Prompt\n","    - Typische Werte: 7-9\n","3. **num_frames**:\n","    \n","    - Anzahl der zu generierenden Frames\n","    - Typische Werte: 16-60 (je nach Modell)\n","4. **fps** (Frames per Second):\n","    \n","    - Bestimmt die Abspielgeschwindigkeit des generierten Videos\n","    - Typische Werte: 7-30\n","\n","<p><font color='blue' size=\"4\">\n","Text-to-Video spezifische Parameter\n","</font></p>\n","\n","\n","5. **height/width**:\n","    - Aufl√∂sung des generierten Videos\n","    - Die optimalen Werte h√§ngen vom verwendeten Modell ab\n","\n","<p><font color='blue' size=\"4\">\n","Image-to-Video spezifische Parameter\n","</font></p>\n","\n","\n","6. **motion_bucket_id**:\n","    \n","    - Steuert die St√§rke der Bewegung im Video\n","    - Bereich: 0-999 (0=minimal, 999=maximal)\n","7. **noise_aug_strength**:\n","    \n","    - Bestimmt, wie stark vom Originalbild abgewichen wird\n","    - Bereich: 0-1 (0=nah am Original, 1=starke Abweichung)\n"],"metadata":{"id":"Si5CXZELbOmw"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Tipps f√ºr Einsteiger\n","</font></p>\n","\n","1. **Hardware-Anforderungen beachten:**\n","    \n","    - Video-Generierung ben√∂tigt viel Grafikspeicher (8GB+ VRAM empfohlen)\n","    - Google Colab kann eine gute Alternative sein\n","2. **Prompt-Engineering:**\n","    \n","    - Detaillierte und klare Prompts f√ºhren zu besseren Ergebnissen\n","    - Experimentiere mit verschiedenen Beschreibungen\n","3. **Parameter schrittweise anpassen:**\n","    \n","    - Beginne mit Standardwerten und passe sie dann an\n","    - Dokumentiere deine Experimente\n","4. **F√ºr Image-to-Video:**\n","    \n","    - Verwende hochwertige Ausgangsbilder\n","    - Die besten Ergebnisse erzielst du mit Bildern im Format 768x768 oder 1024x576"],"metadata":{"id":"fxBMalhDcZ4k"}},{"cell_type":"markdown","source":["# A | Aufgabe\n","---"],"metadata":{"id":"gRJbfUG9-ujC"}},{"cell_type":"markdown","source":["Bereit f√ºr eine neue Herausforderung? In diesen Aufgaben werden Sie die leistungsstarken Vision-Modelle von **MediaPipe** kennenlernen. Mit diesen Modellen k√∂nnen sie u.a. Objekterkennung, Handgesten und Gesichtsausdr√ºcke in Echtzeit analysieren. Entdecken Sie die Potenziale der GenAI. Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen.\n","\n","[MediaPipe](https://mediapipe-studio.webapps.google.com/home)\n","\n","[MediaPipe L√∂sungsleitfaden](https://ai.google.dev/edge/mediapipe/solutions/guide?hl=de)"],"metadata":{"id":"Cb6-1kyIx-RF"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Video-Objekterkennung\n","</font></p>"],"metadata":{"id":"gmigTCGFyI04"}},{"cell_type":"markdown","source":["* **Aufgabe:**\n","    * Installieren Sie die erforderlichen Bibliotheken (MediaPipe, OpenCV).\n","    * Laden Sie ein vortrainiertes Objekterkennungsmodell von MediaPipe herunter.\n","    * Schreiben Sie ein Python-Skript, das ein Bild oder einen Videostream einliest und die erkannten Objekte mit Begrenzungsrahmen und Labels visualisiert.\n","    * Experimentieren Sie mit verschiedenen Bildern und Videostreams und beobachten Sie die Ergebnisse.\n","* **Zusatzaufgabe:** Versuchen Sie die Genauigkeit der Objekterkennung zu verbessern, indem sie verschiedene Modelle oder Parameter ausprobieren.\n"],"metadata":{"id":"_DNE1RJmyNBu"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Handgestenerkennung\n","</font></p>"],"metadata":{"id":"eq_on7-3yZyE"}},{"cell_type":"markdown","source":["* **Aufgabe:**\n","    * Nutzen Sie das MediaPipe-Modell f√ºr die Handgestenerkennung.\n","    * Erstellen Sie ein Programm, das die Positionen der Handgelenke und Finger erkennt und visualisiert.\n","    * Implementieren Sie eine einfache Gestenerkennung, z. B. die Erkennung von \"Daumen hoch\" oder \"Daumen runter\".\n","    * Lassen Sie Ihr Programm auf erkannte Gesten mit einer Ausgabe im Terminal reagieren.\n","* **Zusatzaufgabe:** Die Teilnehmer k√∂nnen komplexere Gesten implementieren oder das Programm mit einer anderen Anwendung (z. B. einem Pr√§sentationsprogramm) verbinden, um Gesten zur Steuerung zu verwenden.\n","\n"],"metadata":{"id":"epm_6Q2Ny9gt"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Gesichtserkennung und -verfolgung\n","</font></p>"],"metadata":{"id":"PucCcraJzJ9i"}},{"cell_type":"markdown","source":["* **Aufgabe:**\n","    * Verwenden Sie das MediaPipe-Modell f√ºr die Gesichtserkennung und -verfolgung.\n","    * Erstellen Sie ein Programm, das Gesichter in einem Videostream erkennt und die Gesichtspunkte (z. B. Augen, Nase, Mund) verfolgt.\n","    * Implementieren Sie eine einfache Gesichtsausdrucksanalyse, z. B. die Erkennung von L√§cheln oder Stirnrunzeln.\n","    * Die Daten die erhoben werden k√∂nnen visuell in Echtzeit dargestellt werden.\n","* **Zusatzaufgabe:** Sie k√∂nnen versuchen, die Gesichtsausdrucksanalyse zu verbessern oder das Programm mit einer anderen Anwendung (z. B. einem Avatar-Programm) zu verbinden, um Gesichtsausdr√ºcke auf den Avatar zu √ºbertragen.\n"],"metadata":{"id":"AFG7kuUZzLvd"}}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["IQm3u7mNZc6b","23F6baGU4Cq2","3464d8Glm0yi","Hgn0xlXqDOyk","ccg_cmsN4chQ","Ku-TomzW498G","7KOoPcwW5H2i","0QVN1QOa5kNr","8gaiVjttbmno","K0TgYNpviMDn","fGd1iijaiSWo","ljyypqKlFt11","ayAtyWH0eH6Y","N9rNF-O1i0cT","bRQRxrNZw_7C","6AXueZa4xPgr","XWKhIzSIbs5M","9zv-Rce0fElT","BWt1r_ruguI0","k54lNATcgMO0","yBUHN0ZOHKwZ","lWa5rMj3G2Wb","8wd5ZR15bxf7","Wou65uivk35y","og_moM7Qk-Vh","6NaQWfRFl_vz","R9u3hT1RIA6Z","iFllFW8wb080","Ej4ajzVLIf00","sox0Vtx-ItWK"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)"],"metadata":{"id":"37rMyCQz2JNw"}},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b>\n","Snippets GenAI\n","</b></font> </br></p>\n","\n","---"],"metadata":{"id":"R5CfUEMJdvFQ"}},{"cell_type":"markdown","source":["# 0 | Setup & Konfiguration\n","---"],"metadata":{"id":"IQm3u7mNZc6b"}},{"cell_type":"markdown","source":["# 0.1 | Install Kursbibliothek üìå"],"metadata":{"id":"qSG-9nZaZj3q"}},{"cell_type":"code","source":["!uv pip install --system -q chromadb\n","\n","!uv pip install --system -q git+https://github.com/ralf-42/genai_lib\n","\n","from genai_lib.chromadb_statistic import (\n","    analyze_collection,\n","    display_chromadb_statistics,\n","    get_collection_chunks,\n","    display_collection_chunks,\n","    export_statistics_to_json\n",")"],"metadata":{"id":"zYXfr5UlZnfk","executionInfo":{"status":"ok","timestamp":1753460981160,"user_tz":-120,"elapsed":6956,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# 0.2 | Import Kursbibliothek üìå"],"metadata":{"id":"8yUdGhEHadmI"}},{"cell_type":"markdown","source":[],"metadata":{"id":"zSZWo_rgZphr"}},{"cell_type":"markdown","source":["# 0.3 | Kursbibliothek - Inhalt"],"metadata":{"id":"DnLL7z-4ocSH"}},{"cell_type":"markdown","source":["\n","\n","üõ†Ô∏è utilities.py"],"metadata":{"id":"F_sLhNUAROy5"}},{"cell_type":"markdown","source":["**Umgebungs- und Setup-Funktionen:**\n","\n","`check_environment()`\n","\n","- Zeigt die aktuelle Python-Version an\n","- Listet alle installierten LangChain-Bibliotheken auf\n","- Unterdr√ºckt st√∂rende Deprecation-Warnungen\n","\n","`install_packages(modules)`\n","\n","- Installiert Python-Module automatisch mit `uv pip install`\n","- Pr√ºft vorher, ob Module bereits verf√ºgbar sind\n","- Optimiert f√ºr Google Colab mit ruhiger Installation\n","\n","`get_ipinfo()`\n","\n","- Ruft Geoinformationen zur aktuellen √∂ffentlichen IP-Adresse ab\n","- Zeigt IP, Standort, Provider und weitere Netzwerkdaten an\n","\n","`setup_api_keys(key_names, create_globals=True)`\n","\n","- L√§dt API-Keys aus Google Colab userdata\n","- Setzt sie als Umgebungsvariablen und optional als globale Variablen\n","- Unterst√ºtzt mehrere Keys gleichzeitig (OpenAI, Anthropic, Hugging Face, etc.)\n","\n","`mprint(text)`\n","\n","- Gibt Text als formatiertes Markdown in Jupyter-Notebooks aus\n","- Nutzt IPython's `display()` und `Markdown()` f√ºr bessere Darstellung\n","\n","`process_response(response)`\n","\n","- Extrahiert strukturierte Informationen aus LLM-Antworten\n","- Parst Token-Nutzungsdaten (Prompt, Completion, Total)\n","- Gibt bereinigten Text und Metadaten als Dictionary zur√ºck\n","\n","\n","---"],"metadata":{"id":"mLJCYHpURRSo"}},{"cell_type":"markdown","source":["\n","\n","üìã prepare_prompt.py"],"metadata":{"id":"W5v9fJMbQ_jK"}},{"cell_type":"markdown","source":["\n","\n","`apply_prepare_framework(task, role, tone, word_limit)`\n","- **Zweck**: Erstellt strukturierte Prompts nach dem PREPARE-Framework\n","- **Parameter**: Aufgabe, Rolle, Tonfall, Wortlimit\n","- **Ausgabe**: Vollst√§ndig formatierter Prompt mit 7 Komponenten (P-R-E-P-A-R-E)\n","- **Anwendung**: Systematische Prompt-Erstellung f√ºr bessere LLM-Ergebnisse\n","\n","---\n"],"metadata":{"id":"HtA0rBK5RC25"}},{"cell_type":"markdown","source":["\n","üé® show_md.py"],"metadata":{"id":"wqZvD_5YRJj8"}},{"cell_type":"markdown","source":["Markdown-Display-Funktionen f√ºr Jupyter Notebooks:\n","\n","- **`show_md(text, prefix)`** - Basis-Markdown-Anzeige mit optionalem Prefix\n","- **`show_title(text)`** - Zeigt Titel mit üí°-Emoji (`# Titel üí°`)\n","- **`show_subtitle(text)`** - Zeigt Untertitel (`## Untertitel`)\n","- **`show_info(text)`** - Info-Meldung mit ‚ÑπÔ∏è-Symbol\n","- **`show_warning(text)`** - Warnung mit ‚ö†Ô∏è-Symbol\n","- **`show_success(text)`** - Erfolgsmeldung mit ‚úÖ-Symbol\n","\n","**Anwendung**: Strukturierte und visuell ansprechende Notebook-Ausgaben\n","\n","---"],"metadata":{"id":"vcbrqq7LRGoy"}},{"cell_type":"markdown","source":["\n","\n","üéØ Typische Anwendung"],"metadata":{"id":"g_WI9yZkRgA5"}},{"cell_type":"markdown","source":["```python\n","# 1. Umgebung pr√ºfen und API-Keys laden\n","check_environment()\n","setup_api_keys([\"OPENAI_API_KEY\"])\n","\n","# 2. Strukturierte Notebook-Ausgaben\n","show_title(\"Mein KI-Projekt\")\n","show_info(\"Experiment gestartet\")\n","\n","# 3. PREPARE-Prompts erstellen\n","prompt = apply_prepare_framework(\n","    task=\"Erkl√§re maschinelles Lernen\",\n","    role=\"KI-Tutor\",\n","    tone=\"verst√§ndlich\"\n",")\n","\n","# 4. LLM-Antworten verarbeiten\n","result = process_response(llm_response)\n","show_success(f\"Antwort erhalten: {result['tokens_total']} Tokens\")\n","```\n","\n","**Ideal f√ºr**: KI-Kurse, Jupyter-Notebooks, LangChain-Projekte, strukturierte Prompt-Engineering"],"metadata":{"id":"XeJ4r80YRWFE"}},{"cell_type":"markdown","source":["# 0.4 | Umgebung einrichten üìå"],"metadata":{"id":"0hI3GKnNbJ99"}},{"cell_type":"code","source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>Umgebung einrichten</font> </br></p>\n","!uv pip install --system -q git+https://github.com/ralf-42/genai_lib\n","from genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages\n","setup_api_keys(['OPENAI_API_KEY', 'HF_TOKEN'], create_globals=False)\n","print()\n","check_environment()\n","print()\n","get_ipinfo()\n","# Bei Bedarf: Trennen zwischen Installationsname () und Importname (f√ºr Python) beide Angaben in Klammern\n","# install_packages([('markitdown[all]', 'markitdown'), 'langchain_chroma', ]"],"metadata":{"id":"xlCNUpx_bOqq","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 0.5 | Standard-Importe f√ºr GenAI"],"metadata":{"id":"23F6baGU4Cq2"}},{"cell_type":"code","source":["# Moderne LangChain-Imports (Version 0.2.0+)\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# Standard-Python-Imports\n","import os\n","from IPython.display import Markdown, display"],"metadata":{"id":"aTr6UN0h4IG-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 0.6 | Namenskonventionen"],"metadata":{"id":"3464d8Glm0yi"}},{"cell_type":"markdown","source":["**Empfohlene Namenskonventionen (snake_case):**\n","\n","**Variablen:**\n","- `user_input` oder `input` - Benutzereingabe\n","- `system_prompt` - Systemnachricht\n","- `chat_history` oder `history` - Chatverlauf\n","- `model_name` - Modellname\n","- `temperature` - Temperatur-Parameter\n","- `response` - API-Antwort\n","- `result` - Verarbeitetes Ergebnis\n","\n","**LangChain-Objekte:**\n","- `prompt_template` - Prompt-Vorlage\n","- `prompt` - Fertiger Prompt\n","- `llm` - Sprachmodell\n","- `chain` - Verarbeitungskette\n","- `parser` - Output-Parser"],"metadata":{"id":"JHxMlnzWm4p8"}},{"cell_type":"markdown","source":["# 0.7 | Temperatur-Guidelines"],"metadata":{"id":"Hgn0xlXqDOyk"}},{"cell_type":"markdown","source":["**Temperatur-Richtlinien f√ºr KI-Modelle**\n","\n","| Temperatur | Kategorie | Wert | Anwendungsbereich | Beschreibung |\n","|------------|-----------|------|-------------------|--------------|\n","| **Faktisch** | `factual` | 0.0 | Mathematik, Code, Datenanalyse | Deterministische Ausgaben f√ºr pr√§zise, reproduzierbare Ergebnisse |\n","| **Ausgewogen** | `balanced` | 0.3 | Allgemeine Fragen, Erkl√§rungen | Leicht variierte Antworten bei gleichbleibender Genauigkeit |\n","| **Kreativ** | `creative` | 0.7 | Brainstorming, Texterstellung | Vielf√§ltige und innovative Antworten f√ºr kreative Aufgaben |\n","| **Sehr kreativ** | `very_creative` | 1.0 | Experimentelle Inhalte | Maximale Kreativit√§t und Unvorhersagbarkeit |\n","\n","**Verwendungshinweise**\n","\n","- **Niedrige Temperatur (0.0-0.3)**: F√ºr sachliche, pr√§zise Antworten\n","- **Mittlere Temperatur (0.4-0.6)**: F√ºr ausgewogene Kommunikation\n","- **Hohe Temperatur (0.7-1.0)**: F√ºr kreative und experimentelle Ausgaben\n","\n"],"metadata":{"id":"ce4MVxa6DXm-"}},{"cell_type":"markdown","source":["# 1 | Erste Schritte\n","---"],"metadata":{"id":"ccg_cmsN4chQ"}},{"cell_type":"markdown","source":["# 1.1 | Einfachster LLM-Aufruf"],"metadata":{"id":"Ku-TomzW498G"}},{"cell_type":"code","source":["# Einfachster LLM-Aufruf\n","llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n","user_input = \"Erkl√§re mir KI in einem Satz\"\n","\n","# Direkter Aufruf\n","response = llm.invoke([HumanMessage(content=user_input)])\n","print(response.content)"],"metadata":{"id":"i7cIWKYm5FD1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1.2 | Mit System-Prompt"],"metadata":{"id":"7KOoPcwW5H2i"}},{"cell_type":"code","source":["# System-Prompt mit modernem Ansatz\n","system_prompt = \"Du bist ein hilfreicher KI-Assistent. Antworte kurz und pr√§zise.\"\n","user_input = \"Was ist Machine Learning?\"\n","\n","messages = [\n","    SystemMessage(content=system_prompt),\n","    HumanMessage(content=user_input)\n","]\n","\n","response = llm.invoke(messages)\n","print(response.content)"],"metadata":{"id":"roPof5IR5NPZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1.3 | Streaming-Ausgabe"],"metadata":{"id":"0QVN1QOa5kNr"}},{"cell_type":"code","source":["# Streaming f√ºr Echtzeit-Ausgabe\n","print(\"ü§ñ Antwort: \", end=\"\")\n","for chunk in llm.stream([HumanMessage(content=\"Erz√§hle eine kurze Geschichte √ºber KI\")]):\n","    print(chunk.content, end=\"\", flush=True)\n","print(\"\\n\")"],"metadata":{"id":"EJVCuJD05pky"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2 | Prompts & Templates\n","---"],"metadata":{"id":"8gaiVjttbmno"}},{"cell_type":"markdown","source":["# 2.1 | Einfacher String-Prompt"],"metadata":{"id":"K0TgYNpviMDn"}},{"cell_type":"code","source":["# Einfacher String-Prompt\n","frage = \"Wer ist Albert Einstein?\"\n","user_input = f\"Du bist ein hilfreicher Assistent. Beantworte die Frage: {frage}\"\n","\n","# Als HumanMessage verwenden\n","response = llm.invoke([HumanMessage(content=user_input)])\n","print(f\"üéØ Antwort: {response.content}\")"],"metadata":{"id":"Crwgzo0XiRHt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.2 | PromptTemplate"],"metadata":{"id":"fGd1iijaiSWo"}},{"cell_type":"code","source":["from langchain_core.prompts import PromptTemplate\n","\n","# Prompt definieren\n","prompt = PromptTemplate.from_template(\n","    \"Systemanweisung: {system_prompt}\\n\"\n","    \"Benutzereingabe: {input}\\n\"\n","    \"Antwort:\"\n",")"],"metadata":{"id":"6NEV8D11iViQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.3 | ChatPromptTemplate"],"metadata":{"id":"ljyypqKlFt11"}},{"cell_type":"code","source":["# Modernes ChatPromptTemplate (einfache Version)\n","prompt_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent.\"),\n","    (\"human\", \"{user_input}\")\n","])"],"metadata":{"id":"tdd5vchMFzml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Direkt mit Chain verwenden (moderne LCEL-Syntax)\n","chain = prompt_template | llm | StrOutputParser()\n","result = chain.invoke({\"user_input\": \"Was ist Machine Learning?\"})\n","\n","print(f\"üéØ Ergebnis: {result}\")"],"metadata":{"id":"OsYo1K4TF2VT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.4 | ChatPromptTemplate - History üìå"],"metadata":{"id":"ayAtyWH0eH6Y"}},{"cell_type":"code","source":["from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","\n","# Erweitert mit Chat-History\n","prompt_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"human\", \"{user_input}\")\n","])"],"metadata":{"id":"x_VtyM8CeL9W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Moderne Chain-Syntax\n","chain = prompt_template | llm | StrOutputParser()\n","\n","# Ausf√ºhrung mit Input-Dictionary\n","result = chain.invoke({\n","    \"system_prompt\": \"Du bist ein Experte f√ºr Wissenschaft.\",\n","    \"chat_history\": [],  # Leer f√ºr erstes Gespr√§ch\n","    \"user_input\": \"Erkl√§re mir die Relativit√§tstheorie\"\n","})\n","\n","print(f\"üìã Ergebnis: {result}\")"],"metadata":{"id":"jM72ewbNFc2H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.5 | FewShotPromptTemplate"],"metadata":{"id":"N9rNF-O1i0cT"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, FewShotChatMessagePromptTemplate\n","from langchain_core.messages import HumanMessage, AIMessage\n","\n","# Beispiele direkt als Liste von (input, output)\n","examples = [\n","    {\"input\": \"Was ist die Hauptstadt von Frankreich?\", \"output\": \"Die Hauptstadt von Frankreich ist Paris.\"},\n","    {\"input\": \"Wie viele Kontinente gibt es?\", \"output\": \"Es gibt sieben Kontinente.\"}\n","]"],"metadata":{"id":"mFxG58QEi8jf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Few-Shot Prompt als Nachrichtenblock\n","few_shot_template = FewShotChatMessagePromptTemplate.from_examples(\n","    examples=examples,\n","    example_prompt=ChatPromptTemplate.from_messages([\n","        (\"human\", \"{user_input}\"),\n","        (\"ai\", \"{output}\")\n","    ])\n",")"],"metadata":{"id":"xcWLkftvvPIx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Finales PromptTemplate mit Systemrolle, Few-Shot-Beispielen, Chat-Verlauf und neuer Eingabe\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    *few_shot.messages,\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"human\", \"{user_input}\")\n","])"],"metadata":{"id":"OsO6aQ7cvQcv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.6 | ChatMessagePromptTemplate"],"metadata":{"id":"bRQRxrNZw_7C"}},{"cell_type":"code","source":["from langchain_core.prompts.chat import ChatMessagePromptTemplate\n","\n","moderator_message_template = ChatMessagePromptTemplate(\n","    role=\"moderator\",\n","    prompt=\"Bitte bleib respektvoll und h√∂flich.\"\n",")"],"metadata":{"id":"GLrSc2XXxDSx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent.\"),\n","    moderator_prompt,  # eigene Rolle\n","    (\"human\", \"{user_input}\")\n","])"],"metadata":{"id":"q5APfPbDxHh2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.7 | partial_variables"],"metadata":{"id":"6AXueZa4xPgr"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","# Prompt mit zwei Variablen\n","template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    (\"human\", \"{user_input}\")\n","])\n","\n","# system_prompt wird hier \"fest verdrahtet\"\n","prompt_with_fixed_system = template.partial(system_prompt=\"Du bist ein KI-Experte f√ºr Geschichte.\")"],"metadata":{"id":"NVs62-IzxWFU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Jetzt muss man beim .invoke() nur noch input √ºbergeben\n","response = (prompt_with_fixed_system | llm | parser).invoke({\n","    \"user_input\": \"Was war die Ursache des Drei√üigj√§hrigen Krieges?\"\n","})"],"metadata":{"id":"zpZX7bAlxa3u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3 | Model & Chains\n","---"],"metadata":{"id":"XWKhIzSIbs5M"}},{"cell_type":"markdown","source":["# 3.1 | Model - simple"],"metadata":{"id":"9zv-Rce0fElT"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","model = \"gpt-4o-mini\"\n","temperature = 0.0\n","llm = ChatOpenAI(model=model, temperatur=temperatur)\n","\n","# Modell aufrufen\n","response = llm.invoke(prompt)"],"metadata":{"id":"0eQCRJ9JfKxU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3.3 | Chain üìå"],"metadata":{"id":"BWt1r_ruguI0"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","\n","model = \"gpt-4o-mini\"\n","temperature = 0.0\n","llm = ChatOpenAI(model=model, temperature=temperature)\n","\n","parser = StrOutputParser()  # Wenn noch kein eigener Parser existiert\n","\n","# LCEL-Chain\n","chain = prompt | llm | parser"],"metadata":{"id":"NpvGhi7fgx8I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Beispielhafte Eingabewerte\n","input_variables = {\n","    \"system_prompt\": system_prompt,\n","    \"chat_history\": history,\n","    \"input\": user_input\n","}\n","\n","# Aufruf der Kette mit Eingabewerten\n","response = chain.invoke(input_variables)"],"metadata":{"id":"uV6TfLiLuDMp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3.3 | Chat-History üìå"],"metadata":{"id":"k54lNATcgMO0"}},{"cell_type":"code","source":["# Historie aktualisieren\n","history.append(HumanMessage(content=user_input))\n","history.append(AIMessage(content=response.content))"],"metadata":{"id":"3IqqDB-1gR8d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3.4 | Konfigurationsfunktion"],"metadata":{"id":"yBUHN0ZOHKwZ"}},{"cell_type":"code","source":["# Verschiedene LLM-Konfigurationen (moderne Syntax)\n","def create_llm(model_name: str = \"gpt-4o-mini\", temp: float = 0.0):\n","    \"\"\"\n","    Erstellt ein LLM mit modernen Parametern\n","    \"\"\"\n","    return ChatOpenAI(\n","        model=model_name,\n","        temperature=temp,\n","        max_tokens=1000,\n","        timeout=30,\n","        max_retries=2\n","    )\n","\n","# Verschiedene Modell-Varianten\n","llm_creative = create_llm(\"gpt-4o-mini\", 0.8)  # Kreativ\n","llm_factual = create_llm(\"gpt-4o-mini\", 0.0)   # Faktisch\n","llm_balanced = create_llm(\"gpt-4o-mini\", 0.3)  # Ausgewogen\n","\n","print(\"ü§ñ LLM-Instanzen erstellt\")"],"metadata":{"id":"B7JMtR4HHO4h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3.5 | Runnables-Methoden"],"metadata":{"id":"lWa5rMj3G2Wb"}},{"cell_type":"markdown","source":["Jedes Runnable unterst√ºtzt standardm√§√üig folgende Methoden:\n","\n","1. **invoke()**: F√ºr einzelne, synchrone Anfragen\n","2. **batch()**: Verarbeitet mehrere Eingaben parallel\n","3. **stream()**: Gibt Teilergebnisse zur√ºck, sobald sie verf√ºgbar sind\n","4. **ainvoke()**: Asynchrone Version von invoke\n","5. **abatch()**: Asynchrone Version von batch\n","6. **astream()**: Asynchrone Version von stream"],"metadata":{"id":"hUrE01P8G58H"}},{"cell_type":"markdown","source":["# 4 | Output Parser\n","---"],"metadata":{"id":"8wd5ZR15bxf7"}},{"cell_type":"markdown","source":["# 4.1 | Simple"],"metadata":{"id":"Wou65uivk35y"}},{"cell_type":"code","source":["result = {\n","    \"text\": response.content,\n","    \"tokens_used\": response.response_metadata[\"token_usage\"][\"total_tokens\"]\n","}"],"metadata":{"id":"UNWKS2VNk6hT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4.2 | StrOutputParser üìå"],"metadata":{"id":"og_moM7Qk-Vh"}},{"cell_type":"code","source":["from langchain_core.output_parsers.string import StrOutputParser\n","\n","parser = StrOutputParser()"],"metadata":{"id":"iJB4yap0lRq3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chain = prompt| llm | parser\n","response = chain.invoke()"],"metadata":{"id":"wM_MqJzYHiJn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4.3 | SimpleJsonOutputParser"],"metadata":{"id":"6NaQWfRFl_vz"}},{"cell_type":"code","source":["from langchain.output_parsers import SimpleJsonOutputParser\n","\n","# Nur wenn keine spezifische Struktur ben√∂tigt wird\n","simple_parser = SimpleJsonOutputParser()"],"metadata":{"id":"XEYphtWRl___"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4.4 | Strukturierter Output mit Pydantic"],"metadata":{"id":"R9u3hT1RIA6Z"}},{"cell_type":"code","source":["from pydantic import BaseModel, Field\n","from langchain_core.output_parsers import PydanticOutputParser\n","\n","# Pydantic-Model definieren\n","class QuestionAnswer(BaseModel):\n","    question: str = Field(description=\"Die gestellte Frage\")\n","    answer: str = Field(description=\"Die Antwort auf die Frage\")\n","    confidence: int = Field(description=\"Vertrauen in die Antwort (0-100)\")\n","    category: str = Field(description=\"Kategorie der Frage\")\n","\n","# Parser erstellen\n","pydantic_parser = PydanticOutputParser(pydantic_object=QuestionAnswer)\n","\n","# Chain mit strukturiertem Output\n","structured_chain = (\n","    ChatPromptTemplate.from_messages([\n","        (\"system\", \"Du bist ein Experte. {format_instructions}\"),\n","        (\"user\", \"{question}\")\n","    ]).partial(format_instructions=pydantic_parser.get_format_instructions())\n","    | llm\n","    | pydantic_parser\n",")\n","\n","# Testen\n","try:\n","    structured_result = structured_chain.invoke({\"question\": \"Was ist Photosynthese?\"})\n","    print(f\"üì¶ Strukturiertes Ergebnis: {structured_result}\")\n","    print(f\"üéØ Antwort: {structured_result.answer}\")\n","    print(f\"üìä Vertrauen: {structured_result.confidence}%\")\n","except Exception as e:\n","    print(f\"‚ùå Pydantic-Parser Fehler: {e}\")"],"metadata":{"id":"97ih8v9QIHp9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5 | Chat-History & Memory\n","---"],"metadata":{"id":"iFllFW8wb080"}},{"cell_type":"markdown","source":["# 5.1 | Einfache History"],"metadata":{"id":"Ej4ajzVLIf00"}},{"cell_type":"code","source":["# Moderne Chat-History-Verwaltung\n","chat_history = []\n","\n","def add_to_history(user_msg: str, ai_msg: str):\n","    \"\"\"\n","    F√ºgt Nachrichten zur Chat-History hinzu\n","    \"\"\"\n","    chat_history.extend([\n","        HumanMessage(content=user_msg),\n","        AIMessage(content=ai_msg)\n","    ])\n","\n","# Chat-Chain mit History\n","history_chain = (\n","    ChatPromptTemplate.from_messages([\n","        (\"system\", \"Du bist ein hilfreicher Assistent.\"),\n","        MessagesPlaceholder(variable_name=\"history\"),\n","        (\"user\", \"{input}\")\n","    ])\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","# Erste Nachricht\n","user_input = \"Mein Name ist Max\"\n","result1 = history_chain.invoke({\"history\": chat_history, \"input\": user_input})\n","add_to_history(user_input, result1)\n","\n","print(f\"ü§ñ Antwort 1: {result1}\")\n","print(f\"üí¨ History-L√§nge: {len(chat_history)}\")"],"metadata":{"id":"7L7eEwMrIjcx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5.2 | Konversation mit History (modern)"],"metadata":{"id":"sox0Vtx-ItWK"}},{"cell_type":"code","source":["def chat_with_memory(user_input: str, max_history: int = 10):\n","    \"\"\"\n","    Chattet mit dem LLM unter Ber√ºcksichtigung der History\n","    \"\"\"\n","    # History begrenzen\n","    recent_history = chat_history[-max_history:] if len(chat_history) > max_history else chat_history\n","\n","    # Chain ausf√ºhren\n","    result = history_chain.invoke({\n","        \"history\": recent_history,\n","        \"input\": user_input\n","    })\n","\n","    # History aktualisieren\n","    add_to_history(user_input, result)\n","\n","    return result\n","\n","# Fortsetzung der Konversation\n","response2 = chat_with_memory(\"Wie war nochmal mein Name?\")\n","print(f\"ü§ñ Antwort 2: {response2}\")"],"metadata":{"id":"EBr8CBP9I2bK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6 | RAG\n","---"],"metadata":{"id":"UIGK8g4ascXW"}},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/under_construction_dall_e_klein.png)"],"metadata":{"id":"ygZwKDrIslRJ"}},{"cell_type":"markdown","source":["# 7 | Multimodal Models\n","---"],"metadata":{"id":"j64yLrhOsyl9"}},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/under_construction_dall_e_klein.png)"],"metadata":{"id":"oM2svs_Os2FJ"}},{"cell_type":"markdown","source":["# 8 | Agents\n","---"],"metadata":{"id":"lemGiPdWtZzo"}},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/under_construction_dall_e_klein.png)"],"metadata":{"id":"QPm280Imtd0l"}},{"cell_type":"markdown","source":["# 9 | Local & Open Source Models\n","---"],"metadata":{"id":"3TA9fSiftLL_"}},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/under_construction_dall_e_klein.png)"],"metadata":{"id":"Auk0z0LmtUnL"}}]}
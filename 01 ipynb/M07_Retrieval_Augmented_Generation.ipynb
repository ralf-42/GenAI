{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyM3URQWpSt+sWgt/5bmximR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<p><font size=\"7\" color='grey'> <b>\n","Anwendung Generativer KI\n","</b></font> </br></p>"],"metadata":{"id":"Ih2CTVBnArVZ"}},{"cell_type":"markdown","source":["<p><font size=\"6\" color='grey'> <b>\n","Modul 07: Retrieval Augmented Generation\n","</b></font> </br></p>\n","\n","\n","---"],"metadata":{"id":"6jJZ7wbdArVc"}},{"cell_type":"markdown","source":["# 1 | Übersicht\n","---"],"metadata":{"id":"oYvUY6gMBKO1"}},{"cell_type":"markdown","metadata":{"id":"pC9A-LaYhsta"},"source":["# 6.1: Einführung in Retrieval-Augmented Generation (RAG)\n","\n","Large Language Models (LLMs), wie sie in LangChain integriert sind, sind leistungsstarke Tools zum Verarbeiten und Verstehen großer Textmengen. Diese Modelle können genutzt werden, um Fragen basierend auf dem Inhalt eines Dokuments zu beantworten, was sie für Aufgaben, die Informationsextraktion und -verständnis erfordern, äußerst wertvoll macht.\n","\n","Eine der Techniken, die in Verbindung mit LLMs zur dokumentenbasierten Beantwortung von Fragen verwendet werden, ist die Retrieval-Augmented Generation (RAG). RAG ist ein hybrider Ansatz, der die Stärken von Informationsabrufsystemen mit den generativen Fähigkeiten von Sprachmodellen kombiniert. Hier ist ein kurzer Überblick über die Funktionsweise von RAG:\n","\n","1. **Abrufphase:** Wenn eine Frage gestellt wird, ruft das RAG-System zunächst relevante Dokumente oder Dokumentsegmente aus einem großen Korpus ab. Dieser Abruf basiert auf der Ähnlichkeit des Inhalts der Dokumente mit der Frage. Die Idee besteht darin, Textbeweise zu finden, die die Antwort enthalten könnten.\n","2. **Augmentationsphase:** Die abgerufenen Dokumente werden dann als Kontextinformationen in ein Sprachmodell eingespeist. Dieser Schritt ist entscheidend, da er dem Sprachmodell spezifische, für die Frage relevante Daten liefert, die in der vorab trainierten Wissensbasis des Modells möglicherweise nicht vorhanden sind.\n","3. **Generierungsphase:** Mit dem Kontext der abgerufenen Dokumente generiert das Sprachmodell eine Antwort. Das Modell synthetisiert die Informationen aus den Dokumenten und verwendet sein Sprach- und Kontextverständnis, um eine kohärente und genaue Antwort zu formulieren.\n","\n","Durch die Kombination des Abrufs relevanter Informationen mit der generativen Leistung von LLMs verbessert RAG effektiv die Fähigkeit des Modells, präzise Antworten basierend auf dem Inhalt eines Dokuments zu liefern. Dieser Ansatz ist besonders nützlich in Szenarien, in denen die direkte Antwort auf eine Frage möglicherweise nicht ohne weiteres in den Trainingsdaten des Modells verfügbar ist und das System externe Beweise abrufen muss, um die Antwortgenerierung zu unterstützen. Dies macht die Integration von LLMs mit RAG durch LangChain zu einer robusten Lösung für die dokumentenbasierte Beantwortung von Fragen, die ein tiefes Verständnis und differenzierte Antworten in verschiedenen Bereichen und Arten von Anfragen ermöglicht.\n","\n","Wir beginnen mit dem Öffnen einer Verbindung zu einem OpenAI LLM-Modell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfISNTpwOKiZ"},"outputs":[],"source":["from langchain.chains.summarize import load_summarize_chain\n","from langchain.document_loaders import PyPDFLoader, TextLoader\n","from langchain import OpenAI, PromptTemplate\n","from langchain_openai import ChatOpenAI\n","from IPython.display import display_markdown\n","\n","MODEL = 'gpt-4o-mini'\n","\n","llm = ChatOpenAI(\n","        model=MODEL,\n","        temperature=0.2,\n","        n=1\n","    )\n"]},{"cell_type":"markdown","metadata":{"id":"OLUWbW7NRp4m"},"source":["Wir werden nun mehrere PDFs laden, die wir mit Fragen abfragen können. Wir können zu diesen Dokumenten Fragen mit Informationen stellen, die nicht Teil des Basismodells sind. Wir erstellen Loader für jedes der PDFs, damit wir sie zur einfachen Abfrage in einen Vektorspeicher laden können.\n","\n","Der folgende Codeausschnitt zeigt, wie man mit einer bestimmten Funktion „load_summarize_chain“ einen Zusammenfassungsprozess mithilfe eines Large Language Model (LLM) mit einem Kettentyp „map_reduce“ einrichtet. Zunächst wird mithilfe des „PyPDFLoader“ ein PDF von der angegebenen URL („https://arxiv.org/pdf/1706.03762“) geladen. Das geladene Dokument wird dann in handhabbare Teile aufgeteilt („load_and_split“). Diese Teile werden in die Zusammenfassungskette („chain.run(docs)“) eingespeist, die den Inhalt verarbeitet und verdichtet. Schließlich wird der zusammengefasste Inhalt im Markdown-Format direkt in der Ausgabeumgebung angezeigt, wodurch sichergestellt wird, dass die Formatierung der Zusammenfassung erhalten bleibt."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wIVM9OwssPZm","outputId":"fcdb3459-5cec-4a71-a8f9-60d636c9a550"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading: https://arxiv.org/pdf/1706.03762\n","Reading: https://arxiv.org/pdf/1810.04805\n","Reading: https://arxiv.org/pdf/2005.14165\n","Reading: https://arxiv.org/pdf/1910.10683\n"]}],"source":["urls = [\n","  \"https://arxiv.org/pdf/1706.03762\",\n","  \"https://arxiv.org/pdf/1810.04805\",\n","  \"https://arxiv.org/pdf/2005.14165\",\n","  \"https://arxiv.org/pdf/1910.10683\"\n","]\n","\n","loaders = []\n","\n","chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n","\n","for url in urls:\n","print(f\"Lesen: {url}\")\n","  loader = PyPDFLoader(url)\n","  loaders.append(loader)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EiuExFf4Z50G"},"source":["Als nächstes laden wir Einbettungen aus den vier Dokumenten in [ChromaDB](https://www.trychroma.com/). Diese Einbettungen ermöglichen es, die Eingabeaufforderung mit Informationen aus den von uns geladenen PDF-Dokumenten zu erweitern."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ms1lqPY_5HTG"},"outputs":[],"source":["from langchain.indexes import VectorstoreIndexCreator\n","from langchain.document_loaders import PyPDFDirectoryLoader\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_community.vectorstores.inmemory import InMemoryVectorStore\n","\n","embeddings_model = OpenAIEmbeddings()\n","index = VectorstoreIndexCreator(embedding=embeddings_model,vectorstore_cls=InMemoryVectorStore).from_loaders(loaders)"]},{"cell_type":"markdown","metadata":{"id":"s3T-ADPcap7Q"},"source":["Nachdem die Einbettungen geladen sind, können wir das Modell nach Informationen abfragen, die nur in den Dokumenten enthalten sind."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"j6C1ACgT5is6","outputId":"1d741d55-d5ff-4343-f555-ca4d2f0c5494"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'The left figure in Figure 2 demonstrates Scaled Dot-Product Attention.'"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["query = \"Which figure demonstrates Scaled Dot-Product Attention?\"\n","\n","index.query(query,llm=llm)"]},{"cell_type":"markdown","metadata":{"id":"ErkO7KR9apEY"},"source":["## Einschränkungen von RAG\n","\n","Language Model Retrieval-Augmented Generation (LLM RAG) kombiniert die Fähigkeiten großer Sprachmodelle mit externen Datenabrufmechanismen. Dieser Ansatz verbessert die Leistung von Sprachmodellen, indem er Zugriff auf spezifische, oft proprietäre Daten bietet, die dem allgemeinen Wissen des vorab trainierten Modells möglicherweise fehlen. Die Wirksamkeit von LLM RAG nimmt jedoch erheblich ab, wenn die erweiterten Daten bereits Allgemeinwissen sind und inhärent im Basismodell enthalten sind.\n","\n","LLM RAG eignet sich hervorragend für Szenarien, in denen Benutzer proprietäre oder hochspezialisierte Informationen benötigen. In Bereichen wie Finanzen, Recht oder technischen Branchen sind spezifische Datensätze, Berichte oder Dokumente für genaue Antworten unerlässlich. Die einzigartige Fähigkeit von RAG, diese externen Daten zu nutzen, gewährleistet hochpräzise und kontextbezogen relevante Antworten. Obwohl das Basismodell umfassend anhand einer breiten Palette öffentlich verfügbarer Informationen trainiert wurde, fehlt ihm möglicherweise die Tiefe oder die neuesten Updates, die für diese Nischenbereiche erforderlich sind. Daher führt die Integration proprietärer Datensätze durch RAG zu genaueren und kontextbezogen angereicherten Ergebnissen.\n","\n","Es ist wichtig zu bedenken, dass sich die Vorteile von LLM RAG erheblich verringern, wenn die zu erweiternden Daten bereits Allgemeinwissen sind. Das Basismodell wird vorab anhand großer Datenmengen trainiert, die ein breites Spektrum allgemeiner Wissensthemen abdecken. Wenn Abfragen Informationen beinhalten, die in diesen allgemeinen Rahmen fallen, generiert das Basismodell daher genaue und informative Antworten, ohne dass eine externe Erweiterung erforderlich ist. In solchen Fällen bietet die Verwendung von RAG keinen nennenswerten Mehrwert und kann zu unnötiger Komplexität und Verarbeitungsaufwand führen.\n","\n","Darüber hinaus führt das Abrufen von Daten, die das Modell bereits gut versteht, zu Ineffizienzen. Das umfangreiche Vortraining des Basismodells umfasst unterschiedliche Texte, was bedeutet, dass seine interne Wissensbasis normalerweise für allgemeine Wissensabfragen ausreicht. Wenn man sich in diesen Fällen auf RAG verlässt, werden die Stärken des Modells daher nicht effektiv genutzt und die Gesamteffizienz des Systems beeinträchtigt.\n","\n","Zusammenfassend lässt sich sagen, dass LLM RAG zwar sehr hilfreich ist, wenn es um die Erweiterung von Sprachmodellen mit proprietären oder hochspezialisierten Daten geht, seine Wirksamkeit jedoch nachlässt, wenn es um allgemeines Wissen geht. Das umfassende Training des Basismodells deckt bereits eine große Menge allgemeiner Informationen ab, sodass eine RAG-Erweiterung in solchen Kontexten überflüssig wird. Setzen Sie RAG daher strategisch ein, da es die größte Verbesserung in Bereichen bietet, in denen Zugriff auf proprietäre Daten erforderlich ist, die das Basismodell möglicherweise nur teilweise abdeckt."]},{"cell_type":"markdown","metadata":{"id":"j7K2Fzfp3Vux"},"source":["# Modul 6 Aufgabe\n","\n","Die erste Aufgabe findet ihr hier: [assignment 6](https://github.com/jeffheaton/app_generative_ai/blob/main/assignments/assignment_yourname_t81_559_class6.ipynb)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NemgD3ZI3Vux"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"WE_upLDqEdo6"},"source":["# 6.2: Einführung in ChromaDB\n","\n","In diesem Modul untersuchen wir die grundlegende Verwendung von Chroma, um Informationen effizient durch Einbettungen zu speichern und abzurufen. Diese grundlegende Technik bildet das Rückgrat zahlreicher fortschrittlicher KI-Anwendungen.\n","\n","* [ChromaDB](https://www.trychroma.com/)\n","\n","Einbettungen dienen als KI-native Darstellung verschiedener Datentypen und sind daher ideal für die Verwendung mit einer breiten Palette KI-gestützter Tools und Algorithmen. Sie können die Essenz von Text, Bildern und bald sogar Audio und Video zusammenfassen.\n","\n","Ein Einbettungsmodell verarbeitet Daten, um eine Einbettung zu erzeugen, die Vektorzahlen erzeugt. Die Entwickler haben das Modell so konzipiert, dass ähnliche Daten, wie etwa Texte mit analogen Bedeutungen oder Bilder mit vergleichbarem Inhalt, Vektoren ergeben, die im Vektorraum näher beieinander liegen, während unähnliche Daten zu weiter entfernten Vektoren führen.\n","\n","Die Architektur von ChromaDB ist in Abbildung 6.ChromaDB dargestellt.\n","\n","**Abbildung 6.ChromaDB: Die Architektur von ChromaDB**\n","![ChromaDB](https://data.heatonresearch.com/images/wustl/app_genai/hrm4.svg)\n","\n","Die Kern-API von ChromaDB besteht aus nur vier API-Aufrufen, von denen der erste den Client einrichtet, den Sie für die Interaktion mit ChromaDB verwenden.\n","\n","```\n","import chromadb\n","client = chromadb.HttpClient()\n","```\n","\n","Mit dem Client können Sie eine Sammlung erstellen, die Dokumente enthält.\n","\n","```\n","collection = client.create_collection(\"sample_collection\")\n","```\n","\n","Als nächstes fügen wir dieser Sammlung Dokumente hinzu.\n","\n","```\n","collection.add(\n"," documents=[\"This is document1\", \"This is document2\"], # we embed for you, or bring your own\n"," metadatas=[{\"source\": \"notion\"}, {\"source\": \"google-docs\"}], # filter on arbitrary metadata!\n"," ids=[\"doc1\", \"doc2\"], # must be unique for each doc\n",")\n","```\n","\n","Wenn alles vorhanden ist, können Sie jetzt eine Abfrage durchführen.\n","\n","```\n","results = collection.query(\n"," query_texts=[\"This is a query document\"],\n"," n_results=2,\n"," # where={\"metadata_field\": \"is_equal_to_this\"}, # optional filter\n"," # where_document={\"$contains\":\"search_string\"}  # optional filter\n",")\n","```\n","\n","## Laden und Abfragen von ChromaDB\n","\n","Wir sehen uns nun ein Beispiel für das Laden und Abfragen von ChromeDB an. Dazu verwenden wir den [SciQ dataset](https://huggingface.co/datasets/allenai/sciq) von HuggingFace. Der SciQ-Datensatz enthält 13.679 Crowdsourcing-Prüfungsfragen zu Naturwissenschaften, unter anderem zu Physik, Chemie und Biologie. Die Fragen sind im Multiple-Choice-Format mit jeweils 4 Antwortoptionen. Zusätzlich haben die Autoren einen Absatz mit Belegen für die richtige Antwort auf die meisten Fragen bereitgestellt.\n","\n","Wir beginnen damit, diese Fragen mit unterstützenden Informationen anzureichern.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":418,"referenced_widgets":["a704672da0a747a5934aa6407492033d","76a508245c0a429bb63a396cd516ec6f","36da1e8096354ebd9c4560f6e0f73b9b","ead7251f07144b3abddc013e626e6631","305f158181094b53b1dbd2225ff82585","978e3056a314459888dcd1f7b2b851fd","65377c9a88624ecaa1f086265a57dfa8","5eaf34d721424a66b9131503dce9343c","b893e4a96c1347eb82ccbea708ce7ac3","8fa2de9477d34c27a89e74cac6615302","b222144ae87949d581f9af7e51c4df04","3949bbe2bc964321a4967f3d35a0ce47","d0edc769c9df48cd92c57622871e972b","65983fa0395545fdacc87a172afa1801","3f191be3a29248c9942c67b2e69add65","081c9b5800eb4dd4a876a6bdaecbddc7","84a4435bd72b46f68d02ad5b5c845f57","27f282606a4940bd910780490248cac6","c813fb9f533d4801b68d58cfd691fae6","9d50278cda5c4a57abfd0d9c20f5593c","b2a4ee7e70d94b9ab88fcfb6e28a1390","bde2eeddd23046ca8f455d07c878db12","cf57d06f8d234d20826cbb3c15a79f65","f2604929fd524ad281fd0b81bea566a0","c436fb6debdc4a748f75355557741c91","a902f87756a54aef9e18673d14eaabc1","00ab922119234584a0db1cd280f5aa3d","f3f832f168684160b54b484a4815b4cf","a438e31f0c504648bacf7e78651b624a","5fb1b3c0bf72411faa94587ec3981d28","02fae6962c154d76b78cd8f8ff2b71f4","f7ef620e4a4d44188f10969f5923b7e1","a2b29c0b7d414878bbc4500bc94b5990","2db6dbe1386545adb67c4c3a85bad302","e6487cf2ce55400c9a0f83ebfcd6e78b","c91b8ae7e3fb4217b334cba94ac74cd4","e8acb72b44514f7c8aa0249d4358bb65","31a78d4f139c44f09166048916db2cc3","647c3d618223421f89f201e2be7b986f","4bfba8ea6789460f95d624edfc3cb381","b0024e57e50e4baf9c8c93cbee2acbef","fb5dc4bfee784a74bf121e1a1197dd20","def5b6ad75ee4431b77a4075fc29f3ca","4eb01518b0884bc083368cf3a22baa99","eea0f1c868224a00b272db6f024a24af","cebdc7a4544e41f39279d48a5b25f837","19f49307149f44f4a05bcfbb8ff17ecb","f0d61d8df7504a1d88e97e8473ec7875","0554fdc81552485b8ed0e9c239ab1db2","f27b39c15f98419c9d058ad946cac45d","736e5bdef2bb41dd8059c45d22a401b2","93b56a3be2db4c71bf163c719a080428","b353d9d9f30940749f3637467b479beb","f91d47de2ae14a04bb6eedb177c9c331","1b0cc239ada3425396ff9e12ff809486","2da5ce1f3e9848829757becab3c6dc98","a7c0a9d178d74de48c9d0337aaa51e46","b3828d39ed6345bb8e34ace51b6afe05","6c67aa00634e4c09817857d40baf6c57","5fa6abcc52664ef190b133c1668fe4cf","ebf7caef33b44bfc8f0407c8f3e396e2","c874f87f25cb48f28b81577e3f123896","cc08e1456128427c8aa917a05e256038","88921507a9224792b171ea88ee847baf","128d37d8f4494cb187c3d601230082a4","c66e37254bd84b4f863c08008f42dafb","2731081dd71746749a07de4515e2ae88","e15b2cb89dad4cd8bc9b3e6d639bf640","18b0fa9b445c44eeab4bde15f96e07e9","a23100320cc04b479382f8f47ee2ff1b","46784c6b6f384264a265213f4d545e0a","9bacf95046d546cfb0a829a0c0542654","4b20634b5cd34556b37521af22bdd28d","eb93502b328d4f12b1f321fe8d79dbcf","f38ee28ad6fc4bcea22f27139512e856","732cc3e746bf4715a63b0233a2e0dfb8","f3f1bd4bdc924de286a62e96a19b49ee","0e1b45a271c343c69f0eb5665986e386","bdd1a59d4f9a4ce1b6882edda35d3cb6","09501d64acb146f4874ff85ca494e7ae","3a71b23530e94790984ad083b9a87e7b","c5cac619d8a447ed9f102cd21a46deed","6742f49dfa874798b232bc5272794911","8a2909c207174553b43ff87040ece214","e41d181dd60a423ba0894ca81f9b939f","cbb36fc3e2724b55a85bf65e85aabc13","95037b62c5904cf1bbe7cd07cc11dea1","12b70e95b13544558f87eb87bdf1e09c"]},"id":"_Ujcx3dpbbBG","outputId":"70a056a0-8b45-4a5a-d74a-19c69ea69fd8"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a704672da0a747a5934aa6407492033d","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/7.02k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3949bbe2bc964321a4967f3d35a0ce47","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/3.99M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cf57d06f8d234d20826cbb3c15a79f65","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/339k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2db6dbe1386545adb67c4c3a85bad302","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/343k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eea0f1c868224a00b272db6f024a24af","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/11679 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2da5ce1f3e9848829757becab3c6dc98","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/1000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2731081dd71746749a07de4515e2ae88","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/1000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0e1b45a271c343c69f0eb5665986e386","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/11679 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Number of questions with support:  10481\n"]}],"source":["# Holen Sie sich den SciQ-Datensatz von HuggingFace\n","from datasets import load_dataset\n","\n","dataset = load_dataset(\"sciq\", split=\"train\")\n","\n","# Filtern Sie den Datensatz, um nur Fragen mit einer Unterstützung einzuschließen\n","dataset = dataset.filter(lambda x: x[\"support\"] != \"\")\n","\n","print(\"Number of questions with support: \", len(dataset))"]},{"cell_type":"markdown","metadata":{"id":"23kAuRFJZqA7"},"source":["Wir beginnen mit der Erstellung eines ChromaDB-Clients und einer Sammlung namens „sciq_supports“. Der Standard-Chroma-Client ist flüchtig, d. h. er speichert nicht auf der Festplatte. In Teil 5 erfahren Sie, wie diese Daten gespeichert werden. Wir erstellen eine neue Chroma-Sammlung, um die unterstützenden Beweise zu speichern. Wir müssen keine Einbettungsfunktion angeben; wir verwenden die Standardfunktion."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2x7LvB0bbBG"},"outputs":[],"source":["import chromadb\n","\n","client = chromadb.Client()\n","\n","collection = client.create_collection(\"sciq_supports\")"]},{"cell_type":"markdown","metadata":{"id":"qlNIQAfWaXPI"},"source":["Als nächstes betten wir die ersten 100 Stützen für dieses Beispiel ein und speichern sie."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rvry6bGQd9B6","outputId":"ea3fe2a5-6b9d-4e4c-e1bf-4241542aa3d4"},"outputs":[{"name":"stderr","output_type":"stream","text":["/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:17<00:00, 4.87MiB/s]\n"]}],"source":["collection.add(\n","    ids=[str(i) for i in range(0, 100)],  # IDs sind nur Zeichenfolgen\n","    documents=dataset[\"support\"][:100],\n","    metadatas=[{\"type\": \"support\"} for _ in range(0, 100)\n","    ],\n",")"]},{"cell_type":"markdown","metadata":{"id":"h4aoE-MsavU0"},"source":["Wir fragen nun die Datenbank nach Unterstützung für jede der Fragen ab."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1nQZ3zSoeZqZ"},"outputs":[],"source":["results = collection.query(\n","    query_texts=dataset[\"question\"][:10],\n","    n_results=1)"]},{"cell_type":"markdown","metadata":{"id":"RxAK8LDLtMGz"},"source":["Wir lassen uns diese Informationen anzeigen und können sehen, welche Teile des Dokuments die einzelnen Fragen unterstützen. Später werden wir sehen, dass RAG diese Unterstützung in die Eingabeaufforderung zur Beantwortung der Frage einspeist."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1vSp-rd6ecAy","outputId":"9d44106a-e4f0-4751-993d-b128586aed8c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Question: What type of organism is commonly used in preparation of foods such as cheese and yogurt?\n","Retrieved support: Agents of Decomposition The fungus-like protist saprobes are specialized to absorb nutrients from nonliving organic matter, such as dead organisms or their wastes. For instance, many types of oomycetes grow on dead animals or algae. Saprobic protists have the essential function of returning inorganic nutrients to the soil and water. This process allows for new plant growth, which in turn generates sustenance for other organisms along the food chain. Indeed, without saprobe species, such as protists, fungi, and bacteria, life would cease to exist as all organic carbon became “tied up” in dead organisms.\n","\n","Question: What phenomenon makes global winds blow northeast to southwest or the reverse in the northern hemisphere and northwest to southeast or the reverse in the southern hemisphere?\n","Retrieved support: Without Coriolis Effect the global winds would blow north to south or south to north. But Coriolis makes them blow northeast to southwest or the reverse in the Northern Hemisphere. The winds blow northwest to southeast or the reverse in the southern hemisphere.\n","\n","Question: Changes from a less-ordered state to a more-ordered state (such as a liquid to a solid) are always what?\n","Retrieved support: Summary Changes of state are examples of phase changes, or phase transitions. All phase changes are accompanied by changes in the energy of a system. Changes from a more-ordered state to a less-ordered state (such as a liquid to a gas) areendothermic. Changes from a less-ordered state to a more-ordered state (such as a liquid to a solid) are always exothermic. The conversion of a solid to a liquid is called fusion (or melting). The energy required to melt 1 mol of a substance is its enthalpy of fusion (ΔHfus). The energy change required to vaporize 1 mol of a substance is the enthalpy of vaporization (ΔHvap). The direct conversion of a solid to a gas is sublimation. The amount of energy needed to sublime 1 mol of a substance is its enthalpy of sublimation (ΔHsub) and is the sum of the enthalpies of fusion and vaporization. Plots of the temperature of a substance versus heat added or versus heating time at a constant rate of heating are calledheating curves. Heating curves relate temperature changes to phase transitions. A superheated liquid, a liquid at a temperature and pressure at which it should be a gas, is not stable. A cooling curve is not exactly the reverse of the heating curve because many liquids do not freeze at the expected temperature. Instead, they form a supercooled liquid, a metastable liquid phase that exists below the normal melting point. Supercooled liquids usually crystallize on standing, or adding a seed crystal of the same or another substance can induce crystallization.\n","\n","Question: What is the least dangerous radioactive decay?\n","Retrieved support: All radioactive decay is dangerous to living things, but alpha decay is the least dangerous.\n","\n","Question: Kilauea in hawaii is the world’s most continuously active volcano. very active volcanoes characteristically eject red-hot rocks and lava rather than this?\n","Retrieved support: Example 3.5 Calculating Projectile Motion: Hot Rock Projectile Kilauea in Hawaii is the world’s most continuously active volcano. Very active volcanoes characteristically eject red-hot rocks and lava rather than smoke and ash. Suppose a large rock is ejected from the volcano with a speed of 25.0 m/s and at an angle 35.0º above the horizontal, as shown in Figure 3.40. The rock strikes the side of the volcano at an altitude 20.0 m lower than its starting point. (a) Calculate the time it takes the rock to follow this path. (b) What are the magnitude and direction of the rock’s velocity at impact?.\n","\n","Question: When a meteoroid reaches earth, what is the remaining object called?\n","Retrieved support: Meteoroids are smaller than asteroids, ranging from the size of boulders to the size of sand grains. When meteoroids enter Earth’s atmosphere, they vaporize, creating a trail of glowing gas called a meteor. If any of the meteoroid reaches Earth, the remaining object is called a meteorite.\n","\n","Question: What kind of a reaction occurs when a substance reacts quickly with oxygen?\n","Retrieved support: A combustion reaction occurs when a substance reacts quickly with oxygen (O 2 ). For example, in the Figure below , charcoal is combining with oxygen. Combustion is commonly called burning, and the substance that burns is usually referred to as fuel. The products of a complete combustion reaction include carbon dioxide (CO 2 ) and water vapor (H 2 O). The reaction typically gives off heat and light as well. The general equation for a complete combustion reaction is:.\n","\n","Question: Organisms categorized by what species descriptor demonstrate a version of allopatric speciation and have limited regions of overlap with one another, but where they overlap they interbreed successfully?.\n","Retrieved support: Ring species Ring species demonstrate a version of allopatric speciation. Imagine populations of the species A. Over the geographic range of A there exist a number of subpopulations. These subpopulations (A1 to A5) and (Aa to Ae) have limited regions of overlap with one another but where they overlap they interbreed successfully. But populations A5 and Ae no longer interbreed successfully – are these populations separate species?  In this case, there is no clear-cut answer, but it is likely that in the link between the various populations will be broken and one or more species may form in the future. Consider the black bear Ursus americanus. Originally distributed across all of North America, its distribution is now much more fragmented. Isolated populations are free to adapt to their own particular environments and migration between populations is limited. Clearly the environment in Florida is different from that in Mexico, Alaska, or Newfoundland. Different environments will favor different adaptations. If, over time, these populations were to come back into contact with one another, they might or might not be able to interbreed successfully - reproductive isolation may occur and one species may become many.\n","\n","Question: Alpha emission is a type of what?\n","Retrieved support: One type of radioactivity is alpha emission. What is an alpha particle? What happens to an alpha particle after it is emitted from an unstable nucleus?.\n","\n","Question: What is the stored food in a seed called?\n","Retrieved support: The stored food in a seed is called endosperm . It nourishes the embryo until it can start making food on its own.\n","\n"]}],"source":["# Drucken Sie die Frage und die dazugehörige Hilfestellung aus\n","for i, q in enumerate(dataset['question'][:10]):\n","print(f\"Frage: {q}\")\n","print(f\"Abgerufener Support: {results['documents'][i][0]}\")\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"xKz6gIaBElCj"},"source":["# 6.3: Einbettungen verstehen\n","\n","Ein [embedding](https://platform.openai.com/docs/guides/embeddings) ist ein Vektor (eine Liste) von Gleitkommazahlen. Der [embedding](https://platform.openai.com/docs/guides/embeddings) zwischen zwei Vektoren misst deren Verwandtschaftsgrad. Kleine Abstände deuten auf einen hohen Verwandtschaftsgrad hin, große Abstände auf einen niedrigen Verwandtschaftsgrad.\n","\n","Die beiden Einbettungsmodelle, zwischen denen Sie in OpenAI wählen können, sind die folgenden:\n","\n","* Text-Einbettung-3-klein\n","* Text-Einbettung-3-groß\n","\n","Die Wahl zwischen den beiden OpenAI-Einbettungsmodellen „text-embedding-3-small“ und „text-embedding-3-large“ hängt von mehreren Faktoren ab, die mit Ihrem spezifischen Anwendungsfall zusammenhängen, darunter Leistungsanforderungen, Rechenressourcen und die Art der Aufgaben, für die Sie die Einbettungen benötigen. Hier sind einige wichtige Überlegungen:\n","\n","1. **Leistung und Genauigkeit**:\n","* **text-embedding-3-large**: Im Allgemeinen erfassen größere Modelle differenziertere und komplexere Beziehungen innerhalb des Textes. Dies führt zu einer besseren Leistung bei Aufgaben, die ein tiefes Verständnis der Sprache erfordern, wie z. B. semantische Ähnlichkeit, Stimmungsanalyse und anspruchsvollere NLP-Aufgaben.\n","* **text-embedding-3-small**: Kleinere Modelle sind möglicherweise nicht so genau oder detailliert wie größere, sie können aber dennoch viele Aufgaben gut bewältigen, insbesondere solche mit geringerer Komplexität oder bei Feinabstimmung auf bestimmte Datensätze.\n","2. **Rechenressourcen:**\n","* **text-embedding-3-large**: Erfordert mehr Rechenleistung und Speicher. Diese Anforderung ist wichtig, wenn Sie das Modell in einer ressourcenbeschränkten Umgebung einsetzen oder eine große Datenmenge in Echtzeit verarbeiten müssen.\n","* **text-embedding-3-small**: Effizientere Ressourcennutzung und daher die bessere Wahl für Anwendungen mit begrenzter Rechenleistung oder bei einem Betrieb in einem Maßstab, bei dem Kosten und Geschwindigkeit entscheidende Faktoren sind.\n","3. **Latenz und Durchsatz:**\n","* **text-embedding-3-large**: Größere Modelle weisen aufgrund ihrer Komplexität normalerweise eine höhere Latenz auf, was sich auf Echtzeitanwendungen auswirken kann.\n","* **text-embedding-3-small**: Geringere Latenz und schnellere Inferenzzeiten kommen Anwendungen zugute, die schnelle Reaktionen erfordern.\n","4. **Kosten:**\n","* **text-embedding-3-large**: Aufgrund des höheren Rechenleistungsbedarfs sind wahrscheinlich höhere Betriebskosten verbunden.\n","* **text-embedding-3-small**: Kostengünstiger, insbesondere bei groß angelegten Bereitstellungen.\n","Anwendungsfallspezifikationen:\n","\n","Das große Modell ist möglicherweise besser für Anwendungen geeignet, die eine hohe Präzision erfordern und bei denen ein detailliertes semantisches Verständnis von entscheidender Bedeutung ist, wie etwa bei der differenzierten Textanalyse oder der fortgeschrittenen KI-Forschung.\n","Das kleine Modell könnte die bessere Wahl für Anwendungen sein, bei denen Geschwindigkeit, Kosten und Effizienz wichtiger sind, wie etwa Echtzeitsysteme, Chatbots oder Anwendungen mit einfacheren Textverarbeitungsanforderungen.\n","\n","Zusammenfassend:\n","\n","* Wählen Sie text-embedding-3-large, wenn:\n","* Sie benötigen hohe Genauigkeit und ein detailliertes semantisches Verständnis.\n","* Sie verfügen über ausreichend Rechenressourcen und ein ausreichendes Budget.\n","* Die Latenz ist kein kritisches Problem.\n","* Wählen Sie text-embedding-3-small, wenn:\n","* Sie legen Wert auf eine effiziente Ressourcennutzung und geringere Kosten.\n","* Sie benötigen schnellere Inferenzzeiten.\n","\n","Die Aufgaben sind weniger komplex oder die Umgebung erfordert eingeschränktere Ressourcen.\n","Durch die Bewertung Ihrer Anforderungen und Einschränkungen können Sie leichter entscheiden, welches Modell Sie verwenden möchten.\n","\n","## Instanziieren eines Einbettungsmodells\n","\n","Für diesen Kurs empfehle ich die Verwendung von **text-embedding-3-small**. Es verfügt über alle Funktionen, die wir benötigen, und wird Ihre Credits noch weiter aufwerten. Beginnen wir mit der Erstellung eines Clients, der dieses Modell verwendet.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wP9LYwNUSKyv"},"outputs":[],"source":["from langchain_openai import OpenAIEmbeddings\n","\n","embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"]},{"cell_type":"markdown","metadata":{"id":"uGs1Fqk0Vu-6"},"source":["Beginnen wir mit dem Verständnis der Grundlagen eines Einbettungsmodells und von Vektoren. Ein Einbettungsmodell ist ein Tool, das jede Textzeichenfolge, unabhängig von ihrer Länge, in einen Vektor umwandeln kann. Dieser Vektor ist eine eindeutige Darstellung der Textzeichenfolge. Wenn die Vektoren für zwei Textzeichenfolgen gleich sind, bedeutet dies, dass die Textzeichenfolgen identisch sind. Wenn die Vektoren unterschiedlich sind, deutet dies darauf hin, dass die Textzeichenfolgen unterschiedlich sind. Dieser Unterschied ist jedoch kein einfacher Vergleich. Sogar zwei sehr unterschiedliche Textzeichenfolgen mit derselben Bedeutung können ähnliche Vektoren erzeugen.\n","\n","Sehen wir uns zunächst Vektoren für einzelne Wörter an."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iq3WUSr7T1QK","outputId":"fec4ed4c-f518-419a-ca1b-80995846fdb5"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'list'>\n"]}],"source":["l1 = embeddings_model.embed_query(\"dog\")\n","l2 = embeddings_model.embed_query(\"Something that is a bit longer than a word.\")\n","\n","print(type(l1))"]},{"cell_type":"markdown","metadata":{"id":"wMqiJ6zFZUue"},"source":["Wie Sie sehen, ist die Ausgabe nur eine normale Python-Liste. Diese Listen sind ziemlich lang."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BNt_opHMX1B7","outputId":"c0c21cc3-b5a0-4909-df01-b17351f4697a"},"outputs":[{"name":"stdout","output_type":"stream","text":["1536\n","1536\n"]}],"source":["print(len(l1))\n","print(len(l2))"]},{"cell_type":"markdown","metadata":{"id":"OrwZGIA-ZbK_"},"source":["Die Länge dieser Zeichenfolge bleibt bei allen Abfragen desselben Modells gleich. Die größere Version des Modells kann zwar besser Vektoren erstellen, die Zeichenfolgen unterscheiden, erfordert aber nicht immer eine größere Vektorlänge, um diese Qualitätssteigerung zu erreichen. Es handelt sich um ein differenziertes Konzept, das einer weiteren Untersuchung bedarf.\n","\n","Wenn wir die eigentliche Liste anzeigen, sehen wir, dass es sich lediglich um eine Sammlung von Zahlen handelt. Hier zeigen wir nur die ersten zehn Elemente an."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MePSP-eGZpWA","outputId":"e9a1e208-4f3c-4555-a3bb-fc99582c20ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.05113774910569191, -0.01870863139629364, -0.004298428073525429, 0.07271610200405121, -0.007174310740083456, -0.014693480916321278, -0.0059395902790129185, 0.005037412978708744, 0.018954960629343987, -0.01090618409216404]\n"]}],"source":["print(l1[:10])"]},{"cell_type":"markdown","metadata":{"id":"EtbM9gKCaqxU"},"source":["## Vektoren vergleichen\n","\n","Um diese Vektoren zu vergleichen, verwenden wir die mathematischen Fähigkeiten von [Numpy](https://numpy.org/). Es gibt viele verschiedene Ansätze, um Vektoren in der Mathematik zu vergleichen. Einige der gängigsten werden hier vorgestellt.\n","\n","* **Skalarprodukt**: Misst den Kosinus des Winkels zwischen zwei Vektoren und gibt so ihre Richtungsähnlichkeit an. Wird zur Bestimmung der Orthogonalität, Projektion und in verschiedenen Anwendungen wie Computergrafik und maschinellem Lernen verwendet.\n","\n","* **Kreuzprodukt**: Berechnet einen Vektor senkrecht zu zwei gegebenen Vektoren im dreidimensionalen Raum, nützlich zum Ermitteln des Normalvektors einer Ebene und in der Physik für Drehmoment- und Drehimpulsberechnungen.\n","\n","* **Euklidische Distanz**: Berechnet die geradlinige Distanz zwischen zwei Vektoren. Wird häufig im maschinellen Lernen für Clustering- und Nächster-Nachbar-Algorithmen verwendet.\n","\n","* **Kosinus-Ähnlichkeit**: Bewertet den Kosinus des Winkels zwischen zwei Vektoren und betont dabei die Ausrichtung statt der Größe. Wird häufig beim Text Mining und Informationsabruf verwendet, um die Ähnlichkeit von Dokumenten zu vergleichen.\n","\n","* **Manhattan-Distanz**: Misst die Summe der absoluten Differenzen zwischen den Komponenten zweier Vektoren, nützlich in gitterbasierten Pfadfindungsalgorithmen und einigen Anwendungen des maschinellen Lernens.\n","\n","OpenAI [suggests](https://platform.openai.com/docs/guides/embeddings/frequently-asked-questions), dass wir Cosinus-Ähnlichkeit verwenden, um ihre Vektoren zu vergleichen, da die Größe erhalten bleibt. Das Vorzeichen der einzelnen Vektornummern ist wichtig, wir möchten es nicht verwerfen.\n","\n","Wir beginnen mit der Konvertierung der beiden Einbettungen von Python-Listen in Numpy-Arrays."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xgKMSKraapv3"},"outputs":[],"source":["import numpy as np\n","\n","# Konvertieren Sie Listen in Numpy-Arrays\n","vec1 = np.array(l1)\n","vec2 = np.array(l2)"]},{"cell_type":"markdown","metadata":{"id":"UVyEFevXgCqR"},"source":["OpenAI gibt an, dass alle Einbettungen auf die Länge 1 normalisiert sind, was häufig [unit vectors](https://en.wikipedia.org/wiki/Unit_vector) genannt wird. Dies bedeutet, dass:\n","\n","* Die Kosinus-Ähnlichkeit kann etwas schneller berechnet werden, wenn man nur ein Skalarprodukt verwendet\n","* Kosinusähnlichkeit und euklidische Distanz führen zu identischen Rankings\n","\n","Lassen Sie uns dies nun in die Praxis umsetzen. Um zu überprüfen, ob diese Einbettungen tatsächlich die Länge 1 haben, können wir eine praktische Funktion verwenden. Diese Funktion ist speziell für die Analyse der Länge eines Vektors konzipiert und ist daher ein nützliches Werkzeug bei unserer Untersuchung von OpenAI-Einbettungen."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0GPYq2wodpyJ"},"outputs":[],"source":["def analyze_vector_length(vector):\n","  # Berechnen Sie die Länge (Norm) des Vektors\n","  length = np.linalg.norm(vector)\n","\n","  # Überprüfen Sie, ob der Vektor ein Einheitsvektor ist\n","  is_unit_vector = np.isclose(length, 1.0)\n","\n","print(f\"Vektor: {Vektor}\")\n","print(f\"Länge des Vektors: {length}\")\n","print(f\"Ist der Vektor ein Einheitsvektor? {'Ja' wenn is_unit_vector sonst 'Nein'}\")\n","\n","  # Normalisieren Sie den Vektor, um ihn zu einem Einheitsvektor zu machen\n","  unit_vector = vector / length\n","\n","  # Überprüfen Sie die Länge des normalisierten Vektors\n","  normalized_length = np.linalg.norm(unit_vector)\n","\n","print(f\"Normalisierter Vektor (Einheitsvektor): {unit_vector}\")\n","print(f\"Länge des normalisierten Vektors: {normalized_length}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GRrx2dZhfrfG","outputId":"f01bf3e7-0232-420e-942a-7ea191b45fff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Vector: [ 0.05113775 -0.01870863 -0.00429843 ...  0.02879578  0.00215999\n","  0.01790806]\n","Length of the vector: 0.9999999997385218\n","Is the vector a unit vector? Yes\n","Normalized vector (unit vector): [ 0.05113775 -0.01870863 -0.00429843 ...  0.02879578  0.00215999\n","  0.01790806]\n","Length of the normalized vector: 1.0000000000000002\n"]}],"source":["analyze_vector_length(vec1)"]},{"cell_type":"markdown","metadata":{"id":"V459AdOAfqmd"},"source":["Als nächstes berechnen wir die tatsächliche Kosinus-Ähnlichkeit zwischen unseren beiden Vektoren."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q2fgizuzXPAT","outputId":"c6bbbd9d-7a90-403f-95be-4173bc206828"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cosine similarity: 0.16737720056743507\n"]}],"source":["import numpy as np\n","\n","# Konvertieren Sie Listen in Numpy-Arrays\n","vec1 = np.array(l1)\n","vec2 = np.array(l2)\n","\n","# Berechnen Sie das Skalarprodukt\n","dot_product = np.dot(vec1, vec2)\n","\n","# Berechnen Sie die Größen (L2-Normen)\n","magnitude_vec1 = np.linalg.norm(vec1)\n","magnitude_vec2 = np.linalg.norm(vec2)\n","\n","# Kosinus-Ähnlichkeit berechnen\n","cosine_similarity = dot_product / (magnitude_vec1 * magnitude_vec2)\n","\n","print(f\"Kosinus-Ähnlichkeit: {cosine_similarity}\")"]},{"cell_type":"markdown","metadata":{"id":"NO3zv6WGhDEw"},"source":["Der Nenner oben ist 1,0, wie hier gezeigt. Dies liegt an der Einheitsvektoreigenschaft. Wir können unseren Einheitsvektorvergleich auf das Skalarprodukt vereinfachen, wie von OpenAI angegeben."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eMKiwXR7hGl1","outputId":"546a02e8-55f4-4ad7-b9d0-d037a78354be"},"outputs":[{"name":"stdout","output_type":"stream","text":["1.0000000478179367\n"]}],"source":["print(magnitude_vec1 * magnitude_vec2)"]},{"cell_type":"markdown","metadata":{"id":"1weLEmSEhfvY"},"source":["Das Skalarprodukt kann wie folgt berechnet werden."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2c981Unthh5h","outputId":"75df5174-178e-4072-b8c8-00f8fdcb1597"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.16737720857106747\n"]}],"source":["print(np.dot(vec1, vec2))"]},{"cell_type":"markdown","metadata":{"id":"rzGVjgjbkeOC"},"source":["## Ähnlichkeiten von Zeichenfolgen auswerten\n","\n","Wir beginnen mit der Erstellung einer einfachen Funktion zum Vergleichen zweier Zeichenfolgen."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ooX8bTmFlFRJ"},"outputs":[],"source":["def compare_str(embeddings_model, text1, text2):\n","    \"\"\"\n","    This function returns the dot product of embeddings for two given text strings.\n","\n","    Parameters:\n","    embeddings_model: The embeddings model to use for generating embeddings.\n","    text1 (str): The first text string.\n","    text2 (str): The second text string.\n","\n","    Returns:\n","    float: The dot product of the embeddings for text1 and text2.\n","    \"\"\"\n","    # Holen Sie sich die Einbettungen für die beiden Textzeichenfolgen\n","    embedding1 = embeddings_model.embed_query(text1)\n","    embedding2 = embeddings_model.embed_query(text2)\n","\n","    # Konvertieren Sie Einbettungen in Numpy-Arrays zur Skalarproduktberechnung\n","    embedding1_array = np.array(embedding1)\n","    embedding2_array = np.array(embedding2)\n","\n","    # Berechnen und Zurückgeben des Skalarprodukts\n","    dot_product = np.dot(embedding1_array, embedding2_array)\n","    return dot_product"]},{"cell_type":"markdown","metadata":{"id":"20QUKA9-qa4J"},"source":["Versuchen wir es mit zwei Beschreibungen eines „Rasenmähers“, in denen nicht viele ähnliche Wörter verwendet werden."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4dgEmHV2lHEs","outputId":"6553350e-fb4b-48e0-e8cf-6dec3fa43657"},"outputs":[{"data":{"text/plain":["0.6298291212454291"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["compare_str(embeddings_model,\n","            \"A machine that helps people to cut grass.\",\n","            \"Device with blades to cut plants under it.\")"]},{"cell_type":"markdown","metadata":{"id":"etJISf2MmcY1"},"source":["Der Wert des Skalarprodukts (Kosinusähnlichkeit) reicht von -1 bis 1. Wir können ihn wie folgt interpretieren:\n","\n","* Hohe Ähnlichkeit: Werte nahe 1 weisen auf eine hohe Ähnlichkeit hin.\n","* Geringe Ähnlichkeit: Werte nahe -1 weisen auf eine hohe Unähnlichkeit hin.\n","* Neutral/Keine Ähnlichkeit: Werte nahe 0 zeigen keine offensichtliche Ähnlichkeit an.\n","\n","Ein Wert von 0,62 bedeutet also, dass sie einigermaßen ähnlich sind. Passen wir ihn an, um einen Rasenmäher mit einem Flugzeug zu vergleichen."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b43n_JgOlWD1","outputId":"429def1f-dbaf-43c8-8b66-c819319451be"},"outputs":[{"data":{"text/plain":["0.2694946993627674"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["compare_str(embeddings_model,\n","            \"A machine that helps people to cut grass.\",\n","            \"Vehicle that flys through the air.\")"]},{"cell_type":"markdown","metadata":{"id":"IBucz-M7nxp3"},"source":["Wir können sehen, dass die Ähnlichkeit geringer ist."]},{"cell_type":"markdown","metadata":{"id":"vBP-I0eqErvN"},"source":["# 6.4: Beantwortung von Fragen über Textdokumente\n","\n","Retrieval-Augmented Generation (RAG) ist eine fortschrittliche Technik, die die Fähigkeiten großer Sprachmodelle (LLMs) durch die Integration externer Daten in den Antwortgenerierungsprozess verbessern soll. Damit RAG wirklich effektiv ist, muss es mit Daten arbeiten, die noch nicht im Basismodell vorhanden sind. Dies ist entscheidend, da der Hauptvorteil von RAG in seiner Fähigkeit liegt, spezifische, oft aktuelle Informationen abzurufen, mit denen das LLM während seines anfänglichen Trainings möglicherweise nicht in Berührung gekommen ist.\n","\n","Diese Funktion macht RAG besonders wertvoll für Unternehmensumgebungen. Unternehmen generieren und speichern große Mengen an proprietären Daten, darunter interne Dokumente, Kundeninformationen und detaillierte Berichte. Diese Daten sind oft unternehmensspezifisch und nicht Teil des öffentlich verfügbaren Korpus, mit dem ein LLM geschult würde. Durch die Verwendung von RAG können Unternehmen ihre eigenen Daten nutzen, um präzisere und kontextrelevantere Antworten von ihren LLMs zu erhalten und so Entscheidungsprozesse und Betriebseffizienz zu verbessern.\n","\n","Um die Fähigkeiten von RAG zu veranschaulichen, verwenden wir einen von mir erstellten Beispieldatensatz, der synthetische Daten von Mitarbeiterbiografien aus fünf fiktiven Unternehmen enthält. Dieser Datensatz soll zeigen, wie RAG bestimmte Informationen, die ein Basismodell nicht von sich aus besitzen würde, effektiv abrufen und nutzen kann. Auf diese Weise liefert er ein klares Beispiel dafür, wie RAG in einem realen Unternehmensumfeld angewendet werden kann.\n","\n","Hier ist ein Beispiel für eine solche generierte Biografie:\n","\n","> Elena Martinez ist eine erfahrene Robotikingenieurin bei FutureTech, einem führenden Innovator im Bereich künstliche Intelligenz und Robotik mit Sitz im Silicon Valley. Mit einem Master-Abschluss in Maschinenbau vom MIT und über einem Jahrzehnt Erfahrung war Elena maßgeblich an der Entwicklung autonomer Robotersysteme beteiligt, die die städtische Mobilität und Zugänglichkeit verbessern sollen. Zu ihren bahnbrechenden Arbeiten gehört die Entwicklung des ersten KI-gesteuerten Roboterassistenten, der nahtlos mit städtischen Umgebungen interagieren kann, um älteren und behinderten Menschen zu helfen. Als leidenschaftliche Verfechterin der Förderung von Frauen in MINT-Fächern leitet Elena auch das Outreach-Programm von FutureTech, das darauf abzielt, die nächste Generation weiblicher Ingenieure durch Workshops und Mentoring zu inspirieren. Ihre Beiträge haben FutureTech nicht nur zu neuen Höhen geführt, sondern auch neue Maßstäbe bei Roboteranwendungen für das Gemeinwohl gesetzt.\n","\n","\n","Diese Biografie zeigt die Art detaillierter, unternehmensspezifischer Informationen, die RAG abrufen und in seine Antworten integrieren kann, wodurch die Relevanz und Genauigkeit der generierten Inhalte verbessert wird. Durch die Integration dieser maßgeschneiderten Daten bereichert RAG nicht nur die Ergebnisse der LLMs, sondern stellt auch sicher, dass die Antworten auf den einzigartigen Kontext und die Bedürfnisse der Organisation abgestimmt sind.\n","\n","Diese Beispieldaten werden unter den folgenden URLs gespeichert. Jede Datei enthält Personen aus einem der Unternehmen.\n","\n","* https://data.heatonresearch.com/data/t81-559/bios/DD.txt\n","* https://data.heatonresearch.com/data/t81-559/bios/FT.txt\n","* https://data.heatonresearch.com/data/t81-559/bios/GS.txt\n","* https://data.heatonresearch.com/data/t81-559/bios/NGS.txt\n","* https://data.heatonresearch.com/data/t81-559/bios/TI.txt\n","\n","Der folgende Code definiert diese URLs und instanziiert ein LLM-Modell.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0QQm0vSmeF5T"},"outputs":[],"source":["from langchain.chains.summarize import load_summarize_chain\n","from langchain.document_loaders import TextLoader\n","from langchain import OpenAI, PromptTemplate\n","from langchain_openai import ChatOpenAI\n","from IPython.display import display_markdown\n","from langchain.indexes import VectorstoreIndexCreator\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_community.vectorstores.inmemory import InMemoryVectorStore\n","from langchain.schema import Document\n","import requests\n","\n","MODEL = 'gpt-4o-mini'\n","\n","llm = ChatOpenAI(\n","        model=MODEL,\n","        temperature=0.2,\n","        n=1\n","    )\n","\n","urls = [\n","    \"https://data.heatonresearch.com/data/t81-559/bios/DD.txt\",\n","    \"https://data.heatonresearch.com/data/t81-559/bios/FT.txt\",\n","    \"https://data.heatonresearch.com/data/t81-559/bios/GS.txt\",\n","    \"https://data.heatonresearch.com/data/t81-559/bios/NGS.txt\",\n","    \"https://data.heatonresearch.com/data/t81-559/bios/TI.txt\"\n","]"]},{"cell_type":"markdown","metadata":{"id":"wmQJoW7Si3Lc"},"source":["\n","Die Funktion verarbeitet ein großes Dokument, indem sie es in kleinere, handhabbare Segmente, sogenannte Chunks, unterteilt, um Einbettungen für den LLM-Abruf in Retrieval-Augmented Generation (RAG) zu erstellen. Der Chunk-Parameter bestimmt die maximale Länge jedes Segments und stellt sicher, dass der Text in Teile zerlegt wird, die vom Sprachmodell effizient verarbeitet und analysiert werden können. Der Overlap-Parameter gibt die Anzahl der Token an, die am Anfang jedes neuen Chunks wiederholt werden sollen, wodurch eine Überlappung zwischen aufeinanderfolgenden Chunks entsteht. Diese Überlappung hilft dabei, die kontextuelle Kontinuität über Chunks hinweg aufrechtzuerhalten und verbessert so die Qualität und Genauigkeit der Einbettungen. Durch die systematische Segmentierung des Dokuments und die Generierung von Einbettungen für jeden Chunk verbessert die Funktion die Fähigkeit des Sprachmodells, relevante Informationen abzurufen, was zu präziseren und kontextuell angemessenen Antworten im RAG-Framework führt."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KNwkulAagbQr"},"outputs":[],"source":["def chunk_text(text, chunk_size, overlap):\n","    chunks = []\n","    for i in range(0, len(text), chunk_size - overlap):\n","        chunks.append(text[i:i + chunk_size])\n","    return chunks"]},{"cell_type":"markdown","metadata":{"id":"Yojogrn2jQjQ"},"source":["In dieser Funktion setzen wir die Parameter für Blockgröße und Überlappung auf die folgenden Werte:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OnPD5l7oghJR"},"outputs":[],"source":["chunk_size = 900\n","overlap = 300"]},{"cell_type":"markdown","metadata":{"id":"tBBhD2Utjn8t"},"source":["Wir verarbeiten Textdateien, die Listen mit Biografien von Personen enthalten. Diese Einstellungen helfen dabei, die großen Textdaten effektiver zu verwalten. Die Blockgröße von 1000 stellt sicher, dass jedes Textsegment oder jeder Block bis zu 1000 Token enthält, was es dem Sprachmodell erleichtert, Einbettungen für jeden Teil zu verarbeiten und zu generieren. Die Überlappung von 200 Token bedeutet, dass jeder neue Block mit den letzten 200 Token des vorherigen Blocks beginnt. Diese Überlappung ist entscheidend, um die Kontinuität des Kontexts zwischen den Blöcken aufrechtzuerhalten, was besonders nützlich ist, wenn biografische Daten verarbeitet werden, bei denen sich Details oft über mehrere Blöcke erstrecken.\n","\n","\n","Der folgende Code liest Textinhalte aus einer Liste von URLs, verarbeitet diese Inhalte, indem er sie in kleinere, handhabbare Blöcke aufteilt, und speichert diese Blöcke als Dokumentobjekte. Zunächst wird eine leere Liste mit dem Namen „documents“ erstellt, um diese Dokumentobjekte aufzunehmen. Der Code durchläuft dann jede URL in der bereitgestellten Liste und gibt eine Meldung aus, die die aktuell verarbeitete URL angibt. Für jede URL ruft er den Inhalt mithilfe der Methode requests.get ab und prüft mit response.raise_for_status() auf HTTP-Fehler. Sobald der Inhalt erfolgreich abgerufen wurde, wird er mithilfe der Funktion chunk_text in kleinere Segmente aufgeteilt. Dabei werden die angegebene Blockgröße und Überlappung berücksichtigt, um die kontextuelle Kontinuität zwischen den Blöcken aufrechtzuerhalten. Diese Blöcke werden dann verwendet, um Dokumentobjekte zu erstellen, die jeweils einen Teil des Textes enthalten. Diese Dokumentobjekte werden anschließend an die Liste „documents“ angehängt, wodurch die großen Textdateien effektiv in kleinere, abrufbare Segmente organisiert werden, die für die weitere Verarbeitung geeignet sind, z. B. zum Generieren von Einbettungen für den LLM-Abruf in Retrieval-Augmented Generation (RAG)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3zoC7v0igkkk","outputId":"9a389c7f-1d52-4642-83f7-f24d42a72c9f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading: https://data.heatonresearch.com/data/t81-559/bios/DD.txt\n","Reading: https://data.heatonresearch.com/data/t81-559/bios/FT.txt\n","Reading: https://data.heatonresearch.com/data/t81-559/bios/GS.txt\n","Reading: https://data.heatonresearch.com/data/t81-559/bios/NGS.txt\n","Reading: https://data.heatonresearch.com/data/t81-559/bios/TI.txt\n"]}],"source":["documents = []\n","\n","for url in urls:\n","print(f\"Lesen: {url}\")\n","    response = requests.get(url)\n","    response.raise_for_status()  # Stellen Sie sicher, dass uns schlechte Antworten auffallen\n","    content = response.text\n","    chunks = chunk_text(content, chunk_size, overlap)\n","    for chunk in chunks:\n","        document = Document(page_content=chunk)\n","        documents.append(document)"]},{"cell_type":"markdown","metadata":{"id":"NM0kGSyMnjEP"},"source":["Wir haben die Biografiedaten geladen und integrieren sie nun mit dem folgenden Code in die ChromaDB-Datenbank. ChromaDB, das ein Einbettungsmodell erfordert, ist standardmäßig auf „all-MiniLM-L6-v2“ eingestellt. Andere [model wrappers](https://docs.trychroma.com/integrations/openai) werden bereitgestellt. Dieses Modell wird von SentenceTransformerEmbeddings bereitgestellt, das hier für Einbettungsfunktionen instanziiert wird. Unter Verwendung von CharacterTextSplitter von langchain_text_splitters werden Dokumente für die Verarbeitung in handhabbare Teile aufgeteilt. Chroma, ein Vektorspeicher, lässt sich nahtlos mit diesen Komponenten verbinden und ermöglicht eine effiziente Speicherung und Abfrage von Einbettungen innerhalb der Datenbank."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460,"referenced_widgets":["68d3bb013df845cbb766debf4092a42f","df54569c71fa4bf1bc9800f5b006e4ce","f1b570993843424588d4c9886cd7f8d3","d3f8d53df7834960bf37dab640ff80eb","3d24a4035b2f4c17ac9d94df3139cbc0","3464bfc97663437d908a27407d9c1817","4f69a1b2e20a4d43be5e78ed31256c5f","7f15910219b4424e8c0aa239271abe60","cd726be433b14a92bfd43b0e63fae456","ec3634f8d68a4ef38601250868373a43","3fa6d2df1c8a4a1994b779cecfa48724","f930c3724d55417987e6405c62fc77f6","6190be32427343638cb01f214f6aad59","c483d715a70e4fc7b6cc3fe4c9161f1d","2b6b3e8e206f49dc9a3de5e0fae0fc66","74fc312878524522afb8b12f153880c3","4203e7fe06cc4e69a95c001e4c0c1695","676b4497687649e3b241c42fffb39d5f","34faf5343a89415ca6050494d495db8e","09d9d0d6f3e5466cb92a777e968b57c4","6218403192ab4045ab0f88a18528d853","2778911f7fbb4978b7e82e7a5d42a133","3ebe3186f7504c478fc78c37d8840630","2632e36ffa7b42a69f68e5c3e3113996","f7b1a79b86f4406eb888233b2f781b97","f39d48d775f448658fb18a26a11e03b0","9cfdeda4169f4e3b96d396bb774069ff","c47aaa18871e46229bb238ca95342050","54239e23ecab43bebc3c2ec357a64214","da9d15e316f14e319ffdd052832c4f6d","cf3b7e99534e4d469d59e03eb0eb41ee","5c3d30696fb4465fbd1df0af8a7310f9","8574170534794badb9d7d8395a3e2933","0a9d1a7adb5e42898c28082f3e9cf32e","618838010882444c9e5cbbe5f8c72afc","6a3ab197863e42c5881b0ac129caca50","5e73526146bb47efa2b308dc2d35008a","519bb649f06e4719982ce7588f9c4b2b","1ab76287868749b8a717a8c1626c925e","e71f543d4739449da03c65fb3026ddee","24ef7cde093645feb32f8acb92265bb5","28a88f02acbb4b38b474c219070317ad","2c27781d15ca4a6e9d775af2952e701d","62933e98a3844a75aa9cc8ad160adc68","a574a476accc483eb9c70c86cfc03204","366967a33f584ce1a1f2d9ca1ee3e53f","e8227ffeefa0472b99a602d15d06be09","73011d7a22474092bc896c16ba56b99f","0d9b67a692de4612a8fb46f65118c87c","b5df88cc12ed43ef81ff5dc4e668513b","76b5503fb9a94ecca076dc0cc8a5776a","eb9729256b2b47efa5cd8393f696e5c3","b25663cd25994c7c9e732878b58749ca","6b959d4da8144fcdb63249e7f14e67bd","6c53836a08a241d2a4ba18fcda3fa57f","c5a518da4d444455b92b4c6c0ef25e32","4b20a23a49884155869d49a1ac1fdf76","cbecc3c56d914069aa870a51c974ac7a","feef3088b25a47c28c730b96b537bcfb","a755624dfd1147e4b96eb6d636760250","f3df4a3a0e0447fd8732e2c9b6abe78d","a7dde37db3bf444480ae24589347b3c6","9c53fe5575074b64b1d56156898193e1","0874eba30374497ea28b1e8430337e4b","03e81b34733b4a6989e80af808ad6dfb","1fa41bc1c41046aaa32a7f02924dd144","eaf503e2a7f5411cac863c93c059c9f9","163bd834a8d142deaac8437f9e45f233","6b980c6821ce4712ab265d4782261621","66bdae3d974f43b4804dab6bddbf341f","ce89717b370845e3ab6e7a384678e0fa","0a407c86444d462da26efcfb2d1f9814","839d57aec9e9461e840f28fe63320c90","6f5f821a5c4949278167b3eb5d8fe808","8a612ea80a554531946fe624e1deff71","d38d0ba254cb417c90338a27ec5153b4","2bd01c618ab346c79be6620a3e4d6e37","06811b0df5834c62a409c043ace5f0bf","5269f9d43cec468abc71d950a2972a6b","206c2733ac4f40ea81b97b1457f92671","a06cdd9cb67a4361a432d4ad4b2b5e39","bde2b14926014c99b205420a68f98ca0","84b6b7907435475887fc51822f3d7bd0","6bf186ca30df4ae4a49fd319291de0c5","a07b7849eb6d4a06ac63324a4f20dd65","c12033a292e244038b60cdd8593281aa","31a69d7afc7b4c889ea84c52e1389107","c4f5f466bcf34d5e8a1fd9f93a34d551","9b5299a5c0dc4c51a997fd043782e3f1","ea14c88c79174f04b00cbca4c04b7817","17d7bcf0ef5e4f2c9b51e243c3d48dc6","ef4a1142789e47caa973209b5e1536d4","ac756588e4f541f78fb8ad7a199741d8","72b31e123a5449f7b0711d615d8cf6b8","7cb3a3aedd96469b835bb08eaf096c01","5b025d6a8f0e410db574ad421e3bf79b","cecf1f88044f4e859e5393699c791b38","7ec3c79ae64e41f1836dd65735ff6dcf","c690ad90184744bfac757b113814992d","54dac098381d48a7a73bc2d439064b7a","6de619a422a949b09462ee5aa5d22913","1e7234a3193b43f1b6cdcc4c7890e267","1223a20abfe745a7b6109b2d43890bfb","351ece73cd194d1ca76a659a6ed035eb","e19be6872e7f4b62bb5fd206bd8d8764","714ca8e211364d5c83de580c62d80438","4c0f09e5e5a1449cbbcaba03aafafb16","3c3a26d6ec86499482f3cded50808a3f","e7af2fc7c6e348119e84b83aea5fd5b5","4573220908094ad2ab3dd430b9c646d3","d3cc91ee69f34653ab8105b9954d5e9d","8fd18c9b7d974d23b7ff6d3676bb58e9","4838a3993a8546da92cad577274ace7d","88370c77fe9746a4b77c369c0ef7cd90","e989bd288482442ebaf63f5baa4919e9","7034eb8eb44f43afa9972290f1c615c9","e43c1c477f214d06916eeebbdb926b05","1186d4d94e4b4c099de5a0d36e44530a","fbbc7ca4072d4f9bad237443e3832a52","27dc08a3c2f34101a0839a016071f3ad","c7e89fea1c8a465bb4734216082261b9"]},"id":"wyBkIzgdPAd-","outputId":"eb2df963-13b0-4150-81c9-8d2a8555718b"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-6-11990a71100e>:10: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n","  embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n","/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  from tqdm.autonotebook import tqdm, trange\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"68d3bb013df845cbb766debf4092a42f","version_major":2,"version_minor":0},"text/plain":["modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f930c3724d55417987e6405c62fc77f6","version_major":2,"version_minor":0},"text/plain":["config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3ebe3186f7504c478fc78c37d8840630","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a9d1a7adb5e42898c28082f3e9cf32e","version_major":2,"version_minor":0},"text/plain":["sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a574a476accc483eb9c70c86cfc03204","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c5a518da4d444455b92b4c6c0ef25e32","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eaf503e2a7f5411cac863c93c059c9f9","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"06811b0df5834c62a409c043ace5f0bf","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9b5299a5c0dc4c51a997fd043782e3f1","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"54dac098381d48a7a73bc2d439064b7a","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d3cc91ee69f34653ab8105b9954d5e9d","version_major":2,"version_minor":0},"text/plain":["1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from langchain_text_splitters import CharacterTextSplitter\n","from langchain_community.embeddings.sentence_transformer import (\n","    SentenceTransformerEmbeddings,\n",")\n","from langchain.vectorstores import Chroma\n","\n","text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n","docs = text_splitter.split_documents(documents)\n","\n","embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n","\n","db = Chroma.from_documents(docs, embedding_function)"]},{"cell_type":"markdown","metadata":{"id":"z7RHY_3h6qQ8"},"source":["Wir sind dabei, eine RAG-Kette (Retrieval-Augmented Generation) mit dem folgenden Code zu konstruieren. Beginnend mit einem Retriever, der aus unserer zuvor konfigurierten ChromaDB-Datenbank initialisiert wurde, werden Dokumente zur Eingabevorbereitung in eine zusammengesetzte Zeichenfolge formatiert. Die Kette enthält einen Frage-Antwort-Fluss: Die formatierten Dokumente dienen neben einem Pass-Through-Fragehandler als Kontext. Unter Verwendung des vom Hub abgerufenen RAG-Modells geht die Kette zu einem Sprachmodell (llm) über. Schließlich werden die Ergebnisse mit StrOutputParser in ein Zeichenfolgenformat analysiert, wodurch der Prozess zur Generierung von Antworten basierend auf abgerufenen Dokumentkontexten abgeschlossen wird."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V2BSjxvDPrND","outputId":"68261c69-fa59-49dd-8092-1a361a686ee3"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/langsmith/client.py:333: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langsmith/client.py:5434: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n","  prompt = loads(json.dumps(prompt_object.manifest))\n"]}],"source":["from langchain import hub\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_core.output_parsers import StrOutputParser\n","\n","rag_prompt = hub.pull(\"rlm/rag-prompt\")\n","\n","def format_documents(documents):\n","    return \"\\n\\n\".join(doc.page_content for doc in documents)\n","\n","retriever = db.as_retriever()\n","\n","qa_chain = (\n","    {\"context\": retriever | format_documents, \"question\": RunnablePassthrough()}\n","    | rag_prompt\n","    | llm\n","    | StrOutputParser()\n",")"]},{"cell_type":"markdown","metadata":{"id":"zIjLhtCr7HLe"},"source":["Wir können jetzt die RAG-Kette aufrufen und sie zu einer der Personen abfragen, die in unseren Beispielbiografiedaten enthalten waren."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"139UgQ5H34sL","outputId":"ae432e2f-076c-434f-f44f-2868646a7132"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"I don't know.\""]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["qa_chain.invoke(\"What company does Elena Martinez work for?\")"]},{"cell_type":"markdown","metadata":{"id":"IGb8_A28nH4L"},"source":["## Die RAG-Kette auseinandernehmen\n","\n","Mehrere Komponenten werden kombiniert, um externe Daten in eine Eingabeaufforderung für den RAG-Zugriff zu bringen. In diesem Abschnitt werden wir jede Komponente untersuchen und sehen, wie sie einzeln funktioniert. Wir beginnen damit, zu sehen, wie ChromaDB aufgefordert wird, und beobachten, wie es relevante Informationen aus dem größeren Satz von Dokumenten abruft. Diese kleinere Teilmenge ermöglicht es, die Daten in den Kontext einzufügen. Selbst wenn das gesamte Quellmaterial in den Kontextpuffer des LLM passen würde, würde RAG die Kosten senken und die Zugriffszeit verbessern, da viel weniger Daten an das LLM gesendet werden müssen."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZMAOksLu30kS","outputId":"517d9f99-d8ec-4dbe-b10e-c43ff0627159"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Document(page_content=\"rofessional life, Samantha is an avid rock climber and enjoys mentoring young women interested in STEM careers, aiming to inspire and cultivate a new generation of tech leaders.\\n\\nSamantha Clarke is a seasoned Project Manager at Digital Dynamics, a leading tech company known for its innovative solutions in digital marketing and AI-driven analytics. With over a decade of experience in the tech industry, Samantha has played a pivotal role in steering complex projects to success, enhancing the company's reputation for efficiency and cutting-edge technology. A graduate of MIT with a degree in Computer Science, she has a passion for integrating user-friendly technology with business needs. Samantha is particularly noted for her leadership in the development of the company's flagship product, the MarketMinder AI, which has revolutionized the way businesses understand consumer behavior. Outside\"), Document(page_content=\"market reach. A graduate of MIT with a degree in Systems Engineering, her expertise lies in integrating cross-functional teams and optimizing workflows. Samantha's leadership was instrumental in the launch of the company's flagship product, the MarketMinder AI, which has since revolutionized data-driven marketing strategies for Digital Dynamics' global clientele. Outside of work, she is an avid rock climber and volunteers her time mentoring young women interested in STEM careers.\\n\\nSamantha Clarke is a seasoned Project Manager at Digital Dynamics, a leading tech company known for its innovative solutions in artificial intelligence and machine learning. With over a decade of experience in the tech industry, Samantha has played a pivotal role in steering complex projects to successful completion, fostering collaboration among cross-functional teams, and driving the adoption of agile methodo\"), Document(page_content='ices. A graduate of MIT with a degree in Computer Science, she has a passion for integrating cutting-edge technologies to enhance operational efficiencies and solve critical challenges. Outside of her professional life, Samantha is an avid rock climber and volunteers her time mentoring young women interested in STEM careers, aiming to inspire the next generation of female tech leaders.\\n\\nSamantha Clarke is a seasoned Project Manager at Global Solutions, an innovative tech company known for its cutting-edge solutions in environmental technology. With over a decade of experience in project management, Samantha has successfully led numerous high-profile projects that focus on sustainable development and renewable energy sources. A graduate of Stanford University with a degree in Environmental Science and Policy, she has a passion for integrating eco-friendly practices into business models. S'), Document(page_content='s for numerous Fortune 500 companies. A graduate of MIT with a degree in Systems Engineering, her passion for technology and problem-solving is matched only by her commitment to mentoring young women entering STEM fields. Outside of work, Samantha is an avid rock climber and volunteers her time at local community centers teaching coding to kids.\\n\\nSamantha Clarke is a seasoned Project Manager at Global Solutions, an innovative tech company known for its cutting-edge solutions in environmental technology. With over a decade of experience in project management, Samantha has successfully led numerous high-profile projects that focus on sustainable development and renewable energy sources. A graduate of Stanford University with a degree in Environmental Science and Policy, she has a passion for integrating eco-friendly practices into business models. Samantha, originally from Seattle, now res')]\n"]}],"source":["# frage es ab\n","query = \"What company does Elena Martinez work for?\"\n","docs = db.similarity_search(query)\n","\n","# Ergebnisse drucken\n","print(docs)"]},{"cell_type":"markdown","metadata":{"id":"ARbwrvd5_Xwf"},"source":["Wie Sie sehen, gibt ChromaDB mehrere Informationsbits zurück, die dem LLM den Kontext zur Beantwortung der Frage liefern. Keine dieser Informationen ist „allgemein bekannt“, sie sind nur in den generierten Biografiedaten vorhanden, die wir in ChromaDB geladen haben.\n","\n","Wir werden uns auch die RAG-Eingabeaufforderungsvorlage ansehen. Wir verwenden die Standardeingabeaufforderungsvorlage von LangChain Hub. Der Text dieser Eingabeaufforderung ist hier. Sowohl die Frage als auch alle unterstützenden Informationen aus der Datenbank werden bereitgestellt."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":89},"id":"OYJD0hgE_bqF","outputId":"164ec10f-e71f-4239-ad0c-e3e17a20dcf9"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/langsmith/client.py:333: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n","  warnings.warn(\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\""]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["rag_prompt = hub.pull(\"rlm/rag-prompt\")\n","rag_prompt.messages[0].prompt.template"]},{"cell_type":"markdown","metadata":{"id":"ct13EpDFArFr"},"source":["Das R in den RAG-Standards steht für „Retrieval“. Übergeben wir die Frage an das Retriever-Objekt, das ChromaDB abfragt, um herauszufinden, welche Dokumente am ehesten übereinstimmen. Sie werden einige Überlappungen und Duplikate sehen. Diese Überlappungen helfen dabei, Kontinuität über Blockgrenzen hinweg sicherzustellen."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wrOXOMe1Axix","outputId":"97755884-188f-4694-b869-bae536e39d1d"},"outputs":[{"data":{"text/plain":["[Document(page_content=\"rofessional life, Samantha is an avid rock climber and enjoys mentoring young women interested in STEM careers, aiming to inspire and cultivate a new generation of tech leaders.\\n\\nSamantha Clarke is a seasoned Project Manager at Digital Dynamics, a leading tech company known for its innovative solutions in digital marketing and AI-driven analytics. With over a decade of experience in the tech industry, Samantha has played a pivotal role in steering complex projects to success, enhancing the company's reputation for efficiency and cutting-edge technology. A graduate of MIT with a degree in Computer Science, she has a passion for integrating user-friendly technology with business needs. Samantha is particularly noted for her leadership in the development of the company's flagship product, the MarketMinder AI, which has revolutionized the way businesses understand consumer behavior. Outside\"),\n"," Document(page_content=\"market reach. A graduate of MIT with a degree in Systems Engineering, her expertise lies in integrating cross-functional teams and optimizing workflows. Samantha's leadership was instrumental in the launch of the company's flagship product, the MarketMinder AI, which has since revolutionized data-driven marketing strategies for Digital Dynamics' global clientele. Outside of work, she is an avid rock climber and volunteers her time mentoring young women interested in STEM careers.\\n\\nSamantha Clarke is a seasoned Project Manager at Digital Dynamics, a leading tech company known for its innovative solutions in artificial intelligence and machine learning. With over a decade of experience in the tech industry, Samantha has played a pivotal role in steering complex projects to successful completion, fostering collaboration among cross-functional teams, and driving the adoption of agile methodo\"),\n"," Document(page_content='ices. A graduate of MIT with a degree in Computer Science, she has a passion for integrating cutting-edge technologies to enhance operational efficiencies and solve critical challenges. Outside of her professional life, Samantha is an avid rock climber and volunteers her time mentoring young women interested in STEM careers, aiming to inspire the next generation of female tech leaders.\\n\\nSamantha Clarke is a seasoned Project Manager at Global Solutions, an innovative tech company known for its cutting-edge solutions in environmental technology. With over a decade of experience in project management, Samantha has successfully led numerous high-profile projects that focus on sustainable development and renewable energy sources. A graduate of Stanford University with a degree in Environmental Science and Policy, she has a passion for integrating eco-friendly practices into business models. S'),\n"," Document(page_content='s for numerous Fortune 500 companies. A graduate of MIT with a degree in Systems Engineering, her passion for technology and problem-solving is matched only by her commitment to mentoring young women entering STEM fields. Outside of work, Samantha is an avid rock climber and volunteers her time at local community centers teaching coding to kids.\\n\\nSamantha Clarke is a seasoned Project Manager at Global Solutions, an innovative tech company known for its cutting-edge solutions in environmental technology. With over a decade of experience in project management, Samantha has successfully led numerous high-profile projects that focus on sustainable development and renewable energy sources. A graduate of Stanford University with a degree in Environmental Science and Policy, she has a passion for integrating eco-friendly practices into business models. Samantha, originally from Seattle, now res')]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["retriever.invoke(\"What company does Elena Martinez work for?\")"]},{"cell_type":"markdown","metadata":{"id":"2JFStTE3CMsE"},"source":["Diese Daten werden mit der Frage in der RAG-Eingabeaufforderung kombiniert, um sie an das LLM zu übermitteln."]},{"cell_type":"markdown","metadata":{"id":"ARR5NeLzQ2Wg"},"source":["## RAG über PDF-Dokumente\n","\n","Wir werden nun untersuchen, wie man RAG mit einem PDF-Dokument verwendet. Wir werden ein PDF besprechen, das ich mit dem Buchersteller erstellt habe, den wir früher in dieser Klasse gesehen haben. Das Buch gehört zum Genre [steampunk](https://en.wikipedia.org/wiki/Steampunk) und trägt den Titel [steampunk](https://en.wikipedia.org/wiki/Steampunk). Wir verwenden denselben Code wie zuvor, außer dass ich das PDF in Text konvertiere, bevor ich Blöcke erstelle. Diese Technik wird Ihnen bei Aufgabe 6 sehr hilfreich sein."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s2vGastaRds8","outputId":"09c29c69-a3e5-48ec-8449-df26759ca7d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading: https://data.heatonresearch.com/data/t81-559/assignments/clockwork.pdf\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/langsmith/client.py:333: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n","  warnings.warn(\n"]}],"source":["import pypdf\n","from io import BytesIO\n","\n","MODEL = 'gpt-4o-mini'\n","\n","llm = ChatOpenAI(\n","        model=MODEL,\n","        temperature=0.2,\n","        n=1\n","    )\n","\n","urls = [\n","    \"https://data.heatonresearch.com/data/t81-559/assignments/clockwork.pdf\"\n","]\n","\n","def extract_pdf_text(pdf_content):\n","    pdf_file = BytesIO(pdf_content)\n","    reader = pypdf.PdfReader(pdf_file)\n","    text = \"\"\n","    for page in reader.pages:\n","        text += page.extract_text()\n","    return text\n","\n","def chunk_text(text, chunk_size, overlap):\n","    chunks = []\n","    for i in range(0, len(text), chunk_size - overlap):\n","        chunks.append(text[i:i + chunk_size])\n","    return chunks\n","\n","chunk_size = 900\n","overlap = 300\n","\n","documents = []\n","\n","for url in urls:\n","print(f\"Lesen: {url}\")\n","    response = requests.get(url)\n","    response.raise_for_status()  # Stellen Sie sicher, dass uns schlechte Antworten auffallen\n","    content = extract_pdf_text(response.content)  # Korrigiert, um Text mit pypdf zu extrahieren\n","    chunks = chunk_text(content, chunk_size, overlap)\n","    for chunk in chunks:\n","        document = Document(page_content=chunk)\n","        documents.append(document)\n","\n","text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n","docs = text_splitter.split_documents(documents)\n","\n","embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n","\n","db = Chroma.from_documents(docs, embedding_function)\n","rag_prompt = hub.pull(\"rlm/rag-prompt\")\n","\n","def format_documents(documents):\n","    return \"\\n\\n\".join(doc.page_content for doc in documents)\n","\n","retriever = db.as_retriever()\n","\n","qa_chain = (\n","    {\"context\": retriever | format_documents, \"question\": RunnablePassthrough()}\n","    | rag_prompt\n","    | llm\n","    | StrOutputParser()\n",")"]},{"cell_type":"markdown","metadata":{"id":"U_WXhc5MU-gU"},"source":["Nachdem wir nun das PDF geladen haben, können wir es abfragen."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":71},"id":"OvwT9uj6R3Nv","outputId":"210059e6-3877-45fe-aeab-71f54ca54948"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Eliza Hawthorne is a determined inventor and a central character in a narrative involving adventure and innovation in a steampunk setting. She seeks to challenge the status quo and uncover the truth behind forces manipulating technology in London. Eliza embodies the spirit of change and empowerment, ready to forge her own path amidst chaos.'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["qa_chain.invoke(\"Who is Eliza Hawthorne?\")"]},{"cell_type":"markdown","metadata":{"id":"-aExzpaOEyrU"},"source":["# 6.5: Einbetten von Datenbanken\n","\n","\n","In diesem Abschnitt untersuchen wir den Prozess der Ausführung von ChromaDB als Server, um den Zugriff auf RAG LLm zu erleichtern. Zunächst laden wir eine lokal gespeicherte Einbettungsdatenbank, die die Grundlage für die Datenverarbeitungsfunktionen unseres Servers bildet. Sobald die Datenbank erfolgreich geladen wurde, zeigen wir, wie sie neu geladen werden kann, um sicherzustellen, dass unser System aktuelle Informationen beibehalten und sich nahtlos von Störungen erholen kann. Schließlich konzentrieren wir uns auf den Zugriff auf die Datenbank über einen HTTP-Client, der es Benutzern ermöglicht, remote mit dem Server zu interagieren und Einbettungen nach Bedarf für effiziente und skalierbare Sprachmodelloperationen abzurufen und zu verwenden. In diesem Teil möchten wir eine umfassende Anleitung zum Einrichten und Verwalten von ChromaDB bereitstellen, damit ein robuster RAG LLm-Zugriff unterstützt wird.\n","\n","Um die Fähigkeiten des Servers zu veranschaulichen, verwenden wir einen von mir erstellten Beispieldatensatz, der synthetische Daten von Mitarbeiterbiografien aus fünf fiktiven Unternehmen enthält. Dieser Datensatz soll zeigen, wie RAG bestimmte Informationen, die ein Basismodell nicht von sich aus besitzen würde, effektiv abrufen und nutzen kann. Auf diese Weise liefert er ein klares Beispiel dafür, wie RAG in einem realen Unternehmensumfeld angewendet werden kann.\n","\n","Hier ist ein Beispiel für eine solche generierte Biografie:\n","\n","> Elena Martinez ist eine erfahrene Robotikingenieurin bei FutureTech, einem führenden Innovator im Bereich künstliche Intelligenz und Robotik mit Sitz im Silicon Valley. Mit einem Master-Abschluss in Maschinenbau vom MIT und über einem Jahrzehnt Erfahrung war Elena maßgeblich an der Entwicklung autonomer Robotersysteme beteiligt, die die städtische Mobilität und Zugänglichkeit verbessern sollen. Zu ihren bahnbrechenden Arbeiten gehört die Entwicklung des ersten KI-gesteuerten Roboterassistenten, der nahtlos mit städtischen Umgebungen interagieren kann, um älteren und behinderten Menschen zu helfen. Als leidenschaftliche Verfechterin der Förderung von Frauen in MINT-Fächern leitet Elena auch das Outreach-Programm von FutureTech, das darauf abzielt, die nächste Generation weiblicher Ingenieure durch Workshops und Mentoring zu inspirieren. Ihre Beiträge haben FutureTech nicht nur zu neuen Höhen geführt, sondern auch neue Maßstäbe bei Roboteranwendungen für das Gemeinwohl gesetzt.\n","\n","\n","Diese Biografie zeigt die Art detaillierter, unternehmensspezifischer Informationen, die RAG abrufen und in seine Antworten integrieren kann, wodurch die Relevanz und Genauigkeit der generierten Inhalte verbessert wird. Durch die Integration dieser maßgeschneiderten Daten bereichert RAG nicht nur die Ergebnisse der LLMs, sondern stellt auch sicher, dass die Antworten auf den einzigartigen Kontext und die Bedürfnisse der Organisation abgestimmt sind.\n","\n","Diese Beispieldaten werden unter den folgenden URLs gespeichert. Jede Datei enthält Personen aus einem der Unternehmen.\n","\n","* https://data.heatonresearch.com/data/t81-559/bios/DD.txt\n","* https://data.heatonresearch.com/data/t81-559/bios/FT.txt\n","* https://data.heatonresearch.com/data/t81-559/bios/GS.txt\n","* https://data.heatonresearch.com/data/t81-559/bios/NGS.txt\n","* https://data.heatonresearch.com/data/t81-559/bios/TI.txt\n","\n","Der folgende Code lädt diese Dokumente und speichert sie in einer ChromaDB-Datenbank. Diesen Code haben wir bereits zuvor gesehen.\n","\n","## Laden Sie die ChromaDB-Datenbank\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":693,"referenced_widgets":["34765850b1b9457484057087b6dc881b","12d77ed541de43e1a11dd88a187f49cc","4ca0fce5c0fa4a388227e62eafb91b5b","9d7a821bb35c4c2c9f2a6b3486d54c8e","ffdafb399e1742fa9705127344217866","bc7efe0fde224bfbb9589480c6b93cc1","2d232b8865d047019ef37f3501187d81","b1313498c29748a78e512b300e381715","4198c65224f24bab9728380f78734f30","0113faed621540398814e84186cd1377","a1689587a1e347d28146fb2f57d7bd1d","62beff48b42b4a5bbc5780535b75674a","a74beabad4ae47a3b9457ca61d9af9bd","e4c945901fdb4f089943919f10683c2f","4ce6a45b185c4ca5b1f07851cf4d31e4","1fa67476de444d3187c428710b414139","c2973e4452cb4785a45b1d39a2216823","28475ebe696541c4a9f1783528a853ef","1758367da24640179dce8317cd99eb2a","e4496edd280d4f34980906c441add2df","b2e4a87007b94380a7e86ff564300b29","82ffd0bf646c4ebc88b39a41712ce3f2","83bae2cde28048d39ab0b9db224d5758","155af4db580b44d1bae399fdc223dc13","c95fa0d27a014a04ba53b0cc4dbf8e1d","6b5ddf46099f4f46b9637302f82c022b","949d7d7ff81b4d629cc271081c1d0c88","f29fd8fdde6e480cb5e1172c5d27ef1d","c8b91d3e8f334a79af569dd903c5ff77","72aaedc34e254df9a17d8905394526e8","986675611a5948adbfe93debcd7e41bb","d91b1d3b68f841b1a5140d0861aee982","fe393ba7c89d464ea21160b4f46d120b","0e8c7b3e367f45e8a6cba82fc85c2a12","99a458b24c614a87a3e4617c154929f4","a370ce792b8d46968b11f3959a8b50ed","64eaf8697f2041c7aa09f38185f1798f","5bcc104127a543dd87ddb55a1af5f980","64b3e3f2e45c4b8dba06f166b3a776cf","0f1534fa86b34c25adf8c5cfb9912723","5b54462c3b1243f6897058ff34a618e0","23a338bdea3d4172a46badbce41e95c4","e38acd94e08a44589d6a4bc0cbe94f24","9209850cf11b4dab92e549cf50d8e154","8f751efffaed4e1c8f18f1b53ab926c0","c7759174a944466b846643264732fb8f","376b9ea754a54167b8f2c8005616e56d","f3b2fac1b03d4581a6f75462bac48696","cfa86180918145efb7a7cbe81379ff2a","1c56160c2c55439aadab7f8eaf5e4120","99748eef9f07451dbd3f6def5ed92112","3643552afb2e4d19bf0f73a14802b60a","c0cfe4404c1143dea9fcb5d7eaa61989","8ec6015870fa48538b4ef94f30498f70","b321e6b5c31a49d0bafce93b1d6b8ad0","2ee8e4c96fba4da081bde8b85be2d2c0","8e607e8e877544e69c3650eb8f180712","c76b4692d2db41589707586eaafde76e","377545edf9474d9581a0d3ce184fd813","4fc1a259b489429ab3aca9154249151d","ec5209021bc74ee2b33c5ba291e79222","9124b6e4731f4ab195dffa891ed3ee35","31d46ccf3060473399e848d5fcac429d","45a89719d4c942bcbb66a09e9b4a67a6","82f34021227c4345b609d0e22809a19d","e957905641244f7393f6f728511758aa","59561bc8eec14df2827fdb760cb8edea","0d6aa5bc1d1145aaabeee6fa6af9ff8d","70edcdef84ef407d94a200348c303e5c","af7cd828ca8a400fa657735c3bdb9ca1","bf42b0eff04344a9b48812beb59bc43d","b6149d22d47b421aa30ee349afbdd865","4d4f165e9247460ba13c4f151dbac7ac","41fa415d44314e0a986c3668da1d3328","cde34242a251447a8f5b3f6ef6479d5e","6095e3cf11a04c9d8dd7ddb7a615df14","18c194e4bb5044e9923989c90124dcc7","e8b9acebac0941c585c3bd3519467d07","3ef35eb1292940848c8b9dbd9d946a1f","52c35bb8cc7c4f7fa6b79648637b0c93","c706def1b1124cb6b4964fc103e9453b","62c25d93360440de9620c61103d42162","2ebdfbc23ed04e4ea5d6e9ba14bd1095","d288bb7067e34a8ca30eb0fa4d0b8512","209183175b98478fa40f2dc41d885ecd","53c9199529a044f9bffa24525273e5f2","5b8dffddc8514c9aa956d7c2a5a49714","2af2a838d4274ac69a7c63b892c77174","8627714191af431b88a490584374d980","1d376608e02c4fc2bae8d03efbf3ac1e","9ab77c7927624cb0979736bf4530922c","1550cfd70b894d508b87b447c2396e47","b891073051204e03b2b4126b411fdfe0","f10a1391bea34119b3678e4dab2a0f03","3ea5e0a1d7ed4e70912886691d4732ed","4820f35d78df4ce38bfeab049c663cc1","beeccc9597d14b56857ac7311e1ff05a","66ac7aab2853438caa49e71f7e9247c9","cd77b711fcc34fafa4d8988eb4867902","27bd1f54104b4ff9a24f6eea294fed57","dbbc2a73b29946bd94aafeb6fe31b724","e55764b1519d4f43ae5cd32ba4622a43","006d1373afad4c07a4f4c1342872d9a7","2bb59e93e4a742b2b5e0f15d508a5b05","9dd8bff72a3a4ae99daaf4a420ef8db3","e283c069d0d9401ca5b98da52cc0d8d6","7acd6da963334224a7e53ad555c18e3e","0499288e1876498eac1505c32b9f7599","b2a23b9479094bacad13cb00f2cf8597","c094628d8203449b86e04dcc876d85f2","98e9106289d64d208fb8b224fd4316e2","24d8cba75e5c4166a354f78021dde3de","7700403398e94841a5e65c19d0cdebc8","f60be2c309a14b55aa99e31721ce25c1","a82033d9450242afbd5b11648c7c4651","b76d6a9f85874573abdf917eb27c6af3","7cb5f2643d684b7b9760dda66c51e907","2ba8fc76fc5d4650aa9af7bf82eec167","0532dce748fa45dbb46f48d8b39d8c5c","9e687c4fa8fe4b62bcc7bddd09c6c7a5","2a7e28165b1b491dab1e1a9663a0e40d"]},"id":"HYtVPMTHExaT","outputId":"6f6bcfe0-bd6c-43dd-8369-7ebe8d461ac8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading: https://data.heatonresearch.com/data/t81-559/bios/DD.txt\n","Reading: https://data.heatonresearch.com/data/t81-559/bios/FT.txt\n","Reading: https://data.heatonresearch.com/data/t81-559/bios/GS.txt\n","Reading: https://data.heatonresearch.com/data/t81-559/bios/NGS.txt\n","Reading: https://data.heatonresearch.com/data/t81-559/bios/TI.txt\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n","  warn_deprecated(\n","/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  from tqdm.autonotebook import tqdm, trange\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34765850b1b9457484057087b6dc881b","version_major":2,"version_minor":0},"text/plain":["modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"62beff48b42b4a5bbc5780535b75674a","version_major":2,"version_minor":0},"text/plain":["config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83bae2cde28048d39ab0b9db224d5758","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0e8c7b3e367f45e8a6cba82fc85c2a12","version_major":2,"version_minor":0},"text/plain":["sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f751efffaed4e1c8f18f1b53ab926c0","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2ee8e4c96fba4da081bde8b85be2d2c0","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"59561bc8eec14df2827fdb760cb8edea","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8b9acebac0941c585c3bd3519467d07","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8627714191af431b88a490584374d980","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"27bd1f54104b4ff9a24f6eea294fed57","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98e9106289d64d208fb8b224fd4316e2","version_major":2,"version_minor":0},"text/plain":["1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from langchain.chains.summarize import load_summarize_chain\n","from langchain.document_loaders import TextLoader\n","from langchain import OpenAI, PromptTemplate\n","from langchain_openai import ChatOpenAI\n","from IPython.display import display_markdown\n","from langchain.indexes import VectorstoreIndexCreator\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_community.vectorstores.inmemory import InMemoryVectorStore\n","from langchain.schema import Document\n","import requests\n","\n","urls = [\n","    \"https://data.heatonresearch.com/data/t81-559/bios/DD.txt\",\n","    \"https://data.heatonresearch.com/data/t81-559/bios/FT.txt\",\n","    \"https://data.heatonresearch.com/data/t81-559/bios/GS.txt\",\n","    \"https://data.heatonresearch.com/data/t81-559/bios/NGS.txt\",\n","    \"https://data.heatonresearch.com/data/t81-559/bios/TI.txt\"\n","]\n","\n","def chunk_text(text, chunk_size, overlap):\n","    chunks = []\n","    for i in range(0, len(text), chunk_size - overlap):\n","        chunks.append(text[i:i + chunk_size])\n","    return chunks\n","\n","chunk_size = 900\n","overlap = 300\n","\n","documents = []\n","\n","for url in urls:\n","print(f\"Lesen: {url}\")\n","    response = requests.get(url)\n","    response.raise_for_status()  # Stellen Sie sicher, dass uns schlechte Antworten auffallen\n","    content = response.text\n","    chunks = chunk_text(content, chunk_size, overlap)\n","    for chunk in chunks:\n","        document = Document(page_content=chunk)\n","        documents.append(document)\n","\n","from langchain_text_splitters import CharacterTextSplitter\n","from langchain_community.embeddings.sentence_transformer import (\n","    SentenceTransformerEmbeddings,\n",")\n","from langchain.vectorstores import Chroma\n","\n","text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n","docs = text_splitter.split_documents(documents)\n","\n","embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n","\n","# db = Chroma.from_documents(Dokumente, Einbettungsfunktion)"]},{"cell_type":"markdown","metadata":{"id":"ELhuBPP-agc_"},"source":["Abschließend laden wir aus diesen Dokumenten eine ChromaDB-Datenbank. Dabei geben wir allerdings zunächst ein Verzeichnis an, in dem diese Daten gespeichert werden."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gYehOnJFJPI1"},"outputs":[],"source":["db = Chroma.from_documents(docs, embedding_function, persist_directory=\"/content/chroma_db\")"]},{"cell_type":"markdown","metadata":{"id":"E2h0nvlva1ah"},"source":["Wir können sehen, dass dies ein reguläres Chroma-Objekt ist."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":274},"id":"F_1FgCiUYzTH","outputId":"222cdfdc-b546-414c-a291-29e0051257a4"},"outputs":[{"data":{"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_community.vectorstores.chroma.Chroma</b><br/>def __init__(collection_name: str=_LANGCHAIN_DEFAULT_COLLECTION_NAME, embedding_function: Optional[Embeddings]=None, persist_directory: Optional[str]=None, client_settings: Optional[chromadb.config.Settings]=None, collection_metadata: Optional[Dict]=None, client: Optional[chromadb.Client]=None, relevance_score_fn: Optional[Callable[[float], float]]=None) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/chroma.py</a>`ChromaDB` vector store.\n","\n","To use, you should have the ``chromadb`` python package installed.\n","\n","Example:\n","    .. code-block:: python\n","\n","            from langchain_community.vectorstores import Chroma\n","            from langchain_community.embeddings.openai import OpenAIEmbeddings\n","\n","            embeddings = OpenAIEmbeddings()\n","            vectorstore = Chroma(&quot;langchain_store&quot;, embeddings)</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 53);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"],"text/plain":["langchain_community.vectorstores.chroma.Chroma"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["type(db)"]},{"cell_type":"markdown","metadata":{"id":"9CM8vZuJbCrn"},"source":["Der folgende Code zeigt die Struktur der eingebetteten Datenbank, wie sie von ChromaDB gespeichert wird."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EhILCyOZMpmk","outputId":"d5c34abd-b9a1-46c0-bddb-5c582535b462"},"outputs":[{"name":"stdout","output_type":"stream","text":["total 5904\n","drwxr-xr-x 3 root root    4096 Jun 26 16:31 .\n","drwxr-xr-x 1 root root    4096 Jun 26 16:30 ..\n","-rw-r--r-- 1 root root 6029312 Jun 26 16:31 chroma.sqlite3\n","drwxr-xr-x 2 root root    4096 Jun 26 16:31 e7f2b890-35c6-4a12-b5ff-a12a08fe90c8\n"]}],"source":["!ls -al /content/chroma_db"]},{"cell_type":"markdown","metadata":{"id":"Z_6oBXkOJqq1"},"source":["## Datenbank von der Festplatte neu laden\n","\n","Wir können diese Daten nun in eine neue speicherbasierte ChromaDB neu laden. Diese Technik ist viel effizienter als das Neuladen der Daten mit der zuvor verwendeten Methode from_documents."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x-2isPrvJw5W"},"outputs":[],"source":["db2 = Chroma(persist_directory=\"/content/chroma_db\", embedding_function=embedding_function)\n"]},{"cell_type":"markdown","metadata":{"id":"X9Mjb9KnbW4T"},"source":["Der folgende Code, den wir zuvor besprochen haben, zeigt, dass die neu geladene Datenbank betriebsbereit ist."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XdHWSI1NJ_WH"},"outputs":[],"source":["from langchain import hub\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_core.output_parsers import StrOutputParser\n","\n","MODEL = 'gpt-4o-mini'\n","\n","llm = ChatOpenAI(\n","        model=MODEL,\n","        temperature=0.2,\n","        n=1\n","    )\n","\n","rag_prompt = hub.pull(\"rlm/rag-prompt\")\n","\n","def format_documents(documents):\n","    return \"\\n\\n\".join(doc.page_content for doc in documents)\n","\n","retriever = db2.as_retriever()\n","\n","qa_chain = (\n","    {\"context\": retriever | format_documents, \"question\": RunnablePassthrough()}\n","    | rag_prompt\n","    | llm\n","    | StrOutputParser()\n",")"]},{"cell_type":"markdown","metadata":{"id":"L0_poIKdbpzo"},"source":["Wir führen eine Abfrage durch, die auf die RAG-fähigen Daten zugreift."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"odbi4SeVKRJ0","outputId":"705fcbb5-6756-413e-f560-000e2852bb6b"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Elena Martinez works for Digital Dynamics.'"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["qa_chain.invoke(\"What company does Elena Martinez work for?\")"]},{"cell_type":"markdown","metadata":{"id":"c-TI6Ft_Jt57"},"source":["## Datenbank als Server ausführen\n","\n","Mit dem folgenden Befehl können Sie einen ChromaDB-Server starten, der das zuvor erstellte Verzeichnis verwendet.\n","\n","```\n","chroma run --host 127.0.0.1 --path /content/chroma_db &\n","```"]},{"cell_type":"markdown","metadata":{"id":"-ySeKBhjcNu0"},"source":["Wir können jetzt einen Client erstellen, um mit diesem Datenbankserver zu kommunizieren."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SPxDQnHTWFxv"},"outputs":[],"source":["import chromadb\n","import langchain_community\n","\n","client = chromadb.HttpClient(host='127.0.0.1', port=8000)\n","db3 = langchain_community.vectorstores.chroma.Chroma(\n","    client=client,embedding_function=embedding_function)"]},{"cell_type":"markdown","metadata":{"id":"3jGOl1PGbh_k"},"source":["Der folgende Code, den wir zuvor besprochen haben, zeigt, dass die HTTP-Datenbank des Clients betriebsbereit ist."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RgiFmGyrWQKw"},"outputs":[],"source":["from langchain import hub\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_core.output_parsers import StrOutputParser\n","\n","MODEL = 'gpt-4o-mini'\n","\n","llm = ChatOpenAI(\n","        model=MODEL,\n","        temperature=0.2,\n","        n=1\n","    )\n","\n","rag_prompt = hub.pull(\"rlm/rag-prompt\")\n","\n","def format_documents(documents):\n","    return \"\\n\\n\".join(doc.page_content for doc in documents)\n","\n","retriever = db3.as_retriever()\n","\n","qa_chain = (\n","    {\"context\": retriever | format_documents, \"question\": RunnablePassthrough()}\n","    | rag_prompt\n","    | llm\n","    | StrOutputParser()\n",")"]},{"cell_type":"markdown","metadata":{"id":"V7Nd5Tuxbldr"},"source":["Wir führen eine Abfrage durch, die auf die RAG-fähigen Daten zugreift."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"wvKB2dBRXFtx","outputId":"e68fb841-f0cf-4861-aa50-e326c0a1f738"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Samantha Clarke works for Digital Dynamics.'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["qa_chain.invoke(\"What company does Elena Martinez work for?\")"]}]}
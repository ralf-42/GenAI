{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["dXyUNJDNOh5U","pC9A-LaYhsta","J_6EHVIcBe0Q","HODQfLqHBnUI","TgfPV_aXBuv9","x1Wb0diiB1yO","ikTJEPGqN9IH"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<p><font size=\"6\" color='grey'> <b>\n","Anwendung Generativer KI\n","</b></font> </br></p>"],"metadata":{"id":"Ih2CTVBnArVZ"}},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b>\n","Large Language Models und Transformer\n","</b></font> </br></p>\n","\n","\n","---"],"metadata":{"id":"6jJZ7wbdArVc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfdhPIzcEYRG","cellView":"form","collapsed":true},"outputs":[],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  Colab-Umfeld</font> </br></p>\n","# Installierte Python Version\n","import sys\n","print(f\"Python Version: \",sys.version)\n","# Installierte LangChain Bibliotheken\n","print()\n","print(\"Installierte LangChain Bibliotheken:\")\n","\n","!pip list | grep '^langchain'\n","# Unterdr√ºckt die \"DeprecationWarning\" von LangChain f√ºr die Memory-Funktionden\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")"]},{"cell_type":"code","source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  SetUp API-Keys (setup_api_keys)</font> </br></p>\n","\n","def setup_api_keys():\n","    \"\"\"Konfiguriert alle ben√∂tigten API-Keys aus Google Colab userdata\"\"\"\n","    from google.colab import userdata\n","    import os\n","    from os import environ\n","\n","    # Dictionary der ben√∂tigten API-Keys\n","    keys = {\n","        'OPENAI_API_KEY': 'OPENAI_API_KEY',\n","        'HF_TOKEN': 'HF_TOKEN',\n","        # Weitere Keys bei Bedarf\n","    }\n","\n","    # Keys in Umgebungsvariablen setzen\n","    for env_var, key_name in keys.items():\n","        environ[env_var] = userdata.get(key_name)\n","\n","    return {k: environ[k] for k in keys.keys()}\n","\n","# Verwendung\n","all_keys = setup_api_keys()\n","# Bei Bedarf einzelne Keys direkt zugreifen\n","# WEATHER_API_KEY = all_keys['WEATHER_API_KEY']"],"metadata":{"cellView":"form","id":"WD3Wwr6sESX8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1 | Large Language Models\n","---"],"metadata":{"id":"dXyUNJDNOh5U"}},{"cell_type":"markdown","source":["LLMs funktionieren im Wesentlichen durch die *Vorhersage des wahrscheinlichsten n√§chsten Worte*s in einem gegebenen Text, basierend auf einer riesigen Menge an Trainingsdaten aus dem Internet. Dieser Prozess wird durch Billiarden von Parametern gesteuert, die w√§hrend des Trainings optimiert werden. Ein entscheidender Aspekt ist die Architektur von **Transfomern**, die es LLMs erlaubt, den gesamten Text **parallel** zu verarbeiten, anstatt Wort f√ºr Wort, und dabei den Kontext durch einen Mechanismus namens **Self Attention** zu ber√ºcksichtigen. Das Ergebnis ist ein System, das erstaunlich fl√ºssige und oft sinnvolle Texte generieren kann, obwohl das genaue *Warum* hinter den Vorhersagen aufgrund der Komplexit√§t und Gr√∂√üe des Modells schwer zu verstehen ist."],"metadata":{"id":"vwFflTpGObBC"}},{"cell_type":"markdown","metadata":{"id":"pC9A-LaYhsta"},"source":["# 2 | Foundation Models\n","---"]},{"cell_type":"markdown","source":["\n","Die Grundlage gro√üer Sprachmodelle (LLMs) sind **Foundation Models**, die anhand vieler Daten trainiert wurden. Sie k√∂nnen f√ºr bestimmte Aufgaben oder Anwendungen angepasst oder optimiert werden. Diese Modelle werden als *Basismodelle* (Foundation Models) bezeichnet, da sie eine grundlegende Ebene an Wissen und F√§higkeiten bieten, auf der spezialisierte Funktionen aufgebaut werden k√∂nnen. Mehrere namhafte Technologieunternehmen und Forschungsorganisationen bieten gro√üe Sprachmodelle an. Zu ihnen z√§hlen OpenAI mit Modellen wie **GPT** (Generative Pre-trained Transformer), Google mit **BERT** (Bidirectional Encoder Representations from Transformers) und anderen Varianten sowie Facebook (Meta), das Modelle wie **RoBERTa** (Robustly Optimized BERT Pretraining Approach) anbietet.\n","\n","Das Trainieren eines gro√üen Sprachmodells von Grund auf erfordert viel Rechenleistung und Fachwissen. Es braucht umfangreiche Daten, die man erst sammeln, bereinigen und verarbeiten muss. Au√üerdem braucht man leistungsstarke Computer, die so viele Daten verarbeiten k√∂nnen. Das kann sehr teuer werden, f√ºr Einzelpersonen und viele Organisationen. Die Entwicklung eines gro√üen Sprachmodells ist sehr anspruchsvoll und dauert oft l√§nger als ein Kurs. In Kursen lernt man, wie man vorhandene Modelle verwendet und optimiert, um bestimmte Probleme zu l√∂sen oder Forschung zu betreiben.\n","\n"],"metadata":{"id":"rhiHq-tbpfRA"}},{"cell_type":"markdown","source":["**Im Kurs verwenden wir:**  \n","- **gpt-4o-mini**: Standardmodell f√ºr die meisten Aufgaben.  \n","- **gpt-4o**: F√ºr detailliertere Antworten.  \n","- **o3-mini**: Optimiert f√ºr logische Schlussfolgerungen.  "],"metadata":{"id":"LiNHLzJxwYbT"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Schrittweiser Modellaufbau\n","</font></p>\n"],"metadata":{"id":"aQqiPx-V3Kio"}},{"cell_type":"markdown","source":["Gerne beschreibe ich das Modell analog zur vorgegebenen Struktur:\n","\n","Im vorliegenden Code wird ein Chatbot-Modell schrittweise aufgebaut. Der Modellaufbau ist modular und kann als `template` f√ºr nachfolgende Konversationsmodelle verwendet werden.\n","\n","Diese strukturierte Aufteilung macht das Modell robust, wartbar und erweiterbar f√ºr verschiedene Anwendungsf√§lle im Bereich der Konversations-KI.\n","\n"],"metadata":{"id":"tiFCe1fe3hEa"}},{"cell_type":"code","source":["# Abschnitt 0: Installation und API-Key\n","!uv pip install --system --prerelease allow langchain_community langchain_openai"],"metadata":{"id":"JvzHRtXd3hEd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Erl√§uterung der externen Funktionen:**\n","\n","- `from IPython.display import display, Markdown`: Erlaubt die Darstellung von Output in verschiedenen Formaten, einschlie√ülich formatiertem Markdown, in Jupyter Notebooks.\n","- `from langchain_openai import ChatOpenAI`: Importiert die Klasse `ChatOpenAI`, die zur Integration von OpenAI's Chatbot-Funktionalit√§ten in die LangChain-Bibliothek genutzt wird.\n","- `from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder`: Erlaubt die Definition von Vorlagen f√ºr Chat-Prompts und Platzhalter f√ºr Nachrichten in LangChain.\n","- `from langchain.schema import HumanMessage, AIMessage, SystemMessage`: Definiert verschiedene Nachrichtentypen in LangChain, die zwischen menschlichen, KI- und Systemnachrichten unterscheiden.\n","- `from langchain_core.output_parsers.string import StrOutputParser`: Importiert eine Klasse, die zum Parsen von KI-Output in String-Format in der LangChain-Core-Bibliothek verwendet wird."],"metadata":{"id":"Fo0Tm9U56qVM"}},{"cell_type":"code","source":["# Abschnitt 1: Importe\n","from IPython.display import display, Markdown\n","\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain.schema import HumanMessage, AIMessage, SystemMessage\n","from langchain_core.output_parsers.string import StrOutputParser\n","\n","# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATURE = 0.0\n","\n","DEFAULT_SYSTEM = \"\"\"\n","Du bist ein kompetenter KI-Assistent mit breitem Fachwissen.\n","\n","Deine Antworten sind:\n","- Klar strukturiert und mit Markdown formatiert\n","- Praxisorientiert und direkt umsetzbar\n","- Basierend auf aktuellem Kenntnisstand\n","- Mit passenden Beispielen versehen\n","- In verst√§ndlicher Sprache formuliert\n","\n","Bei deiner Arbeit:\n","- Analysierst du Fragen sorgf√§ltig\n","- Gibst pr√§zise und relevante Antworten\n","- Erkennst den Kontext der Anfrage\n","- Bietest bei Bedarf weiterf√ºhrende Informationen\n","- Bleibst sachlich und neutral\n","\n","Formatiere alle Antworten in Markdown f√ºr optimale Lesbarkeit.\n","\"\"\"\n","\n","# Abschnitt 3: Chat-Komponenten initialisieren\n","llm = ChatOpenAI(temperature=TEMPERATURE, model=MODEL)\n","\n","chat_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    MessagesPlaceholder(variable_name=\"history\"),\n","    (\"human\", \"{question}\")\n","])\n","\n","# Abschnitt 4: Funktionen definieren\n","def format_history(history: list) -> list:\n","    \"\"\"Folistiert listKonversationsverlauf f√ºr das Prompt-Template.\"\"\"\n","    return [\n","        HumanMessage(content=h.content) if isinstance(h, HumanMessage)\n","        else AIMessage(content=h.content) if isinstance(h, AIMessage)\n","        else h\n","        for h in history\n","    ]\n","\n","def process_conversation(question: str, history: list) -> str:\n","    \"\"\"Verlistitet eine Konversation und gibt die Antwort zur√ºck.\"\"\"\n","    conversation_chain = chat_prompt | llm | StrOutputParser()\n","    response = conversation_chain.invoke(\n","        {\"system_prompt\": DEFAULT_SYSTEM, \"history\": format_history(history), \"question\": question}\n","    )\n","    return response\n","\n","def interact_with_ai(question: str, history: list):\n","    \"\"\"F√ºhrt eine einzelne Interaktion mit der KI durch.\"\"\"\n","    print(f\"\\nüßë‚Äçü¶± Mensch: {question}\")\n","    response = process_conversation(question, history)\n","    print(\"\\nü§ñ KI:\", end=\" \")\n","    display(Markdown(response))\n","    history.extend([HumanMessage(content=question), AIMessage(content=response)])\n","\n","# Abschnitt 5: Hauptprogramm\n","history = [SystemMessage(content=DEFAULT_SYSTEM)]\n","test_questions = [\n","    \"Mein Name ist Ralf\",\n","    \"Warum ist der Himmel blau?\",\n","    \"Und warum ist er manchmal rot?\",\n","    \"Wie ist mein Name?\"\n","]\n","\n","for question in test_questions:\n","    interact_with_ai(question, history)"],"metadata":{"id":"9y7B2qfuB2Xs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Neue Konversation starten\n","</font></p>"],"metadata":{"id":"TWq5rQ4AeIU6"}},{"cell_type":"code","source":["# Abschnitt 5: Hauptprogramm\n","history = [SystemMessage(content=DEFAULT_SYSTEM)]\n","question = \"Erstelle eine Tabelle der f√ºnf bev√∂lkerungsreichsten L√§ndern mit Bev√∂lkerung und BIP.\"\n","interact_with_ai(question, history)"],"metadata":{"id":"bep9LTAvdiT2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J_6EHVIcBe0Q"},"source":["# 3 | Textgenerierung\n","---"]},{"cell_type":"markdown","source":["Die Generierung von Text geh√∂rt zu den h√§ufigsten Anwendungsf√§llen f√ºr **Large Language Models (LLMs)**. Die Erstellung von nat√ºrlichem Text folgen einem einem √§hnlichen Prinzip wie zur Codegenerierung.  \n","\n","Anstatt eine interaktive Chatumgebung zu verwenden, erfolgt die Textgenerierung durch gezielte **Eingabeaufforderungen (Prompts)**, die an **LangChain** gesendet werden. Das Modell verarbeitet diese Anfragen und liefert darauf basierend den generierten Text.  \n","\n","Der folgende Code zeigt, wie ein LLM zur Generierung von Text abgefragt werden kann."],"metadata":{"id":"Q6qNoTVI7Jp_"}},{"cell_type":"markdown","metadata":{"id":"DB0IAW8vBJLV"},"source":["<p><font color='black' size=\"5\">\n","Muster zur Textgenerierung\n","</font></p>\n","\n","Bei der grundlegenden Textgenerierung gibt es verschiedene **Eingabeaufforderungsmuster (Prompting-Methoden)**, die sich je nach Detailgrad der bereitgestellten Informationen unterscheiden. Diese Muster beeinflussen, wie das **Large Language Model (LLM)** den gew√ºnschten Text erzeugt.  \n","\n","Im Folgenden werden verschiedene dieser Muster untersucht und analysiert, wie sie sich auf die Qualit√§t und Struktur der generierten Inhalte auswirken.\n","\n","* Zero-Shot-Prompt\n","* One-Shot-Prompt\n","* Few-Shot-Prompt"]},{"cell_type":"markdown","metadata":{"id":"9GeVTcwLl4xi"},"source":["<p><font color='black' size=\"5\">\n","Zero-Shot Textgenerierung\n","</font></p>\n","\n","Ein Zero-Shot-Prompt zur Texterstellung ist eine Technik, bei der ein Sprachmodell ohne vorherige Anpassung oder spezielles Training auf eine bestimmte Aufgabe direkt mit einer einzigen Eingabeaufforderung genutzt wird. Um diesen Ansatz erfolgreich anzuwenden, ist es entscheidend, einen pr√§zisen und ausf√ºhrlichen Prompt zu formulieren, der genau beschreibt, welche Art von Inhalt generiert werden soll. Dabei sollten Stil, Struktur und relevante Details oder Einschr√§nkungen klar angegeben werden. Beispielsweise kann f√ºr eine gesch√§ftliche E-Mail der gew√ºnschte Tonfall (formell oder informell), die wesentlichen Inhalte (wie Termin, Thema, Teilnehmer) sowie eine Handlungsaufforderung spezifiziert werden. Da das Modell allein auf die im Prompt enthaltenen Informationen angewiesen ist, sollte das gew√ºnschte Ergebnis m√∂glichst eindeutig formuliert sein. Diese Methode ist flexibel einsetzbar und erlaubt die Erstellung vielf√§ltiger Textarten, ohne dass eine vorherige Modellanpassung erforderlich ist.  \n","\n","Das folgende Beispiel demonstriert eine Zero-Shot-Eingabeaufforderung: Es werden verschiedene Anforderungen gestellt und Informationen zu einem Studierenden bereitgestellt, jedoch ohne eine explizite Vorlage f√ºr das Sprachmodell vorzugeben."]},{"cell_type":"code","source":["# Abschnitt 1: Importe\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain.schema import SystemMessage, HumanMessage, AIMessage\n","\n","# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATURE = 0.2\n","DEFAULT_SYSTEM = \"\"\"\n","Du bist ein hilfreicher KI-Assistent, der bei der Erstellung von Empfehlungsschreiben hilft.\n","\"\"\"\n","\n","# Abschnitt 3: Chat-Komponenten initialisieren\n","llm = ChatOpenAI(temperature=TEMPERATURE, model=MODEL)\n","chat_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    MessagesPlaceholder(\"messages\"),\n","    (\"human\", \"{human_input}\")\n","])\n","\n","# Abschnitt 4: Funktionen definieren\n","def format_history(history: list[dict]) -> list[dict]:\n","    \"\"\"Formatiert den Konversationsverlauf f√ºr die Anzeige.\"\"\"\n","    formatted_history = []\n","    for message in history:\n","        if isinstance(message, HumanMessage):\n","            formatted_history.append({\"role\": \"user\", \"content\": message.content})\n","        elif isinstance(message, AIMessage):\n","            formatted_history.append({\"role\": \"assistant\", \"content\": message.content})\n","    return formatted_history\n","\n","def process_conversation(question: str, history: list, llm: ChatOpenAI, chat_prompt: ChatPromptTemplate) -> str:\n","    \"\"\"Verarbeitet eine einzelne Konversation.\"\"\"\n","    messages = chat_prompt.format_messages(\n","        system_prompt=DEFAULT_SYSTEM,\n","        messages=history,\n","        human_input=question\n","    )\n","    response = llm.invoke(messages)  # Verwendung von invoke statt __call__\n","    history.append(HumanMessage(content=question))\n","    history.append(response)\n","    return response.content\n","\n","def interact_with_ai(question: str, history: list):\n","    \"\"\"F√ºhrt eine einzelne Interaktion mit der KI durch.\"\"\"\n","    response = process_conversation(question, history, llm, chat_prompt)\n","    print(response)"],"metadata":{"id":"uMZrQ2UQKJKo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 5: Hauptprogramm\n","history = [SystemMessage(content=DEFAULT_SYSTEM)]\n","question = [\n","    \"\"\"\n","    Erstellen Sie ein positives Empfehlungsschreiben f√ºr Maria Weber, eine meiner Studierenden im Kurs ML 401 an der Technischen Universit√§t M√ºnchen. Mein Name ist Dr. Julia Berger. Sie bewirbt sich f√ºr einen Master of Science in Data Science. Geben Sie mir nur den Haupttext des Schreibens, keine Kopf- oder Fu√üzeile. Formatieren Sie in Markdown.\n","\n","    Unten ist ihre Anfrage:\n","\n","    Sehr geehrte Frau Dr. Berger,\n","\n","    ich hoffe, es geht Ihnen gut! Ich bin Maria Weber (Matrikelnr.: 7890), eine Absolventin der TU M√ºnchen, die im Juni 2023 meinen Bachelor in Wirtschaftsmathematik abgeschlossen habe.\n","\n","    Im Wintersemester 2022/23 hatte ich das Privileg, Ihren Kurs ML 401: Grundlagen des maschinellen Lernens zu besuchen. Der Kurs war ein Wahlpflichtfach in meinem Bachelorprogramm. Die Inhalte haben mich sehr begeistert und ich habe mich durchgehend intensiv eingebracht, was sich in der Note 1,0 widerspiegelte.\n","\n","    Nach meinem Abschluss mit einer Gesamtnote von 1,1 ‚Äì der besten Note meines Jahrgangs ‚Äì arbeite ich als Data Analytics Consultant bei der Firma DataTech GmbH. Meine Hauptaufgabe liegt in der Entwicklung von KI-gest√ºtzten Analysetools f√ºr Gesch√§ftsprozesse. Um meine Expertise im Bereich Data Science zu vertiefen, m√∂chte ich berufsbegleitend einen Master in Data Science absolvieren. Ich sch√§tze Ihre Forschungsarbeit sehr (ich verfolge regelm√§√üig Ihre Publikationen und habe sie auch in meinem Team vorgestellt) und Ihre Einsch√§tzung w√§re f√ºr meine Bewerbung sehr wertvoll.\n","\n","    Ich bewerbe mich f√ºr folgende Programme:\n","    - ETH Z√ºrich, Master of Science in Data Science\n","    - TU Berlin, Master of Science in Data Engineering\n","\n","    D√ºrfte ich Sie um ein Empfehlungsschreiben f√ºr diese Bewerbungen bitten? Meinen Lebenslauf habe ich beigef√ºgt und ich stelle Ihnen gerne weitere Informationen zur Verf√ºgung.\n","\n","    Vielen Dank f√ºr die Ber√ºcksichtigung meiner Anfrage.\n","\n","    Mit freundlichen Gr√º√üen\n","    Maria Weber\n","    \"\"\"\n","]\n","interact_with_ai(question, history)"],"metadata":{"id":"bGp3dfribNW1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XhpXOo-xOair"},"source":["<p><font color='black' size=\"5\">\n","One-Shot Textgenerierung\n","</font></p>\n","\n","Ein One-Shot-Prompt zur Textgenerierung ist eine Methode, bei der ein Sprachmodell mit einer einzigen, detaillierten Eingabe angewiesen wird, auf deren Basis es einen zusammenh√§ngenden Text erstellt. Um diese Technik wirkungsvoll einzusetzen, sollte der Prompt klar und pr√§zise formuliert sein und alle relevanten Informationen sowie den gew√ºnschten Kontext enthalten. Dabei ist es ratsam, Stil, Ton und spezifische inhaltliche Aspekte genau zu definieren.  \n","\n","M√∂chte man beispielsweise eine atmosph√§rische Beschreibung einer K√ºstenstadt generieren lassen, sollten wesentliche Merkmale wie Tageszeit, Stimmung und bestimmte visuelle oder emotionale Eindr√ºcke explizit benannt werden. Eine solche detaillierte Vorgabe erleichtert es dem Modell, die Anforderungen zu erfassen und passgenaue Inhalte zu erstellen.  \n","\n","Nachdem der Prompt formuliert wurde, kann er direkt in das Textgenerierungstool eingegeben werden. Falls das Ergebnis nicht vollst√§ndig den Erwartungen entspricht, l√§sst sich der Prompt gezielt anpassen, um die Qualit√§t und Relevanz der Ausgabe weiter zu optimieren."]},{"cell_type":"code","source":["# Abschnitt 5: Hauptprogramm\n","history = [SystemMessage(content=DEFAULT_SYSTEM)]\n","question = \"\"\"\n","Basierend auf diesem Beispiel, erstelle ein neues akademisches Empfehlungsschreiben.\n","Behalte den professionellen Ton bei, aber passe die Details an:\n","\n","BEISPIEL:\n","\"Ich freue mich sehr, Maria Weber f√ºr den Master of Science in Data Science zu empfehlen. Als ihre Dozentin im Kurs ML 401: Grundlagen des maschinellen Lernens an der TU M√ºnchen konnte ich ihre au√üergew√∂hnlichen akademischen F√§higkeiten beobachten. Frau Weber schloss den anspruchsvollen Kurs mit der Note 1,0 ab und demonstrierte dabei ein tiefgreifendes Verst√§ndnis f√ºr maschinelles Lernen und dessen praktische Anwendungen.\n","\n","Besonders beeindruckend war ihre F√§higkeit, komplexe Konzepte nicht nur zu verstehen, sondern auch auf reale Problemstellungen anzuwenden. In ihrer Position als Data Analytics Consultant bei DataTech GmbH setzt sie diese F√§higkeiten erfolgreich in der Entwicklung KI-gest√ºtzter Analysetools ein.\n","\n","Mit ihrem Bachelorabschluss in Wirtschaftsmathematik (Note 1,1) als Jahrgangsbeste hat Frau Weber bereits bewiesen, dass sie h√∂chsten akademischen Anforderungen gerecht wird. Ihre analytischen F√§higkeiten, gepaart mit ihrer Lernbereitschaft und ihrem Engagement, machen sie zu einer idealen Kandidatin f√ºr ein weiterf√ºhrendes Studium im Bereich Data Science.\n","\n","Aufgrund ihrer bisherigen Leistungen und ihres Potenzials empfehle ich Frau Weber nachdr√ºcklich f√ºr den Master of Science in Data Science. Sie wird zweifellos einen wertvollen Beitrag zum Programm leisten.\"\n","\n","ANWEISUNGEN:\n","1. Erstelle ein neues Empfehlungsschreiben mit √§hnlicher Struktur\n","2. √Ñndere:\n","- Namen\n","- Studienfach\n","- Universit√§ten\n","- Kurse\n","- Noten\n","- aktuelle Berufst√§tigkeit\n","3. Behalte bei:\n","- Vier-Absatz-Struktur\n","- Professionellen Ton\n","- Spezifische Beispiele f√ºr Leistungen\n","- Klare Empfehlung am Ende\n","\n","GEW√úNSCHTES FORMAT:\n","- Nur Haupttext\n","- Keine Anrede/Gru√üformel\n","- In Markdown formatiert\n","\"\"\"\n","\n","interact_with_ai(question, history)"],"metadata":{"id":"5-thhsZp-VdA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DYr9PH7ZOeH3"},"source":["<p><font color='black' size=\"5\">\n","Few-Shot Textgenerierung\n","</font></p>\n","\n","Ein Few-Shot-Prompt gibt einem Modell eine begrenzte Anzahl von Beispielen, um dessen Reaktion gezielt zu steuern. Diese Methode eignet sich besonders f√ºr Sprach- oder Bildgenerierungsmodelle, da sie dem Modell hilft, Muster oder Stile anhand weniger Eingaben zu erkennen und entsprechend nachzubilden.  \n","\n","Ein typischer Few-Shot-Prompt f√ºr ein Textgenerierungsmodell enth√§lt mehrere Beispielpaare aus Eingaben und den gew√ºnschten Ausgaben. Dadurch erh√§lt das Modell eine Orientierung und kann √§hnliche Inhalte mit h√∂herer Pr√§zision erzeugen. Dieser Ansatz erm√∂glicht eine gezielte Anpassung der Modellantworten, ohne dass eine umfassende Trainingsphase erforderlich ist.  \n","\n","Few-Shot-Prompting verbessert die Qualit√§t und Konsistenz der generierten Texte und macht das Modell flexibler einsetzbar ‚Äì insbesondere f√ºr Aufgaben, die kreative Feinabstimmung oder spezifische Stilvorgaben erfordern."]},{"cell_type":"code","source":["# Abschnitt 5: Hauptprogramm\n","history = [SystemMessage(content=DEFAULT_SYSTEM)]\n","question = \"\"\"\n","    Basierend auf diesen Beispielen, erstelle ein neues akademisches Empfehlungsschreiben.\n","    Behalte den professionellen Ton bei, aber passe die Details an:\n","\n","    BEISPIEL 1:\n","    \"Ich freue mich sehr, Maria Weber f√ºr den Master of Science in Data Science zu empfehlen. Als ihre Dozentin im Kurs ML 401: Grundlagen des maschinellen Lernens an der TU M√ºnchen konnte ich ihre au√üergew√∂hnlichen akademischen F√§higkeiten beobachten. Frau Weber schloss den anspruchsvollen Kurs mit der Note 1,0 ab und demonstrierte dabei ein tiefgreifendes Verst√§ndnis f√ºr maschinelles Lernen und dessen praktische Anwendungen.\n","\n","    Besonders beeindruckend war ihre F√§higkeit, komplexe Konzepte nicht nur zu verstehen, sondern auch auf reale Problemstellungen anzuwenden. In ihrer Position als Data Analytics Consultant bei DataTech GmbH setzt sie diese F√§higkeiten erfolgreich in der Entwicklung KI-gest√ºtzter Analysetools ein.\n","\n","    Mit ihrem Bachelorabschluss in Wirtschaftsmathematik (Note 1,1) als Jahrgangsbeste hat Frau Weber bereits bewiesen, dass sie h√∂chsten akademischen Anforderungen gerecht wird. Ihre analytischen F√§higkeiten, gepaart mit ihrer Lernbereitschaft und ihrem Engagement, machen sie zu einer idealen Kandidatin f√ºr ein weiterf√ºhrendes Studium im Bereich Data Science.\n","\n","    Aufgrund ihrer bisherigen Leistungen und ihres Potenzials empfehle ich Frau Weber nachdr√ºcklich f√ºr den Master of Science in Data Science. Sie wird zweifellos einen wertvollen Beitrag zum Programm leisten.\"\n","\n","    BEISPIEL 2:\n","    \"Mit gro√üer √úberzeugung empfehle ich Thomas Bauer f√ºr den Master of Science in Robotik. Als sein Betreuer im Kurs ROB 301: Einf√ºhrung in die Robotik am Karlsruher Institut f√ºr Technologie habe ich sein au√üerordentliches technisches Talent kennengelernt. Herr Bauer erreichte im Kurs die Note 1,3 und zeigte dabei besondere St√§rken in der praktischen Roboterprogrammierung.\n","\n","    Seine innovative Herangehensweise an komplexe Problemstellungen war bemerkenswert. Als Werkstudent bei RoboTech entwickelte er bereits erfolgreich autonome Navigationssysteme f√ºr Industrieroboter und bewies damit seine F√§higkeit, theoretisches Wissen praktisch umzusetzen.\n","\n","    Sein Bachelorabschluss in Mechatronik (Note 1,4) unterstreicht seine solide technische Grundausbildung. Herr Bauer zeichnet sich durch systematisches Denken, Kreativit√§t bei der L√∂sungsfindung und ausgepr√§gte Teamf√§higkeit aus - Eigenschaften, die f√ºr das angestrebte Masterstudium essentiell sind.\n","\n","    Ich empfehle Herrn Bauer mit Nachdruck f√ºr den Master of Science in Robotik. Seine Motivation und sein technisches Verst√§ndnis werden ihn zu einem wertvollen Mitglied des Programms machen.\"\n","\n","    BEISPIEL 3:\n","    \"Es ist mir eine Freude, Lisa Schmidt f√ºr den Master of Science in K√ºnstlicher Intelligenz zu empfehlen. W√§hrend ihres Besuchs meines Kurses AI 501: Deep Learning an der RWTH Aachen bewies sie au√üergew√∂hnliche F√§higkeiten im Bereich der k√ºnstlichen Intelligenz. Frau Schmidt erzielte die Note 1,2 und entwickelte dabei innovative L√∂sungsans√§tze f√ºr komplexe Deep-Learning-Aufgaben.\n","\n","    In ihrer Rolle als KI-Entwicklerin bei AI Solutions hat sie ihr Talent bereits unter Beweis gestellt. Ihre selbstst√§ndig entwickelten Neural-Network-Architekturen f√ºr Bilderkennungssysteme zeigen ihre F√§higkeit, theoretische Konzepte in praktische Anwendungen zu √ºberf√ºhren.\n","\n","    Mit einem Bachelorabschluss in Informatik (Note 1,3) verf√ºgt Frau Schmidt √ºber eine exzellente Grundlage f√ºr weiterf√ºhrende Studien. Ihre Kombination aus technischem Verst√§ndnis, Kreativit√§t und analytischem Denken macht sie zu einer vielversprechenden Kandidatin f√ºr ein anspruchsvolles Masterprogramm.\n","\n","    Ich empfehle Frau Schmidt uneingeschr√§nkt f√ºr den Master of Science in K√ºnstlicher Intelligenz. Ihre bisherigen Leistungen und ihr Enthusiasmus f√ºr das Fachgebiet lassen keinen Zweifel an ihrem zuk√ºnftigen Erfolg.\"\n","\n","    ANWEISUNGEN:\n","    1. Erstelle ein neues Empfehlungsschreiben mit √§hnlicher Struktur\n","    2. √Ñndere:\n","        - Namen\n","        - Studienfach\n","        - Universit√§ten\n","        - Kurse\n","        - Noten\n","        - aktuelle Berufst√§tigkeit\n","    3. Behalte bei:\n","        - Vier-Absatz-Struktur\n","        - Professionellen Ton\n","        - Spezifische Beispiele f√ºr Leistungen\n","        - Klare Empfehlung am Ende\n","\n","    GEW√úNSCHTES FORMAT:\n","    - Nur Haupttext\n","    - Keine Anrede/Gru√üformel\n","    - In Markdown formatiert\n","\n","    MUSTER F√úR ABS√ÑTZE:\n","    1. Einleitung mit Empfehlung und Kontext\n","    2. Spezifische Leistungen und praktische Erfahrung\n","    3. Akademischer Hintergrund und pers√∂nliche Eigenschaften\n","    4. Abschlie√üende Empfehlung und Zukunftsprognose\n","    \"\"\"\n","\n","interact_with_ai(question, history)"],"metadata":{"id":"fXXPYh02Nerm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oFDE3DRgHvq1"},"source":["<p><font color='black' size=\"5\">\n","Synthetische Daten generieren\n","</font></p>\n","\n","LLMs (Large Language Models) eignen sich zur Erzeugung synthetischer Daten, was besonders n√ºtzlich f√ºr Testszenarien ist, die realistische Informationen oder eine breite demografische Vielfalt erfordern. Diese Modelle k√∂nnen beispielsweise detaillierte Biografien f√ºr verschiedene Berufsgruppen generieren, wodurch realit√§tsnahe Daten f√ºr Simulationen, die Entwicklung von Testalgorithmen oder das Training anderer KI-Systeme bereitgestellt werden.  \n","\n","Ein m√∂glicher Anwendungsfall w√§re die Erstellung synthetischer Biografien f√ºr Berufsgruppen wie Softwareentwickler, Kinderkrankenschwestern, Finanzanalysten, Naturwissenschaftslehrer an Universit√§ten oder Marketingmanager. Jede dieser Biografien k√∂nnte individuelle Karrierewege, Qualifikationen und berufliche Erfahrungen umfassen, um vielseitige und praxisnahe Tests zu erm√∂glichen. Auf diese Weise tragen synthetische Daten zur Verbesserung der Systemleistung bei, ohne auf echte personenbezogene Informationen zur√ºckzugreifen."]},{"cell_type":"code","source":["# Abschnitt 1: Importe\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain.schema import SystemMessage, HumanMessage, AIMessage\n","\n","# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATURE = 0.2\n","DEFAULT_SYSTEM = \"\"\"\n","Du bist ein Assistent, der synthetische Daten f√ºr eine Person in dem von Ihnen angegebenen Berufsfeld generiert. Erstelle eine kurze Biografie der Person, die nicht l√§nger als 5 S√§tze ist. Erw√§hne die Berufsbezeichnung nicht ausdr√ºcklich.\n","\"\"\"\n","\n","# Abschnitt 3: Chat-Komponenten initialisieren\n","llm = ChatOpenAI(temperature=TEMPERATURE, model=MODEL)\n","chat_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    MessagesPlaceholder(\"messages\"),\n","    (\"human\", \"{human_input}\")\n","])\n","\n","# Abschnitt 4: Funktionen definieren\n","def format_history(history: list[dict]) -> list[dict]:\n","    \"\"\"Formatiert den Konversationsverlauf f√ºr die Anzeige.\"\"\"\n","    formatted_history = []\n","    for message in history:\n","        if isinstance(message, HumanMessage):\n","            formatted_history.append({\"role\": \"user\", \"content\": message.content})\n","        elif isinstance(message, AIMessage):\n","            formatted_history.append({\"role\": \"assistant\", \"content\": message.content})\n","    return formatted_history\n","\n","def process_conversation(question: str, history: list, llm: ChatOpenAI, chat_prompt: ChatPromptTemplate) -> str:\n","    \"\"\"Verarbeitet eine einzelne Konversation.\"\"\"\n","    messages = chat_prompt.format_messages(\n","        system_prompt=DEFAULT_SYSTEM,\n","        messages=history,\n","        human_input=question\n","    )\n","    response = llm.invoke(messages)\n","    history.append(HumanMessage(content=question))\n","    history.append(response)\n","    return response.content\n","\n","def interact_with_ai(question: str, history: list):\n","    \"\"\"F√ºhrt eine einzelne Interaktion mit der KI durch.\"\"\"\n","    response = process_conversation(question, history, llm, chat_prompt)\n","    print(response)\n","\n","# Abschnitt 5: Hauptprogramm\n","history = [SystemMessage(content=DEFAULT_SYSTEM)]\n","question = [\"Softwareentwickler\"]\n","\n","interact_with_ai(question, history)"],"metadata":{"id":"mgPNU5OGN_cS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qjLdt7kMOP-Q"},"source":["Nun wird eine CSV-Datei mit mehreren zuf√§llig generierten Biografien erstellt."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U25feB-bIHSY"},"outputs":[],"source":["CAREER = [\n","\"Luftballon-Verk√§ufer\",\n","\"Kinderkrankenschwester\",\n","\"Talkmaster\",\n","\"Hochschullehrer Wirtschaftswissenschaften\",\n","\"Zirkus-Zauberer\"\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kh4UDM16Jxq7"},"outputs":[],"source":["from tqdm import tqdm\n","\n","# Verwenden Sie tqdm, um den Fortschrittsbalken anzuzeigen\n","history = [SystemMessage(content=DEFAULT_SYSTEM)]\n","for i in tqdm(range(5), desc=\"Generating Careers\"):\n","    history = [SystemMessage(content=DEFAULT_SYSTEM)]\n","    career_choice = CAREER[i]\n","    result = interact_with_ai(career_choice, history)\n","    print(result)"]},{"cell_type":"markdown","metadata":{"id":"HODQfLqHBnUI"},"source":["# 4 | Textzusammenfassung\n","---"]},{"cell_type":"markdown","source":["Gro√üe Sprachmodelle (LLMs) wie GPT-4 fassen Texte zusammen, indem sie zentrale Inhalte extrahieren und kompakt wiedergeben. Sie erfassen den Kontext sowie semantische Strukturen und erstellen eine pr√§gnante Version, die die Hauptaussagen erh√§lt. Durch ihre F√§higkeit, verschiedene Textarten zu analysieren ‚Äì von Fachartikeln bis hin zu narrativen Texten - liefern sie verst√§ndliche und relevante Zusammenfassungen. Ihre Flexibilit√§t in Bezug auf L√§nge und inhaltliche Schwerpunkte macht sie zu einem effizienten Werkzeug f√ºr die schnelle Informationsverarbeitung."],"metadata":{"id":"l-LRWCEb7X-5"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Einzelnes PDF zusammenfassen\n","</font></p>\n","\n","Zun√§chst wird die Zusammenfassung einer einzelnen PDF-Datei betrachtet. LangChang nutzt spezielle Dokumentlader, um verschiedene Dateiformate wie PDFs zu verarbeiten. F√ºr unterschiedliche Datentypen stehen spezifische Lader zur Verf√ºgung. Im folgenden Code wird eine PDF-Datei geladen und mithilfe einer allgemeinen Systemaufforderung analysiert und zusammengefasst."],"metadata":{"id":"l6nozFN3ZmH6"}},{"cell_type":"code","source":["# Abschnitt 0: Installation und API-Key\n","!uv pip install --system --prerelease allow langchain-community langchain_openai pypdf"],"metadata":{"id":"o_DUnw6NaNmS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OLUWbW7NRp4m"},"source":["Der folgende Code demonstriert die Nutzung der Funktion **‚Äûload_summarize_chain‚Äú**, um mithilfe eines Large Language Models (LLM) und des Kettentyps **‚Äûmap_reduce‚Äú** eine Zusammenfassung zu erstellen. Zun√§chst wird das PDF-Dokument *Attention Is All You Need* √ºber den **‚ÄûPyPDFLoader‚Äú** von der angegebenen URL (**‚Äûhttps://arxiv.org/pdf/1706.03762‚Äú**) geladen. Anschlie√üend wird der Inhalt durch **‚Äûload_and_split‚Äú** in kleinere Abschnitte unterteilt. Diese Segmente werden dann der Verarbeitungskette (**‚Äûchain.run(docs)‚Äú**) √ºbergeben, die den Text verarbeitet und verdichtet. Schlie√ülich erfolgt die Ausgabe des zusammengefassten Inhalts im **Markdown-Format**, um die urspr√ºngliche Formatierung beizubehalten."]},{"cell_type":"code","source":["# Abschnitt 1: Importe\n","from IPython.display import display, Markdown\n","\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain.schema import HumanMessage, AIMessage, SystemMessage\n","from langchain_core.output_parsers.string import StrOutputParser\n","\n","# Importe f√ºr einzelne Zusammenfassung\n","from langchain.chains.summarize import load_summarize_chain\n","from langchain.document_loaders import PyPDFLoader, TextLoader\n","from langchain import PromptTemplate\n","\n","# Importe f√ºr mehrere Zusammenfassungen\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain.schema.document import Document\n","\n","# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"  # Aktuellstes GPT-4 Modell\n","TEMPERATURE = 0.0\n","\n","TEMPLATE = \"\"\"\n","Schreiben Sie eine kurze Zusammenfassung der pr√§sentierten Informationen. Schreiben Sie die Zusammenfassung auf Deutsch.\n","\n","{text}\n","\n","ZUSAMMENFASSUNG:\"\"\"\n","\n","# Abschnitt 3: Chat-Komponenten initialisieren\n","llm = ChatOpenAI(temperature=TEMPERATURE, model=MODEL)\n","\n","# Abschnitt 4: Funktionen definieren\n","\n","def load_document(url: str):\n","    \"\"\"L√§dt ein Dokument von einer URL und splittet es in Chunks\"\"\"\n","    loader = PyPDFLoader(url)\n","    return loader.load_and_split()\n","\n","def summarize_document(docs, chain):\n","    \"\"\"Fasst ein Dokument zusammen\"\"\"\n","    # Dokumente zu einem String zusammenf√ºgen\n","    text = \" \".join([doc.page_content for doc in docs])\n","    return chain.invoke({\"text\": text})\n","\n","# Create prompt template using LCEL\n","prompt = PromptTemplate.from_template(TEMPLATE)\n","\n","# Initialize chain with custom prompt and LCEL\n","chain = prompt | llm | StrOutputParser()\n","\n","# Abschnitt 5: Hauptprogramm\n","url = \"https://arxiv.org/pdf/1706.03762\"\n","docs = load_document(url)\n","summary = summarize_document(docs, chain)\n","display(Markdown(\"## ‚ú® Zusammenfassung:\"))\n","display(Markdown(summary))"],"metadata":{"id":"B5cC13qPSs2a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D3MVbKWNsLL3"},"source":["<p><font color='black' size=\"5\">\n","Mehrere PDFs zusammenfassen\n","</font></p>\n","\n","Diese vier wissenschaftlichen Arbeiten haben wesentliche Fortschritte im Bereich der generativen KI und der Verarbeitung nat√ºrlicher Sprache (NLP) erm√∂glicht.  \n","\n","1. **\"Attention Is All You Need\" (Vaswani et al., 2017)** f√ºhrte die **Transformer-Architektur** ein, die sich als grundlegendes Modell f√ºr moderne NLP-Systeme etabliert hat. Der Verzicht auf rekurrente Strukturen und die Einf√ºhrung des Selbstaufmerksamkeitsmechanismus erm√∂glichten effizientere und leistungsf√§higere Modelle wie GPT und BERT.  \n","\n","2. **\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" (Devlin et al., 2018)** revolutionierte die Kontextverarbeitung in NLP-Modellen durch **bidirektionales Training**, das es erm√∂glichte, sowohl vorhergehende als auch nachfolgende W√∂rter gleichzeitig zu analysieren. Dadurch erzielte BERT erhebliche Verbesserungen bei zahlreichen NLP-Aufgaben.  \n","\n","3. **\"Language Models are Few-Shot Learners\" (Brown et al., 2020)** untersuchte das Potenzial **gro√üskalierter Transformer-Modelle** am Beispiel von GPT-3. Die Studie zeigte, dass die Leistung solcher Modelle durch Skalierung verbessert wird und sie auch ohne aufgabenspezifisches Training effektive Ergebnisse liefern k√∂nnen.  \n","\n","4. **\"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" (Raffel et al., 2019)** pr√§sentierte **T5 (Text-to-Text Transfer Transformer)**, ein Modell, das alle NLP-Aufgaben in ein einheitliches Text-zu-Text-Format √ºberf√ºhrt. Dadurch wurde das Transferlernen zwischen verschiedenen Aufgaben vereinfacht.  \n","\n","Diese Arbeiten bilden zusammen die Grundlage f√ºr viele moderne NLP-Modelle und deren Anwendungen in generativer KI."]},{"cell_type":"code","source":["# Abschnitt 5: Hauptprogramm - LCEL-Version\n","urls = [\n","    \"https://arxiv.org/pdf/1706.03762\",\n","    \"https://arxiv.org/pdf/1810.04805\",\n","    \"https://arxiv.org/pdf/2005.14165\",\n","    \"https://arxiv.org/pdf/1910.10683\"\n","]\n","\n","summaries = []\n","\n","for url in urls:\n","    print(f\"Verarbeite Artikel: {url}\")\n","    docs = load_document(url)                       # Dokument laden\n","    summary_part = summarize_document(docs, chain)  # Zusammenfassung erstellen\n","    display(Markdown(\"## ‚ú® Zusammenfassung:\"))\n","    display(Markdown(summary_part))                 # Zusammenfassung ausgeben\n","    summaries.append(summary_part)                  # Zusammenfassungen sammeln"],"metadata":{"id":"hq4d3S0Xo5Nv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QE79kzVUTZ15"},"source":["Nachdem die einzelnen Artikel zusammengefasst wurden, folgt die Erstellung einer umfassenden √úbersicht durch die Kombination dieser Zusammenfassungen. Hierf√ºr werden die Texte zun√§chst zu einer langen Zeichenfolge zusammengef√ºhrt.  \n","\n","Das Endergebnis wird im **Markdown-Format** ausgegeben, um eine klare Struktur und Lesbarkeit zu gew√§hrleisten. Auf diese Weise entsteht eine pr√§gnante, aber inhaltlich umfassende Synthese der Originalartikel."]},{"cell_type":"code","source":["# Gesamtzusammenfassung erstellen\n","gesamt_text = \" \".join(summaries)\n","gesamt_zusammenfassung = chain.invoke({\"text\": gesamt_text})\n","\n","display(Markdown(\"## ‚ú® Gesamt-Zusammenfassung:\"))\n","display(Markdown(gesamt_zusammenfassung))"],"metadata":{"id":"_cUtG7B_WZpb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TgfPV_aXBuv9"},"source":["# 5 | Textklassifizierung\n","---"]},{"cell_type":"markdown","source":["Gro√üe Sprachmodelle (LLMs) haben die Textklassifizierung grundlegend ver√§ndert, indem sie einen flexibleren und effizienteren Ansatz als herk√∂mmliche maschinelle Lernmethoden bieten. W√§hrend traditionelle Verfahren umfangreiche, manuell annotierte Datens√§tze ben√∂tigten, um Modelle zu trainieren, bieten LLMs eine zeitsparende Alternative.  \n","\n","Durch die **Few-Shot Klassifizierung**, die mit beschriftete Beispiele funktioniert, k√∂nnen LLMs Texte allein auf Basis weniger nat√ºrlichsprachlichen Beispiele kategorisieren. Diese F√§higkeit beruht auf dem umfassenden Wissen, das die Modelle w√§hrend ihres Trainings mit gro√üen Textkorpora erworben haben.  \n","\n","Dieser Ansatz reduziert den Aufwand f√ºr die Einrichtung einer Textklassifizierung erheblich und erm√∂glicht eine schnelle Anpassung an neue Anforderungen. Dadurch eignen sich LLMs f√ºr verschiedene Anwendungsf√§lle, darunter **Stimmungsanalyse, Themenkategorisierung, Spam-Erkennung und Inhaltsmoderation**.  \n","\n","Im folgenden Code werden SMS-Texte analysiert. Anschlie√üend erfolgt deren Klassifizierung mithilfe eines LLMs."],"metadata":{"id":"XB3SnNhx7eWv"}},{"cell_type":"markdown","metadata":{"id":"8cdf6QNZrC02"},"source":["Im folgenden Programm erfolgt eine Klassifizierung der SMS-Texte.\n"]},{"cell_type":"code","source":["# Abschnitt 0: Installation und API-Key\n","!uv pip install --system --prerelease allow langchain-community langchain_openai pypdf"],"metadata":{"id":"O0F_978YOcfS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 1: Importe\n","import pandas as pd\n","import os\n","from IPython.display import display, Markdown\n","\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain.schema import HumanMessage, AIMessage, SystemMessage\n","from langchain_core.output_parsers.string import StrOutputParser\n","\n","# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"  # Aktuellstes GPT-4 Modell\n","TEMPERATURE = 0.0\n","\n","DEFAULT_SYSTEM = \"\"\"\n","Du bist ein Experte f√ºr die Klassifikation von SMS-Nachrichten.\n","\n","Deine Aufgabe:\n","- Klassifiziere SMS-Nachrichten als 'spam' oder 'ham'\n","- Gib nur das Klassifikationsergebnis zur√ºck\n","- Verwende 'sonstiges' wenn keine klare Zuordnung m√∂glich ist\n","- Keine zus√§tzlichen Erkl√§rungen\n","\n","Beachte die Beispiele f√ºr die Klassifikation:\n","- spam: Werbung, Gewinnspiele, kostenpflichtige Dienste\n","- ham: Normale Kommunikation, pers√∂nliche Nachrichten\n","\"\"\"\n","\n","# Abschnitt 3: Chat-Komponenten initialisieren\n","llm = ChatOpenAI(temperature=TEMPERATURE, model=MODEL)\n","\n","chat_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    MessagesPlaceholder(variable_name=\"history\"),\n","    (\"human\", \"{question}\")\n","])\n","\n","# Abschnitt 4: Funktionen definieren\n","def format_history(history: list) -> list:\n","    \"\"\"Formatiert den Konversationsverlauf f√ºr das Prompt-Template.\"\"\"\n","    return [\n","        HumanMessage(content=h.content) if isinstance(h, HumanMessage)\n","        else AIMessage(content=h.content) if isinstance(h, AIMessage)\n","        else h\n","        for h in history\n","    ]\n","\n","def process_conversation(question: str, history: list) -> str:\n","    \"\"\"Verarbeitet eine Konversation und gibt die Antwort zur√ºck.\"\"\"\n","    conversation_chain = chat_prompt | llm | StrOutputParser()\n","    response = conversation_chain.invoke(\n","        {\"system_prompt\": DEFAULT_SYSTEM, \"history\": format_history(history), \"question\": question}\n","    )\n","    return response\n","\n","def create_training_examples() -> str:\n","    \"\"\" Erstellt Trainingsbeispiele aus den Beispiel-SMS \"\"\"\n","    examples = []\n","    for label, messages in train_sms.items():\n","        for msg in messages:\n","            examples.append(f\"{label}\\t{msg}\")\n","    return \"\\n\".join(examples)\n","\n","def classify(sms_text: str, history: list) -> str:\n","    \"\"\" Klassifiziert eine einzelne SMS-Nachricht \"\"\"\n","    # Erstelle Prompt mit Trainingsbeispielen\n","    full_prompt = f\"\"\"\n","    Klassifiziere die folgende SMS basierend auf diesen Beispielen:\n","\n","    {create_training_examples()}\n","\n","    Gebe als Antwort nur \"spam\" oder \"ham\" zur√ºck.\n","    Wenn keine Entscheidung m√∂glich ist dann \"sonstiges\".\n","    Gebe keine Begr√ºndung.\n","\n","    Hier ist die SMS:\n","    {sms_text}\n","    \"\"\"\n","    response = process_conversation(full_prompt, history)\n","    return response.strip()\n","\n","# Abschnitt 5: Hauptprogramm\n","history = [SystemMessage(content=DEFAULT_SYSTEM)]\n","\n","# Beispiel-SMS f√ºr das Training\n","train_sms = {\n","    'spam': [\n","        \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n","        \"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, ¬£1.50 to rcv\",\n","        \"WINNER!! As a valued network customer you have been selected to receivea ¬£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\"\n","    ],\n","    'ham': [\n","        \"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\",\n","        \"Ok lar... Joking wif u oni...\",\n","        \"U dun say so early hor... U c already then say...\"\n","    ]\n","}\n","\n","test_sms = [\n","    \"WINNER!! As a valued customer you have won ¬£1000 prize!\",\n","    \"Hey, what time is the meeting tomorrow?\",\n","    \"Free entry! Win an iPhone now! Click here:\",\n","    \"I'll be home in 10 minutes\"\n","]\n","\n","display(Markdown(\"## ‚ú® Klassifiziere Test-SMS:\"))\n","display(Markdown(\"---\"))\n","\n","# SMS-Klassifizierung\n","for i, sms in enumerate(test_sms):\n","    result = classify(sms, history)\n","    print(f\"\\nTest SMS #{i+1}:\")\n","    print(f\"Text: {sms}\")\n","    print(f\"Klassifizierung: {result}\")"],"metadata":{"id":"U-5JUYXRYvrJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ibm452B-uURM"},"source":["<p><font color='black' size=\"5\">\n","Namen klassifizieren\n","</font></p>\n","\n","Die Analyse von W√∂rtern in Texten stellt f√ºr k√ºnstliche Intelligenz eine interessante Herausforderung dar, besonders bei der Zero-Shot Klassifizierung ohne Trainingsdaten. Hierbei muss das Modell spontan entscheiden, ob ein Wort ein Name oder ein regul√§res Wort ist. Im folgenden Text soll getestet werden, ob eine generative KI in der Lage ist, W√∂rter korrekt in die Kategorien \"Name\" oder \"Nicht-Name\" einzuordnen, ohne zuvor mit √§hnlichen Beispielen trainiert worden zu sein. Die zu analysierenden W√∂rter enthalten sowohl gew√∂hnliche Begriffe als auch verschiedene Arten von Namen wie Vornamen, Nachnamen oder Firmennamen. Die zentrale Frage lautet: Kann das Modell durch sein grundlegendes Sprachverst√§ndnis diese Unterscheidung treffen?"]},{"cell_type":"markdown","metadata":{"id":"taGW_e6xvKJM"},"source":["Nun wird ein Large Language Model (LLM) definiert, das den Text analysiert."]},{"cell_type":"markdown","metadata":{"id":"4r8S8zI0ua8U"},"source":["Letztendlich wird eine einzelne Eingabeaufforderung genutzt, um diese Namen zu identifizieren und zu kategorisieren."]},{"cell_type":"code","source":["# Abschnitt 1: Importe\n","from IPython.display import display, Markdown\n","\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain.schema import HumanMessage, AIMessage, SystemMessage\n","from langchain_core.output_parsers.string import StrOutputParser\n","\n","# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"  # Aktuellstes GPT-4 Modell\n","TEMPERATURE = 0.0\n","\n","DEFAULT_SYSTEM = \"\"\"\n","Du bist ein pr√§ziser Namensextraktor. Deine Aufgabe:\n","- Extrahiere alle Namen von Menschen, Produkten und Unternehmen aus dem Text\n","- Gib nur die gefundenen Namen zur√ºck, einer pro Zeile\n","- Keine zus√§tzlichen Erkl√§rungen oder Formatierungen\n","- Keine Duplikate\n","\"\"\"\n","\n","SAMPLE = \"\"\"\n","Anna und Sarah sind seit dem Studium befreundet. Nach ihrem Abschluss hatten sie ein Vorstellungsgespr√§ch bei Frau Weber f√ºr eine Stelle bei der Technovision GmbH, deren Hauptprodukt Futurtech hei√üt und von der Programmiererin Maria Schmidt entwickelt wurde. Nach dem erfolgreichen Gespr√§ch trafen sie sich mit ihren Freunden Rafael und Aida zum Feiern. An ihrem ersten Arbeitstag lernten sie drei weitere Mitarbeiter kennen: Richard, Lisa und Pia. Sp√§ter kam noch Matthias M√ºller zum Team dazu.\n","\"\"\"\n","\n","# Abschnitt 3: Chat-Komponenten initialisieren\n","llm = ChatOpenAI(temperature=TEMPERATURE, model=MODEL)\n","\n","chat_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    MessagesPlaceholder(variable_name=\"history\"),\n","    (\"human\", \"{question}\")\n","])\n","\n","# Abschnitt 4: Funktionen definieren\n","def format_history(history: list) -> list:\n","    \"\"\"Formatiert den Konversationsverlauf f√ºr das Prompt-Template.\"\"\"\n","    return [\n","        HumanMessage(content=h.content) if isinstance(h, HumanMessage)\n","        else AIMessage(content=h.content) if isinstance(h, AIMessage)\n","        else h\n","        for h in history\n","    ]\n","\n","def process_conversation(question: str, history: list) -> str:\n","    \"\"\"Verarbeitet eine Konversation und gibt die Antwort zur√ºck.\"\"\"\n","    conversation_chain = chat_prompt | llm | StrOutputParser()\n","    response = conversation_chain.invoke(\n","        {\"system_prompt\": DEFAULT_SYSTEM, \"history\": format_history(history), \"question\": question}\n","    )\n","    return response\n","\n","def extract_names(text: str, history: list) -> str:\n","    \"\"\" Extrahiert Namen aus einem Text \"\"\"\n","    response = process_conversation(text, history)\n","    return response.strip()\n","\n","# Abschnitt 5: Hauptprogramm\n","history = [SystemMessage(content=DEFAULT_SYSTEM)]\n","\n","# Namen extrahieren und anzeigen\n","names = extract_names(SAMPLE, history)\n","display(Markdown(\"## üìõ Gefundene Namen:\"))\n","display(Markdown(names))"],"metadata":{"id":"aPwDOP8IZTeS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x1Wb0diiB1yO"},"source":["# 6 | LLM schreibt ein Buch\n","---"]},{"cell_type":"markdown","source":["Dieser Abschnitt zeigt, wie ein Large Language Model (LLM) beim Schreiben eines Buches genutzt werden kann. Da ein Buch eine komplexe, langfristige Aufgabe darstellt, erfolgt der Prozess schrittweise.  \n","\n","Zun√§chst wird ein **Thema** gew√§hlt, auf dessen Basis das LLM einen Titel und eine kurze Zusammenfassung erstellt. Diese Zusammenfassung dient als inhaltlicher Leitfaden. Anschlie√üend wird ein **Inhaltsverzeichnis** generiert, das die Kapitel und Abschnitte strukturiert.  \n","\n","Jedes **Kapitel** wird in separaten Sitzungen mit dem LLM ausgearbeitet, wobei die zuvor erstellte Gliederung zur Orientierung dient. Dieser iterative Ansatz gew√§hrleistet Konsistenz und erm√∂glicht eine detaillierte Ausarbeitung.  \n","\n","Durch eine methodische Vorgehensweise verwandelt sich eine anf√§ngliche Idee in ein gut strukturiertes Buch. F√ºr kreative Ergebnisse wird ein Sprachmodell mit einer Temperatur von 0,7 verwendet."],"metadata":{"id":"VPzcHtnM7k-f"}},{"cell_type":"code","source":["!uv pip install --system --prerelease allow pdfkit\n","!sudo apt-get install -qq -y wkhtmltopdf > /dev/null 2>&1"],"metadata":{"id":"_Vn1VtpB4xB7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 1: Importe\n","from IPython.display import display, Markdown\n","import markdown\n","import pdfkit\n","\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain.schema import HumanMessage, SystemMessage, AIMessage\n","from langchain_core.output_parsers.string import StrOutputParser\n","\n","# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"  # Aktuellstes GPT-4 Modell\n","TEMPERATURE = 0.7\n","\n","DEFAULT_SYSTEM = \"\"\"\n","Du bist ein kreativer Autor und Schriftsteller.\n","\n","Deine Texte sind:\n","- Fesselnd und gut strukturiert\n","- Reich an Details und Beschreibungen\n","- Konsistent im Stil und der Erz√§hlweise\n","- Mit klarer Handlung und Charakterentwicklung\n","- In ansprechendem Deutsch verfasst\n","\n","Bei deinem Schreiben:\n","- Entwickelst du √ºberzeugende Charaktere\n","- Erschaffst atmosph√§rische Szenen\n","- Beh√§ltst den roten Faden bei\n","- Verwendest kreative Sprachbilder\n","- Achtest auf Spannung und Dynamik\n","\"\"\"\n","\n","NUM_CHAPTER = 10\n","SUBJECT = \"Sci-Fi Fantasy Thriller\"\n","\n","# Prompt-Templates\n","TITLE_PROMPT = ChatPromptTemplate.from_template(\n","    \"Gebe mir einen zuf√§lligen Titel f√ºr ein Buch zum Thema {subject}. \"\n","    \"Gibt nur den Titel zur√ºck, keinen zus√§tzlichen Text.\"\n",")\n","\n","SYNOPSIS_PROMPT = ChatPromptTemplate.from_template(\n","    \"Gebe mir eine Inhaltsangabe f√ºr ein Buch zum Thema {subject}. \"\n","    \"Gebe nur die Inhaltsangabe zur√ºck, keinen zus√§tzlichen Text.\"\n",")\n","\n","TOC_PROMPT = ChatPromptTemplate.from_template(\n","    \"Gebe mir ein Inhaltsverzeichnis f√ºr ein Buch mit dem Titel {title} \"\n","    \"f√ºr ein Buch zum Thema {subject}. Die Buchzusammenfassung lautet {synopsis}. \"\n","    \"Gebe das Inhaltsverzeichnis als liste von Kapitel√ºberschriften zur√ºck. \"\n","    \"Erstelle nicht mehr als {num_chapters} Kapitel. \"\n","    \"Trenne Kapitelnummer und Kapitel√ºberschrift mit einem Pipe-Zeichen '|'. \"\n","    \"Gebe nur die Kapitelnamen zur√ºck, keinen zus√§tzlichen Text.\"\n",")\n","\n","BOOK_PROMPT = ChatPromptTemplate.from_template(\n","    \"Schreibe Kapitel {chapter_num} mit dem Titel {chapter_title} f√ºr ein Buch \"\n","    \"mit dem Titel {title} zum Thema {subject}. Die Buchzusammenfassung lautet {synopsis}. \"\n","    \"Das Inhaltsverzeichnis lautet {toc}. \"\n","    \"Gebe nur den Kapiteltext zur√ºck, keine Kapitel√ºberschrift, keinen Kapiteltitel, \"\n","    \"keine Kapitelnummer, keinen zus√§tzlichen Text.\"\n",")\n","\n","# Abschnitt 3: Chat-Komponenten initialisieren\n","llm = ChatOpenAI(temperature=TEMPERATURE, model=MODEL)"],"metadata":{"id":"o0SHZTYMbvZW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k5D_R3w8B1yj"},"source":["Einfache Hilfsfunktionen werden implementiert, um das LLM mit einer benutzerdefinierten Systemaufforderung zu befragen. Diese Aufforderung informiert das Modell dar√ºber, dass es beim Schreiben eines Buches assistiert."]},{"cell_type":"code","source":["# Abschnitt 4: Funktionen definieren\n","def process_conversation(prompt: ChatPromptTemplate, **kwargs) -> str:\n","    \"\"\" Verarbeitet eine Konversation mit dem LLM unter Verwendung eines ChatPromptTemplates. \"\"\"\n","    formatted_prompt = prompt.format_messages(**kwargs)\n","    response = llm.invoke(formatted_prompt)\n","    return response.content.strip(\" '\\\"\")\n","\n","def render_chapter(\n","    chapter_num: int,\n","    chapter_title: str,\n","    title: str,\n","    subject: str,\n","    synopsis: str,\n","    toc: str\n",") -> str:\n","    \"\"\" Rendert ein Kapitel basierend auf den √ºbergebenen Parametern. \"\"\"\n","    txt = process_conversation(\n","        BOOK_PROMPT,\n","        chapter_num=chapter_num,\n","        chapter_title=chapter_title,\n","        title=title,\n","        subject=subject,\n","        synopsis=synopsis,\n","        toc=toc\n","    )\n","    return txt\n","\n","def markdown_string_zu_pdf(markdown_string, ausgabedatei):\n","    \"\"\" Konvertiert einen Markdown-String in eine PDF-Datei. \"\"\"\n","    html = markdown.markdown(markdown_string)\n","    html = f\"\"\"\n","    <!DOCTYPE html>\n","    <html lang=\"de\">\n","    <head>\n","        <meta charset=\"UTF-8\">\n","    </head>\n","    <body>\n","        {html}\n","    </body>\n","    </html>\n","    \"\"\"\n","    try:\n","        pdfkit.from_string(html, ausgabedatei)\n","        print(f\"PDF erfolgreich erstellt: {ausgabedatei}\")\n","    except Exception as e:\n","        print(f\"Fehler bei der PDF-Erstellung: {e}\")"],"metadata":{"id":"GlEpB8XEdWzO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FOWb0XT7RlYg"},"source":["\n","<p><font color='black' size=\"5\">\n","Titel, Zusammenfassung, Inhaltsverzeichnis\n","</font></p>\n","\n","F√ºr dieses Buch kann der Benutzer das Thema √ºber die Variable `SUBJECT` festlegen. Anschlie√üend wird das LLM angewiesen, einen zuf√§lligen Titel basierend auf diesem Thema zu generieren. Dabei wird darauf geachtet, dass die Eingabeaufforderung pr√§zise formuliert ist, um zu vermeiden, dass das LLM zus√§tzliche Einleitungen wie ‚ÄûHier ist ein zuf√§lliger Titel‚Äú hinzuf√ºgt."]},{"cell_type":"code","source":["# Abschnitt 5: Hauptprogramm\n","title = process_conversation(TITLE_PROMPT, subject=SUBJECT)\n","print(f\"Buchtitel: {title}\")"],"metadata":{"id":"F-JliP77dRoI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fXyOF_oYtMlR"},"source":["Nachdem der Titel festgelegt wurde, kann nun eine zuf√§llige Zusammenfassung des Buches generiert werden. Diese gibt eine erste inhaltliche Orientierung und dient als Grundlage f√ºr die weitere Strukturierung."]},{"cell_type":"code","source":["synopsis = process_conversation(SYNOPSIS_PROMPT, subject=SUBJECT)\n","display(Markdown(\"## ‚ú® Zusammenfassung:\"))\n","display(Markdown(synopsis))"],"metadata":{"id":"RnY-qJcNdjCk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U6ju-lJEttjw"},"source":["Nun wird das Inhaltsverzeichnis erstellt, wobei alle zuvor generierten Informationen ber√ºcksichtigt werden. Ein spezifisches Format wird vorgegeben, um eine klare Struktur zu gew√§hrleisten. Obwohl die Kapitelnummern leicht ableitbar w√§ren, werden sie explizit angefordert, da das LLM sie ohnehin bereitstellen m√∂chte. Der Ansatz erleichtert eine konsistente Struktur und erm√∂glicht es, die Nummerierung sp√§ter gezielt zu entfernen, falls erforderlich."]},{"cell_type":"code","source":["toc = process_conversation(\n","    TOC_PROMPT,\n","    title=title,\n","    subject=SUBJECT,\n","    synopsis=synopsis,\n","    num_chapters=NUM_CHAPTER\n",")\n","toc = [chapter.strip().split(\"|\") for chapter in toc.split(\"\\n\") if chapter.strip()]\n","display(Markdown(\"## üóíÔ∏è Inhaltsverzeichnis:\"))\n","for chapter_num, chapter_title in toc:\n","    display(Markdown(f\"{chapter_num}. {chapter_title}\"))"],"metadata":{"id":"KxySgphUdsST"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vMXLYGi_RxNp"},"source":["\n","<p><font color='black' size=\"5\">\n","Erstellen der Kapitel des Buches\n","</font></p>\n","\n","\n","Nun wird eine Funktion erstellt, die den Text eines Kapitels generiert. Damit das LLM √ºber ausreichend Kontext verf√ºgt, werden die Zusammenfassung, das Inhaltsverzeichnis und die entsprechende Kapitelnummer √ºbergeben."]},{"cell_type":"code","source":["book_content = f\"# {title}\"\n","for chapter_num, chapter_title in toc:\n","    print(f\"Generiere Kapitel {chapter_num}: {chapter_title}\")\n","    chapter_text = render_chapter(\n","        chapter_num,\n","        chapter_title,\n","        title,\n","        SUBJECT,\n","        synopsis,\n","        toc\n","    )\n","    book_content += f\"\\n## {chapter_title}\\n{chapter_text}\"\n","\n","markdown_string_zu_pdf(book_content, \"mein_buch.pdf\")"],"metadata":{"id":"8feMLfRHd1w4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NTsMT8sYYh6C"},"source":["Das generierte Buch steht nun zum Download bereit."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pqcpXCsABZZo"},"outputs":[],"source":["from google.colab import files\n","files.download(\"mein_buch.pdf\")"]},{"cell_type":"markdown","source":["# A | Aufgabe\n","---"],"metadata":{"id":"ikTJEPGqN9IH"}},{"cell_type":"markdown","source":["Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen."],"metadata":{"id":"eGjopsS35P_r"}},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Sentiment-Analyse von Produktbewertungen mit LLM Zero-Shot-Klassifizierung\n","</font></p>\n","\n","**Aufgabenstellung**\n","Entwickeln Sie ein Python-Programm, das mithilfe eines Large Language Models (LLM) Produktbewertungen in die Kategorien \"Positiv\", \"Neutral\" oder \"Negativ\" einordnet und zus√§tzlich die betroffenen Produktaspekte (Qualit√§t, Preis, Lieferung, Service) identifiziert.\n","\n","**Lernziele**\n","Nach Abschluss dieser √úbung k√∂nnen Sie:\n","- Zero-Shot-Klassifizierung f√ºr mehrere Kategorien implementieren\n","- Aspekt-basierte Sentiment-Analyse durchf√ºhren\n","- Komplexe Prompts f√ºr Mehrfachklassifizierung erstellen\n","- Strukturierte Ausgaben aus LLM-Antworten generieren\n","\n","**Aufgabendetails**\n","\n","```python\n","# Beispieldaten\n","reviews = [\n","    \"Die Qualit√§t des Produkts ist hervorragend, allerdings finde ich den Preis zu hoch.\",\n","    \"Schnelle Lieferung, guter Service, faire Preise - besser geht es nicht!\",\n","    \"Nach zwei Wochen ging das Ger√§t kaputt. Der Kundenservice war bei der Reklamation leider keine Hilfe.\",\n","    \"Durchschnittliche Qualit√§t, erf√ºllt seinen Zweck. Lieferung dauerte etwas l√§nger als angegeben.\"\n","]\n","```"],"metadata":{"id":"lzlpv_R4OHPR"}},{"cell_type":"markdown","source":["**Beispiel-Prompt**\n","```\n","Analysiere die folgende Produktbewertung hinsichtlich:\n","1. Gesamtsentiment (Positiv/Neutral/Negativ)\n","2. Erw√§hnte Produktaspekte (Qualit√§t/Preis/Lieferung/Service)\n","3. Sentiment pro Aspekt\n","\n","Bewertung: {review}\n","\n","Antworte im Format:\n","Gesamtsentiment: [Kategorie]\n","Identifizierte Aspekte:\n","- [Aspekt]: [Sentiment]\n","Begr√ºndung: [Analyse]\n","```"],"metadata":{"id":"5gffAlWTOwXS"}}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyN74c+sdz3Rf9JcHlkksi1i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<p><font size=\"7\" color='grey'> <b>\n","Anwendung Generativer KI\n","</b></font> </br></p>"],"metadata":{"id":"Ih2CTVBnArVZ"}},{"cell_type":"markdown","source":["<p><font size=\"6\" color='grey'> <b>\n","Modul 08: Multimodal und Text-zu-Bild\n","</b></font> </br></p>\n","\n","\n","---"],"metadata":{"id":"6jJZ7wbdArVc"}},{"cell_type":"markdown","source":["# 1 | Übersicht\n","---"],"metadata":{"id":"oYvUY6gMBKO1"}},{"cell_type":"markdown","metadata":{"id":"v2MPPX0c1pHi"},"source":["# Teil 10.1: StreamLit in Google Colab ausführen\n","\n","In diesem Modul erkunden wir Streamlit, ein leistungsstarkes Tool zum Erstellen interaktiver Webanwendungen. Streamlit bietet eine einfache und intuitive Möglichkeit, Benutzeroberflächen für unsere generativen KI-Anwendungen zu erstellen, sodass Benutzer in Echtzeit mit KI-Modellen interagieren können. Durch die Verwendung von Streamlit können wir die Zugänglichkeit und Benutzerfreundlichkeit unserer KI-Lösungen verbessern und es einfacher machen, Ergebnisse zu präsentieren, Benutzereingaben zu akzeptieren und Daten zu visualisieren – alles über eine webbasierte Schnittstelle, ohne dass umfassende Kenntnisse in der Webentwicklung erforderlich sind.\n","\n","* [StreamLit](https://streamlit.io/)\n","* [StreamLit Documentation](https://docs.streamlit.io/)\n","\n","Streamlit ist eine Open-Source-Python-Bibliothek, mit der sich benutzerdefinierte Webanwendungen für Projekte im Bereich maschinelles Lernen und Datenwissenschaft einfach erstellen und teilen lassen. Streamlit wurde mit Blick auf Einfachheit entwickelt und ermöglicht es Entwicklern, Datenskripte mit nur wenigen Codezeilen in interaktive Apps umzuwandeln. Es bietet integrierte Komponenten wie Schieberegler, Schaltflächen, Texteingabe und Diagramme, die mühelos in eine Anwendung integriert werden können. Streamlit aktualisiert die App automatisch, wenn der Code aktualisiert wird, und bietet eine sofortige Vorschau der Änderungen. Dies macht es zu einem idealen Tool zum schnellen Erstellen und Bereitstellen von Schnittstellen für KI-Modelle, Datenvisualisierungen und verschiedene Datenwissenschaftsaufgaben.\n","\n","\n","## Hallo Welt StreamLit\n","\n","Als Nächstes sehen wir uns eine einfache „Hallo Welt!“-Anwendung in Streamlit an, um eine praktische Einführung in deren Funktionalität zu erhalten. Dieses grundlegende Beispiel zeigt, wie Sie mit nur wenigen Zeilen Python-Code eine Streamlit-App erstellen und ausführen. Nachdem wir die App erstellt haben, untersuchen wir, wie Sie sie mit Google Colab bereitstellen und über eine Weboberfläche zugänglich machen.\n","\n","Wir müssen eine Datei „app.py“ schreiben, und das tun wir mit der Zeile „%%writefile app.py“, einem magischen Befehl, der in Umgebungen wie Jupyter Notebook verwendet wird, um den Code zu erstellen und in einer neuen Datei namens „app.py“ zu speichern. Im Skript importiert die Anweisung „import streamlit as st“ die Streamlit-Bibliothek und weist ihr zur einfacheren Verwendung den Alias ​​„st“ zu. Die Zeile „st.write(„Hello World“)“ ist ein einfacher Befehl in Streamlit, der den Text „Hello World“ an die Anwendungsschnittstelle ausgibt. Wenn dieses Skript mit Streamlit ausgeführt wird, startet es eine Web-App, die die Nachricht „Hello World“ anzeigt und als grundlegendes Beispiel dafür dient, wie Streamlit eine Benutzeroberfläche erstellen kann."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q-HB2MnV5Fhb"},"outputs":[],"source":["%%writefile app.py\n","import streamlit as st\n","\n","st.write(\"Hello World\")"]},{"cell_type":"markdown","metadata":{"id":"MnAV_bNbYDdX"},"source":["\n","In diesem Modul verwenden wir npx, um einen sicheren Tunnel zum Streamlit-Server zu erstellen, der in unserer Google Colab-Umgebung läuft. Da Colab in einer Remote-Umgebung ohne direkten Zugriff auf localhost ausgeführt wird, können wir durch Tunneln die Streamlit-App im Web verfügbar machen und mit ihr interagieren. Um diesen Tunnel einzurichten, verwenden wir den folgenden Befehl, um das erforderliche Passwort abzurufen. Dieses Passwort entspricht der IP-Adresse des Colab-Knotens, auf dem unser Streamlit-Server läuft. Mithilfe dieser IP-Adresse können wir unsere Verbindung durch den Tunnel authentifizieren und so einen sicheren Pfad für den Zugriff auf die Streamlit-App von unserem lokalen Browser aus bereitstellen."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pp9t944r3hm8"},"outputs":[],"source":["!curl https://loca.lt/mytunnelpassword"]},{"cell_type":"markdown","metadata":{"id":"In8q5jmZcY_o"},"source":["Als nächstes starten wir StreamLit, damit es im Hintergrund unserer CoLab-Instanz ausgeführt wird."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TDddusH22OKo"},"outputs":[],"source":["!streamlit run app.py &>/content/logs.txt &"]},{"cell_type":"markdown","metadata":{"id":"l_4oECKMcdtp"},"source":["Wir sind jetzt bereit, den Tunnel zu starten. Sie erhalten eine URL für den Zugriff auf Ihre StreamLit-App. Sie müssen das zuvor erhaltene Passwort/die IP-Adresse eingeben. Dieser Befehl wird so lange ausgeführt, bis Sie ihn stoppen."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c0tTbyVC2SMp"},"outputs":[],"source":["!npx localtunnel --port 8501"]},{"cell_type":"markdown","metadata":{"id":"kO2L7d_9Rm67"},"source":["## Einfache StreamLit-Eingabe\n","\n","Nun sehen wir uns ein Beispiel an, das zeigt, wie man einen mathematischen Ausdruck vom Benutzer über eine einfache Schnittstelle akzeptiert und auswertet. Der folgende Code erstellt eine einfache Streamlit-Anwendung, die einen mathematischen Ausdruck als Eingabe verwendet und auswertet und dabei sicherstellt, dass nur gültige Eingaben akzeptiert werden.\n","\n","Das Skript beginnt mit dem Importieren der erforderlichen Bibliotheken streamlit und re. Es definiert eine Funktion evaluate_expression(expression), die einen regulären Ausdruck (re.match) verwendet, um nur Zahlen, Operatoren (+, -, *, /) und Klammern in der Eingabe zuzulassen. Wenn die Eingabe gültig ist, versucht es, den Ausdruck mithilfe der eval()-Funktion von Python auszuwerten. Wenn die Auswertung fehlschlägt, wird eine Fehlermeldung zurückgegeben. Die Streamlit-App selbst besteht aus einem Titel, „Simple Expression Evaluator“, und einem Texteingabefeld, in das der Benutzer einen mathematischen Ausdruck eingeben kann. Sobald der Benutzer den Ausdruck eingibt, wird das Ergebnis berechnet und auf der Seite angezeigt. Wenn die Eingabe ungültige Zeichen enthält, benachrichtigt die App den Benutzer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aq-XOzLWRvBK"},"outputs":[],"source":["%%writefile app.py\n","import streamlit as st\n","import re\n","\n","def evaluate_expression(expression):\n","    # Erlaube nur Zahlen, Operatoren und Klammern\n","    if re.match(r'^[0-9+\\-*/(). ]+$', expression):\n","        try:\n","            # Bewerten Sie den Ausdruck\n","            result = eval(expression)\n","            return result\n","        except Exception as e:\n","            return f\"Error: {str(e)}\"\n","    else:\n","        return \"Invalid input: Only numbers and mathematical operators are allowed.\"\n","\n","# Streamlit-App\n","st.title(\"Simple Expression Evaluator\")\n","\n","# Benutzereingabe\n","expression = st.text_input(\"Enter a mathematical expression:\", \"\")\n","\n","# Ergebnisse auswerten und anzeigen\n","if expression:\n","    result = evaluate_expression(expression)\n","    st.write(f\"Result: {result}\")"]},{"cell_type":"markdown","metadata":{"id":"rLEnh-CFdl-R"},"source":["Wie zuvor holen wir uns das Passwort, starten den Server und anschließend den Tunnel."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wmMu1L6eRv_G"},"outputs":[],"source":["!curl https://loca.lt/mytunnelpassword"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9C6r3pS_RySE"},"outputs":[],"source":["!streamlit run app.py &>/content/logs.txt &"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nleI8bVhR2_L"},"outputs":[],"source":["!npx localtunnel --port 8501"]},{"cell_type":"markdown","metadata":{"id":"cDSVQC8y1pHi"},"source":["# Modul 10 Aufgabe\n","\n","Die erste Aufgabe findet ihr hier: [assignment 10](https://github.com/jeffheaton/app_generative_ai/blob/main/assignments/assignment_yourname_t81_559_class10.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"nCxMjTmHH3Et"},"source":["# Teil 10.2: StreamLit-Einführung\n","\n","\n","In diesem Modul erkunden wir Streamlit, ein leistungsstarkes Tool zum Erstellen interaktiver Webanwendungen mit minimalem Code. Wir werden zwei praktische Beispiele durchgehen, um seine Fähigkeiten zu demonstrieren:\n","\n","1. Generator für Tilgungstabellen: Im ersten Beispiel verwenden wir die Benutzeroberflächenelemente von Streamlit, um einen einfachen Generator für Tilgungstabellen zu erstellen. Sie werden sehen, wie einfach es ist, Benutzereingaben zu erfassen, zu verarbeiten und die Ergebnisse in einem interaktiven Format anzuzeigen.\n","\n","2. Multimodale Bildtransformation: Das zweite Beispiel zeigt die Leistungsfähigkeit multimodaler Modelle. Wir erstellen eine Anwendung, mit der Sie Bilder hochladen und diese mithilfe von KI in Cartoon-Versionen umwandeln können. Dadurch erhalten Sie ein Verständnis dafür, wie Streamlit die Bildverarbeitung und Anzeigeergebnisse effektiv handhaben kann.\n","\n","Am Ende dieses Moduls verfügen Sie über ein solides Verständnis für die Verwendung von Streamlit zum Erstellen sowohl datengesteuerter als auch KI-gestützter Anwendungen.\n","\n","## Beispiel für eine Kreditberechnung\n","\n","\n","Dieser Code erstellt einen Tilgungsrechner mithilfe von Streamlit, einer Bibliothek zum Erstellen interaktiver Webanwendungen in Python. Die Anwendung besteht aus einer Funktion zum Berechnen des Tilgungsplans und einer einfachen Benutzeroberfläche zum Eingeben von Kreditdetails und Anzeigen der Ergebnisse.\n","\n","Die Funktion ```calculate_amortization``` verwendet drei Parameter: ```loan_amount```, ```annual_rate``` und ```years```. Sie berechnet zunächst den monatlichen Zinssatz und die Gesamtzahl der Zahlungen während der Laufzeit des Kredits. Anhand dieser berechnet sie die monatliche Zahlung anhand der Standardformel zur Tilgung von Krediten. Anschließend erstellt sie iterativ einen Tilgungsplan, berechnet die Zins- und Tilgungsanteile jeder Zahlung und aktualisiert den Restbetrag. Die Ergebnisse werden in einer Liste gespeichert, die dann in einen Pandas ```DataFrame`` mit Spalten für jeden Monat, Zahlungsbetrag, gezahlte Tilgung, gezahlte Zinsen und Restbetrag umgewandelt wird.\n","\n","Die Benutzeroberfläche der Streamlit-App beginnt mit dem Titel „Kredittilgungsrechner“. Sie bietet dem Benutzer Eingabefelder, in denen er den Kreditbetrag, den jährlichen Zinssatz und die Kreditlaufzeit in Jahren angeben kann. Diese Eingaben verwenden Streamlits number_input-Methode, mit der Benutzer Werte einfach anpassen können.\n","\n","Wenn Sie auf die Schaltfläche „Amortisierungstabelle berechnen“ klicken, ruft die Anwendung die Funktion calculate_amortization mit den Eingabewerten auf und zeigt den resultierenden DataFrame an. Außerdem wird der monatliche Zahlungsbetrag angezeigt. Darüber hinaus generiert die Anwendung eine herunterladbare CSV-Datei der Amortisierungstabelle, sodass Benutzer die Ergebnisse lokal speichern können. Diese Funktionalität wird durch die Methode st.download_button bereitgestellt, die die CSV-Daten des DataFrame übernimmt und als herunterladbare Datei anbietet."]},{"cell_type":"code","source":["%%writefile app.py\n","\n","import streamlit as st\n","import pandas as pd\n","\n","# Funktion zur Berechnung der Kreditamortisierung\n","def calculate_amortization(loan_amount, annual_rate, years):\n","    monthly_rate = annual_rate / 12 / 100\n","    num_payments = years * 12\n","    monthly_payment = loan_amount * (monthly_rate * (1 + monthly_rate) ** num_payments) / ((1 + monthly_rate) ** num_payments - 1)\n","\n","    # Tilgungsplan erstellen\n","    amortization_data = []\n","    balance = loan_amount\n","    for i in range(1, num_payments + 1):\n","        interest_payment = balance * monthly_rate\n","        principal_payment = monthly_payment - interest_payment\n","        balance -= principal_payment\n","        amortization_data.append([i, monthly_payment, principal_payment, interest_payment, max(balance, 0)])\n","\n","    # DataFrame erstellen\n","    df = pd.DataFrame(amortization_data, columns=['Month', 'Payment', 'Principal', 'Interest', 'Balance'])\n","    return df\n","\n","# Streamlit App\n","st.title('Loan Amortization Calculator')\n","\n","# Eingabefelder\n","loan_amount = st.number_input('Loan Amount', value=300000.0, min_value=0.0, step=1000.0)\n","annual_rate = st.number_input('Annual Interest Rate (%)', value=7.5, min_value=0.0, step=0.1)\n","years = st.number_input('Loan Term (years)', value=30, min_value=1, step=1)\n","\n","# Tilgungsplan berechnen\n","if st.button('Calculate Amortization Table'):\n","    amortization_df = calculate_amortization(loan_amount, annual_rate, years)\n","    st.write(f'Monthly Payment: ${amortization_df[\"Payment\"][0]:,.2f}')\n","    st.dataframe(amortization_df)\n","\n","    # Herunterladbare CSV\n","    csv = amortization_df.to_csv(index=False)\n","    st.download_button(label=\"Download Amortization Table as CSV\", data=csv, file_name='amortization_schedule.csv', mime='text/csv')\n"],"metadata":{"id":"_d1jBuL-wiyh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Als Nächstes erhalten wir das Passwort für unseren StreamLit-Server, den wir gerade starten."],"metadata":{"id":"OWnPCS8v_iYS"}},{"cell_type":"code","source":["!curl https://loca.lt/mytunnelpassword"],"metadata":{"id":"Uisp3omywt5V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Wir starten den StreamLit-Server und erhalten seine URL. Sie benötigen das obige Passwort, wenn Sie auf die angegebene URL zugreifen."],"metadata":{"id":"AZHCdk7j_mUQ"}},{"cell_type":"code","source":["!streamlit run app.py server1 &>/content/logs.txt &\n","!npx --yes localtunnel --port 8501"],"metadata":{"id":"fap8dxKTw1xF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Cartoon-Bild\n","\n","Dieser Code erstellt eine Streamlit-Anwendung, die hochgeladene Bilder mithilfe eines multimodalen Large Language Model (LLM) und Bildgenerierungstools in cartoonartige Versionen umwandelt. Die Anwendung ermöglicht es Benutzern, ein Bild hochzuladen, verarbeitet es mit einem KI-Modell und generiert eine cartoonartige Version, die dann heruntergeladen werden kann.\n","\n","Die Kernfunktionalität ist in der Funktion „modify_image“ gekapselt. Diese Funktion beginnt mit der Initialisierung des GPT-Modells, genauer gesagt „gpt-4o-mini“. Wenn ein Bild hochgeladen wird, wird es in eine base64-codierte Zeichenfolge konvertiert, um die Kommunikation mit dem LLM zu erleichtern. Es wird eine Nachricht erstellt, die eine Textaufforderung und das base64-codierte Bild kombiniert und dann an das GPT-Modell gesendet wird, um eine geänderte Aufforderung zu generieren. Die Antwort des Modells enthält eine Aufforderung, die das Bild im Cartoon-Stil darstellen soll.\n","\n","Nach Erhalt dieser Cartoon-Eingabeaufforderung verwendet die Funktion das DALL-E-Modell („dall-e-3“) von OpenAI, um basierend auf der Eingabeaufforderung ein neues Bild zu generieren. Der Bildgenerierungsprozess gibt eine URL zurück, die dann abgerufen und zur weiteren Verarbeitung in ein PIL-Bildobjekt konvertiert wird.\n","\n","Die Streamlit-App-Oberfläche beginnt mit einem Titel: „Cartoonify Image with Multimodal LLM“. Sie bietet einen Bild-Uploader mit der Methode st.file_uploader, mit dem Benutzer Bilder im JPEG- oder PNG-Format hochladen können. Sobald ein Bild hochgeladen ist, wird es mit der Methode st.image von Streamlit angezeigt.\n","\n","Anschließend wird die Funktion „modify_image“ aufgerufen, um das hochgeladene Bild zu verarbeiten und in einen Cartoon-Stil umzuwandeln. Während dieser Vorgang abläuft, wird eine Meldung („Cartoon-Version wird erstellt …“) angezeigt, die auf die laufende Aktivität hinweist. Das Cartoon-Bild wird dann in der App angezeigt.\n","\n","Schließlich bietet die App die Möglichkeit, das Cartoon-Bild herunterzuladen. Dies wird erreicht, indem das geänderte Bild in einem Puffer gespeichert und Streamlits st.download_button verwendet wird, damit der Benutzer es im JPEG-Format herunterladen kann. Dies bietet Benutzern eine nahtlose Möglichkeit, das transformierte Bild nicht nur anzuzeigen, sondern auch lokal zu speichern."],"metadata":{"id":"gJAC0eCn3Qaf"}},{"cell_type":"code","source":["%%writefile app.py\n","\n","import streamlit as st\n","from langchain_core.messages import HumanMessage\n","from langchain_openai import ChatOpenAI\n","import base64\n","import httpx\n","from openai import OpenAI\n","from PIL import Image\n","from io import BytesIO\n","import matplotlib.pyplot as plt\n","import requests\n","\n","def modify_image(image, prompt):\n","    # Initialisieren des GPT-Modells\n","    model = ChatOpenAI(model=\"gpt-4o-mini\")\n","\n","    # Konvertieren Sie das hochgeladene Bild in Base64\n","    buffered = BytesIO()\n","    image.save(buffered, format=\"JPEG\")\n","    image_data = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n","\n","    # Erstellen Sie eine Nachricht mit Text und Bild\n","    message = HumanMessage(\n","        content=[\n","            {\"type\": \"text\", \"text\": prompt},\n","            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}},\n","        ],\n","    )\n","\n","    # Erhalten Sie eine Antwort mit einer geänderten Eingabeaufforderung von GPT\n","    response = model.invoke([message])\n","    cartoon_prompt = response.content\n","\n","    # Initialisieren Sie das DALL-E-Modell, um das Bild zu generieren\n","    client = OpenAI()\n","\n","    # Generieren Sie das Bild basierend auf der von GPT generierten Cartoon-Eingabeaufforderung\n","    response = client.images.generate(\n","        model=\"dall-e-3\",\n","        prompt=cartoon_prompt,\n","        size=\"1024x1024\",\n","        quality=\"standard\",\n","        n=1,\n","    )\n","\n","    # Holen Sie sich die Bild-URL von DALL-E\n","    image_url = response.data[0].url\n","\n","    # Holen Sie sich das generierte Bild\n","    response2 = requests.get(image_url)\n","    img = Image.open(BytesIO(response2.content))\n","\n","    return img\n","\n","# Streamlit-App\n","st.title(\"Cartoonify Image with Multimodal LLM\")\n","\n","# Bild-Upload\n","uploaded_image = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n","\n","if uploaded_image is not None:\n","    # Öffnen Sie das Bild mit PIL\n","    image = Image.open(uploaded_image)\n","\n","    # Das Originalbild anzeigen\n","    st.image(image, caption='Uploaded Image', use_column_width=True)\n","\n","    # Ändern Sie das Bild so, dass es wie ein Cartoon aussieht\n","    st.write(\"Generating cartoon version...\")\n","    cartoon_img = modify_image(image, \"Output a prompt that would render this image as a cartoon.\")\n","\n","    # Zeigen Sie das Cartoon-Bild an\n","    st.image(cartoon_img, caption='Cartoonified Image', use_column_width=True)\n","\n","    # Bieten Sie eine Option zum Herunterladen des Cartoon-Bildes an\n","    buffered = BytesIO()\n","    cartoon_img.save(buffered, format=\"JPEG\")\n","    st.download_button(\n","        label=\"Download Cartoon Image\",\n","        data=buffered.getvalue(),\n","        file_name=\"cartoon_image.jpg\",\n","        mime=\"image/jpeg\"\n","    )\n"],"metadata":{"id":"QGOuOGgD3Syp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Als Nächstes erhalten wir das Passwort für unseren StreamLit-Server, den wir gerade starten."],"metadata":{"id":"bkmzsav3AFS5"}},{"cell_type":"code","source":["!curl https://loca.lt/mytunnelpassword"],"metadata":{"id":"AU4hiwY-3o2m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Wir starten den StreamLit-Server und erhalten seine URL. Sie benötigen das obige Passwort, wenn Sie auf die angegebene URL zugreifen."],"metadata":{"id":"o_Aah1tqAJ42"}},{"cell_type":"code","source":["!streamlit run app.py server1 &>/content/logs.txt &\n","!npx --yes localtunnel --port 8501"],"metadata":{"id":"waRVCr4y3rJj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LCQ-dqRTH9Fx"},"source":["# Teil 10.3: Den Streamlit-Status verstehen\n","\n","Streamlit ist ein leistungsstarkes Framework zum Erstellen interaktiver Webanwendungen in Python. Beim Erstellen dynamischerer und komplexerer Apps müssen Sie jedoch möglicherweise Informationen zwischen Benutzerinteraktionen beibehalten, z. B. vorherige Eingaben verfolgen, Berechnungen speichern oder Auswahlen speichern. Hier wird die Statusverwaltung von Streamlit unverzichtbar.\n","\n","In Streamlit bezieht sich „Status“ auf den Mechanismus zum Speichern und Verwalten von Daten während einer Benutzersitzung. Ohne Statusverwaltung löst jede Interaktion (wie das Klicken auf eine Schaltfläche oder das Ändern einer Eingabe) eine erneute Ausführung der gesamten App aus, wobei alle Werte zurückgesetzt werden, die Sie möglicherweise beibehalten möchten. Mit „Status“ können Sie diese Werte beibehalten und erweiterte und reaktionsfähigere Verhaltensweisen in Ihrer App ermöglichen.\n","\n","### Wie Streamlit den Status verarbeitet\n","Streamlit bietet eine einfache Möglichkeit, den Status über st.session_state zu handhaben, ein spezielles Objekt, das Werte während der Sitzung eines Benutzers speichert. Dieses Objekt funktioniert wie ein Wörterbuch, in dem Sie Werte in verschiedenen Teilen Ihrer App speichern, aktualisieren und abrufen können. Durch die Verwendung des Sitzungsstatus können Sie:\n","\n","* Behalten Sie Benutzereingabewerte bei, wenn Sie durch verschiedene Komponenten Ihrer App navigieren.\n","* Speichern Sie Berechnungsergebnisse oder Benutzerauswahlen, um sie in nachfolgenden Interaktionen zu verwenden oder zu ändern.\n","* Erstellen Sie komplexere Apps, die mehrere Schritte, Datenverarbeitung und Interaktivität umfassen.\n","\n","## Die Leistungsfähigkeit von Streamlit State: Ein Beispiel für einen Kreditrechner\n","In diesem Modul untersuchen wir die Zustandsverwaltung anhand eines Kredittilgungsrechners als praktisches Beispiel. Diese App ermöglicht es Benutzern, verschiedene Kreditparameter einzugeben, Berechnungen durchzuführen und die Ergebnisse im Sitzungsstatus zu speichern, um sie einfach vergleichen zu können. Durch die Nutzung der Zustandsverwaltung ermöglicht die App Benutzern, verschiedene Kreditszenarien zu untersuchen, ohne ihre vorherigen Berechnungen zu verlieren, und bietet so ein umfassenderes, interaktiveres Erlebnis.\n","\n","Bevor wir uns mit den Einzelheiten des Kreditrechnerbeispiels befassen, behandeln wir zunächst die Grundlagen der effektiven Verwendung von st.session_state in Streamlit-Anwendungen. Diese Grundlage vermittelt das erforderliche Wissen, um erweiterte Funktionen zu implementieren und Ihre Apps wirklich dynamisch zu gestalten.\n","\n","Beginnen wir damit, zu verstehen, wie st.session_state funktioniert und wie es genutzt werden kann, um Informationen über verschiedene Teile einer Streamlit-App hinweg beizubehalten.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gA3YGzPOZD_e"},"outputs":[],"source":["%%writefile app.py\n","\n","import streamlit as st\n","import pandas as pd\n","\n","# Funktion zur Berechnung der Kreditamortisierung\n","def calculate_amortization(loan_amount, annual_rate, years):\n","    monthly_rate = annual_rate / 12 / 100\n","    num_payments = years * 12\n","    monthly_payment = loan_amount * (monthly_rate * (1 + monthly_rate) ** num_payments) / ((1 + monthly_rate) ** num_payments - 1)\n","\n","    # Tilgungsplan erstellen\n","    amortization_data = []\n","    balance = loan_amount\n","    total_interest = 0\n","    for i in range(1, num_payments + 1):\n","        interest_payment = balance * monthly_rate\n","        principal_payment = monthly_payment - interest_payment\n","        balance -= principal_payment\n","        total_interest += interest_payment\n","        amortization_data.append([i, monthly_payment, principal_payment, interest_payment, max(balance, 0)])\n","\n","    # DataFrame erstellen\n","    df = pd.DataFrame(amortization_data, columns=['Month', 'Payment', 'Principal', 'Interest', 'Balance'])\n","    return df, monthly_payment, total_interest\n","\n","# Initialisieren Sie den Sitzungsstatus, um Berechnungen zu speichern\n","if 'calculations' not in st.session_state:\n","    st.session_state['calculations'] = []\n","\n","# Streamlit App\n","st.title('Loan Amortization Calculator')\n","\n","# Eingabefelder\n","loan_amount = st.number_input('Loan Amount', value=300000.0, min_value=0.0, step=1000.0)\n","annual_rate = st.number_input('Annual Interest Rate (%)', value=7.5, min_value=0.0, step=0.1)\n","years = st.number_input('Loan Term (years)', value=30, min_value=1, step=1)\n","\n","# Tilgungsplan berechnen\n","if st.button('Calculate Amortization Table'):\n","    amortization_df, monthly_payment, total_interest = calculate_amortization(loan_amount, annual_rate, years)\n","    st.write(f'Monthly Payment: ${monthly_payment:,.2f}')\n","    st.write(f'Total Interest Paid: ${total_interest:,.2f}')\n","    st.dataframe(amortization_df)\n","\n","    # Aktuelle Berechnung im Sitzungsstatus speichern\n","    st.session_state['calculations'].append({\n","        'Loan Amount': loan_amount,\n","        'Annual Rate (%)': annual_rate,\n","        'Years': years,\n","        'Monthly Payment': monthly_payment,\n","        'Total Interest': total_interest\n","    })\n","\n","# Gespeicherte Berechnungen anzeigen\n","if st.session_state['calculations']:\n","    st.subheader('Saved Loan Calculations')\n","    comparison_df = pd.DataFrame(st.session_state['calculations'])\n","    st.dataframe(comparison_df)\n","\n","    # Herunterladbare CSV aller gespeicherten Berechnungen\n","    csv = comparison_df.to_csv(index=False)\n","    st.download_button(label=\"Download Comparison Table as CSV\", data=csv, file_name='loan_comparisons.csv', mime='text/csv')\n","\n","    # Alle Berechnungen löschen\n","    if st.button('Clear All Calculations'):\n","        st.session_state['calculations'].clear()\n"]},{"cell_type":"markdown","source":["\n","Der bereitgestellte Code zeigt, wie Sie mithilfe des Sitzungsstatus von Streamlit einen interaktiven Tilgungsrechner erstellen, mit dem Benutzer mehrere Kreditszenarien vergleichen können. Die App definiert zunächst eine Funktion zur Berechnung des Tilgungsplans für einen bestimmten Kreditbetrag, Zinssatz und eine bestimmte Kreditlaufzeit. Der Hauptteil der App initialisiert den Sitzungsstatus und erstellt eine Liste zum Speichern von Kreditberechnungen. Diese Liste ist entscheidend, damit Benutzerdaten während der gesamten Interaktion mit der App gespeichert bleiben.\n","\n","Wenn der Benutzer die Kreditparameter eingibt und auf die Schaltfläche „Tilgungstabelle berechnen“ klickt, berechnet die App die monatliche Zahlung und die Gesamtzinsen anhand der angegebenen Werte. Diese Ergebnisse werden dann in der Sitzungsstatusliste als Wörterbuch gespeichert, das den Kreditbetrag, den Zinssatz, die Kreditlaufzeit, die monatliche Zahlung und die Gesamtzinsen enthält. Dadurch kann die App jede neue Berechnung speichern, ohne zuvor eingegebene Daten zu verlieren.\n","\n","Die App prüft, ob im Sitzungsstatus gespeicherte Berechnungen vorhanden sind. Falls vorhanden, wird eine Vergleichstabelle mit allen gespeicherten Kreditszenarien angezeigt, einschließlich Kreditbetrag, Zinssatz, Laufzeit, monatlicher Zahlung und Gesamtzinsen. Darüber hinaus bietet sie eine Schaltfläche „Vergleichstabelle als CSV herunterladen“, mit der Benutzer ihre Vergleichsdaten zur weiteren Analyse herunterladen können. Außerdem ist eine Schaltfläche „Alle Berechnungen löschen“ enthalten, die den Sitzungsstatus löscht und die App auf ihren ursprünglichen Zustand zurücksetzt. Dieser Code unterstreicht die Leistungsfähigkeit von st.session_state beim Erstellen einer interaktiveren und benutzerfreundlicheren App, indem Informationen über mehrere Benutzerinteraktionen hinweg beibehalten werden.\n","\n","Als Nächstes erhalten wir das Passwort für unseren StreamLit-Server, den wir gerade starten.\n"],"metadata":{"id":"GWNmAgVuba_O"}},{"cell_type":"code","source":["!curl https://loca.lt/mytunnelpassword\n"],"metadata":{"id":"nUaNcodZbdoO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Wir starten den StreamLit-Server und erhalten seine URL. Sie benötigen das obige Passwort, wenn Sie auf die angegebene URL zugreifen."],"metadata":{"id":"m9hk-1zAbgGn"}},{"cell_type":"code","source":["!streamlit run app.py server1 &>/content/logs.txt &\n","!npx --yes localtunnel --port 8501"],"metadata":{"id":"BgvEx3Yubk6X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J9eBgkWzID5M"},"source":["# Teil 10.4: Erstellen einer Chat-Anwendung\n","\n","In diesem Modul führen wir Sie durch den Prozess der Erstellung einer StreamLit-basierten LLM-Chat-Anwendung. Um alles zugänglich und unkompliziert zu halten, führen wir unsere App mit Google Colab aus. Darüber hinaus stellen wir Ihnen llm_util.py vor, ein Hilfsskript, das die Arbeit mit LangChain-kompatiblen großen Sprachmodellen (Large Language Models, LLMs) erleichtern soll. In diesem Beispiel verwenden wir OpenAIs LLM, um unsere Chat-Anwendung zu betreiben. Am Ende dieses Moduls verfügen Sie über eine funktionsfähige, interaktive Chat-App und ein solides Verständnis dafür, wie Sie LLMs mit StreamLit in Ihre Projekte integrieren können.\n","\n","Wir werden nun drei Dateien erstellen:\n","\n","* **app.py** – Die wichtigste StreamLit-Chatanwendung.\n","* **llm_util.py** – Das LLM-Dienstprogramm, das es uns ermöglicht, jedes LangChain LLM für unsere Chat-Anwendung zu nutzen.\n","* **llms.yaml** – Eine Konfigurationsdatei, um zu definieren, welche LLMs wir verwenden werden; in diesem Beispiel ist es OpenAI.\n","\n","## Chat-Anwendung\n","\n","Der folgende Code richtet eine einfache Chatbot-Anwendung mithilfe der OpenAI-API und Streamlit ein, einer Python-Bibliothek, mit der sich Web-Apps ganz einfach erstellen lassen. Das Skript mit dem Namen app.py importiert zunächst verschiedene Module, die für seine Funktionalität erforderlich sind. Es verwendet „openai.OpenAI“ zur Interaktion mit dem OpenAI-Sprachmodell, „streamlit“ zur Erstellung der Benutzeroberfläche und „sys“ zur Verarbeitung von Befehlszeilenargumenten. Darüber hinaus importiert es benutzerdefinierte Dienstprogramme aus „llm_util“ sowie bestimmte Komponenten aus der LangChain-Bibliothek, darunter „ConversationSummaryMemory“, „PromptTemplate“ und „ConversationChain“. Diese Tools sind entscheidend für die Verwaltung des Konversationskontexts, die Erstellung von Eingabevorlagen und die Erleichterung von Konversationsinteraktionen.\n","\n","Das Skript prüft zunächst, ob die erforderlichen Befehlszeilenargumente angegeben sind. Es erwartet ein Sprachmodellprofil als Argument. Wenn dies nicht angegeben ist, wird die Ausführung angehalten und der Benutzer aufgefordert, die erforderlichen Eingaben zu machen. Dieser Mechanismus stellt sicher, dass vor dem Starten des Chatbots das entsprechende Sprachmodell ausgewählt wird.\n","\n","Das Herzstück der Chatbot-Einrichtung ist die Funktion create_chatbot. Diese Funktion richtet den Chatbot mithilfe des angegebenen Sprachmodells ein und enthält eine Speicherkomponente, „ConversationSummaryMemory“, die den Kontext der laufenden Konversation verfolgt. Durch die Nutzung dieses Speichers kann der Chatbot im Laufe der Zeit kohärentere und kontextbezogenere Antworten generieren. Darüber hinaus definiert die Funktion mithilfe von LangChains PromptTemplate eine einfache Vorlage, die formatiert, wie der Konversationsverlauf und die Benutzereingabe kombiniert werden. Anschließend gibt sie ein ConversationChain-Objekt zurück, das diese Konversationen verarbeitet und sicherstellt, dass die Antworten vom gesamten Chatverlauf beeinflusst werden.\n","\n","Um eine interaktive Schnittstelle zu erstellen, verwendet das Skript Streamlit. Es beginnt mit der Festlegung eines Titels, „Chat“, für die Anwendung. Um den Status des Chatbots zu verwalten und den Überblick über die Konversation zu behalten, verwendet das Skript den Sitzungsstatusmechanismus von Streamlit. Es prüft, ob bereits eine Chatbot-Instanz (st.session_state.chat) vorhanden ist. Wenn nicht, initialisiert es eine neue mit dem über die Befehlszeile bereitgestellten LLM-Profil. Ebenso richtet es eine Liste zum Speichern der Konversationsnachrichten (st.session_state.messages) ein, falls diese noch nicht vorhanden ist.\n","\n","Der Gesprächsverlauf wird dann durchlaufen und mithilfe der chatspezifischen Funktionen von Streamlit angezeigt. Jede Nachricht, egal ob vom Benutzer oder vom Assistenten, wird in der Chat-Oberfläche wiedergegeben. Dadurch wird ein fortlaufendes Protokoll des Gesprächs erstellt, das dem Benutzer eine fortlaufende Ansicht seiner Interaktion mit dem Chatbot bietet.\n","\n","Die Benutzerinteraktion erfolgt über ein Chat-Eingabefeld, das von st.chat_input bereitgestellt wird. Wenn der Benutzer eine Eingabe macht, hängt das Skript die Nachricht an das Konversationsprotokoll an und zeigt sie in der Benutzeroberfläche an. Der Chatbot verarbeitet diese Eingabe dann mithilfe der Vorhersagemethode von ConversationChain und generiert eine Antwort basierend auf dem Kontext der Konversation. Diese Antwort wird im Chatfenster angezeigt und dem Sitzungsstatus hinzugefügt, wodurch sichergestellt wird, dass der gesamte Dialog gespeichert und für nachfolgende Interaktionen verwendet wird.\n","\n","Im Wesentlichen nutzt dieses Skript die Speicherverwaltungs- und Vorlagenfunktionen von LangChain, um einen dynamischen Chatbot zu erstellen, während Streamlit eine benutzerfreundliche Schnittstelle für Echtzeitinteraktionen bietet. Die Kombination dieser Tools ermöglicht es dem Chatbot, den Kontext während der gesamten Konversation beizubehalten, was zu aussagekräftigeren und kohärenteren Austauschen führt. Da außerdem ein Sprachmodellprofil als Befehlszeilenargument erforderlich ist, ist das Skript flexibel und an verschiedene LLM-Konfigurationen anpassbar, sodass der Benutzer das Verhalten des Chatbots je nach seinen spezifischen Anforderungen anpassen kann."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nEQ9c5Akqj_R","outputId":"8c5eeabd-f406-4c8f-f330-52ed4e285985"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing app.py\n"]}],"source":["%%writefile app.py\n","from openai import OpenAI\n","import streamlit as st\n","from llm_util import *\n","from langchain.memory import ConversationSummaryMemory\n","from langchain.prompts.prompt import PromptTemplate\n","from langchain.chains import ConversationChain\n","import sys\n","\n","# Dies ruft alle Kommandozeilenargumente als Liste ab\n","arguments = sys.argv\n","if len(sys.argv) != 2:\n","    print(\"Please specify the llm to use as the first argument\")\n","    st.stop()\n","else:\n","    profile = sys.argv[1]\n","\n","\n","def create_chatbot(llm):\n","    memory = ConversationSummaryMemory(llm=llm)\n","\n","    template = \"\"\"{history}\\n{input}\\n\\n\n","    \"\"\"\n","    PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n","    return ConversationChain(llm=llm, prompt=PROMPT, memory=memory, verbose=False)\n","\n","\n","st.title(\"Chat\")\n","\n","if \"chat\" not in st.session_state:\n","    client = open_llm(profile)\n","    st.session_state.chat = create_chatbot(client)\n","\n","if \"messages\" not in st.session_state:\n","    st.session_state.messages = []\n","\n","for message in st.session_state.messages:\n","    with st.chat_message(message[\"role\"]):\n","        st.markdown(\n","            message[\"content\"],\n","        )\n","\n","if prompt := st.chat_input(\"What is up?\"):\n","    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n","    with st.chat_message(\"user\"):\n","        st.markdown(\n","            prompt,\n","        )\n","\n","    with st.chat_message(\"assistant\"):\n","        response = st.session_state.chat.predict(input=prompt)\n","        st.markdown(\n","            response,\n","        )\n","    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"]},{"cell_type":"markdown","metadata":{"id":"HSKyHUHqhCcx"},"source":["## LLM-Dienstprogramm\n","\n","\n","Der folgende Code ermöglicht der Chat-Anwendung, verschiedene von LangChain unterstützte Sprachmodelle basierend auf einer Konfigurationsdatei zu verwenden. Das Skript llm_util.py dient als Dienstprogramm, das verschiedene Sprachmodelle dynamisch lädt und initialisiert, indem es Konfigurationen verwendet, die in einer YAML-Datei (llms.yaml) angegeben sind. Dieser Ansatz bietet eine flexible Möglichkeit, das Sprachmodell zu ändern, ohne den Hauptanwendungscode zu ändern, und ermöglicht so einfaches Experimentieren und Anpassen."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bXml9nrGqj_R","outputId":"5400a880-69fd-4b87-b11a-32e8132e06ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing llm_util.py\n"]}],"source":["%%writefile llm_util.py\n","import yaml\n","\n","# Laden Sie die YAML-Datei\n","def load_yaml(file_path):\n","    with open(file_path, \"r\") as file:\n","        return yaml.safe_load(file)\n","\n","\n","# Funktion zum dynamischen Importieren einer Klasse basierend auf einem Stringpfad (z. B. „langchain_community.chat_models.ChatOllama“)\n","def get_class(class_path):\n","    module_path, class_name = class_path.rsplit(\".\", 1)\n","    module = __import__(module_path, fromlist=[class_name])\n","    return getattr(module, class_name)\n","\n","\n","# Open Language Model Server-Funktion\n","def open_llm(server_name):\n","    config = load_yaml(\"llms.yaml\")\n","    for server in config[\"servers\"]:\n","        if server[\"name\"] == server_name:\n","            class_path = server[\"class\"]\n","            clazz = get_class(class_path)\n","            # Entfernen Sie „Klasse“ und „Name“ aus den Parametern, da sie für die Initialisierung nicht benötigt werden.\n","            params = {k: v for k, v in server.items() if k not in [\"class\", \"name\"]}\n","\n","            return clazz(**params)\n","    raise ValueError(f\"Server '{server_name}' not found\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wk6T0aJEqj_S","outputId":"207c08e7-ee31-4752-e632-087d9cdccc40"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing llms.yaml\n"]}],"source":["%%writefile llms.yaml\n","\n","servers:\n","  - name: server1\n","    class: langchain_openai.ChatOpenAI\n","    model: gpt-4o-mini\n","    temperature: 0\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eMqVgvmxhySA"},"source":["Als Nächstes erhalten wir das Passwort für unseren StreamLit-Server, den wir gerade starten."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0iJmAPR8c-JE","outputId":"bfb03037-7bf6-45ef-bfb1-8315823f15c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["34.138.208.107"]}],"source":["!curl https://loca.lt/mytunnelpassword"]},{"cell_type":"markdown","metadata":{"id":"H3lKrMcvh3_F"},"source":["Wir starten den StreamLit-Server und erhalten seine URL. Sie benötigen das obige Passwort, wenn Sie auf die angegebene URL zugreifen."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YSLPrByztQOe","outputId":"1eebbf3a-c3bf-4205-abf0-87c53f5397aa"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[K\u001b[?25hyour url is: https://lucky-colts-reply.loca.lt\n","^C\n"]}],"source":["!streamlit run app.py server1 &>/content/logs.txt &\n","!npx --yes localtunnel --port 8501"]},{"cell_type":"markdown","metadata":{"id":"OXvoDfPzkgwc"},"source":["Die folgende Konfigurationsdatei zeigt, wie andere LLM-Server genutzt werden:\n","\n","* server1 - Ollama\n","* server2 - OpenAI\n","* Server3 – Bedrock (AWS)\n","* server4 - LMStudio"]},{"cell_type":"markdown","metadata":{"id":"LLPuHtYQqj_S"},"source":["```\n","servers:\n","  - name: server1\n","    class: langchain_community.chat_models.ChatOllama\n","    base_url: http://localhost:11434\n","    model: llama2\n","    temperature: 0\n","  - name: server2\n","    class: langchain_openai.ChatOpenAI\n","    model: gpt-4o-mini\n","    temperature: 0\n","  - name: server3\n","    class: langchain_aws.ChatBedrock\n","    model_id: amazon.titan-text-express-v1\n","    model_kwargs:\n","      temperature: 0.1\n","  - name: server4\n","    class: langchain_openai.ChatOpenAI\n","    base_url: http://localhost:1234/v1/\n","    model: TheBloke/Mistral-7B-Instruct-v0.1-GGUF\n","    openai_api_key: None\n","    temperature: 0\n","  ```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rUH52Q_wknk0"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"syjSu_ZJIJyH"},"source":["# Teil 10.5: Multimodale Chat-Anwendung\n","\n","In diesem Modul führen wir Sie durch den Prozess der Erstellung einer multimodalen StreamLit-basierten LLM-Chat-Anwendung. Um alles zugänglich und unkompliziert zu halten, führen wir unsere App mit Google Colab aus. Darüber hinaus stellen wir Ihnen llm_util.py vor, ein Hilfsskript, das die Arbeit mit LangChain-kompatiblen großen Sprachmodellen (Large Language Models, LLMs) erleichtern soll. In diesem Beispiel verwenden wir OpenAIs LLM, um unsere Chat-Anwendung zu betreiben. Am Ende dieses Moduls verfügen Sie über eine funktionsfähige, interaktive Chat-App und ein solides Verständnis dafür, wie Sie LLMs mit StreamLit in Ihre Projekte integrieren können.\n","\n","Wir werden nun drei Dateien erstellen:\n","\n","* **app.py** – Die wichtigste StreamLit-Chatanwendung.\n","* **llm_util.py** – Das LLM-Dienstprogramm, das es uns ermöglicht, jedes LangChain LLM für unsere Chat-Anwendung zu nutzen.\n","* **llms.yaml** – Eine Konfigurationsdatei, um zu definieren, welche LLMs wir verwenden werden; in diesem Beispiel ist es OpenAI.\n","\n","Diese Chat-Anwendung ermöglicht das Anhängen von Bildern an die Konversation.\n","\n","## Chat-Anwendung\n","\n","Um Ihren Chatbot mit multimodalen Funktionen zu erweitern, müssen Sie Ihre Streamlit-Anwendung so ändern, dass sie sowohl Text- als auch Bildeingaben von Benutzern akzeptiert. Ersetzen Sie zunächst die einfache Texteingabe durch ein Formular, das ein Textfeld und einen Bild-Uploader enthält. So können Benutzer Nachrichten eingeben und optional Bilder anhängen.\n","\n","Wenn ein Benutzer das Formular absendet, verarbeiten Sie die Eingaben, indem Sie eine Nachrichteninhaltsstruktur erstellen, die sowohl den Text als auch die Bilddaten enthält. Lesen Sie für das Bild die hochgeladene Datei, kodieren Sie sie in Base64 und formatieren Sie sie in eine Daten-URL. Dadurch werden die Bilddaten für die Übermittlung an das Sprachmodell in einem kompatiblen Format vorbereitet.\n","\n","Verwenden Sie statt einer einfachen Konversationskette eine Sprachmodellschnittstelle wie ChatOpenAI, die Nachrichten mit Text und Bildern verarbeiten kann. Rufen Sie das Sprachmodell mit der richtig formatierten Nachricht auf, die den Text des Benutzers und die codierten Bilddaten enthält.\n","\n","Verwalten Sie den Konversationsverlauf mithilfe des Sitzungsstatus, um den Kontext über Interaktionen hinweg beizubehalten. Aktualisieren Sie die Chat-Oberfläche, um sowohl die Eingaben des Benutzers (Text und Bilder) als auch die Antworten des Assistenten anzuzeigen. Stellen Sie sicher, dass die Antworten des Assistenten in der Oberfläche entsprechend wiedergegeben werden.\n","\n","Durch die Implementierung dieser Änderungen unterstützt Ihr Chatbot multimodale Interaktionen und ermöglicht Benutzern die Teilnahme an ausführlicheren Gesprächen, die Text- und Bildinformationen kombinieren. Dieses Setup nutzt Streamlit für die Benutzeroberfläche und LangChain zusammen mit den Sprachmodellen von OpenAI für die Verarbeitung, sodass der Chatbot von Benutzern angehängte Bilder interpretieren und darauf reagieren kann."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y5s2drJ6IJyI"},"outputs":[],"source":["%%writefile app.py\n","import streamlit as st\n","from openai import OpenAI\n","from llm_util import *\n","import base64\n","from langchain_core.messages import HumanMessage\n","from langchain_openai import ChatOpenAI\n","import sys\n","\n","# Dies ruft alle Kommandozeilenargumente als Liste ab\n","arguments = sys.argv\n","if len(sys.argv) != 2:\n","    print(\"Please specify the llm to use as the first argument\")\n","    st.stop()\n","else:\n","    profile = sys.argv[1]\n","\n","st.title(\"Chat with Image Support\")\n","\n","if \"chat\" not in st.session_state:\n","    client = open_llm(profile)\n","    st.session_state.chat = client  # Angenommen, dies gibt eine ChatOpenAI-Instanz zurück\n","\n","if \"messages\" not in st.session_state:\n","    st.session_state.messages = []\n","\n","# Vorhandene Nachrichten anzeigen\n","for message in st.session_state.messages:\n","    with st.chat_message(message[\"role\"]):\n","        st.markdown(message[\"content\"])\n","\n","# Erstellen eines Formulars für Benutzereingaben\n","with st.form(\"chat_form\"):\n","    prompt = st.text_input(\"Enter your message:\")\n","    uploaded_file = st.file_uploader(\"Upload an image (optional)\", type=[\"jpg\", \"jpeg\", \"png\"])\n","    submit_button = st.form_submit_button(\"Send\")\n","\n","if submit_button:\n","    # Erstellen des Inhalts der Benutzernachricht\n","    message_content = []\n","    if prompt:\n","        message_content.append({\"type\": \"text\", \"text\": prompt})\n","    if uploaded_file is not None:\n","        # Lesen Sie die Bilddaten und kodieren Sie sie in Base64\n","        image_bytes = uploaded_file.read()\n","        image_type = uploaded_file.type  # z. B. „image/jpeg“\n","        image_data = base64.b64encode(image_bytes).decode(\"utf-8\")\n","        # Binden Sie die Bilddaten in den Nachrichteninhalt ein\n","        message_content.append(\n","            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:{image_type};base64,{image_data}\"}}\n","        )\n","    # Erstellen Sie die HumanMessage\n","    message = HumanMessage(content=message_content)\n","    # Benutzernachricht an Sitzungsstatus anhängen\n","    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n","    with st.chat_message(\"user\"):\n","        if prompt:\n","            st.markdown(prompt)\n","        if uploaded_file is not None:\n","            st.image(uploaded_file)\n","    # Erhalten Sie eine Antwort vom LLM\n","    response = st.session_state.chat.invoke([message])\n","    # Antwort des Assistenten an Nachrichten anhängen\n","    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response.content})\n","    with st.chat_message(\"assistant\"):\n","        st.markdown(response.content)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2G2UnJkhIJyJ"},"source":["## LLM-Dienstprogramm\n","\n","\n","Der folgende Code ermöglicht der Chat-Anwendung, verschiedene von LangChain unterstützte Sprachmodelle basierend auf einer Konfigurationsdatei zu verwenden. Das Skript llm_util.py dient als Dienstprogramm, das verschiedene Sprachmodelle dynamisch lädt und initialisiert, indem es Konfigurationen verwendet, die in einer YAML-Datei (llms.yaml) angegeben sind. Dieser Ansatz bietet eine flexible Möglichkeit, das Sprachmodell zu ändern, ohne den Hauptanwendungscode zu ändern, und ermöglicht so einfaches Experimentieren und Anpassen."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8S8T4f_wIJyJ"},"outputs":[],"source":["%%writefile llm_util.py\n","import yaml\n","\n","# Laden Sie die YAML-Datei\n","def load_yaml(file_path):\n","    with open(file_path, \"r\") as file:\n","        return yaml.safe_load(file)\n","\n","\n","# Funktion zum dynamischen Importieren einer Klasse basierend auf einem Stringpfad (z. B. „langchain_community.chat_models.ChatOllama“)\n","def get_class(class_path):\n","    module_path, class_name = class_path.rsplit(\".\", 1)\n","    module = __import__(module_path, fromlist=[class_name])\n","    return getattr(module, class_name)\n","\n","\n","# Open Language Model Server-Funktion\n","def open_llm(server_name):\n","    config = load_yaml(\"llms.yaml\")\n","    for server in config[\"servers\"]:\n","        if server[\"name\"] == server_name:\n","            class_path = server[\"class\"]\n","            clazz = get_class(class_path)\n","            # Entfernen Sie „Klasse“ und „Name“ aus den Parametern, da sie für die Initialisierung nicht benötigt werden.\n","            params = {k: v for k, v in server.items() if k not in [\"class\", \"name\"]}\n","\n","            return clazz(**params)\n","    raise ValueError(f\"Server '{server_name}' not found\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"57Ggm-HKIJyK"},"outputs":[],"source":["%%writefile llms.yaml\n","\n","servers:\n","  - name: server1\n","    class: langchain_openai.ChatOpenAI\n","    model: gpt-4o-mini\n","    temperature: 0\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MQxOMf6IIJyK"},"source":["Als Nächstes erhalten wir das Passwort für unseren StreamLit-Server, den wir gerade starten."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fQJfqE59IJyK"},"outputs":[],"source":["!curl https://loca.lt/mytunnelpassword"]},{"cell_type":"markdown","metadata":{"id":"ClGJkMfRIJyL"},"source":["Wir starten den StreamLit-Server und erhalten seine URL. Sie benötigen das obige Passwort, wenn Sie auf die angegebene URL zugreifen."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UDFwlE51IJyL"},"outputs":[],"source":["!streamlit run app.py server1 &>/content/logs.txt &\n","!npx --yes localtunnel --port 8501"]}]}
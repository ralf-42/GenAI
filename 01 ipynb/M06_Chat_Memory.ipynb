{"cells":[{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)"],"metadata":{"id":"9JKGDENeDeB0"},"id":"9JKGDENeDeB0"},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b> Chat und Memory </b></font> </br></p>\n","\n","---"],"metadata":{"id":"bxo8g0twDZK5"},"id":"bxo8g0twDZK5"},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","collapsed":true,"id":"dfdhPIzcEYRG","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  Colab-Umfeld</font> </br></p>\n","# Installierte Python Version\n","import sys\n","print(f\"Python Version: \",sys.version)\n","# Installierte LangChain Bibliotheken\n","print()\n","print(\"Installierte LangChain Bibliotheken:\")\n","\n","!pip list | grep '^langchain'\n","# Unterdr√ºckt die \"DeprecationWarning\" von LangChain f√ºr die Memory-Funktionden\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")"],"id":"dfdhPIzcEYRG"},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WD3Wwr6sESX8"},"outputs":[],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  SetUp API-Keys (setup_api_keys)</font> </br></p>\n","def setup_api_keys():\n","    \"\"\"Konfiguriert alle ben√∂tigten API-Keys aus Google Colab userdata\"\"\"\n","    from google.colab import userdata\n","    import os\n","    from os import environ\n","\n","    # Dictionary der ben√∂tigten API-Keys\n","    keys = {\n","        'OPENAI_API_KEY': 'OPENAI_API_KEY',\n","        'HF_TOKEN': 'HF_TOKEN',\n","        # Weitere Keys bei Bedarf\n","    }\n","\n","    # Keys in Umgebungsvariablen setzen\n","    for env_var, key_name in keys.items():\n","        environ[env_var] = userdata.get(key_name)\n","\n","    return {k: environ[k] for k in keys.keys()}\n","\n","# Verwendung\n","all_keys = setup_api_keys()\n","# Bei Bedarf einzelne Keys direkt zugreifen\n","# WEATHER_API_KEY = all_keys['WEATHER_API_KEY']"],"id":"WD3Wwr6sESX8"},{"cell_type":"markdown","source":["**NEW Structure**\n","\n","+ 4. ChatMessageHistory-Implementierungen\n","+ 1. Token-Limit (M06 Abschnitt 3)\n","+ 6. Vektorbasierte Speichersysteme\n","+ 5. Mehrschichtige Memory-Architekturen\n","+ 3. Zusammenfassung (M06 Abschnitt 4)\n","+ del: Erweiterte persistente Speicherung (M06 Abschnitt 5 - Komplexversion)\n","+ 2. Nachrichten-Limit (M06 Abschnitt 2)\n","+ del: Vollst√§ndige Konversationsverwaltung (M06 Abschnitt 1)\n","..."],"metadata":{"id":"S6TZE41ImL3M"},"id":"S6TZE41ImL3M"},{"cell_type":"markdown","source":["Manuelle Konversationsverwaltung aus M05 \"history\" √ºbernehmen."],"metadata":{"id":"mYcyugWB93I9"},"id":"mYcyugWB93I9"},{"cell_type":"markdown","source":["# Vereinfachtes Beispiel: Persistenter Chat mit ChatMessageHistory\n","\n","Ich habe das Beispiel deutlich vereinfacht und auf das Wesentliche reduziert, ganz im Stil der kompakteren Beispiele aus M06. Das neue Beispiel konzentriert sich auf die Kernelemente:\n","\n","## Hauptmerkmale des vereinfachten Beispiels\n","\n","- **Minimale Imports**: Nur die notwendigsten Bibliotheken\n","- **Einfachere Datenbank-Einrichtung**: Eine knappe Funktion zum Erstellen der erforderlichen Tabelle\n","- **Reduzierte Funktionalit√§t**: Fokus auf den Chat-Kern ohne erweiterte Session-Management-Funktionen\n","- **Vereinfachte Hauptschleife**: Standardm√§√üig eine einzelne Sitzung mit einfachem Ein-/Ausgabe-Fluss\n","\n","## Struktur des Codes\n","\n","Der Code folgt weiterhin der klaren Strukturierung aus M06:\n","\n","1. **Importe**: Nur die wichtigsten Bibliotheken f√ºr die Funktionalit√§t\n","2. **Konstanten**: Minimale Konfiguration mit Modellname, Datenbankpfad und Systemprompt\n","3. **Datenbank einrichten**: Einfache Funktion zum Erstellen der Datenbanktabelle\n","4. **Chat-Komponenten**: Funktionen zum Abrufen der Chathistorie und Erstellen der Chain\n","5. **Chat-Funktion**: Hauptfunktion f√ºr die Interaktion mit dem Benutzer\n","6. **Hauptfunktion**: Einfacher Einstiegspunkt zum Starten des Chats\n","\n","## Kernkonzepte\n","\n","Das vereinfachte Beispiel demonstriert immer noch die wichtigsten Konzepte:\n","\n","1. **Verwendung von SQLChatMessageHistory**: F√ºr die persistente Speicherung von Nachrichten\n","2. **Integration mit RunnableWithMessageHistory**: F√ºr einfache Verkn√ºpfung von Chains mit persistentem Speicher\n","3. **Persistenz √ºber SQLite**: F√ºr zuverl√§ssige Datenspeicherung zwischen Programmausf√ºhrungen\n","\n","## Wie es funktioniert\n","\n","1. Eine SQLite-Datenbank wird eingerichtet, um Chatnachrichten zu speichern\n","2. Eine eindeutige Session-ID wird generiert, um den Chat zu identifizieren\n","3. Die Chat-Schleife erm√∂glicht Eingaben und zeigt Antworten an\n","4. Alle Nachrichten werden automatisch in der Datenbank gespeichert\n","5. Die Geschichte bleibt erhalten, auch wenn das Programm neu gestartet wird\n","\n","Dieses vereinfachte Beispiel demonstriert dennoch den wichtigen Kernpunkt: Die einfache Integration von ChatMessageHistory-Implementierungen in LangChain f√ºr persistenten Speicher."],"metadata":{"id":"Mqu0e6RAoHUO"},"id":"Mqu0e6RAoHUO"},{"cell_type":"code","source":["#\n","# Beispiel f√ºr 1. ChatMessageHistory-Implementierungen\n","#\n","# Abschnitt 1: Importe\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.runnables.history import RunnableWithMessageHistory\n","from langchain_openai import ChatOpenAI\n","from langchain_community.chat_message_histories import SQLChatMessageHistory\n","from IPython.display import display, Markdown\n","import sqlite3\n","import uuid\n","\n","# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","DB_PATH = \"chat_history.db\"\n","DEFAULT_SYSTEM = \"Du bist ein hilfreicher Assistent.\"\n","\n","# Abschnitt 3: Datenbank einrichten\n","def setup_database():\n","    \"\"\"Initialisiert die SQLite-Datenbank f√ºr Chat-Historien.\"\"\"\n","    conn = sqlite3.connect(DB_PATH)\n","    cursor = conn.cursor()\n","\n","    # Tabelle erstellen, falls sie nicht existiert\n","    cursor.execute(\"\"\"\n","    CREATE TABLE IF NOT EXISTS message_store (\n","        session_id TEXT,\n","        message BLOB,\n","        index INTEGER\n","    )\n","    \"\"\")\n","\n","    conn.commit()\n","    conn.close()\n","    print(\"Datenbank ist bereit.\")\n","\n","# Abschnitt 4: Chat-Komponenten\n","def get_chat_history(session_id):\n","    \"\"\"Gibt ein ChatMessageHistory-Objekt zur√ºck.\"\"\"\n","    return SQLChatMessageHistory(\n","        session_id=session_id,\n","        connection_string=f\"sqlite:///{DB_PATH}\"\n","    )\n","\n","def create_chat_chain():\n","    \"\"\"Erstellt die Chat-Chain mit History-Unterst√ºtzung.\"\"\"\n","    # LLM initialisieren\n","    llm = ChatOpenAI(model=MODEL, temperature=0)\n","\n","    # Prompt mit History-Platzhalter\n","    prompt = ChatPromptTemplate.from_messages([\n","        (\"system\", DEFAULT_SYSTEM),\n","        MessagesPlaceholder(variable_name=\"history\"),\n","        (\"human\", \"{input}\")\n","    ])\n","\n","    # Basis-Chain\n","    chain = prompt | llm\n","\n","    # Chain mit History erweitern\n","    chain_with_history = RunnableWithMessageHistory(\n","        chain,\n","        get_chat_history,\n","        input_messages_key=\"input\",\n","        history_messages_key=\"history\"\n","    )\n","\n","    return chain_with_history\n","\n","# Abschnitt 5: Chat-Funktion\n","def chat():\n","    \"\"\"F√ºhrt eine einfache Chat-Konversation mit persistenter Geschichte.\"\"\"\n","    # Setup\n","    setup_database()\n","    chain = create_chat_chain()\n","\n","    # Session-ID generieren\n","    session_id = str(uuid.uuid4())[:8]\n","    print(f\"Chat gestartet. Session-ID: {session_id}\")\n","    print(\"Tippe 'exit' zum Beenden.\")\n","\n","    # Chat-Schleife\n","    while True:\n","        # Benutzereingabe\n","        user_input = input(\"\\nMensch: \")\n","        if user_input.lower() == 'exit':\n","            break\n","\n","        # Antwort mit History generieren\n","        response = chain.invoke(\n","            {\"input\": user_input},\n","            config={\"configurable\": {\"session_id\": session_id}}\n","        )\n","\n","        # Antwort anzeigen\n","        print(f\"\\nKI: {response.content}\")\n","\n","# Abschnitt 6: Hauptfunktion\n","def main():\n","    \"\"\"Startet die Chat-Anwendung.\"\"\"\n","    chat()\n","\n","# Wenn direkt ausgef√ºhrt\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"MwEAu4i4nOiz"},"id":"MwEAu4i4nOiz","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Vektorbasierte Speichersysteme - Einfaches Beispiel\n","\n","Dieses Beispiel demonstriert, wie man vektorbasierte Speichersysteme mit LangChain f√ºr ein semantisches Chatbot-Ged√§chtnis implementiert. Im Gegensatz zu einfachen sequentiellen Speichermethoden erm√∂glicht dieser Ansatz das \"Erinnern\" an fr√ºhere Gespr√§che basierend auf semantischer √Ñhnlichkeit.\n","\n","## Kernkonzept vektorbasierter Speichersysteme\n","\n","Bei diesem Ansatz werden:\n","\n","1. **Alle Nachrichten in Vektoren umgewandelt** (mittels Embedding-Modellen)\n","2. **In einer Vektordatenbank gespeichert** (hier: Chroma)\n","3. **Semantisch durchsucht** (anhand von √Ñhnlichkeit zur aktuellen Frage)\n","\n","Statt nur die letzten N Nachrichten zu speichern, kann der Chatbot gezielt relevante Informationen aus der gesamten Konversationshistorie abrufen.\n","\n","## Vorteile dieses Ansatzes\n","\n","- **Semantisches Ged√§chtnis**: Findet relevante Informationen basierend auf Bedeutung, nicht Position\n","- **Skalierbarkeit**: Funktioniert gut mit gro√üen Konversationshistorien\n","- **Selektiver Abruf**: Speichert die vollst√§ndige Konversation, ruft aber nur relevante Teile ab\n","\n","## Wie das Beispiel funktioniert\n","\n","Das Beispiel ist in zwei Phasen aufgeteilt:\n","\n","### Phase 1: Informationen sammeln\n","Der Nutzer teilt verschiedene Informationen √ºber Filme mit, die in der Vektordatenbank gespeichert werden.\n","\n","### Phase 2: Ged√§chtnis testen\n","Nach L√∂schen des kurzfristigen Chat-Verlaufs werden Fragen gestellt, die Informationen aus Phase 1 abrufen.\n","\n","Die Demonstration zeigt, wie der Chatbot sich an fr√ºhere Informationen \"erinnern\" kann, auch wenn sie nicht mehr im direkten Kontext sind, indem er:\n","1. Die aktuelle Frage in einen Vektor umwandelt\n","2. √Ñhnliche Vektoren (fr√ºhere Konversationsteile) in der Datenbank findet\n","3. Diese relevanten Teile in den Prompt einf√ºgt\n","4. Eine informierte Antwort generiert\n","\n","## Technische Komponenten\n","\n","- **OpenAIEmbeddings**: Wandelt Text in Vektoren um\n","- **Chroma**: Leichtgewichtige Vektordatenbank zur Speicherung\n","- **VectorStoreRetrieverMemory**: LangChain-Komponente zur Integration von Vektorspeichern als Ged√§chtnis\n","- **Retriever**: Konfiguriert f√ºr √Ñhnlichkeitssuche mit den Top 3 Ergebnissen\n","\n","## Anwendungsbereiche\n","\n","Dieser Ansatz ist besonders n√ºtzlich f√ºr:\n","- Chatbots mit langen Konversationshistorien\n","- Assistenten, die sich an spezifische Details aus fr√ºheren Gespr√§chen erinnern m√ºssen\n","- Systeme, die gro√üe Mengen an Kontextinformationen effizient verwalten m√ºssen\n","\n","Vektorbasierte Speichersysteme stellen eine erhebliche Verbesserung gegen√ºber einfachen sequentiellen Speichermethoden dar und sind eine zentrale Komponente moderner KI-Assistenten."],"metadata":{"id":"jOIW4whioe_Y"},"id":"jOIW4whioe_Y"},{"cell_type":"code","source":["# Abschnitt 1: Importe\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_community.vectorstores import Chroma\n","from langchain.memory import VectorStoreRetrieverMemory\n","from langchain_core.runnables import RunnablePassthrough\n","from IPython.display import display, Markdown\n","import os\n","import uuid\n","\n","# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","EMBEDDING_MODEL = \"text-embedding-ada-002\"\n","VECTOR_DB_PATH = \"chroma_db\"\n","TEMPERATURE = 0.0\n","\n","SYSTEM_PROMPT = \"\"\"Du bist ein hilfreicher Assistent mit semantischem Ged√§chtnis.\n","Anders als bei einfachen Chats kannst du vergangene Konversationen durchsuchen,\n","um passende Informationen zu finden - auch wenn sie vor l√§ngerer Zeit erw√§hnt wurden.\n","\n","Wenn du auf fr√ºhere Informationen zur√ºckgreifst, erw√§hne kurz, dass du dich an dieses\n","Thema erinnerst, damit der Nutzer den Kontext besser versteht.\"\"\"\n","\n","# Abschnitt 3: Vektorspeicher einrichten\n","def setup_vector_memory():\n","    \"\"\"Richtet den Vektorspeicher und das Memory ein.\"\"\"\n","    # Embeddings-Modell initialisieren\n","    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)\n","\n","    # Chroma-Vektordatenbank initialisieren oder laden\n","    vectorstore = Chroma(\n","        collection_name=\"chat_history\",\n","        embedding_function=embeddings,\n","        persist_directory=VECTOR_DB_PATH\n","    )\n","\n","    # Retriever mit moderierten Parametern konfigurieren\n","    retriever = vectorstore.as_retriever(\n","        search_type=\"similarity\",  # √Ñhnlichkeitssuche verwenden\n","        search_kwargs={\"k\": 3}     # Top 3 √§hnlichste Eintr√§ge abrufen\n","    )\n","\n","    # Vektorspeicher-Memory erstellen\n","    memory = VectorStoreRetrieverMemory(\n","        retriever=retriever,\n","        memory_key=\"relevant_history\"\n","    )\n","\n","    return memory, vectorstore\n","\n","# Abschnitt 4: Chat-Komponenten initialisieren\n","def initialize_chat_components(memory):\n","    \"\"\"Initialisiert die LLM-Komponenten und Chain mit Vektorspeicher-Memory.\"\"\"\n","    llm = ChatOpenAI(\n","        model=MODEL,\n","        temperature=TEMPERATURE\n","    )\n","\n","    # Prompt Template mit Platzhalter f√ºr relevante Ged√§chtnisinhalte\n","    prompt = ChatPromptTemplate.from_messages([\n","        (\"system\", SYSTEM_PROMPT),\n","        (\"system\", \"Relevante fr√ºhere Konversation:\\n{relevant_history}\"),\n","        MessagesPlaceholder(variable_name=\"chat_history\"),\n","        (\"human\", \"{input}\")\n","    ])\n","\n","    # Eine einfache Liste f√ºr den aktuellen Chat-Verlauf\n","    chat_history = []\n","\n","    # Funktion zum Laden des Ged√§chtnisses und Aktualisieren des Chat-Verlaufs\n","    def get_memory_and_chat_history(input_dict):\n","        # Relevante Erinnerungen abrufen\n","        memory_variables = memory.load_memory_variables({\"prompt\": input_dict[\"input\"]})\n","\n","        # Alles zusammenf√ºhren\n","        return {\n","            \"relevant_history\": memory_variables[\"relevant_history\"],\n","            \"chat_history\": input_dict[\"chat_history\"],\n","            \"input\": input_dict[\"input\"]\n","        }\n","\n","    # Chain erstellen\n","    chain = RunnablePassthrough.assign(\n","        context=get_memory_and_chat_history\n","    ) | {\n","        \"relevant_history\": lambda x: x[\"context\"][\"relevant_history\"],\n","        \"chat_history\": lambda x: x[\"chat_history\"],\n","        \"input\": lambda x: x[\"input\"]\n","    } | prompt | llm\n","\n","    return chain, chat_history\n","\n","# Abschnitt 5: Interaktionsfunktion\n","def interact_with_ai(question, chain, memory, chat_history):\n","    \"\"\"Verarbeitet eine Konversation mit Vektorspeicher-Ged√§chtnis.\"\"\"\n","    # Nutzereingabe anzeigen\n","    display(Markdown(f\"### üßë‚Äçü¶± Mensch:\"))\n","    display(Markdown(question))\n","\n","    # Antwort mit Vektorspeicher-Ged√§chtnis generieren\n","    response = chain.invoke({\n","        \"input\": question,\n","        \"chat_history\": chat_history\n","    })\n","\n","    # Antwort anzeigen\n","    display(Markdown(f\"### ü§ñ KI:\"))\n","    display(Markdown(response.content))\n","    print()\n","\n","    # Aktualisiere den Chat-Verlauf f√ºr den aktuellen Kontext\n","    chat_history.append({\"role\": \"human\", \"content\": question})\n","    chat_history.append({\"role\": \"assistant\", \"content\": response.content})\n","\n","    # In den Vektorspeicher speichern (als Key-Value-Paar f√ºr bessere Abrufbarkeit)\n","    memory_key = f\"conversation_{uuid.uuid4()}\"\n","    memory.save_context(\n","        {\"prompt\": question},\n","        {\"response\": response.content}\n","    )\n","\n","    return response.content\n","\n","# Abschnitt 6: Hauptfunktion\n","def main():\n","    \"\"\"Hauptfunktion f√ºr die Chat-Anwendung mit Vektorspeicher.\"\"\"\n","    display(Markdown(\"# üß† Vektorbasierter Memory-Chat\"))\n","\n","    # Vektorspeicher und Chat-Komponenten einrichten\n","    memory, vectorstore = setup_vector_memory()\n","    chain, chat_history = initialize_chat_components(memory)\n","\n","    # Beispielunterhaltung in zwei Phasen, um den langfristigen Abruf zu demonstrieren\n","    print(\"--- Phase 1: Informationen sammeln ---\")\n","    questions_phase1 = [\n","        \"Mein Lieblingsfilm ist Inception.\",\n","        \"Ich finde die Schauspieler Leonardo DiCaprio und Ellen Page darin gro√üartig.\",\n","        \"Der Regisseur Christopher Nolan hat auch The Dark Knight und Interstellar gedreht.\",\n","        \"Ich mag besonders die Szene mit dem rotierenden Hotelflur.\",\n","        \"Ein anderer Film, den ich empfehlen kann, ist The Matrix.\"\n","    ]\n","\n","    for question in questions_phase1:\n","        interact_with_ai(question, chain, memory, chat_history)\n","\n","    # Chat-History zur√ºcksetzen f√ºr Phase 2 (simuliert neue Session)\n","    chat_history.clear()\n","\n","    print(\"\\n--- Phase 2: Ged√§chtnis testen ---\")\n","    questions_phase2 = [\n","        \"Welcher Film hat einen rotierenden Hotelflur?\",\n","        \"Wer hat den Film Inception gedreht?\",\n","        \"Welche Filme habe ich dir empfohlen?\",\n","        \"Welche Schauspieler mochte ich in Inception?\"\n","    ]\n","\n","    for question in questions_phase2:\n","        interact_with_ai(question, chain, memory, chat_history)\n","\n","    # Speichere die Vektordatenbank f√ºr zuk√ºnftige Sitzungen\n","    vectorstore.persist()\n","    print(f\"Vektordatenbank wurde unter '{VECTOR_DB_PATH}' gespeichert.\")\n","\n","# Wenn direkt ausgef√ºhrt, starte die Hauptfunktion\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"PD2SezZcog5x"},"id":"PD2SezZcog5x","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Mehrschichtige Memory-Architekturen - Code-Beispiel\n","\n","Das folgende Beispiel demonstriert eine mehrschichtige Memory-Architektur mit LangChain, die verschiedene Ged√§chtnistypen kombiniert, um einen umfassenderen Kontext f√ºr den LLM-Assistenten zu schaffen.\n","\n","## Kernkonzept der mehrschichtigen Memory-Architektur\n","\n","Die Implementierung kombiniert drei Ged√§chtnistypen mit unterschiedlichen St√§rken:\n","\n","1. **Kurzzeitged√§chtnis** (`ConversationBufferWindowMemory`): Speichert die letzten 3 Nachrichten vollst√§ndig, f√ºr unmittelbaren Kontext\n","2. **Zusammenfassungsged√§chtnis** (`ConversationSummaryMemory`): Komprimiert die gesamte vorherige Konversation in eine Zusammenfassung\n","3. **Entit√§tsged√§chtnis** (`ConversationEntityMemory`): Extrahiert und speichert Informationen √ºber erw√§hnte Entit√§ten (Personen, Orte, Konzepte)\n","\n","Diese werden mit `CombinedMemory` zu einer einheitlichen Ged√§chtnisstruktur zusammengefasst, die alle Vorteile kombiniert.\n","\n","## Vorteile dieses Ansatzes\n","\n","- **Effiziente Token-Nutzung**: Jeder Ged√§chtnistyp fokussiert sich auf unterschiedliche Aspekte, was den Kontext optimiert\n","- **Langzeitinformationen**: Wichtige Informationen werden auch √ºber lange Konversationen hinweg bewahrt\n","- **Kontextbewusstsein**: Der Assistent kann sowohl auf unmittelbare Nachrichten als auch auf langfristige Informationen zugreifen\n","\n","## Beispielerkl√§rung\n","\n","Der Code demonstriert:\n","\n","1. **Einrichtung der Memory-Schichten** mit unterschiedlichen Parametern f√ºr jede Schicht\n","2. **Integration mit LangChain** √ºber ein angepasstes Prompt-Template, das alle Ged√§chtnisarten einbezieht\n","3. **Konversationsablauf** mit Speicherung und Abruf aus allen Ged√§chtnisschichten\n","4. **Memory-Inspektion** zur Veranschaulichung des Inhalts der verschiedenen Ged√§chtnistypen\n","\n","## Beispielunterhaltung\n","\n","Die Beispielunterhaltung zeigt, wie der Assistent Informationen aus verschiedenen Schichten nutzt:\n","- Er merkt sich pers√∂nliche Details (Name, Wohnort) mit dem Entit√§tsged√§chtnis\n","- Er beh√§lt unmittelbare Konversationselemente mit dem Kurzzeitged√§chtnis\n","- Er erfasst √ºbergeordnete Themen mit dem Zusammenfassungsged√§chtnis\n","\n","## Anwendungsbereiche\n","\n","Dieser Ansatz ist besonders n√ºtzlich f√ºr:\n","- Pers√∂nliche Assistenten, die sich Nutzerdetails merken m√ºssen\n","- Fachberater, die komplexe Themen diskutieren und dabei Kontext behalten m√ºssen\n","- Informationssysteme, die sowohl schnellen Zugriff auf aktuelle als auch langfristige Informationen ben√∂tigen\n","\n","Die mehrschichtige Memory-Architektur stellt einen fortschrittlichen Ansatz dar, der die verschiedenen Speichermethoden aus den vorherigen Modulen zu einer leistungsf√§higeren Gesamtl√∂sung kombiniert."],"metadata":{"id":"LVptbXzioAqP"},"id":"LVptbXzioAqP"},{"cell_type":"code","source":["#\n","# Mehrschichtig\n","#\n","# Abschnitt 1: Importe\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain.memory import (\n","    ConversationBufferWindowMemory,\n","    ConversationSummaryMemory,\n","    ConversationEntityMemory,\n","    CombinedMemory\n",")\n","from IPython.display import display, Markdown\n","\n","# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATURE = 0.0\n","\n","SYSTEM_PROMPT = \"\"\"Du bist ein hilfsbereiter Assistent mit Zugriff auf verschiedene Ged√§chtnisschichten:\n","1. Kurzzeitged√§chtnis: Die letzten 3 Nachrichten direkt im Kontext\n","2. Zusammenfassungsged√§chtnis: Die Kernthemen der bisherigen Unterhaltung\n","3. Entit√§tsged√§chtnis: Wichtige Informationen √ºber erw√§hnte Personen, Orte oder Dinge\n","\n","Deine Aufgabe ist es, alle verf√ºgbaren Informationen zu nutzen, um hilfreich zu antworten.\"\"\"\n","\n","# Abschnitt 3: Memory-Schichten einrichten\n","def setup_memory_layers():\n","    \"\"\"Richtet die verschiedenen Memory-Schichten ein.\"\"\"\n","    llm = ChatOpenAI(model=MODEL, temperature=TEMPERATURE)\n","\n","    # Schicht 1: Fenster-basiertes Kurzzeitged√§chtnis (letzte 3 Nachrichtenpaare)\n","    buffer_memory = ConversationBufferWindowMemory(\n","        k=3,\n","        return_messages=True,\n","        memory_key=\"buffer_history\",\n","        input_key=\"input\",\n","        output_key=\"output\"\n","    )\n","\n","    # Schicht 2: Zusammenfassungs-Ged√§chtnis (komprimierte fr√ºhere Unterhaltung)\n","    summary_memory = ConversationSummaryMemory(\n","        llm=llm,\n","        memory_key=\"summary_history\",\n","        return_messages=False,\n","        input_key=\"input\",\n","        output_key=\"output\"\n","    )\n","\n","    # Schicht 3: Entit√§ts-Ged√§chtnis (Informationen √ºber erw√§hnte Entit√§ten)\n","    entity_memory = ConversationEntityMemory(\n","        llm=llm,\n","        memory_key=\"entity_store\",\n","        return_messages=False,\n","        input_key=\"input\",\n","        output_key=\"output\",\n","        k=3  # Limitiert auf die 3 relevantesten Entit√§ten\n","    )\n","\n","    # Alle Ged√§chtnisschichten kombinieren\n","    combined_memory = CombinedMemory(\n","        memories=[buffer_memory, summary_memory, entity_memory]\n","    )\n","\n","    return combined_memory\n","\n","# Abschnitt 4: Chat-Komponenten initialisieren\n","def initialize_chat_components(memory):\n","    \"\"\"Initialisiert die LLM-Komponenten und Chain mit kombiniertem Memory.\"\"\"\n","    llm = ChatOpenAI(\n","        model=MODEL,\n","        temperature=TEMPERATURE\n","    )\n","\n","    # Prompt Template mit Platzhaltern f√ºr die verschiedenen Ged√§chtnisarten\n","    prompt = ChatPromptTemplate.from_messages([\n","        (\"system\", SYSTEM_PROMPT),\n","        (\"system\", \"Zusammenfassung der bisherigen Konversation:\\n{summary_history}\"),\n","        (\"system\", \"Wichtige Entit√§ten:\\n{entity_store}\"),\n","        MessagesPlaceholder(variable_name=\"buffer_history\"),\n","        (\"human\", \"{input}\")\n","    ])\n","\n","    # Funktion, die alle Memory-Variablen in die Chain-Eingabe integriert\n","    def get_memory_and_input(input_dict):\n","        memory_variables = memory.load_memory_variables({\"input\": input_dict[\"input\"]})\n","        merged = {**memory_variables, **input_dict}\n","        return merged\n","\n","    # Chain erstellen\n","    chain = (\n","        RunnablePassthrough.assign(memory_variables=get_memory_and_input)\n","        | RunnablePassthrough.assign(\n","            buffer_history=lambda x: x[\"memory_variables\"][\"buffer_history\"],\n","            summary_history=lambda x: x[\"memory_variables\"][\"summary_history\"],\n","            entity_store=lambda x: x[\"memory_variables\"][\"entity_store\"]\n","        )\n","        | prompt\n","        | llm\n","    )\n","\n","    return chain, memory\n","\n","# Abschnitt 5: Interaktionsfunktion\n","def interact_with_ai(question, chain, memory):\n","    \"\"\"Verarbeitet eine Konversation und gibt die Antwort zur√ºck.\"\"\"\n","    # Nutzereingabe anzeigen\n","    display(Markdown(f\"### üßë‚Äçü¶± Mensch:\"))\n","    display(Markdown(question))\n","\n","    # Antwort mit mehrschichtigem Memory generieren\n","    response = chain.invoke({\"input\": question})\n","\n","    # Antwort anzeigen\n","    display(Markdown(f\"### ü§ñ KI:\"))\n","    display(Markdown(response.content))\n","    print()\n","\n","    # Speichere den Dialog im Memory\n","    memory.save_context({\"input\": question}, {\"output\": response.content})\n","\n","    return response.content\n","\n","# Abschnitt 6: Memory-Inspektion\n","def inspect_memory(memory):\n","    \"\"\"Zeigt den Inhalt der verschiedenen Memory-Schichten an.\"\"\"\n","    memory_vars = memory.load_memory_variables({})\n","\n","    display(Markdown(\"## üîç Memory-Inspektion\"))\n","\n","    # Kurzzeitged√§chtnis anzeigen\n","    display(Markdown(\"### üìã Kurzzeitged√§chtnis (Letzte Nachrichten):\"))\n","    if \"buffer_history\" in memory_vars and memory_vars[\"buffer_history\"]:\n","        for msg in memory_vars[\"buffer_history\"]:\n","            role = \"üßë‚Äçü¶± Mensch\" if msg.type == \"human\" else \"ü§ñ KI\"\n","            display(Markdown(f\"**{role}:** {msg.content}\"))\n","    else:\n","        display(Markdown(\"*Noch keine Nachrichten im Kurzzeitged√§chtnis.*\"))\n","\n","    # Zusammenfassung anzeigen\n","    display(Markdown(\"### üìù Zusammenfassungsged√§chtnis:\"))\n","    if \"summary_history\" in memory_vars and memory_vars[\"summary_history\"]:\n","        display(Markdown(memory_vars[\"summary_history\"]))\n","    else:\n","        display(Markdown(\"*Noch keine Zusammenfassung vorhanden.*\"))\n","\n","    # Entit√§ten anzeigen\n","    display(Markdown(\"### üè∑Ô∏è Entit√§tsged√§chtnis:\"))\n","    if \"entity_store\" in memory_vars and memory_vars[\"entity_store\"]:\n","        entity_text = memory_vars[\"entity_store\"]\n","        display(Markdown(entity_text))\n","    else:\n","        display(Markdown(\"*Noch keine Entit√§ten gespeichert.*\"))\n","\n","    print()\n","\n","# Abschnitt 7: Hauptfunktion\n","def main():\n","    \"\"\"Hauptfunktion f√ºr die Chat-Anwendung.\"\"\"\n","    display(Markdown(\"# üß† Mehrschichtiges Memory-Chat-Beispiel\"))\n","\n","    # Memory-Schichten und Chat-Komponenten einrichten\n","    memory = setup_memory_layers()\n","    chain, memory = initialize_chat_components(memory)\n","\n","    # Beispielunterhaltung\n","    test_questions = [\n","        \"Mein Name ist Max und ich lebe in Berlin.\",\n","        \"Ich interessiere mich f√ºr K√ºnstliche Intelligenz und maschinelles Lernen.\",\n","        \"Vor allem neuronale Netze finde ich spannend.\",\n","        \"Ich studiere gerade an der TU Berlin.\",\n","        \"Kannst du mir etwas √ºber Reinforcement Learning erz√§hlen?\",\n","        \"Was sind die Unterschiede zwischen √ºberwachtem und un√ºberwachtem Lernen?\",\n","        \"K√∂nntest du dich daran erinnern, wo ich studiere und wie ich hei√üe?\",\n","        \"Welche Themen haben wir bisher besprochen?\"\n","    ]\n","\n","    for i, question in enumerate(test_questions, 1):\n","        print(f\"\\n--- Interaktion {i} ---\")\n","        interact_with_ai(question, chain, memory)\n","\n","        # Nach jeder zweiten Interaktion das Memory inspizieren\n","        if i % 2 == 0:\n","            inspect_memory(memory)\n","\n","# Wenn direkt ausgef√ºhrt, starte die Hauptfunktion\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"cD6X4a8Zny8h"},"id":"cD6X4a8Zny8h","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"fe8a8ba4","metadata":{"id":"fe8a8ba4"},"source":["# 1 | Intro\n","---\n"]},{"cell_type":"markdown","source":["\n","\n","Gro√üe Sprachmodelle (LLMs) erm√∂glichen eine nat√ºrliche Interaktion √§hnlich menschlicher Gespr√§che. Sie sind in der Lage, auf vorherige Informationen innerhalb eines Dialogs Bezug zu nehmen. In diesem Modul werden wir verschiedene Ans√§tze zur Verwaltung des Ged√§chtnisses eines LLMs untersuchen.\n","\n","Da der interne Kontextpuffer eines LLMs begrenzt ist, ben√∂tigen wir Strategien, um den Chat-Verlauf zu speichern und effizient zu nutzen. LangChain bietet verschiedene L√∂sungsans√§tze, die wir in diesem Modul erkunden werden:\n"],"metadata":{"id":"EmMVp49lhkOh"},"id":"EmMVp49lhkOh"},{"cell_type":"markdown","source":["1. **Persistenter Speicher**: Langfristige Speicherung √ºber Sitzungen hinweg\n","2. **Token-basierte Limitierung**: Ber√ºcksichtigung der tats√§chlichen Tokenlimits des LLM\n","3. **Zusammenfassung von Konversationen**: Komprimieren l√§ngerer Konversationen\n","4. **Begrenzung der Nachrichtenanzahl**: Verwendung eines \"Fensters\" der letzten N Nachrichten\n","5. **Manuelle Konversationsverwaltung**: Einfache Speicherung und Weiterleitung der kompletten Konversationshistorie\n","\n","\n","Jeder dieser Ans√§tze hat seine eigenen Vor- und Nachteile, die wir im Folgenden genauer untersuchen werden.\n"],"metadata":{"id":"cAOs2SNwhmJ-"},"id":"cAOs2SNwhmJ-"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Message-Typen\n","</font></p>"],"metadata":{"id":"kYpOM3O-BJuO"},"id":"kYpOM3O-BJuO"},{"cell_type":"markdown","source":["Die memory-Module von LangChain erwarten Message-Objekte wie SystemMessage, HumanMessage, AIMessage.\n","\n","Sie helfen LangChain (und dem Modell), den Kontext richtig einzuordnen:    \n","Wer sagt was? Welche Rolle hat diese Nachricht im Dialog?\n","\n","Das ist besonders wichtig, wenn man mit ConversationBufferMemory, ConversationSummaryMemory usw. arbeitest, weil diese listenweise Messages speichern und sp√§ter wiederverwenden."],"metadata":{"id":"SeApNepSBjqR"},"id":"SeApNepSBjqR"},{"cell_type":"code","source":["# Abschnitt 0: Installation und API-Key\n","!uv pip install --system --prerelease allow -q langchain_community langchain_openai"],"metadata":{"id":"WHJhHEoqkIeS"},"id":"WHJhHEoqkIeS","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K2lC_JK3guaj"},"outputs":[],"source":["from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n","from langchain_openai import ChatOpenAI\n","from IPython.display import display, Markdown"],"id":"K2lC_JK3guaj"},{"cell_type":"code","source":["messages = [\n","    SystemMessage(\n","        content=\"Du bist ein kompetenter und hilfreicher Assistent.\"\n","    ),\n","    HumanMessage(\n","        content=\"Nenne mir die Hauptstadt von Usbekistan?\"\n","    ),\n","]"],"metadata":{"id":"zIOnQp7j9Drq"},"execution_count":null,"outputs":[],"id":"zIOnQp7j9Drq"},{"cell_type":"code","source":["type(messages)"],"metadata":{"id":"6KDVJpDa92h6"},"execution_count":null,"outputs":[],"id":"6KDVJpDa92h6"},{"cell_type":"code","source":["MODEL = 'gpt-4o-mini'\n","TEMPERATURE = 0.0\n","\n","llm = ChatOpenAI(model=MODEL, temperature= TEMPERATURE)\n","response = llm.invoke(messages)"],"metadata":{"id":"z0yFqHlZ9Kts"},"execution_count":null,"outputs":[],"id":"z0yFqHlZ9Kts"},{"cell_type":"code","source":["display(Markdown(\"## üì£ Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response.content))"],"metadata":{"id":"UvDvAWVd9WiF"},"execution_count":null,"outputs":[],"id":"UvDvAWVd9WiF"},{"cell_type":"markdown","metadata":{"id":"7a9a2437"},"source":["# 2 | Persistenter Speicher\n","---\n"],"id":"7a9a2437"},{"cell_type":"markdown","source":["\n","\n","Die M√∂glichkeit eines ChatBots, sich an vorherige Unterhaltungen zu erinnern, spielt eine zentrale Rolle f√ºr eine fl√ºssige und konsistente Kommunikation. Ein **persistenter Speicher** erlaubt es, Gespr√§chsdaten √ºber mehrere Sitzungen hinweg zu sichern und wiederzuverwenden. Dadurch kann der ChatBot nicht nur den Kontext besser erfassen, sondern auch langfristige Nutzerpr√§ferenzen ber√ºcksichtigen.\n","\n","Dieser Abschnitt untersucht verschiedene Speicherl√∂sungen f√ºr ChatBots und deren effiziente Integration. Dabei werden unterschiedliche Methoden analysiert ‚Äì von einfachen Dateisystemen √ºber relationale und NoSQL-Datenbanken bis hin zu spezialisierten Speichern f√ºr gro√üe Sprachmodelle. Zudem wird aufgezeigt, wie gespeicherte Informationen verwaltet, aktualisiert und gesch√ºtzt werden, um eine ausgewogene Kombination aus Leistungsf√§higkeit, Datenschutz und Benutzerfreundlichkeit zu gew√§hrleisten.\n","\n","Die Implementierung eines dauerhaften Speichers erm√∂glicht es einem ChatBot, nicht nur die Interaktion mit Nutzern zu verbessern, sondern auch personalisierte, kontextbezogene und langfristig relevante Dialoge zu f√ºhren.\n","\n","F√ºr die nachfolgende Version wird die Klasse **BaseMemory** f√ºr einen persistenten Chat verwendet. Der Chat-Verlauf wird in einer JSON-Datei gespeichert. Diese Klasse dient als abstrakte Basisklasse, von der spezifische Memory-Implementierungen abgeleitet werden, um Kontexte und Zwischenergebnisse in den Arbeitsabl√§ufen (Chains) zu speichern und wiederzuverwenden.\n"],"metadata":{"id":"qKkboVWsiMaD"},"id":"qKkboVWsiMaD"},{"cell_type":"markdown","source":["## 2.1 | Minimalversion\n"],"metadata":{"id":"ncw0vI8MAJrt"},"id":"ncw0vI8MAJrt"},{"cell_type":"code","source":["import os\n","from openai import OpenAI\n","\n","def chat():\n","    # Initialisierung des OpenAI-Clients\n","    client = OpenAI()\n","    file = \"chat_history.txt\"\n","\n","    print(\"Chatbot gestartet.\")\n","    print(\"Befehle: 'history' zeigt Verlauf, 'exit' beendet Chat\")\n","\n","    # Chat-Kontext initialisieren\n","    messages = [\n","        {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher Assistent. Antworte auf Deutsch und sei pr√§zise.\"}\n","    ]\n","\n","    # Lade bestehende Historie beim Start, falls vorhanden\n","    try:\n","        if os.path.exists(file) and os.path.getsize(file) > 0:\n","            print(\"Lade vorherige Chathistorie...\")\n","            chat_pairs = []\n","            with open(file, 'r', encoding='utf-8') as f:\n","                content = f.read()\n","                segments = content.split(\"\\n\\n\")\n","\n","                for segment in segments:\n","                    if segment.strip() and \"Benutzer: \" in segment and \"KI: \" in segment:\n","                        parts = segment.split(\"KI: \", 1)\n","                        if len(parts) == 2:\n","                            user_part = parts[0].replace(\"Benutzer: \", \"\", 1).strip()\n","                            ai_part = parts[1].strip()\n","                            if user_part and ai_part:\n","                                chat_pairs.append((user_part, ai_part))\n","\n","            # F√ºge die letzten 5 Austausche zum Kontext hinzu (falls vorhanden)\n","            for user_msg, ai_msg in chat_pairs[-10:]:\n","                messages.append({\"role\": \"user\", \"content\": user_msg})\n","                messages.append({\"role\": \"assistant\", \"content\": ai_msg})\n","\n","            print(f\"{len(chat_pairs)} fr√ºhere Unterhaltungen gefunden, die letzten {min(10, len(chat_pairs))} werden im Kontext verwendet.\")\n","    except Exception as e:\n","        print(f\"Fehler beim Laden der Historie: {e}\")\n","\n","    # Endlosschleife f√ºr den Chat\n","    while True:\n","        user_input = input(\"\\nBenutzer: \")\n","\n","        if user_input == \"exit\":\n","            print(\"Chat beendet.\")\n","            break\n","        elif user_input == \"history\":\n","            try:\n","                with open(file, 'r', encoding='utf-8') as f:\n","                    print(\"\\n--- Chatverlauf ---\")\n","                    print(f.read())\n","                    print(\"--- Ende des Verlaufs ---\\n\")\n","            except FileNotFoundError:\n","                print(\"Keine Chathistorie gefunden.\")\n","            continue\n","\n","        # Benutzer-Nachricht zum Kontext hinzuf√ºgen\n","        messages.append({\"role\": \"user\", \"content\": user_input})\n","\n","        # OpenAI API-Anfrage mit vollem Kontext\n","        try:\n","            response = client.chat.completions.create(\n","                model=\"gpt-4o-mini\",\n","                messages=messages,\n","                max_tokens=1000\n","            )\n","            ai_response = response.choices[0].message.content\n","\n","            # Antwort zum Kontext hinzuf√ºgen f√ºr zuk√ºnftige Anfragen\n","            messages.append({\"role\": \"assistant\", \"content\": ai_response})\n","\n","            # Kontext begrenzen, um Token zu sparen (beh√§lt nur die letzten 10 Nachrichten + System-Prompt)\n","            if len(messages) > 11:  # System-Prompt + 10 Nachrichten\n","                messages = [messages[0]] + messages[-10:]\n","\n","        except Exception as e:\n","            ai_response = f\"Fehler: {str(e)}\"\n","\n","        print(f\"KI: {ai_response}\")\n","\n","        # Speichern in einfacher Textdatei\n","        with open(file, 'a', encoding='utf-8') as f:\n","            f.write(f\"Benutzer: {user_input}\\nKI: {ai_response}\\n\\n\")"],"metadata":{"id":"pF-E3tIDAKky"},"execution_count":null,"outputs":[],"id":"pF-E3tIDAKky"},{"cell_type":"code","source":["chat()"],"metadata":{"id":"WdxQR3IsAM0a"},"execution_count":null,"outputs":[],"id":"WdxQR3IsAM0a"},{"cell_type":"markdown","source":["## 2.2 | Komfortversion\n"],"metadata":{"id":"gNaHt9iuAfzs"},"id":"gNaHt9iuAfzs"},{"cell_type":"code","source":["# Abschnitt 1: Importe\n","import json\n","import os\n","from datetime import datetime\n","from pathlib import Path\n","from langchain_core.memory import BaseMemory\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate\n","from langchain_core.messages import SystemMessage\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain.memory import ConversationSummaryBufferMemory\n","from pydantic import BaseModel, Field\n","\n","from IPython.display import display, Markdown"],"metadata":{"id":"XIxfBMsQ87Jc"},"execution_count":null,"outputs":[],"id":"XIxfBMsQ87Jc"},{"cell_type":"code","source":["# Abschnitt 2: Konstanten und Klassen\n","MODEL = \"gpt-4o-mini\"\n","DEFAULT_MEMORY_FILE = \"chat_memory.json\"\n","SYSTEM_PROMPT = \"\"\"Du bist ein kompetenter KI-Assistent mit breitem Fachwissen.\n","\n","Deine Antworten sind:\n","- Klar strukturiert und mit Markdown formatiert\n","- Praxisorientiert und direkt umsetzbar\n","- Basierend auf aktuellem Kenntnisstand\n","- Mit passenden Beispielen versehen\n","- In verst√§ndlicher Sprache formuliert\"\"\"\n","\n","class EnhancedJSONMemory(BaseMemory, BaseModel):\n","    \"\"\"Erweiterte Memory-Implementierung mit JSON-Speicherung, Statistiken und Zusammenfassung.\"\"\"\n","    file_path: str = Field(default=DEFAULT_MEMORY_FILE)\n","    summary_memory: object = Field(default=None)\n","    chat_data: dict[str, object] = Field(default_factory=lambda: {\n","        \"conversations\": [],\n","        \"metadata\": {\n","            \"created_at\": datetime.now().isoformat(),\n","            \"last_modified\": datetime.now().isoformat(),\n","            \"total_interactions\": 0,\n","            \"model_name\": MODEL,\n","            \"version\": \"2.0\"\n","        },\n","        \"statistics\": {\n","            \"average_response_length\": 0,\n","            \"total_chars_exchanged\": 0\n","        },\n","        \"summary\": \"\"\n","    })\n","\n","    def __init__(self, **kwargs) -> None:\n","        \"\"\"Initialisiert die Memory-Klasse und l√§dt existierende Chatverl√§ufe.\"\"\"\n","        super().__init__(**kwargs)\n","        self.summary_memory = ConversationSummaryBufferMemory(\n","            llm=ChatOpenAI(temperature=0, model=MODEL),\n","            max_token_limit=2000,\n","            memory_key=\"summary\",\n","            human_prefix=\"Mensch\",\n","            ai_prefix=\"KI\"\n","        )\n","        self.load_history()\n","\n","    def format_timestamp(self, timestamp_str: str) -> str:\n","        \"\"\"Formatiert einen ISO-Zeitstempel in ein lesbares Format.\"\"\"\n","        return datetime.fromisoformat(timestamp_str).strftime(\"%d.%m.%Y %H:%M:%S\")\n","\n","    def update_statistics(self, user_input: str, response: str) -> None:\n","        \"\"\"Aktualisiert die Chatstatistiken.\"\"\"\n","        stats = self.chat_data[\"statistics\"]\n","        total_responses = len(self.chat_data[\"conversations\"])\n","\n","        if total_responses > 0:\n","            current_total = stats[\"average_response_length\"] * (total_responses - 1)\n","            stats[\"average_response_length\"] = round((current_total + len(response)) / total_responses, 2)\n","\n","        stats[\"total_chars_exchanged\"] += len(user_input) + len(response)\n","\n","    def update_summary(self, user_input: str, response: str) -> None:\n","        \"\"\"Aktualisiert die Konversationszusammenfassung.\"\"\"\n","        self.summary_memory.save_context({\"input\": user_input}, {\"output\": response})\n","        self.chat_data[\"summary\"] = self.summary_memory.load_memory_variables({}).get(\"summary\", \"\")\n","\n","    def save_context(self, inputs: dict[str], outputs: dict[str, str]) -> None:\n","        \"\"\"Speichert den aktuellen Kontext mit erweiterten Metadaten.\"\"\"\n","        self.chat_data[\"conversations\"].append({\n","            \"timestamp\": datetime.now().isoformat(),\n","            \"user_input\": inputs[\"input\"],\n","            \"response\": outputs[\"text\"],\n","            \"interaction_id\": len(self.chat_data[\"conversations\"]) + 1\n","        })\n","        self.chat_data[\"metadata\"][\"last_modified\"] = datetime.now().isoformat()\n","        self.chat_data[\"metadata\"][\"total_interactions\"] += 1\n","\n","        self.update_statistics(inputs[\"input\"], outputs[\"text\"])\n","        self.update_summary(inputs[\"input\"], outputs[\"text\"])\n","        self._save_to_file()\n","\n","    def _save_to_file(self) -> None:\n","        \"\"\"Speichert den Chatverlauf in der JSON-Datei.\"\"\"\n","        try:\n","            with Path(self.file_path).open(\"w\", encoding=\"utf-8\") as file:\n","                json.dump(self.chat_data, file, ensure_ascii=False, indent=2)\n","        except Exception as e:\n","            md_print(f\"‚ö†Ô∏è **Fehler beim Speichern der Historie:** {str(e)}\")\n","\n","    def load_history(self) -> None:\n","        \"\"\"L√§dt die Chathistorie aus der JSON-Datei.\"\"\"\n","        if not Path(self.file_path).exists():\n","            return\n","\n","        try:\n","            with Path(self.file_path).open(\"r\", encoding=\"utf-8\") as file:\n","                loaded_data = json.load(file)\n","\n","            if \"metadata\" in loaded_data and \"version\" in loaded_data[\"metadata\"]:\n","                self.chat_data = loaded_data\n","                md_print(\"‚úÖ **Konversationshistorie wurde erfolgreich geladen.**\")\n","\n","                # Lade vorhandene Konversationen in das Summary Memory\n","                for conv in self.chat_data[\"conversations\"]:\n","                    self.summary_memory.save_context(\n","                        {\"input\": conv[\"user_input\"]},\n","                        {\"output\": conv[\"response\"]}\n","                    )\n","            else:\n","                md_print(\"‚ö†Ô∏è **Veraltetes Historienformat erkannt. Erstelle neue Historie.**\")\n","        except json.JSONDecodeError as e:\n","            md_print(f\"‚ö†Ô∏è **Fehler beim Laden der JSON-Datei:** {str(e)}\")\n","\n","    def load_memory_variables(self, inputs: dict[str]) -> dict[str, list[dict[str]]]:\n","        \"\"\"L√§dt die Chatvariablen f√ºr den Prompt.\"\"\"\n","        return {\"history\": self.chat_data[\"conversations\"], \"summary\": self.chat_data[\"summary\"]}\n","\n","    @property\n","    def memory_variables(self) -> list[str]:\n","        \"\"\"Definiert die verf√ºgbaren Memory-Variablen.\"\"\"\n","        return [\"history\", \"summary\"]\n","\n","    def clear(self) -> None:\n","        \"\"\"L√∂scht die Chathistorie und speichert den zur√ºckgesetzten Zustand.\"\"\"\n","        # Zur√ºcksetzen des Konversations-Arrays\n","        self.chat_data[\"conversations\"] = []\n","        # Zur√ºcksetzen der Zusammenfassung\n","        self.chat_data[\"summary\"] = \"\"\n","        # Zur√ºcksetzen der Statistiken\n","        self.chat_data[\"statistics\"] = {\n","            \"average_response_length\": 0,\n","            \"total_chars_exchanged\": 0\n","        }\n","        # Metadaten aktualisieren\n","        self.chat_data[\"metadata\"][\"last_modified\"] = datetime.now().isoformat()\n","        self.chat_data[\"metadata\"][\"total_interactions\"] = 0\n","\n","        # Summary Memory zur√ºcksetzen\n","        self.summary_memory.clear()\n","\n","        # Gespeicherte Datei aktualisieren\n","        self._save_to_file()\n","\n","        md_print(\"üßπ **Chathistorie wurde erfolgreich gel√∂scht.**\")\n","\n","    def show_history(self) -> None:\n","        \"\"\"Zeigt eine detaillierte Konversationshistorie und Zusammenfassung.\"\"\"\n","        if not self.chat_data[\"conversations\"]:\n","            md_print(\"‚ùå **Keine Konversationshistorie vorhanden.**\")\n","            return\n","\n","        history_markdown = []\n","        history_markdown.append(\"\\n## üìä Chat-Statistiken:\")\n","        stats = self.chat_data[\"statistics\"]\n","        meta = self.chat_data[\"metadata\"]\n","        history_markdown.append(f\"- **Gesamtinteraktionen:** {meta['total_interactions']}\")\n","        history_markdown.append(f\"- **Durchschnittliche Antwortl√§nge:** {stats['average_response_length']} Zeichen\")\n","        history_markdown.append(f\"- **Gesamter Zeichenaustausch:** {stats['total_chars_exchanged']} Zeichen\")\n","\n","        if self.chat_data[\"summary\"]:\n","            history_markdown.append(\"\\n## üìù Zusammenfassung der Konversation:\")\n","            history_markdown.append(self.chat_data[\"summary\"])\n","\n","        history_markdown.append(\"\\n## üìú Detaillierte Konversationshistorie:\")\n","        for conv in self.chat_data[\"conversations\"]:\n","            history_markdown.append(\"\\n\" + \"=\"*50)\n","            history_markdown.append(f\"### üî¢ Interaktion #{conv['interaction_id']}\")\n","            history_markdown.append(f\"**üïí Zeitpunkt:** {self.format_timestamp(conv['timestamp'])}\")\n","            history_markdown.append(f\"**üßë‚Äçü¶± Mensch:** {conv['user_input']}\")\n","            history_markdown.append(f\"**ü§ñ KI:**\\n{conv['response']}\")\n","\n","        md_print(\"\\n\".join(history_markdown))\n"],"metadata":{"id":"CaWtSTHR9kgd"},"execution_count":null,"outputs":[],"id":"CaWtSTHR9kgd"},{"cell_type":"code","source":["# Abschnitt 3: Chat Komponenten\n","# LLM und Memory initialisieren\n","llm = ChatOpenAI(model=MODEL, temperature=0.0)\n","memory = EnhancedJSONMemory()\n","\n","# Prompt Template und Chat Sequence erstellen\n","prompt = ChatPromptTemplate.from_messages([\n","    SystemMessage(content=SYSTEM_PROMPT),\n","    (\"system\", \"Zusammenfassung der bisherigen Konversation:\\n{summary}\\n\"),\n","    (\"system\", \"Bisheriger Chatverlauf:\\n{history}\\n\"),\n","    (\"human\", \"{input}\")\n","])\n","\n","# Hilfsfunktion f√ºr Formatierung\n","def format_history(history):\n","    return \"\\n\".join([f\"Mensch: {entry['user_input']}\\nKI: {entry['response']}\" for entry in history])\n","\n","# Chat Sequence erstellen\n","chat_sequence = (\n","    {\n","        \"history\": lambda x: format_history(memory.load_memory_variables({})[\"history\"]),\n","        \"summary\": lambda x: memory.load_memory_variables({})[\"summary\"],\n","        \"input\": lambda x: x[\"input\"]\n","    }\n","    | prompt\n","    | llm\n",")"],"metadata":{"id":"28Qv2Sm59m6l"},"execution_count":null,"outputs":[],"id":"28Qv2Sm59m6l"},{"cell_type":"code","source":["# Abschnitt 4: Funktionen\n","def md_print(text):\n","    \"\"\"Gibt Text als Markdown aus, wenn m√∂glich, sonst als normalen Text.\"\"\"\n","    try:\n","        # Pr√ºfen, ob wir in einem Jupyter Notebook sind\n","        is_notebook = 'ipykernel' in sys.modules\n","        if is_notebook:\n","            display(Markdown(text))\n","        else:\n","            print(text)\n","    except NameError:\n","        # Wenn sys nicht definiert ist, nehmen wir an, dass wir nicht in einem Notebook sind\n","        print(text)\n","\n","def run_chat(chat_sequence, memory) -> None:\n","    \"\"\"F√ºhrt die Chat-Schleife aus.\"\"\"\n","    md_print(\"## Chat gestartet\\n\")\n","    md_print(\"**Befehle:**\\n- `history`: Zeigt die Chathistorie\\n- `clear`: L√∂scht die Chathistorie\\n- `exit` oder `tsch√ºss`: Beendet den Chat\\n\")\n","\n","    # Pr√ºfen, ob wir in einem Jupyter Notebook sind\n","    try:\n","        is_notebook = 'ipykernel' in sys.modules\n","    except NameError:\n","        # sys ist nicht definiert, also sind wir nicht in einem Notebook\n","        is_notebook = False\n","\n","    while True:\n","        # Die Eingabeaufforderung bleibt ein normales print, da input() damit arbeitet\n","        if is_notebook:\n","            from IPython.display import clear_output\n","            # In Notebooks k√∂nnen wir es sch√∂ner machen mit garantierter Breitenreduzierung\n","            from ipywidgets import widgets\n","\n","            # Container-Widget mit fester Breite erstellen\n","            container = widgets.HBox(\n","                layout=widgets.Layout(\n","                    width='60%',       # Container auf 60% der verf√ºgbaren Breite beschr√§nken\n","                    margin='0px'\n","                )\n","            )\n","\n","            # Textfeld erstellen (nimmt die volle Breite des Containers ein)\n","            user_input_widget = widgets.Text(\n","                description=\"üß†\",\n","                placeholder=\"Deine Nachricht hier...\",\n","                style={'description_width': '40px'},\n","                layout=widgets.Layout(width='100%')  # F√ºllt den Container vollst√§ndig aus\n","            )\n","\n","            # Das Textfeld in den Container einf√ºgen und anzeigen\n","            container.children = [user_input_widget]\n","            display(container)\n","\n","            # Warten auf Eingabe\n","            user_input = input(\"Eingabe: \")\n","            user_input_widget.value = user_input\n","        else:\n","            user_input = input(\"\\U0001F9B1 Mensch: \")\n","\n","        if user_input.lower() in [\"exit\", \"tsch√ºss\"]:\n","            break\n","        elif user_input.lower() == \"history\":\n","            memory.show_history()\n","            continue\n","        elif user_input.lower() == \"clear\":\n","            memory.clear()\n","            continue\n","\n","        response = chat_sequence.invoke({\"input\": user_input})\n","        memory.save_context({\"input\": user_input}, {\"text\": response.content})\n","\n","        # Markdown-Ausgabe f√ºr die Antwort\n","        md_print(f\"\\n## ü§ñ KI:\\n{response.content}\\n\")"],"metadata":{"id":"PpFbPKZq9qvH"},"execution_count":null,"outputs":[],"id":"PpFbPKZq9qvH"},{"cell_type":"code","source":["# Abschnitt 5: Hauptfunktion\n","def main() -> None:\n","    \"\"\"Hauptfunktion zum Starten des Chat-Bots.\"\"\"\n","    md_print(\"## ü§ñ KI-Chat mit Ged√§chtnis\")\n","\n","    # sys importieren, wenn noch nicht geschehen\n","    try:\n","        import sys\n","    except ImportError:\n","        pass\n","\n","    # Chat-Komponenten sind bereits global definiert\n","    run_chat(chat_sequence, memory)"],"metadata":{"id":"INS1Ykc89sJF"},"execution_count":null,"outputs":[],"id":"INS1Ykc89sJF"},{"cell_type":"code","source":["# Ausf√ºhren, wenn direkt gestartet\n","main()"],"metadata":{"id":"3L89ijxT9XPz"},"execution_count":null,"outputs":[],"id":"3L89ijxT9XPz"},{"cell_type":"markdown","source":["# 3 | Token-Limit\n","---\n"],"metadata":{"id":"7-GWGzh-iBGC"},"id":"7-GWGzh-iBGC"},{"cell_type":"markdown","source":["\n","\n","Die vorherigen Ans√§tze begrenzten die Anzahl der gespeicherten Nachrichten. In der Praxis ist jedoch das **Token-Limit** des Modells entscheidend. `ConversationTokenBufferMemory` adressiert dieses Problem, indem es den Kontext basierend auf der tats√§chlichen Token-Anzahl begrenzt."],"metadata":{"id":"Td23eDsfiCla"},"id":"Td23eDsfiCla"},{"cell_type":"code","source":["# Abschnitt 1: Importe\n","from IPython.display import display, Markdown\n","from langchain.chains import ConversationChain\n","from langchain.memory import ConversationTokenBufferMemory\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import PromptTemplate\n","from langchain.schema import AIMessage"],"metadata":{"id":"iLUCnCgcvKpM"},"execution_count":null,"outputs":[],"id":"iLUCnCgcvKpM"},{"cell_type":"code","source":["# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","MAX_TOKEN_LIMIT = 1024\n","DEFAULT_SYSTEM = \"\"\"\n","Du bist ein kompetenter KI-Assistent mit breitem Fachwissen.\n","\n","Deine Antworten sind:\n","- Klar strukturiert und mit Markdown formatiert\n","- Praxisorientiert und direkt umsetzbar\n","- Basierend auf aktuellem Kenntnisstand\n","- Mit passenden Beispielen versehen\n","- In verst√§ndlicher Sprache formuliert\n","\n","Aktuelle Konversation:\n","{history}\n","Mensch: {input}\n","KI:\n","\"\"\""],"metadata":{"id":"0aBGv_RGwSjh"},"execution_count":null,"outputs":[],"id":"0aBGv_RGwSjh"},{"cell_type":"code","source":["# Abschnitt 3: Funktionen definieren\n","def interact_with_ai(question):\n","    \"\"\" F√ºhrt eine Interaktion mit der KI durch und verarbeitet die Antwort. \"\"\"\n","    # Zeige die Frage an\n","    display(Markdown(f\"**üßë‚Äçü¶± Mensch:**\"))\n","    display(Markdown(f\"{question}\"))\n","\n","    # KI-Antwort abrufen\n","    response = chain.invoke({\"input\": question})\n","\n","    # Antwort je nach Typ verarbeiten\n","    if isinstance(response, dict) and \"response\" in response:\n","        response_text = response[\"response\"]\n","    elif isinstance(response, AIMessage):\n","        response_text = response.content\n","    else:\n","        response_text = str(response)\n","\n","    # Antwort anzeigen\n","    display(Markdown(f\"**ü§ñ KI:**\"))\n","    display(Markdown(f\"{response_text}\"))\n","\n","    # Konversation im Memory speichern\n","    memory.save_context({\"input\": question}, {\"output\": response_text})\n","\n","    # Kein return, um doppelte Ausgabe zu vermeiden\n","    return None"],"metadata":{"id":"nUCbVrPlwT8y"},"execution_count":null,"outputs":[],"id":"nUCbVrPlwT8y"},{"cell_type":"code","source":["# Abschnitt 4: LLM und Kette initialisieren\n","llm = ChatOpenAI(\n","    model_name=MODEL,\n","    temperature=0.0\n",")\n","\n","memory = ConversationTokenBufferMemory(\n","    llm=llm,\n","    max_token_limit=MAX_TOKEN_LIMIT,\n","    return_messages=True\n",")\n","\n","# Prompt Template erstellen\n","prompt = PromptTemplate(\n","    input_variables=[\"history\", \"input\"],\n","    template=DEFAULT_SYSTEM\n",")\n","\n","# Conversation Chain erstellen\n","chain = ConversationChain(\n","    llm=llm,\n","    memory=memory,\n","    prompt=prompt,\n","    verbose=False\n",")"],"metadata":{"id":"aCl7mvN2wZCF"},"execution_count":null,"outputs":[],"id":"aCl7mvN2wZCF"},{"cell_type":"code","source":["# Abschnitt 5: Interaktionen mit dem Modell\n","interact_with_ai(\"Mein Name ist Ralf\")\n","interact_with_ai(\"Warum ist der Himmel blau?\")\n","interact_with_ai(\"Und warum ist er manchmal rot?\")\n","interact_with_ai(\"Wie ist mein Name?\")"],"metadata":{"id":"o2pjNJV0wavF"},"execution_count":null,"outputs":[],"id":"o2pjNJV0wavF"},{"cell_type":"markdown","metadata":{"id":"7b95a6bf"},"source":["Gespeicherten Kontext nach Ausf√ºhrung:"],"id":"7b95a6bf"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9d8ca33b"},"outputs":[],"source":["# Gespeicherter Kontext anzeigen\n","display(Markdown(\"## ‚ú® Gespeicherter Kontext:\"))\n","display(Markdown(\"---\"))\n","\n","# Lade den Speicher\n","memory_ai = memory.load_memory_variables({})[\"history\"]\n","\n","# Erstelle eine formatierte Markdown-Ausgabe f√ºr alle Nachrichten\n","for msg in memory_ai:\n","    markdown_content = f\"**Typ:** {msg.type}\\n\\n**Inhalt:** {msg.content}\"\n","    display(Markdown(markdown_content))"],"id":"9d8ca33b"},{"cell_type":"markdown","metadata":{"id":"4b4bd50e"},"source":["Der Hauptvorteil dieses Ansatzes ist die pr√§zise Kontrolle √ºber die tats√§chliche Token-Anzahl, die ans Modell √ºbergeben wird. Statt willk√ºrlich die letzten N Nachrichten zu speichern, beh√§lt diese Implementierung so viele Nachrichten wie m√∂glich innerhalb des Token-Limits bei.\n","\n"],"id":"4b4bd50e"},{"cell_type":"code","source":["#\n","# NEW besser als Version oben\n","#\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_openai import ChatOpenAI\n","from langchain_core.messages import HumanMessage, AIMessage\n","\n","# Abschnitt 2: Konstanten definieren (bleibt gleich)\n","MODEL = \"gpt-4o-mini\"\n","MAX_TOKEN_LIMIT = 1024\n","SYSTEM_TEMPLATE = \"\"\"\n","Du bist ein kompetenter KI-Assistent mit breitem Fachwissen.\n","\n","Deine Antworten sind:\n","- Klar strukturiert und mit Markdown formatiert\n","- Praxisorientiert und direkt umsetzbar\n","- Basierend auf aktuellem Kenntnisstand\n","- Mit passenden Beispielen versehen\n","- In verst√§ndlicher Sprache formuliert\n","\"\"\"\n","\n","# Abschnitt 4: LLM und Memory initialisieren\n","llm = ChatOpenAI(\n","    model_name=MODEL,\n","    temperature=0.0\n",")\n","\n","# Memory mit MessagesPlaceholder statt ConversationTokenBufferMemory\n","from langchain.memory import ConversationTokenBufferMemory\n","\n","memory = ConversationTokenBufferMemory(\n","    llm=llm,\n","    max_token_limit=MAX_TOKEN_LIMIT,\n","    return_messages=True\n",")\n","\n","# Prompt mit ChatPromptTemplate statt PromptTemplate\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", SYSTEM_TEMPLATE),\n","    MessagesPlaceholder(variable_name=\"history\"),\n","    (\"human\", \"{input}\")\n","])\n","\n","# LCEL Chain statt ConversationChain\n","def load_memory(_):\n","    return memory.chat_memory.messages\n","\n","def save_memory(input_and_output):\n","    user_input = input_and_output[\"input\"]\n","    ai_output = input_and_output[\"output\"]\n","    memory.save_context({\"input\": user_input}, {\"output\": ai_output})\n","    return ai_output\n","\n","# Die LCEL Chain-Definition\n","chain = (\n","    RunnablePassthrough.assign(history=load_memory)\n","    | prompt\n","    | llm\n","    | StrOutputParser()  # Garantiert String-Output\n","    | save_memory\n",")\n","\n","# Vereinfachte interact_with_ai Funktion\n","def interact_with_ai(question):\n","    \"\"\" F√ºhrt eine Interaktion mit der KI durch und verarbeitet die Antwort. \"\"\"\n","    # Zeige die Frage an\n","    display(Markdown(f\"**üßë‚Äçü¶± Mensch:**\"))\n","    display(Markdown(f\"{question}\"))\n","\n","    # KI-Antwort abrufen - immer als String dank StrOutputParser\n","    response_text = chain.invoke({\"input\": question})\n","\n","    # Antwort anzeigen\n","    display(Markdown(f\"**ü§ñ KI:**\"))\n","    display(Markdown(f\"{response_text}\"))\n","\n","    # Memory wird bereits in der Chain gespeichert (save_memory Funktion)\n","\n","    # Kein return, um doppelte Ausgabe zu vermeiden\n","    return None"],"metadata":{"id":"ZkJ9LKYCt-RN"},"id":"ZkJ9LKYCt-RN","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4 | Zusammenfassung\n","---"],"metadata":{"id":"3NVtW3sKiGfB"},"id":"3NVtW3sKiGfB"},{"cell_type":"markdown","source":["\n","\n","\n","Die bisher betrachteten Ans√§tze haben entweder die vollst√§ndige Historie oder einen Teil davon bewahrt. Eine Alternative ist, l√§ngere Konversationen zusammenzufassen, um wichtige Informationen zu behalten, ohne die Token-Limits zu √ºberschreiten.\n","\n","LangChain's `ConversationSummaryMemory` speichert und aktualisiert eine Zusammenfassung der Konversation, anstatt √§ltere Chatverl√§ufe zu vergessen. Dies erm√∂glicht die Bewahrung wichtiger Informationen und h√§lt den Speicher effizient."],"metadata":{"id":"SkwJX0VPiHpK"},"id":"SkwJX0VPiHpK"},{"cell_type":"code","execution_count":null,"metadata":{"id":"47a38211"},"outputs":[],"source":["# Abschnitt 1: Importe\n","from IPython.display import display, Markdown\n","\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.messages import HumanMessage, AIMessage\n","from langchain_openai import ChatOpenAI\n","from langchain.memory import ConversationSummaryMemory\n","from langchain_core.runnables import RunnablePassthrough\n","\n","# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","\n","conversations = [\n","    \"Mein Name ist Ralf\",\n","    \"Warum ist der Himmel blau?\",\n","    \"Und warum ist er manchmal rot?\",\n","    \"Wie ist mein Name?\"\n","]\n","\n","# Abschnitt 3: Funktionen definieren\n","def interact_with_ai(question):\n","    \"\"\"F√ºhrt eine einzelne Interaktion mit der KI durch und verarbeitet die Antwort.\"\"\"\n","    display(Markdown(f\"**üßë‚Äçü¶± Mensch:**\"))\n","    display(Markdown(f\"{question}\"))\n","\n","    response = chain.invoke({\"input\": question})\n","\n","    # Antwort extrahieren\n","    if isinstance(response, dict) and \"response\" in response:\n","        response_text = response[\"response\"]\n","    elif isinstance(response, AIMessage):\n","        response_text = response.content\n","    else:\n","        response_text = str(response)\n","\n","    display(Markdown(f\"**ü§ñ KI:**\"))\n","    display(Markdown(f\"{response_text}\"))\n","    print()\n","\n","    # Speichere die Konversation im Memory\n","    memory.save_context({\"input\": question}, {\"output\": response_text})\n","\n","    return response_text\n","\n","# Abschnitt 4: Chain und Memory initialisieren\n","# LLM initialisieren\n","llm = ChatOpenAI(\n","    temperature=0,\n","    model_name=MODEL\n",")\n","\n","# Memory initialisieren\n","memory = ConversationSummaryMemory(\n","    llm=llm,\n","    return_messages=True,\n","    max_token_limit=2000\n",")\n","\n","# Prompt Template erstellen\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent. Nutze vorherige Konversationen f√ºr kontextbezogene Antworten.\"),\n","    MessagesPlaceholder(variable_name=\"history\"),\n","    (\"human\", \"{input}\")\n","])\n","\n","# Chain erstellen\n","chain = (\n","    RunnablePassthrough.assign(\n","        history=lambda x: memory.load_memory_variables({})[\"history\"]\n","    )\n","    | prompt\n","    | llm\n",")\n","\n","# Abschnitt 5: Nachrichten verarbeiten und Ergebnisse anzeigen\n","for user_input in conversations:\n","    interact_with_ai(user_input)"],"id":"47a38211"},{"cell_type":"markdown","metadata":{"id":"59b1182e"},"source":["Nach der Ausf√ºhrung k√∂nnen wir die erzeugte Zusammenfassung betrachten:"],"id":"59b1182e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"acff52ee"},"outputs":[],"source":["# Ausgabe der Zusammenfassung mit display(Markdown())\n","display(Markdown(\"## Aktuelle Zusammenfassung:\"))\n","summary_ai = memory.load_memory_variables({})[\"history\"]\n","display(Markdown(f\"{summary_ai[0].content}\"))"],"id":"acff52ee"},{"cell_type":"markdown","metadata":{"id":"289eb07e"},"source":["Dieser Ansatz ist besonders n√ºtzlich f√ºr l√§ngere Konversationen, da er die wichtigsten Informationen extrahiert und kompakt beh√§lt, statt nur die letzten N Nachrichten. Dadurch bleibt der Kontext erhalten, ohne das Token-Limit zu sprengen."],"id":"289eb07e"},{"cell_type":"markdown","source":["# 5 | Nachrichten-Limit\n","---\n"],"metadata":{"id":"jUrNYwNah2W0"},"id":"jUrNYwNah2W0"},{"cell_type":"markdown","source":["\n","\n","Manchmal m√∂chten wir die Anzahl der Nachrichten begrenzen, die ein Modell als Kontext erh√§lt, anstatt die komplette Historie zu verwenden. Dies kann sinnvoll sein, wenn:\n","\n","1. Der j√ºngste Kontext am relevantesten ist\n","2. Wir Tokens sparen wollen\n","3. Wir eine fokussiertere Antwort ohne zu viel Ablenkung durch √§ltere Themen w√ºnschen\n","\n"],"metadata":{"id":"d523zHw0D2-B"},"id":"d523zHw0D2-B"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","ConversationBufferWindowMemory\n","</font></p>"],"metadata":{"id":"RkAAy6L5h4eL"},"id":"RkAAy6L5h4eL"},{"cell_type":"code","source":["# Abschnitt 0: Installation und API-Key\n","!uv pip install --system --prerelease allow -q langchain_community langchain_openai"],"metadata":{"id":"rG1ndc8NfMea"},"id":"rG1ndc8NfMea","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"cd0358c8","metadata":{"id":"cd0358c8"},"outputs":[],"source":["# Abschnitt 1: Importe\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain.memory import ConversationBufferWindowMemory\n","from langchain.schema import SystemMessage, HumanMessage, AIMessage\n","from IPython.display import display, Markdown\n","from operator import itemgetter"]},{"cell_type":"code","source":["# Abschnitt 2: Konstanten\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATURE = 0.0\n","DEFAULT_SYSTEM = \"Du bist ein hilfsbereiter KI-Assistent.\"\n","MAX_HISTORY = 3  # Anzahl der zu speichernden Nachrichtenpaare\n","\n","TEST_QUESTIONS = [\n","    \"Mein Lieblingsgericht ist Lasagne. Antworte kurz.\",\n","    \"Wie viele Tage hat eine Woche? Antworte kurz.\",\n","    \"Wie viele Tage hat ein Monat? Antworte kurz.\",\n","    \"Wie viele Tage hat ein Jahr? Antworte kurz.\",\n","    \"Was ist mein Lieblingsgericht?\"\n","]"],"metadata":{"id":"O80lYM50qb8l"},"id":"O80lYM50qb8l","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 3: Chat-Komponenten initialisieren\n","llm = ChatOpenAI(temperature=TEMPERATURE, model=MODEL)\n","\n","# Memory mit k=3 f√ºr Fenstergr√∂√üe\n","memory = ConversationBufferWindowMemory(\n","    k=MAX_HISTORY,\n","    return_messages=True,\n","    memory_key=\"history\"\n",")\n","\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", DEFAULT_SYSTEM),\n","    MessagesPlaceholder(variable_name=\"history\"),\n","    (\"human\", \"{input}\")\n","])\n","\n","def get_chat_history(input_dict: dict) -> dict:\n","    \"\"\"L√§dt die Chat-Historie aus dem Memory.\"\"\"\n","    memory_vars = memory.load_memory_variables({})\n","    return {\n","        \"history\": memory_vars[\"history\"],\n","        \"input\": input_dict[\"input\"]\n","    }\n","\n","# Chain mit Pipe erstellen\n","chain = get_chat_history | prompt | llm"],"metadata":{"id":"zivpRnk8qdSW"},"id":"zivpRnk8qdSW","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 4: Funktionen definieren\n","def interact_with_ai(question):\n","    \"\"\"F√ºhrt eine Interaktion mit der KI durch und verarbeitet die Antwort.\"\"\"\n","    # Ausgabe der Frage\n","    display(Markdown(f\"\\nüßë‚Äçü¶± Mensch:\"))\n","    display(Markdown(question))\n","    display(Markdown(\"\\nü§ñ KI: \"))\n","\n","    # KI-Antwort einholen\n","    response = chain.invoke({\"input\": question})\n","\n","    # Antworttext extrahieren\n","    response_text = response.content if isinstance(response, AIMessage) else str(response)\n","\n","    # Formatierte Ausgabe erstellen\n","    display(Markdown(response_text))\n","    print()\n","\n","    # Konversation im Memory speichern\n","    memory.save_context({\"input\": question}, {\"output\": response_text})\n","\n","    return response_text"],"metadata":{"id":"RJaoO8Jyqf9C"},"id":"RJaoO8Jyqf9C","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 5: Hauptprogramm\n","\n","for question in TEST_QUESTIONS:\n","    interact_with_ai(question)"],"metadata":{"id":"JJ83yHdTqh9L"},"id":"JJ83yHdTqh9L","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"6430a41d","metadata":{"id":"6430a41d"},"source":["Nach der Ausf√ºhrung k√∂nnen wir den gespeicherten Kontext untersuchen:"]},{"cell_type":"code","execution_count":null,"id":"2d0083b7","metadata":{"id":"2d0083b7"},"outputs":[],"source":["print(\"\\nGespeicherter Kontext:\")\n","memory_ai = memory.load_memory_variables({})[\"history\"]\n","for i in range(len(memory_ai)):\n","    print(memory_ai[i].content)"]},{"cell_type":"markdown","id":"083e7fc3","metadata":{"id":"083e7fc3"},"source":["Der Parameter `k` in `ConversationBufferWindowMemory` steuert, wie viele Nachrichten-Paare (Frage und Antwort) dem LLM als Kontext √ºbergeben werden. Mit `k=3` werden nur die letzten drei Konversationsrunden ber√ºcksichtigt, auch wenn die gesamte Konversation l√§nger ist.\n","\n"]},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Memory-Funktion mit LCEL\n","</font></p>"],"metadata":{"id":"sWquJY0Dh9P9"},"id":"sWquJY0Dh9P9"},{"cell_type":"markdown","source":["Alternativ k√∂nnen wir auch unsere eigene Fenster-Implementierung erstellen:"],"metadata":{"id":"yuvIAYB9h-ef"},"id":"yuvIAYB9h-ef"},{"cell_type":"code","execution_count":null,"id":"820b1c1a","metadata":{"id":"820b1c1a"},"outputs":[],"source":["# Abschnitt 1: Importe\n","from IPython.display import display, Markdown\n","\n","from langchain_core.runnables import RunnableLambda, RunnableMap\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import PromptTemplate\n","from langchain.schema import AIMessage"]},{"cell_type":"code","source":["# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","MAX_HISTORY = 3  # Maximale Anzahl an gespeicherten Konversationsbeitr√§gen\n","\n","DEFAULT_SYSTEM = \"\"\"Du bist ein kompetenter KI-Assistent mit breitem Fachwissen.\n","\n","Deine Antworten sind:\n","- Klar strukturiert und mit Markdown formatiert\n","- Praxisorientiert und direkt umsetzbar\n","- Basierend auf aktuellem Kenntnisstand\n","- Mit passenden Beispielen versehen\n","- In verst√§ndlicher Sprache formuliert\n","\n","Aktuelle Konversation:\n","{history}\n","Mensch: {input}\n","KI:\"\"\"\n","\n","TEST_QUESTIONS = [\n","    \"Mein Lieblingsgericht ist Lasagne. Antworte kurz.\",\n","    \"Wie viele Tage hat eine Woche? Antworte kurz.\",\n","    \"Wie viele Tage hat ein Monat? Antworte kurz.\",\n","    \"Wie viele Tage hat ein Jahr? Antworte kurz.\",\n","    \"Was ist mein Lieblingsgericht?\"\n","]"],"metadata":{"id":"zfaTDBiDq6je"},"id":"zfaTDBiDq6je","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 3: Funktionen\n","# Speicher f√ºr Konversationshistorie manuell verwalten\n","conversation_history: list[str] = []\n","\n","def update_memory(question: str, response: str) -> list[str]:\n","    \"\"\"F√ºgt neue Konversationsbeitr√§ge zur Historie hinzu.\"\"\"\n","    global conversation_history\n","    conversation_history.append(f\"Mensch: {question}\\nKI: {response}\")\n","\n","    # Begrenze die Historie auf die letzten N Eintr√§ge\n","    if len(conversation_history) > MAX_HISTORY:\n","        conversation_history = conversation_history[-MAX_HISTORY:]\n","\n","    return conversation_history\n","\n","def format_history(history: list[str]) -> str:\n","    \"\"\"Formatiert die Historie f√ºr die Eingabe an das Modell.\"\"\"\n","    return \"\\n\".join(history)\n","\n","# Interaktionen mit dem Modell\n","def interact_with_ai(question):\n","    \"\"\"F√ºhrt eine einzelne Interaktion mit der KI durch.\"\"\"\n","    # Ausgabe der Frage\n","    display(Markdown(f\"\\nüßë‚Äçü¶± Mensch:\"))\n","    display(Markdown(question))\n","    display(Markdown(\"\\nü§ñ KI: \"))\n","\n","    # KI-Antwort einholen\n","    response = chain.invoke({\"input\": question})\n","\n","    # Antworttext extrahieren\n","    response_text = response.content if isinstance(response, AIMessage) else str(response)\n","\n","    # Formatierte Ausgabe erstellen\n","    display(Markdown(response_text))\n","    print()\n","\n","    # Speicher aktualisieren\n","    update_memory(question, response_text)\n","\n","    return response_text"],"metadata":{"id":"mKJz2wOcq-wE"},"id":"mKJz2wOcq-wE","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 4: LLM und LCEL-Chain initialisieren\n","llm = ChatOpenAI(\n","    model_name=MODEL,\n","    temperature=0.0\n",")\n","\n","prompt = PromptTemplate(\n","    input_variables=[\"history\", \"input\"],\n","    template=DEFAULT_SYSTEM\n",")\n","\n","# RunnableMap f√ºr Input-Transformation\n","prepare_input = RunnableLambda(lambda x: {\n","    \"history\": format_history(conversation_history),\n","    \"input\": x[\"input\"]\n","})\n","\n","# LCEL-Chain erstellen\n","chain = prepare_input | prompt | llm"],"metadata":{"id":"Rr1sdAOjrCf1"},"id":"Rr1sdAOjrCf1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 5: Hauptprogramm\n","\n","for question in TEST_QUESTIONS:\n","    interact_with_ai(question)"],"metadata":{"id":"yHQuvlq4rLnL"},"id":"yHQuvlq4rLnL","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"3e6f7a0f","metadata":{"id":"3e6f7a0f"},"source":["Diese Implementierung nutzt eine einfache Liste, um die letzten N Konversationspaare zu speichern. Sie bietet mehr Kontrolle, erfordert jedoch auch mehr manuellen Code.\n","\n"]},{"cell_type":"markdown","source":["# 6 | Manuelle Verwaltung\n","---\n"],"metadata":{"id":"svg9v5Oahoh2"},"id":"svg9v5Oahoh2"},{"cell_type":"markdown","source":["\n","\n","Im ersten Beispiel werden wir einen Chatbot schrittweise mit einer manuellen Konversationsverwaltung aufbauen. Dieser Ansatz bietet die volle Kontrolle, indem wir die Konversationshistorie explizit verwalten und weitergeben."],"metadata":{"id":"DLm3AoGxjbL_"},"id":"DLm3AoGxjbL_"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2e3ef096"},"outputs":[],"source":["# Abschnitt 1: Importe\n","from IPython.display import display, Markdown\n","\n","from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough, RunnableLambda"],"id":"2e3ef096"},{"cell_type":"code","source":["# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATURE = 0.0\n","MEMORY_WINDOW = 100 # hoher Wert, praktisch keine Beschr√§nkung der Anzahl\n","\n","DEFAULT_SYSTEM = \"\"\"\n","Du bist ein kompetenter KI-Assistent mit breitem Fachwissen.\n","\n","Deine Antworten sind:\n","- Klar strukturiert und mit Markdown formatiert\n","- Praxisorientiert und direkt umsetzbar\n","- Basierend auf aktuellem Kenntnisstand\n","- Mit passenden Beispielen versehen\n","- In verst√§ndlicher Sprache formuliert\n","\n","Bei deiner Arbeit:\n","- Analysierst du Fragen sorgf√§ltig\n","- Gibst pr√§zise und relevante Antworten\n","- Erkennst den Kontext der Anfrage\n","- Bietest bei Bedarf weiterf√ºhrende Informationen\n","- Bleibst sachlich und neutral\n","\n","Formatiere alle Antworten in Markdown f√ºr optimale Lesbarkeit.\n","\"\"\"\n","\n","# Testfragen\n","test_questions = [\n","    \"Mein Lieblingsgericht ist Lasagne. Antworte kurz.\",\n","    \"Wie viele Tage hat eine Woche? Antworte kurz.\",\n","    \"Wie viele Tage hat ein Monat? Antworte kurz.\",\n","    \"Wie viele Tage hat ein Jahr? Antworte kurz.\",\n","    \"Was ist mein Lieblingsgericht?\"\n","]"],"metadata":{"id":"5q4rAMhfk-mi"},"execution_count":null,"outputs":[],"id":"5q4rAMhfk-mi"},{"cell_type":"code","source":["# Abschnitt 3: Chat-Komponenten initialisieren\n","llm = ChatOpenAI(\n","    temperature=TEMPERATURE,\n","    model=MODEL\n",")\n","\n","chat_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"human\", \"{question}\")\n","])\n","\n","def create_chain_input(input_dict: dict[str]) -> dict[str]:\n","    \"\"\"Erstellt den Input f√ºr die Chain mit History.\"\"\"\n","    return {\n","        \"question\": input_dict[\"question\"],\n","        \"system_prompt\": DEFAULT_SYSTEM,\n","        \"chat_history\": input_dict[\"history\"]\n","    }\n","\n","# LCEL Chain mit History erstellen\n","chat_chain = (\n","    RunnableLambda(create_chain_input)\n","    | chat_prompt\n","    | llm\n","    | StrOutputParser()\n",")"],"metadata":{"id":"j0cLXpxqlAYg"},"execution_count":null,"outputs":[],"id":"j0cLXpxqlAYg"},{"cell_type":"code","source":["# Abschnitt 4: Funktionen definieren\n","def format_history(history: list) -> list:\n","    \"\"\"  Formatiert den Konversationsverlauf. \"\"\"\n","    return history[-MEMORY_WINDOW:]\n","\n","def interact_with_ai(question: str, history: list) -> list:\n","    \"\"\" Verarbeitet eine Konversation mit der KI. \"\"\"\n","    # Nutzereingabe anzeigen\n","    display(Markdown(f\"### üßë‚Äçü¶± Mensch:\"))\n","    display(Markdown(question))\n","\n","    # Antwort mit History generieren\n","    response = chat_chain.invoke({\n","        \"question\": question,\n","        \"history\": format_history(history)\n","    })\n","\n","    # Antwort anzeigen\n","    display(Markdown(f\"### ü§ñ KI:\"))\n","    display(Markdown(response))\n","    print()\n","\n","    # Konversationsverlauf aktualisieren\n","    return history + [\n","        HumanMessage(content=question),\n","        AIMessage(content=response)\n","    ]"],"metadata":{"id":"3_i1y_1DlmpT"},"execution_count":null,"outputs":[],"id":"3_i1y_1DlmpT"},{"cell_type":"code","source":["# Abschnitt 5: Hauptprogramm\n","\n","# Konversationsverlauf initialisieren\n","history = [SystemMessage(content=DEFAULT_SYSTEM)]\n","\n","# Testfragen durchlaufen\n","for question in test_questions:\n","    history = interact_with_ai(question, history)"],"metadata":{"id":"VcpkmeQElFCT"},"execution_count":null,"outputs":[],"id":"VcpkmeQElFCT"},{"cell_type":"code","source":["#\n","# NEW f√ºr Kurs-Update - ersetzt die Version oben!\n","#\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.messages import HumanMessage, AIMessage\n","from IPython.display import display, Markdown\n","\n","# Konstanten\n","MODEL = \"gpt-4o-mini\"\n","DEFAULT_SYSTEM = \"Du bist ein freundlicher, sachkundiger AI-Assistent.\"\n","MEMORY_LIMIT = 3  # Begrenzt die History auf die letzten 3 Nachrichtenpaare\n","\n","# Chat-Modell und Prompt in einem Schritt\n","chat_model = ChatOpenAI(temperature=0.7, model=MODEL)\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", DEFAULT_SYSTEM),\n","    MessagesPlaceholder(variable_name=\"history\"),\n","    (\"human\", \"{question}\")\n","])\n","\n","# Kompakte Chat-Chain\n","chat_chain = prompt | chat_model\n","\n","def chat(question, history=None):\n","    \"\"\"Einfache Konversationsfunktion mit Anzeige und begrenzter History-Verwaltung.\"\"\"\n","    if history is None:\n","        history = []\n","\n","    # History auf die letzten MEMORY_LIMIT Nachrichten begrenzen\n","    # Da jedes Paar aus 2 Messages besteht (Human + AI), multiplizieren wir mit 2\n","    if len(history) > MEMORY_LIMIT * 2:\n","        history = history[-(MEMORY_LIMIT * 2):]\n","\n","    # Nutzereingabe anzeigen\n","    display(Markdown(f\"### üßë‚Äçü¶± Mensch:\\n{question}\"))\n","\n","    # Antwort generieren\n","    response = chat_chain.invoke({\"question\": question, \"history\": history})\n","\n","    # Antwort anzeigen\n","    display(Markdown(f\"### ü§ñ KI:\\n{response.content}\"))\n","\n","    # Aktualisierte History zur√ºckgeben\n","    return history + [HumanMessage(content=question), response]\n","\n","# Beispielverwendung\n","history = []\n","for frage in [\"Frage 1\", \"Frage 2\", \"Frage 3\", \"Frage 4\", \"Frage 5\"]:\n","    history = chat(frage, history)\n","    print(f\"History-L√§nge: {len(history)} Nachrichten\")"],"metadata":{"id":"v3DGl-PQdEuC"},"execution_count":null,"outputs":[],"id":"v3DGl-PQdEuC"},{"cell_type":"markdown","metadata":{"id":"92c7cbac"},"source":["Bei diesem Ansatz speichern wir die gesamte Konversationshistorie in einer Liste und reichen sie bei jeder neuen Anfrage mit. Vorteil ist die volle Kontrolle und der Zugriff auf den gesamten Kontext. Der Nachteil: bei langen Unterhaltungen kann der Kontext die Token-Limits des Modells √ºberschreiten.\n","\n"],"id":"92c7cbac"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Framework-Komponenten\n","</font></p>"],"metadata":{"id":"ZLBcxiHdhux9"},"id":"ZLBcxiHdhux9"},{"cell_type":"markdown","source":["\n","\n","Als Alternative zur manuellen Verwaltung bietet LangChain spezialisierte Komponenten zur Konversationsverwaltung. Diese automatisieren viele der manuellen Schritte und bieten eine strukturiertere Herangehensweise."],"metadata":{"id":"sBGRhPwIhyxq"},"id":"sBGRhPwIhyxq"},{"cell_type":"code","execution_count":null,"metadata":{"id":"7182fbf8"},"outputs":[],"source":["# Abschnitt 1: Importe\n","from IPython.display import display, Markdown\n","from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_community.chat_message_histories import ChatMessageHistory\n","from langchain_core.runnables.history import RunnableWithMessageHistory"],"id":"7182fbf8"},{"cell_type":"code","source":["# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATURE = 0.0\n","DEFAULT_SYSTEM = \"\"\"\n","Du bist ein kompetenter KI-Assistent mit breitem Fachwissen.\n","Deine Antworten sind:\n","- Klar strukturiert und mit Markdown formatiert\n","- Praxisorientiert und direkt umsetzbar\n","- Basierend auf aktuellem Kenntnisstand\n","- Mit passenden Beispielen versehen\n","- In verst√§ndlicher Sprache formuliert\n","Bei deiner Arbeit:\n","- Analysierst du Fragen sorgf√§ltig\n","- Gibst pr√§zise und relevante Antworten\n","- Erkennst den Kontext der Anfrage\n","- Bietest bei Bedarf weiterf√ºhrende Informationen\n","- Bleibst sachlich und neutral\n","Formatiere alle Antworten in Markdown f√ºr optimale Lesbarkeit.\n","\"\"\""],"metadata":{"id":"gdq5nmgQm6UR"},"execution_count":null,"outputs":[],"id":"gdq5nmgQm6UR"},{"cell_type":"code","source":["# Abschnitt 3: Historienverwaltung und Chat-Komponenten initialisieren\n","# Dictionary f√ºr Sitzungs-Historien\n","session_histories = {}\n","\n","# Hilfsfunktion zur Verwaltung der ChatMessageHistory\n","def get_session_history(session_id):\n","    if session_id not in session_histories:\n","        # Erstelle eine neue History, wenn keine existiert\n","        history = ChatMessageHistory()\n","        # F√ºge die System-Nachricht zur History hinzu\n","        history.add_message(SystemMessage(content=DEFAULT_SYSTEM))\n","        session_histories[session_id] = history\n","    return session_histories[session_id]\n","\n","# LLM-Instanz initialisieren\n","llm = ChatOpenAI(\n","    temperature=TEMPERATURE,\n","    model=MODEL\n",")\n","\n","# Chat-Prompt mit Platzhalter f√ºr die Historie\n","chat_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", DEFAULT_SYSTEM),\n","    MessagesPlaceholder(variable_name=\"history\"),\n","    (\"human\", \"{question}\")\n","])\n","\n","# Basis-Chain ohne Historie erstellen\n","chain = chat_prompt | llm | StrOutputParser()\n","\n","# Chain mit Message-History-Funktionalit√§t erweitern\n","chain_with_history = RunnableWithMessageHistory(\n","    chain,\n","    get_session_history,  # Unsere verbesserte History-Management-Funktion\n","    input_messages_key=\"question\",\n","    history_messages_key=\"history\"\n",")"],"metadata":{"id":"1RSEAlLkm8SI"},"execution_count":null,"outputs":[],"id":"1RSEAlLkm8SI"},{"cell_type":"code","source":["# Abschnitt 4: Funktionen definieren\n","def interact_with_ai(question: str, session_id: str = \"kontinuierliche_session\") -> str:\n","    \"\"\"Verarbeitet eine Konversation und gibt die Antwort zur√ºck.\"\"\"\n","    # Nutzereingabe anzeigen\n","    display(Markdown(f\"### üßë‚Äçü¶± Mensch:\"))\n","    display(Markdown(question))\n","\n","    # Antwort mit History generieren\n","    response = chain_with_history.invoke(\n","        {\"question\": question},\n","        config={\"configurable\": {\"session_id\": session_id}}\n","    )\n","\n","    # Debug-Ausgabe: Zeige die aktuelle History an\n","    history = session_histories[session_id]\n","\n","    # Antwort anzeigen\n","    display(Markdown(f\"### ü§ñ KI:\"))\n","    display(Markdown(response))\n","    print()\n","\n","    return response"],"metadata":{"id":"UFv9qO_um-K5"},"execution_count":null,"outputs":[],"id":"UFv9qO_um-K5"},{"cell_type":"code","source":["# Abschnitt 5: Hauptprogramm\n","# Konstante Session-ID definieren\n","KONSTANTE_SESSION_ID = \"kontinuierliche_session\"\n","\n","# Testfragen\n","test_questions = [\n","    \"Mein Lieblingsgericht ist Lasagne. Antworte kurz.\",\n","    \"Wie viele Tage hat eine Woche? Antworte kurz.\",\n","    \"Wie viele Tage hat ein Monat? Antworte kurz.\",\n","    \"Wie viele Tage hat ein Jahr? Antworte kurz.\",\n","    \"Was ist mein Lieblingsgericht?\"\n","]\n","\n","# Testfragen durchlaufen mit konstanter Session-ID\n","for question in test_questions:\n","    interact_with_ai(question, session_id=KONSTANTE_SESSION_ID)"],"metadata":{"id":"ME-KIsRgnAFc"},"execution_count":null,"outputs":[],"id":"ME-KIsRgnAFc"},{"cell_type":"code","source":["# Optional: Ausgabe der vollst√§ndigen Konversation am Ende\n","def zeige_konversation(session_id):\n","    \"\"\" Zeigt die vollst√§ndige Konversationshistorie f√ºr eine bestimmte Session an. \"\"\"\n","    markdown_text = \"## Vollst√§ndige Konversation\\n\"\n","\n","    if session_id in session_histories:\n","        for i, msg in enumerate(session_histories[session_id].messages):\n","            # Bestimme die Rolle der Nachricht\n","            if isinstance(msg, SystemMessage):\n","                rolle = \"‚öôÔ∏è System\"\n","            elif isinstance(msg, HumanMessage):\n","                rolle = \"üßë‚Äçü¶± Mensch\"\n","            else:\n","                rolle = \"ü§ñ KI\"\n","\n","            # K√ºrze den Inhalt, wenn er zu lang ist\n","            inhalt = msg.content\n","            if len(inhalt) > 50:\n","                inhalt = f\"{inhalt[:50]}...\"\n","\n","            # F√ºge Nachricht zum Markdown-Text hinzu\n","            markdown_text += f\"**{i}.** [{rolle}]: {inhalt}\\n\\n\"\n","    else:\n","        markdown_text += \"*Keine Konversationshistorie f√ºr diese Session gefunden.*\\n\\n\"\n","\n","    # Zeige den Markdown-Text an\n","    display(Markdown(markdown_text))\n","\n","# Beispielaufruf\n","zeige_konversation(KONSTANTE_SESSION_ID)"],"metadata":{"id":"5CH5mnNInP2U"},"execution_count":null,"outputs":[],"id":"5CH5mnNInP2U"},{"cell_type":"markdown","metadata":{"id":"01eb41f9"},"source":["Dieser Ansatz nutzt `RunnableWithMessageHistory` und `ChatMessageHistory` von LangChain, um Konversationen zu verwalten. Vorteile sind die einfachere Handhabung und die M√∂glichkeit, mehrere Konversationen mit unterschiedlichen Session-IDs zu verwalten.\n","\n"],"id":"01eb41f9"},{"cell_type":"markdown","metadata":{"id":"5cfbf26a"},"source":["# A | Aufgaben\n","---\n"],"id":"5cfbf26a"},{"cell_type":"markdown","source":["\n","\n","Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen.\n"],"metadata":{"id":"pwmSuPsGiQp6"},"id":"pwmSuPsGiQp6"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","KI-gest√ºtztes Notizbuch mit automatischer Kategorisierung\n","</font></p>\n","\n","\n","Entwickeln Sie ein KI-Notizbuch, das Eingaben von Nutzern speichert, automatisch kategorisiert (z. B. \"Technologie\", \"Privat\", \"Aufgaben\") und kontextbezogene Vorschl√§ge liefert. Verwenden Sie dazu ein LangChain Memory-Konzept, z.B. ConversationBufferMemory, f√ºr eine effiziente Verwaltung.\n"],"metadata":{"id":"1UViO3SJiSXy"},"id":"1UViO3SJiSXy"},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Chatbot: Nachrichten- vs. Summary-Speicher\n","</font></p>\n","\n","\n","Erstellen Sie einen einfachen Chatbot mit LangChain, der zwischen Nachrichten- und Summary-Speicher unterscheidet. Analysieren Sie, wie sich die Speichertypen auf die Qualit√§t der Antworten auswirken.\n","\n"],"metadata":{"id":"T6HXRFbXjIVL"},"id":"T6HXRFbXjIVL"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Automatische Protokollerstellung f√ºr Meetings\n","</font></p>\n","\n","\n","Entwickeln Sie ein System, das Gespr√§chsverl√§ufe speichert und automatisch eine zusammenfassende Chat-Notiz erstellt. Nutzen Sie z.B. ConversationSummaryMemory, um die wichtigsten Punkte aus langen Gespr√§chen zu extrahieren.\n","\n"],"metadata":{"id":"9eIl83Z-CqT8"},"id":"9eIl83Z-CqT8"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Virtuelle Assistenten mit eigenem Ged√§chtnis\n","</font></p>\n","\n","\n","Eine Datei wird bereitgestellt, die ein Gespr√§ch zwischen zwei virtuellen Assistenten enth√§lt. Jede Antwort soll aus genau einem Satz bestehen. Die Datei ist folgenderma√üen aufgebaut:\n","\n","|Assistent|Eingabe|\n","|---|---|\n","|botA|Ich hei√üe Max.|\n","|botB|Mein Name ist Emma.|\n","|botA|Ich wohne in Berlin.|\n","|botB|Ich lebe in Hamburg.|\n","|botA|Wo wohne ich und wie hei√üe ich?|\n","|botB|Wo lebe ich?|\n","\n","**Aufgabe**  \n","Schreiben Sie ein Programm, das zwei virtuelle Assistenten simuliert, die jeweils ein eigenes Ged√§chtnis haben. Die Eingaben aus der Datei werden an den jeweiligen Assistenten geschickt, und ihre Antworten sollen in einer neuen Datei gespeichert werden.\n","\n","**Erwartete Ausgabe**  \n","Die erwartete Ausgabe k√∂nnte wie folgt aussehen:\n","\n","|Assistent|Antwort|\n","|---|---|\n","|botA|Hallo Max.|\n","|botB|Hallo Emma.|\n","|botA|Danke f√ºr diese Information.|\n","|botB|Danke f√ºr diese Information.|\n","|botA|Hallo Max, du wohnst in Berlin und dein Name ist Max.|\n","|botB|Hallo Emma, du lebst in Hamburg.|\n","\n","**Hinweise**\n","\n","- Die Antworten m√ºssen nicht exakt mit der Beispielausgabe √ºbereinstimmen, sollten aber sinngem√§√ü √§hnlich sein.\n","- Jeder Assistent hat ein eigenes Ged√§chtnis und sollte sich nur an seine eigenen Informationen erinnern.\n","- Das Programm soll die Antworten in einer neuen Datei im gleichen Tabellenformat speichern."],"metadata":{"id":"CwYae6YZCsto"},"id":"CwYae6YZCsto"}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"colab":{"provenance":[],"collapsed_sections":["ncw0vI8MAJrt","gNaHt9iuAfzs","svg9v5Oahoh2"]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}
{"cells":[{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)"],"metadata":{"id":"9JKGDENeDeB0"},"id":"9JKGDENeDeB0"},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b> Chat & Memory </b></font> </br></p>\n","\n","---"],"metadata":{"id":"bxo8g0twDZK5"},"id":"bxo8g0twDZK5"},{"cell_type":"code","source":["#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n","!uv pip install --system -q git+https://github.com/ralf-42/Python_Modules\n","from genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages\n","setup_api_keys(['OPENAI_API_KEY', 'HF_TOKEN'], create_globals=False)\n","print()\n","check_environment()\n","print()\n","get_ipinfo()\n","# Bei Bedarf: Trennen zwischen Installationsname () und Importname (f√ºr Python) beide Angaben in Klammern\n","# install_packages([('markitdown[all]', 'markitdown'), 'langchain_chroma', ]"],"metadata":{"id":"YQaYpJJwoxVT"},"id":"YQaYpJJwoxVT","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"fe8a8ba4","metadata":{"id":"fe8a8ba4"},"source":["# 1 | Intro\n","---\n"]},{"cell_type":"markdown","source":["Ein Gespr√§ch mit jemandem zu f√ºhren, der nach jedem Satz vergisst, was zuvor gesagt wurde, w√§re √§u√üerst frustrierend. Genau dieses Problem l√∂st **Memory** in der K√ºnstlichen Intelligenz.\n","\n","**Warum braucht KI ein Ged√§chtnis?**\n","\n","Large Language Models wie GPT sind von Natur aus **zustandslos** ‚Äì sie verf√ºgen √ºber kein eingebautes Ged√§chtnis. Jede Anfrage wird isoliert verarbeitet, ohne Bezug zu vorherigen Interaktionen. F√ºr sinnvolle Gespr√§che und intelligente Assistenten ist dies jedoch unbrauchbar.\n","\n","\n","Dieses Kapitel behandelt drei fundamentale Memory-Typen ‚Äì vom einfachen Zwischenspeicher bis zu ausgekl√ºgelten Wissensdatenbanken. Dabei werden konkrete Technologien und Implementierungsans√§tze vorgestellt, die sich unmittelbar in eigenen Projekten einsetzen lassen."],"metadata":{"id":"EmMVp49lhkOh"},"id":"EmMVp49lhkOh"},{"cell_type":"markdown","source":["\n","\n","Die folgenden Abschnitte f√ºhren systematisch durch die Welt von Chat & Memory und deren praktische Anwendung.\n"],"metadata":{"id":"UC42_OiEgxG4"},"id":"UC42_OiEgxG4"},{"cell_type":"markdown","source":["[Kontextfenster](https://editor.p5js.org/ralf.bendig.rb/full/tLnUgyZRK)"],"metadata":{"id":"8BcPHAt_gxkJ"},"id":"8BcPHAt_gxkJ"},{"cell_type":"markdown","source":["| Typ               | Beschreibung                  | Beispiel                                                           | Technologie - Beispiele                                                     | Speicherort      |\n","| ----------------- | ----------------------------- | ------------------------------------------------------------------ | --------------------------------------------------------------------------- | ---------------- |\n","| Kurzzeit-Memory<br> (tempor√§r) | Innerhalb einer Sitzung       | ChatGPT erinnert sich an das, was du vor 3 Nachrichten gesagt hast | `Python Liste`<br><br>`ConversationBufferMemory`<br><br>`ConversationSummaryMemory` | nur im RAM       |\n","| Langzeit-Memory   | √úber mehrere Sitzungen hinweg | KI merkt sich deinen Namen, Interessen etc.                        | JSON-Datei + `ConversationChain`                                            | Festplatte       |\n","| Externes Memory   | Via Datenbanken, Dateien etc. | RAG-Systeme, Notizsysteme                                          | `Chroma` + `RetrievalQA`                                                    | Wissensdatenbank |"],"metadata":{"id":"nk5e8YcSppnt"},"id":"nk5e8YcSppnt"},{"cell_type":"markdown","source":["# 2 | Kurzzeit-Memory\n","---"],"metadata":{"id":"4jBnf3n8r2qF"},"id":"4jBnf3n8r2qF"},{"cell_type":"markdown","source":["Tempor√§res Memory bildet die Grundlage jeder KI-Konversation. Es speichert den unmittelbaren Gespr√§chsverlauf einer Sitzung und erm√∂glicht es der KI, auf vorherige Nachrichten Bezug zu nehmen.\n","Funktionsweise\n","Das System h√§lt die letzten Nachrichten im Arbeitsspeicher vor und f√ºgt sie bei jeder neuen Anfrage als Kontext hinzu. Dadurch entsteht der Eindruck eines zusammenh√§ngenden Gespr√§chs, obwohl das zugrundeliegende Modell weiterhin zustandslos arbeitet."],"metadata":{"id":"oleRtwersLmJ"},"id":"oleRtwersLmJ"},{"cell_type":"markdown","source":["## 2.1 | Python Liste"],"metadata":{"id":"ESERG3-dr7GA"},"id":"ESERG3-dr7GA"},{"cell_type":"code","source":["# Importe\n","from langchain_openai import ChatOpenAI\n","from langchain.schema import HumanMessage, AIMessage, SystemMessage\n","from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.output_parsers.string import StrOutputParser"],"metadata":{"id":"9y7B2qfuB2Xs"},"execution_count":null,"outputs":[],"id":"9y7B2qfuB2Xs"},{"cell_type":"code","source":["# Konstanten & Propmpt-Template\n","model_name = \"gpt-4o-mini\"\n","temperature = 0\n","\n","system_prompt = \"Du bist ein hilfreicher und humorvoller KI-Assistent\"\n","\n","# Die Chat-Prompt definieren mit drei Variablen: system_prompt, chat_history, question\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"human\", \"{user_input}\")\n","])"],"metadata":{"id":"YJVTOv9TH2lT"},"execution_count":null,"outputs":[],"id":"YJVTOv9TH2lT"},{"cell_type":"code","source":["# LLM definieren\n","llm = ChatOpenAI(model=model_name, temperature=temperature)\n","\n","# Parser\n","parser = StrOutputParser()\n","\n","# Die Konversationskette definieren\n","chain = prompt | llm | parser"],"metadata":{"id":"AkkALStvH4ul"},"execution_count":null,"outputs":[],"id":"AkkALStvH4ul"},{"cell_type":"code","source":["# Funktion definieren\n","def interact_with_ai(system_prompt, chat_history, user_input):\n","    \"\"\"F√ºhrt eine einzelne Interaktion mit der KI durch.\"\"\"\n","\n","    # Aufruf der Kette\n","    parameter = {\n","        'system_prompt': system_prompt,\n","        'chat_history': chat_history,\n","        'user_input': user_input\n","    }\n","    response = chain.invoke(parameter)\n","\n","    # Ausgabe\n","    mprint(\"### üßë‚Äçü¶± Mensch:\")\n","    mprint(user_input)\n","\n","    mprint(\"### ü§ñ KI:\")\n","    mprint(response)\n","\n","    # Memory-Management\n","    chat_history.extend([\n","        HumanMessage(content=user_input),\n","        AIMessage(content=response)\n","    ])\n","\n","    return chat_history"],"metadata":{"id":"kzhjBJEsH9h1"},"execution_count":null,"outputs":[],"id":"kzhjBJEsH9h1"},{"cell_type":"code","source":["# Abschnitt 5: Hauptprogramm\n","\n","# Historie wird initialisiert\n","chat_history = [SystemMessage(content=system_prompt)]\n","\n","# Liste mit user-input\n","user_input = \"Mein Name ist Ralf\"\n","interact_with_ai(system_prompt, chat_history, user_input)\n","\n","# Liste mit user-input\n","user_input = \"Hast Du Dir meinen Namen gemerkt?\"\n","interact_with_ai(system_prompt, chat_history, user_input)"],"metadata":{"id":"TGuKwTyVJ-wt"},"execution_count":null,"outputs":[],"id":"TGuKwTyVJ-wt"},{"cell_type":"code","source":["# Python Liste\n","chat_history"],"metadata":{"id":"y5TdeRrG2icv"},"id":"y5TdeRrG2icv","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.2 | CoversationBufferMemory"],"metadata":{"id":"1gue2wy2sNY9"},"id":"1gue2wy2sNY9"},{"cell_type":"code","source":["# Importe LangChain\n","from langchain_openai import ChatOpenAI\n","from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n","from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.output_parsers.string import StrOutputParser"],"metadata":{"id":"WfeTn04Vs8eB"},"id":"WfeTn04Vs8eB","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tempor√§rer Speicher & Prompt-Template & Parser\n","memory = ConversationBufferMemory(return_messages=True)\n","\n","# Prompt-Vorlage mit Platzhalter f√ºr den bisherigen Chat-Verlauf\n","prompt = ChatPromptTemplate.from_messages([\n","    ('system', \"Du bist ein hilfreicher und humorvoller KI-Assistent\"),\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"human\", \"{user_input}\")\n","])"],"metadata":{"id":"Puwv1cLvuLM0"},"id":"Puwv1cLvuLM0","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LLM & Chain definieren\n","model_name = \"gpt-4o-mini\"\n","temperature = 0\n","llm = ChatOpenAI(model=model_name, temperature=temperature)\n","\n","# Parser\n","parser = StrOutputParser()\n","\n","chain = prompt | llm | parser"],"metadata":{"id":"zQEeOIicuPAH"},"id":"zQEeOIicuPAH","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Beispiel-Interaktionen\n","\n","# 1. Eingabe speichern\n","user_input = \"Hallo, ich bin Ralf.\"\n","\n","parameter = {}\n","parameter['chat_history'] = memory.chat_memory.messages\n","parameter['user_input'] = user_input\n","response = chain.invoke(parameter)\n","\n","mprint(f\">>ü§ñ {response}\")\n","\n","memory.chat_memory.add_user_message(user_input)\n","memory.chat_memory.add_ai_message(response)"],"metadata":{"id":"a2U364AFuegJ"},"id":"a2U364AFuegJ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. Folgeeingabe\n","user_input = \"Wei√üt du noch, wie ich hei√üe?\",\n","\n","parameter = {}\n","parameter['chat_history'] = memory.chat_memory.messages\n","parameter['user_input'] = user_input\n","response = chain.invoke(parameter)\n","\n","mprint(response)\n","\n","memory.chat_memory.add_user_message(user_input)\n","memory.chat_memory.add_ai_message(response)"],"metadata":{"id":"JTmgE4davO8P"},"id":"JTmgE4davO8P","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Memory\n","memory.chat_memory.messages"],"metadata":{"id":"xQLQDxgw2YbB"},"id":"xQLQDxgw2YbB","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.3 | ConversationSummaryMemory"],"metadata":{"id":"3MF-bxiisUpw"},"id":"3MF-bxiisUpw"},{"cell_type":"code","source":["# Memory & Propmt-Template & Parser\n","memory = ConversationSummaryMemory(llm=llm, return_messages=True)\n","\n","# # Prompt mit chat_history-Placeholder!\n","# prompt = ChatPromptTemplate.from_messages([\n","#     ('system', \"Du bist ein hilfreicher und humorvoller KI-Assistent.\"),\n","#     MessagesPlaceholder(variable_name=\"chat_history\"),\n","#     (\"human\", \"{user_input}\")\n","# ])"],"metadata":{"id":"A1lnWBOb2Dzx"},"id":"A1lnWBOb2Dzx","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Chain (manuell gebaut)\n","# chain = prompt | llm | parser"],"metadata":{"id":"1YNSxHfR2F5m"},"id":"1YNSxHfR2F5m","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Interaktion mit Memory-Nutzung\n","def interact_with_ai(memory, input):\n","    # Verlauf laden\n","    history_vars = memory.load_memory_variables({})\n","\n","    # Ausgabe anzeigen\n","    mprint(f\"###üßë‚Äçü¶± Mensch:\\n {user_input}\")\n","\n","    # Chain mit Kontext aus Memory aufrufen\n","    parameter = {}\n","    parameter['chat_history'] = history_vars[\"history\"]\n","    parameter['user_input'] = user_input\n","    response = chain.invoke(parameter)\n","\n","\n","    # response = chain.invoke({\n","    #     \"input\": question,\n","    #     \"chat_history\": history_vars[\"history\"]\n","    # })\n","\n","    mprint(f\"### ü§ñ KI:\\n {response}\\n\")\n","\n","    # neuen Dialog abspeichern\n","    memory.save_context({\"input\": input}, {\"output\": response})"],"metadata":{"id":"PNqhe9Z82PcX"},"id":"PNqhe9Z82PcX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test-Dialog\n","user_input_list = [\n","    \"Mein Name ist Ralf\",\n","    \"Warum ist der Himmel blau?\",\n","    \"Warum ist er manchmal rot?\",\n","    \"Wie hei√üe ich?\"\n","]\n","\n","for user_input in user_input_list:\n","    interact_with_ai(memory, user_input)"],"metadata":{"id":"ltU9HMvT2RzD"},"id":"ltU9HMvT2RzD","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Summary\n","memory.load_memory_variables({})['history']"],"metadata":{"id":"9TaWzNyr1q29"},"id":"9TaWzNyr1q29","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3 | Langzeit-Memory\n","---"],"metadata":{"id":"DzD3P1mr28MT"},"id":"DzD3P1mr28MT"},{"cell_type":"markdown","source":["Ein Langzeit-Memory erm√∂glicht es einer KI, √ºber mehrere Sitzungen hinweg Informationen zu behalten ‚Äì √§hnlich wie ein Mensch sich an fr√ºhere Gespr√§che oder Fakten erinnert."],"metadata":{"id":"M0CREbJ05oK3"},"id":"M0CREbJ05oK3"},{"cell_type":"code","source":["# Import LangChain\n","from langchain.memory import ConversationBufferMemory\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain.schema import messages_to_dict\n","\n","# Import Dateiformat\n","import json"],"metadata":{"id":"guLfekZpY4Lf"},"id":"guLfekZpY4Lf","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Funktionen f√ºr Memory-Verwaltung & Chat\n","def load_memory():\n","    \"\"\"L√§dt gespeicherte Chat-Historie aus JSON-Datei\"\"\"\n","    try:\n","        with open(\"chat_memory.json\", \"r\") as f:\n","            return messages_from_dict(json.load(f))\n","    except:\n","        return []\n","\n","\n","def save_memory(messages):\n","    \"\"\"Speichert Chat-Historie in JSON-Datei\"\"\"\n","    with open(\"chat_memory.json\", \"w\") as f:\n","        json.dump(messages_to_dict(messages), f)\n","\n","\n","def chat(memory, chain):\n","    \"\"\" Hauptfunktion f√ºr den Chat \"\"\"\n","\n","    print(\"ü§ñ Chat gestartet. 'exit' zum Beenden.\\n\")\n","\n","    # Chat-Schleife\n","    while True:\n","        user_input = input(\"üßë Frage: \")\n","\n","        # Exit-Bedingung pr√ºfen\n","        if user_input.lower() in [\"exit\", \"quit\"]:\n","            print(\"üëã Tsch√ºss!\")\n","            break\n","\n","        # Antwort generieren\n","        response = chain.invoke({\n","            \"user_input\": user_input,\n","            \"chat_history\": memory.chat_memory.messages\n","        })\n","\n","        print(f\"ü§ñ {response}\\n\")\n","\n","        # Memory aktualisieren und speichern\n","        memory.save_context({\"user_input\": user_input}, {\"output\": response})\n","        save_memory(memory.chat_memory.messages)"],"metadata":{"id":"nAiCufqtZsPg"},"id":"nAiCufqtZsPg","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Memory initialisieren und laden\n","memory = ConversationBufferMemory(return_messages=True)\n","memory.chat_memory.messages = load_memory()\n","\n","# Prompt-Template erstellen\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent.\"),\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"human\", \"{user_input}\")\n","])\n","\n","# Modell initialisieren\n","llm = ChatOpenAI(temperature=0)\n","\n","# Parser erstellen\n","parser = StrOutputParser()\n","\n","# Chain aus Komponenten zusammensetzen\n","chain = prompt | llm | parser"],"metadata":{"id":"BGdVlJGXZwVB"},"id":"BGdVlJGXZwVB","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Chat - Teil 1\n","chat(memory, chain)"],"metadata":{"id":"vObidWEuZ37R"},"id":"vObidWEuZ37R","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Chat - Teil 2\n","chat(memory, chain)"],"metadata":{"id":"XqxqLbuGBP76"},"id":"XqxqLbuGBP76","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["[json-Formatter](https://jsonformatter.info/)"],"metadata":{"id":"KYDcFxKnRJd0"},"id":"KYDcFxKnRJd0"},{"cell_type":"markdown","source":["# 4 | Externes Memory\n","---\n"],"metadata":{"id":"nDONejjp2_sm"},"id":"nDONejjp2_sm"},{"cell_type":"markdown","source":["Ein externes Memory bedeutet, dass sich ein KI-System nicht alles selbst merken muss, sondern bei Bedarf Wissen von au√üen abruft. Das wird h√§ufig mit einer Vektor-Datenbanken umgesetzt ‚Äì das Herzst√ºck vieler moderner Chatbots mit ‚ÄûLangzeitwissen‚Äú.\n","\n","Eine Vektordatenbank speichert Texte, Bilder oder andere Inhalte in einer numerischen Form (sogenannte Embeddings), damit sie schnell durchsucht werden k√∂nnen."],"metadata":{"id":"UfUBG8lf5cAo"},"id":"UfUBG8lf5cAo"},{"cell_type":"code","source":["# Import LangChain\n","from langchain.vectorstores import Chroma\n","from langchain.chains import RetrievalQA\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings"],"metadata":{"id":"IcRlh3Jg3Kk0"},"id":"IcRlh3Jg3Kk0","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Datenbank vorbereiten\n","texte = [\n","    \"Python ist eine vielseitige Programmiersprache.\",\n","    \"K√ºnstliche Intelligenz bezeichnet die Simulation menschlicher Intelligenz durch Maschinen.\"\n","]\n","\n","# Embedding & Chroma-Vektordatenbank\n","embedding = OpenAIEmbeddings()\n","vectordb = Chroma.from_texts(texte, embedding=embedding, persist_directory=\"chroma_db\")"],"metadata":{"id":"A36ro4DHkY59"},"id":"A36ro4DHkY59","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LLM + Retriever\n","model_name = \"gpt-4o-mini\"\n","temperature = 0\n","llm = ChatOpenAI(model=model_name, temperature=temperature)\n","\n","retriever = vectordb.as_retriever()"],"metadata":{"id":"BAIn99kPkdlD"},"id":"BAIn99kPkdlD","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# RetrievalQA-Chain mit invoke-kompatiblem Format\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm=llm,\n","    retriever=retriever,\n","    return_source_documents=False\n",")"],"metadata":{"id":"G3-u9isik9Qr"},"id":"G3-u9isik9Qr","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Beispiel-Konversation mit invoke()\n","user_input_list = [\n","                \"Was ist Python?\",\n","                \"Und was ist KI?\",\n","                \"Was wei√üt du √ºber Java?\"  # Nicht in Datenbank enthalten\n","]\n","\n","for user_input in user_input_list:\n","    mprint(f\"###üßë‚Äçüíª Mensch:\\n {user_input}\")\n","    response = qa_chain.invoke({\"query\": user_input})\n","    mprint(f\"###ü§ñ KI:\\n {response}\\n\")"],"metadata":{"id":"T2wLvnpnlFjT"},"id":"T2wLvnpnlFjT","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5 | Langzeit vs. Externes Memory\n","---\n"],"metadata":{"id":"qiHYQxlBdlJu"},"id":"qiHYQxlBdlJu"},{"cell_type":"markdown","source":["\n","\n","| **Aspekt** | **Langzeit-Memory** | **Externes Memory** |\n","|------------|---------------------|---------------------|\n","| **Zweck** | Pers√∂nliche Erinnerungen an Gespr√§chsverlauf | Geteiltes Wissen und Dokumentation |\n","| **Speicherart** | Chronologischer Chat-Verlauf | Semantisch durchsuchbare Wissensbasis |\n","| **Technologie** | `ConversationBufferMemory` + JSON-Datei | Vektor-Datenbank (Chroma) + `RetrievalQA` |\n","| **Multi-User-Zugriff** | ‚ùå Nein (benutzerspezifisch) | ‚úÖ **Ja (gemeinsam nutzbar)** |\n","| **Speicherort** | Festplatte (z.B. `chat_memory.json`) | Wissensdatenbank mit Embeddings |\n","| **Inhalt** | \"Du hei√üt Ralf\", \"Du magst Python\" | \"Python ist eine Programmiersprache\" |\n","| **Zugriffsmethode** | Chronologisch, sequenziell | Semantische Suche nach Relevanz |\n","| **Skalierbarkeit** | Begrenzt durch Chat-L√§nge | Sehr skalierbar (gro√üe Datenmengen) |\n","| **Anwendungsfall** | Personalisierte Chatbots, Assistenten | Firmenwiki, Support-Systeme, RAG |\n","| **Beispiel aus Notebook** | \"KI merkt sich deinen Namen\" | \"RAG-Systeme, Notizsysteme\" |\n","| **Datenformat** | Nachrichten-Objekte (HumanMessage, AIMessage) | Text-Chunks mit Vektorisierung |\n","| **Persistierung** | `load_memory()` / `save_memory()` | `Chroma(persist_directory=\"...\")` |\n","| **Kontext-Nutzung** | Gesamter bisheriger Chat-Verlauf | Nur relevante Dokumente werden abgerufen |\n","| **Update-H√§ufigkeit** | Nach jeder User-Interaktion | Bei Bedarf (neue Dokumente hinzuf√ºgen) |\n","| **Typische Gr√∂√üe** | Klein bis mittel (wenige KB bis MB) | Gro√ü (GB bis TB m√∂glich) |\n"],"metadata":{"id":"2LHxhnSHeTtp"},"id":"2LHxhnSHeTtp"},{"cell_type":"markdown","source":["# 6 | Mehr Tokens ‚â† Bessere Chat-Memory\n","---"],"metadata":{"id":"0kBaYmRFdI7N"},"id":"0kBaYmRFdI7N"},{"cell_type":"markdown","source":["Mehr Tokens in einem Kontextfenster eines Large Language Models (LLMs) bedeuten nicht automatisch eine bessere Verarbeitung. Dies ist besonders relevant im Kontext von **Chat & Memory**, wo Modelle versuchen, l√§ngere Gespr√§chsverl√§ufe oder gespeicherte Erinnerungen zu nutzen."],"metadata":{"id":"ZvY5VNktdgTp"},"id":"ZvY5VNktdgTp"},{"cell_type":"markdown","source":["\n","\n","1. **Rauschen und Irrelevanz**\n","   Ein gr√∂√üeres Kontextfenster erlaubt zwar die Verarbeitung l√§ngerer Chat-Historien, aber nicht alle Informationen sind f√ºr die aktuelle Anfrage relevant. Das Modell muss wichtige Details von unwichtigen trennen, was mit wachsendem Kontext schwieriger wird.\n","\n","2. **Abnehmende Aufmerksamkeit**\n","   LLMs nutzen Aufmerksamkeitsmechanismen. Bei sehr langen Chat-Kontexten verteilt sich die Aufmerksamkeit √ºber viele Tokens, sodass entscheidende Informationen weniger stark gewichtet werden k√∂nnen.\n","\n","3. **Fehlerakkumulation**\n","   In Chats k√∂nnen kleine Missverst√§ndnisse oder falsche Annahmen durch l√§ngere Kontexte verst√§rkt werden. Dies kann zu Antworten f√ºhren, die auf alten, irrelevanten Informationen basieren.\n","\n","4. **Verarbeitungsgrenzen und Latenz**\n","   Gr√∂√üere Kontextfenster f√ºhren zu mehr Rechenaufwand, l√§ngeren Antwortzeiten und h√∂herem Ressourcenverbrauch ‚Äì ohne dass die Qualit√§t proportional steigt.\n","\n","5. **Warum das Thema weiterhin wichtig ist**\n","   Mit der zunehmenden Nutzung von **Memory-Funktionen** in Chatbots stellt sich die Frage, wie Erinnerungen strukturiert, gefiltert und priorisiert werden. Nur durch intelligentes Kontext- und Memory-Management lassen sich wirklich relevante Informationen nutzen, ohne das Modell mit unn√∂tigen Daten zu √ºberfluten.\n","\n"],"metadata":{"id":"we0u2rKCelAK"},"id":"we0u2rKCelAK"},{"cell_type":"markdown","source":["\n","<p><font color='darkblue' size=\"4\">\n","‚ÑπÔ∏è <b>Fazit</b>\n","</font></p>\n","\n","Ein gro√ües Kontextfenster allein l√∂st nicht die Herausforderungen von Chat & Memory. Entscheidend sind clevere Strategien, um Informationen zu selektieren, zusammenzufassen und zielgerichtet einzusetzen.\n"],"metadata":{"id":"JwPZ2gnAeqHi"},"id":"JwPZ2gnAeqHi"},{"cell_type":"markdown","metadata":{"id":"5cfbf26a"},"source":["# A | Aufgaben\n","---\n"],"id":"5cfbf26a"},{"cell_type":"markdown","source":["\n","\n","Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen.\n"],"metadata":{"id":"pwmSuPsGiQp6"},"id":"pwmSuPsGiQp6"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","KI-gest√ºtztes Notizbuch mit automatischer Kategorisierung\n","</font></p>\n","\n","\n","Entwickeln Sie ein KI-Notizbuch, das Eingaben von Nutzern speichert, automatisch kategorisiert (z. B. \"Technologie\", \"Privat\", \"Aufgaben\") und kontextbezogene Vorschl√§ge liefert. Verwenden Sie dazu ein LangChain Memory-Konzept, z.B. ConversationBufferMemory, f√ºr eine effiziente Verwaltung.\n"],"metadata":{"id":"1UViO3SJiSXy"},"id":"1UViO3SJiSXy"},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Chatbot: Nachrichten- vs. Summary-Speicher\n","</font></p>\n","\n","\n","Erstellen Sie einen einfachen Chatbot mit LangChain, der zwischen Nachrichten- und Summary-Speicher unterscheidet. Analysieren Sie, wie sich die Speichertypen auf die Qualit√§t der Antworten auswirken.\n","\n"],"metadata":{"id":"T6HXRFbXjIVL"},"id":"T6HXRFbXjIVL"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Automatische Protokollerstellung f√ºr Meetings\n","</font></p>\n","\n","\n","Entwickeln Sie ein System, das Gespr√§chsverl√§ufe speichert und automatisch eine zusammenfassende Chat-Notiz erstellt. Nutzen Sie z.B. ConversationSummaryMemory, um die wichtigsten Punkte aus langen Gespr√§chen zu extrahieren.\n","\n"],"metadata":{"id":"9eIl83Z-CqT8"},"id":"9eIl83Z-CqT8"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Virtuelle Assistenten mit eigenem Ged√§chtnis\n","</font></p>\n","\n","\n","Eine Datei wird bereitgestellt, die ein Gespr√§ch zwischen zwei virtuellen Assistenten enth√§lt. Jede Antwort soll aus genau einem Satz bestehen. Die Datei ist folgenderma√üen aufgebaut:\n","\n","|Assistent|Eingabe|\n","|---|---|\n","|botA|Ich hei√üe Max.|\n","|botB|Mein Name ist Emma.|\n","|botA|Ich wohne in Berlin.|\n","|botB|Ich lebe in Hamburg.|\n","|botA|Wo wohne ich und wie hei√üe ich?|\n","|botB|Wo lebe ich?|\n","\n","**Aufgabe**  \n","Schreiben Sie ein Programm, das zwei virtuelle Assistenten simuliert, die jeweils ein eigenes Ged√§chtnis haben. Die Eingaben aus der Datei werden an den jeweiligen Assistenten geschickt, und ihre Antworten sollen in einer neuen Datei gespeichert werden.\n","\n","**Erwartete Ausgabe**  \n","Die erwartete Ausgabe k√∂nnte wie folgt aussehen:\n","\n","|Assistent|Antwort|\n","|---|---|\n","|botA|Hallo Max.|\n","|botB|Hallo Emma.|\n","|botA|Danke f√ºr diese Information.|\n","|botB|Danke f√ºr diese Information.|\n","|botA|Hallo Max, du wohnst in Berlin und dein Name ist Max.|\n","|botB|Hallo Emma, du lebst in Hamburg.|\n","\n","**Hinweise**\n","\n","- Die Antworten m√ºssen nicht exakt mit der Beispielausgabe √ºbereinstimmen, sollten aber sinngem√§√ü √§hnlich sein.\n","- Jeder Assistent hat ein eigenes Ged√§chtnis und sollte sich nur an seine eigenen Informationen erinnern.\n","- Das Programm soll die Antworten in einer neuen Datei im gleichen Tabellenformat speichern."],"metadata":{"id":"CwYae6YZCsto"},"id":"CwYae6YZCsto"}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"colab":{"provenance":[],"collapsed_sections":["fe8a8ba4","4jBnf3n8r2qF","ESERG3-dr7GA","1gue2wy2sNY9","3MF-bxiisUpw","DzD3P1mr28MT","nDONejjp2_sm","qiHYQxlBdlJu","0kBaYmRFdI7N","5cfbf26a"],"toc_visible":true},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}
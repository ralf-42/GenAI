{"cells":[{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)"],"metadata":{"id":"9JKGDENeDeB0"},"id":"9JKGDENeDeB0"},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b> Chat & Memory </b></font> </br></p>\n","\n","---"],"metadata":{"id":"bxo8g0twDZK5"},"id":"bxo8g0twDZK5"},{"cell_type":"code","source":["#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n","!uv pip install --system -q git+https://github.com/ralf-42/Python_Modules\n","from genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages\n","setup_api_keys(['OPENAI_API_KEY', 'HF_TOKEN'], create_globals=False)\n","print()\n","check_environment()\n","print()\n","get_ipinfo()\n","# Bei Bedarf: Trennen zwischen Installationsname () und Importname (f√ºr Python) beide Angaben in Klammern\n","# install_packages([('markitdown[all]', 'markitdown'), 'langchain_chroma', ]"],"metadata":{"id":"YQaYpJJwoxVT"},"id":"YQaYpJJwoxVT","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title üõ†Ô∏è Installationen{ display-mode: \"form\" }\n","install_packages(['chromadb', ])"],"metadata":{"id":"pgVkt6Nbt-By"},"id":"pgVkt6Nbt-By","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"fe8a8ba4","metadata":{"id":"fe8a8ba4"},"source":["# 1 | Intro\n","---\n"]},{"cell_type":"markdown","source":["Ein Gespr√§ch mit jemandem zu f√ºhren, der nach jedem Satz vergisst, was zuvor gesagt wurde, w√§re √§u√üerst frustrierend. Genau dieses Problem l√∂st **Memory** in der K√ºnstlichen Intelligenz.\n","\n","**Warum braucht KI ein Ged√§chtnis?**\n","\n","Large Language Models wie GPT sind von Natur aus **zustandslos** ‚Äì sie verf√ºgen √ºber kein eingebautes Ged√§chtnis. Jede Anfrage wird isoliert verarbeitet, ohne Bezug zu vorherigen Interaktionen. F√ºr sinnvolle Gespr√§che und intelligente Assistenten ist dies jedoch unbrauchbar.\n","\n","\n","Dieses Kapitel behandelt drei fundamentale Memory-Typen ‚Äì vom einfachen Zwischenspeicher bis zu ausgekl√ºgelten Wissensdatenbanken. Dabei werden konkrete Technologien und Implementierungsans√§tze vorgestellt, die sich unmittelbar in eigenen Projekten einsetzen lassen."],"metadata":{"id":"EmMVp49lhkOh"},"id":"EmMVp49lhkOh"},{"cell_type":"markdown","source":["\n","\n","Die folgenden Abschnitte f√ºhren systematisch durch die Welt von Chat & Memory und deren praktische Anwendung.\n"],"metadata":{"id":"UC42_OiEgxG4"},"id":"UC42_OiEgxG4"},{"cell_type":"markdown","source":["[Kontextfenster](https://editor.p5js.org/ralf.bendig.rb/full/tLnUgyZRK)"],"metadata":{"id":"8BcPHAt_gxkJ"},"id":"8BcPHAt_gxkJ"},{"cell_type":"markdown","source":["| Typ               | Beschreibung                  | Beispiel                                                           | Technologie - Beispiele                                                     | Speicherort      |\n","| ----------------- | ----------------------------- | ------------------------------------------------------------------ | --------------------------------------------------------------------------- | ---------------- |\n","| Kurzzeit-Memory<br> (tempor√§r) | Innerhalb einer Sitzung       | ChatGPT erinnert sich an das, was du vor 3 Nachrichten gesagt hast | `Python Liste`<br>`ConversationBufferMemory`<br>`ConversationSummaryMemory` | nur im RAM       |\n","| Langzeit-Memory   | √úber mehrere Sitzungen hinweg | KI merkt sich deinen Namen, Interessen etc.                        | JSON-Datei + `ConversationChain`                                            | Festplatte       |\n","| Externes Memory   | Via Datenbanken, Dateien etc. | RAG-Systeme, Notizsysteme                                          | `Chroma` + `RetrievalQA`                                                    | Wissensdatenbank |"],"metadata":{"id":"nk5e8YcSppnt"},"id":"nk5e8YcSppnt"},{"cell_type":"markdown","source":["# 2 | Kurzzeit-Memory\n","---"],"metadata":{"id":"4jBnf3n8r2qF"},"id":"4jBnf3n8r2qF"},{"cell_type":"markdown","source":["Tempor√§res Memory bildet die Grundlage jeder KI-Konversation. Es speichert den unmittelbaren Gespr√§chsverlauf einer Sitzung und erm√∂glicht es der KI, auf vorherige Nachrichten Bezug zu nehmen.\n","Funktionsweise\n","Das System h√§lt die letzten Nachrichten im Arbeitsspeicher vor und f√ºgt sie bei jeder neuen Anfrage als Kontext hinzu. Dadurch entsteht der Eindruck eines zusammenh√§ngenden Gespr√§chs, obwohl das zugrundeliegende Modell weiterhin zustandslos arbeitet."],"metadata":{"id":"oleRtwersLmJ"},"id":"oleRtwersLmJ"},{"cell_type":"markdown","source":["## 2.1 | Python Liste"],"metadata":{"id":"ESERG3-dr7GA"},"id":"ESERG3-dr7GA"},{"cell_type":"code","source":["# Importe\n","from langchain_openai import ChatOpenAI\n","from langchain.schema import HumanMessage, AIMessage, SystemMessage\n","from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.output_parsers.string import StrOutputParser"],"metadata":{"id":"9y7B2qfuB2Xs"},"execution_count":null,"outputs":[],"id":"9y7B2qfuB2Xs"},{"cell_type":"code","source":["# Konstanten & Propmpt-Template\n","model_name = \"gpt-4o-mini\"\n","temperature = 0\n","\n","system_prompt = \"Du bist ein hilfreicher und humorvoller KI-Assistent\"\n","\n","# Die Chat-Prompt definieren mit drei Variablen: system_prompt, history, question\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    MessagesPlaceholder(variable_name=\"history\"),\n","    (\"human\", \"{input}\")\n","])"],"metadata":{"id":"YJVTOv9TH2lT"},"execution_count":null,"outputs":[],"id":"YJVTOv9TH2lT"},{"cell_type":"code","source":["# LLM definieren\n","llm = ChatOpenAI(model=model_name, temperature=temperature)\n","\n","# Parser\n","parser = StrOutputParser()\n","\n","# Die Konversationskette definieren\n","chain = prompt | llm | parser"],"metadata":{"id":"AkkALStvH4ul"},"execution_count":null,"outputs":[],"id":"AkkALStvH4ul"},{"cell_type":"code","source":["# Funktion definieren\n","def interact_with_ai(input, history):\n","    \"\"\"F√ºhrt eine einzelne Interaktion mit der KI durch.\"\"\"\n","\n","    # -- Aufruf der Kette\n","    response = chain.invoke(\n","        {\"system_prompt\": system_prompt, \"history\": history, \"input\": input}\n","    )\n","\n","    # -- Ausgabe\n","    mprint(\"### üßë‚Äçü¶± Mensch:\")\n","    question = input.replace(\"\\n\", \"<br>\")\n","    mprint(input)\n","\n","    mprint(\"### ü§ñ KI:\")\n","    mprint(response)\n","\n","    # -- Memory-Mangament\n","    history.extend([HumanMessage(content=question), AIMessage(content=response)])"],"metadata":{"id":"kzhjBJEsH9h1"},"execution_count":null,"outputs":[],"id":"kzhjBJEsH9h1"},{"cell_type":"code","source":["# Abschnitt 5: Hauptprogramm\n","\n","# Historie wird initialisiert\n","history = [SystemMessage(content=system_prompt)]\n","\n","# Liste mit user-input\n","user_input = \"Mein Name ist Ralf\"\n","interact_with_ai(user_input, history)\n","\n","# Liste mit user-input\n","user_input = \"Hast Du Dir meinen Namen gemerkt?\"\n","interact_with_ai(user_input, history)"],"metadata":{"id":"TGuKwTyVJ-wt"},"execution_count":null,"outputs":[],"id":"TGuKwTyVJ-wt"},{"cell_type":"code","source":["# Python Liste\n","history"],"metadata":{"id":"y5TdeRrG2icv"},"id":"y5TdeRrG2icv","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.2 | CoversationBufferMemory"],"metadata":{"id":"1gue2wy2sNY9"},"id":"1gue2wy2sNY9"},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from langchain.memory import ConversationBufferMemory\n","from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.output_parsers.string import StrOutputParser"],"metadata":{"id":"WfeTn04Vs8eB"},"id":"WfeTn04Vs8eB","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tempor√§rer Speicher & Prompt-Template & Parser\n","memory = ConversationBufferMemory(return_messages=True)\n","\n","# Prompt-Vorlage mit Platzhalter f√ºr den bisherigen Chat-Verlauf\n","prompt = ChatPromptTemplate.from_messages([\n","    ('system', \"Du bist ein hilfreicher und humorvoller KI-Assistent\"),\n","    MessagesPlaceholder(variable_name=\"history\"),\n","    (\"human\", \"{input}\")\n","])\n","\n","# Parser\n","parser = StrOutputParser()"],"metadata":{"id":"Puwv1cLvuLM0"},"id":"Puwv1cLvuLM0","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LLM & Chain definieren\n","model_name = \"gpt-4o-mini\"\n","temperature = 0\n","llm = ChatOpenAI(model=model_name, temperature=temperature)\n","\n","chain = prompt | llm | parser"],"metadata":{"id":"zQEeOIicuPAH"},"id":"zQEeOIicuPAH","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Beispiel-Interaktionen\n","# 1. Eingabe speichern\n","response = chain.invoke({\n","    \"input\": \"Hallo, ich bin Ralf.\",\n","    \"history\": memory.chat_memory.messages\n","})\n","\n","mprint(response)\n","\n","memory.chat_memory.add_user_message(\"Hallo, ich bin Ralf.\")\n","memory.chat_memory.add_ai_message(response)"],"metadata":{"id":"a2U364AFuegJ"},"id":"a2U364AFuegJ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. Folgeeingabe\n","response = chain.invoke({\n","    \"input\": \"Wei√üt du noch, wie ich hei√üe?\",\n","    \"history\": memory.chat_memory.messages\n","})\n","\n","mprint(response)\n","\n","memory.chat_memory.add_user_message(\"Wei√üt du noch, wie ich hei√üe?\")\n","memory.chat_memory.add_ai_message(response)"],"metadata":{"id":"JTmgE4davO8P"},"id":"JTmgE4davO8P","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Memory\n","memory.chat_memory.messages"],"metadata":{"id":"xQLQDxgw2YbB"},"id":"xQLQDxgw2YbB","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.3 | ConversationSummaryMemory"],"metadata":{"id":"3MF-bxiisUpw"},"id":"3MF-bxiisUpw"},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from langchain.memory import ConversationSummaryMemory\n","from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.output_parsers.string import StrOutputParser"],"metadata":{"id":"K3r2LcDVxauN"},"execution_count":null,"outputs":[],"id":"K3r2LcDVxauN"},{"cell_type":"code","source":["# Memory & Propmt-Template & Parser\n","memory = ConversationSummaryMemory(llm=llm, return_messages=True)\n","\n","# Prompt mit history-Placeholder!\n","prompt = ChatPromptTemplate.from_messages([\n","    ('system', \"Du bist ein hilfreicher und humorvoller KI-Assistent.\"),\n","    MessagesPlaceholder(variable_name=\"history\"),\n","    (\"human\", \"{input}\")\n","])\n","\n","# Parser\n","parser = StrOutputParser()"],"metadata":{"id":"A1lnWBOb2Dzx"},"id":"A1lnWBOb2Dzx","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LLM & Chain\n","model_name = \"gpt-4o-mini\"\n","temperature = 0\n","llm = ChatOpenAI(model=model_name, temperature=temperature)\n","\n","# Chain (manuell gebaut)\n","chain = prompt | llm | parser"],"metadata":{"id":"1YNSxHfR2F5m"},"id":"1YNSxHfR2F5m","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Interaktion mit Memory-Nutzung\n","def interact_with_ai(question):\n","    # Verlauf laden\n","    history_vars = memory.load_memory_variables({})\n","\n","    # Ausgabe anzeigen\n","    mprint(f\"###üßë‚Äçü¶± Mensch:\\n {question}\")\n","\n","    # Chain mit Kontext aus Memory aufrufen\n","    response = chain.invoke({\n","        \"input\": question,\n","        \"history\": history_vars[\"history\"]\n","    })\n","\n","    mprint(f\"### ü§ñ KI:\\n {response}\\n\")\n","\n","    # neuen Dialog abspeichern\n","    memory.save_context({\"input\": question}, {\"output\": response})"],"metadata":{"id":"PNqhe9Z82PcX"},"id":"PNqhe9Z82PcX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test-Dialog\n","user_input = [\n","    \"Mein Name ist Ralf\",\n","    \"Warum ist der Himmel blau?\",\n","    \"Warum ist er manchmal rot?\",\n","    \"Wie hei√üe ich?\"\n","]\n","\n","for input_text in user_input:\n","    interact_with_ai(input_text)"],"metadata":{"id":"ltU9HMvT2RzD"},"id":"ltU9HMvT2RzD","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Summary\n","memory.load_memory_variables({})['history']"],"metadata":{"id":"9TaWzNyr1q29"},"id":"9TaWzNyr1q29","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3 | Langzeit-Memory\n","---"],"metadata":{"id":"DzD3P1mr28MT"},"id":"DzD3P1mr28MT"},{"cell_type":"markdown","source":["Ein Langzeit-Memory erm√∂glicht es einer KI, √ºber mehrere Sitzungen hinweg Informationen zu behalten ‚Äì √§hnlich wie ein Mensch sich an fr√ºhere Gespr√§che oder Fakten erinnert."],"metadata":{"id":"M0CREbJ05oK3"},"id":"M0CREbJ05oK3"},{"cell_type":"code","source":["import json\n","from langchain.memory import ConversationBufferMemory\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.output_parsers import StrOutputParser"],"metadata":{"id":"zKxBdgHK5oun"},"id":"zKxBdgHK5oun","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Funktionen\n","def chat():\n","    \"\"\"Hauptfunktion f√ºr den Chat\"\"\"\n","    # Memory und Chain setup\n","    memory = ConversationBufferMemory(return_messages=True)\n","    memory.chat_memory.messages = load_memory()\n","\n","    chain = (ChatPromptTemplate.from_messages([\n","        (\"system\", \"Du bist ein hilfreicher Assistent.\"),\n","        MessagesPlaceholder(variable_name=\"history\"),\n","        (\"human\", \"{input}\")\n","    ]) | ChatOpenAI(temperature=0) | StrOutputParser())\n","\n","    print(\"ü§ñ Chat gestartet. 'exit' zum Beenden.\\n\")\n","\n","    # Chat-Schleife\n","    while True:\n","        user_input = input(\"üßë Frage: \")\n","\n","        if user_input.lower() in [\"exit\", \"quit\"]:\n","            print(\"üëã Tsch√ºss!\")\n","            break\n","\n","        # Antwort generieren\n","        response = chain.invoke({\n","            \"input\": user_input,\n","            \"history\": memory.chat_memory.messages\n","        })\n","\n","        print(f\"ü§ñ {response}\\n\")\n","\n","        # Memory aktualisieren\n","        memory.save_context({\"input\": user_input}, {\"output\": response})\n","        save_memory(memory.chat_memory.messages)\n","\n","def load_memory():\n","    \"\"\"L√§dt gespeicherte Chat-Historie\"\"\"\n","    try:\n","        with open(\"chat_memory.json\", \"r\") as f:\n","            from langchain.schema import messages_from_dict\n","            return messages_from_dict(json.load(f))\n","    except:\n","        return []\n","\n","def save_memory(messages):\n","    \"\"\"Speichert Chat-Historie\"\"\"\n","    from langchain.schema import messages_to_dict\n","    with open(\"chat_memory.json\", \"w\") as f:\n","        json.dump(messages_to_dict(messages), f)"],"metadata":{"id":"a0E5_eAaBMno"},"id":"a0E5_eAaBMno","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Starten\n","if __name__ == \"__main__\":\n","    chat()"],"metadata":{"id":"XqxqLbuGBP76"},"id":"XqxqLbuGBP76","execution_count":null,"outputs":[]},{"cell_type":"code","source":["chat()"],"metadata":{"id":"Karzpjkrj7lN"},"id":"Karzpjkrj7lN","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4 | Externes Memory\n","---\n"],"metadata":{"id":"nDONejjp2_sm"},"id":"nDONejjp2_sm"},{"cell_type":"markdown","source":["Ein externes Memory bedeutet, dass sich ein KI-System nicht alles selbst merken muss, sondern bei Bedarf Wissen von au√üen abruft. Das wird h√§ufig mit einer Vektor-Datenbanken umgesetzt ‚Äì das Herzst√ºck vieler moderner Chatbots mit ‚ÄûLangzeitwissen‚Äú.\n","\n","Eine Vektordatenbank speichert Texte, Bilder oder andere Inhalte in einer numerischen Form (sogenannte Embeddings), damit sie schnell durchsucht werden k√∂nnen."],"metadata":{"id":"UfUBG8lf5cAo"},"id":"UfUBG8lf5cAo"},{"cell_type":"code","source":["from langchain.vectorstores import Chroma\n","from langchain.chains import RetrievalQA\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings"],"metadata":{"id":"IcRlh3Jg3Kk0"},"id":"IcRlh3Jg3Kk0","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Datenbank vorbereiten\n","texts = [\n","    \"Python ist eine vielseitige Programmiersprache.\",\n","    \"K√ºnstliche Intelligenz bezeichnet die Simulation menschlicher Intelligenz durch Maschinen.\"\n","]\n","\n","# Embedding & Chroma-Vektordatenbank\n","embedding = OpenAIEmbeddings()\n","vectordb = Chroma.from_texts(texts, embedding=embedding, persist_directory=\"chroma_db\")"],"metadata":{"id":"A36ro4DHkY59"},"id":"A36ro4DHkY59","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LLM + Retriever\n","model_name = \"gpt-4o-mini\"\n","temperature = 0\n","llm = ChatOpenAI(model=model_name, temperature=temperature)\n","\n","retriever = vectordb.as_retriever()"],"metadata":{"id":"BAIn99kPkdlD"},"id":"BAIn99kPkdlD","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# RetrievalQA-Chain mit invoke-kompatiblem Format\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm=llm,\n","    retriever=retriever,\n","    return_source_documents=False\n",")"],"metadata":{"id":"G3-u9isik9Qr"},"id":"G3-u9isik9Qr","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Beispiel-Konversation mit invoke()\n","fragen = [\n","    \"Was ist Python?\",\n","    \"Und was ist KI?\",\n","    \"Was wei√üt du √ºber Java?\"  # Nicht in Datenbank enthalten\n","]\n","\n","for frage in fragen:\n","    mprint(f\"###üßë‚Äçüíª Mensch:\\n {frage}\")\n","    antwort = qa_chain.invoke({\"query\": frage})\n","    mprint(f\"###ü§ñ KI:\\n {antwort['result']}\\n\")"],"metadata":{"id":"T2wLvnpnlFjT"},"id":"T2wLvnpnlFjT","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5 | Mehr Tokens ‚â† Bessere Chat-Memory"],"metadata":{"id":"0kBaYmRFdI7N"},"id":"0kBaYmRFdI7N"},{"cell_type":"markdown","source":["Mehr Tokens in einem Kontextfenster eines Large Language Models (LLMs) bedeuten nicht automatisch eine bessere Verarbeitung. Dies ist besonders relevant im Kontext von **Chat & Memory**, wo Modelle versuchen, l√§ngere Gespr√§chsverl√§ufe oder gespeicherte Erinnerungen zu nutzen.\n","\n","1. **Rauschen und Irrelevanz**\n","   Ein gr√∂√üeres Kontextfenster erlaubt zwar die Verarbeitung l√§ngerer Chat-Historien, aber nicht alle Informationen sind f√ºr die aktuelle Anfrage relevant. Das Modell muss wichtige Details von unwichtigen trennen, was mit wachsendem Kontext schwieriger wird.\n","\n","2. **Abnehmende Aufmerksamkeit**\n","   LLMs nutzen Aufmerksamkeitsmechanismen. Bei sehr langen Chat-Kontexten verteilt sich die Aufmerksamkeit √ºber viele Tokens, sodass entscheidende Informationen weniger stark gewichtet werden k√∂nnen.\n","\n","3. **Fehlerakkumulation**\n","   In Chats k√∂nnen kleine Missverst√§ndnisse oder falsche Annahmen durch l√§ngere Kontexte verst√§rkt werden. Dies kann zu Antworten f√ºhren, die auf alten, irrelevanten Informationen basieren.\n","\n","4. **Verarbeitungsgrenzen und Latenz**\n","   Gr√∂√üere Kontextfenster f√ºhren zu mehr Rechenaufwand, l√§ngeren Antwortzeiten und h√∂herem Ressourcenverbrauch ‚Äì ohne dass die Qualit√§t proportional steigt.\n","\n","5. **Warum das Thema weiterhin wichtig ist**\n","   Mit der zunehmenden Nutzung von **Memory-Funktionen** in Chatbots stellt sich die Frage, wie Erinnerungen strukturiert, gefiltert und priorisiert werden. Nur durch intelligentes Kontext- und Memory-Management lassen sich wirklich relevante Informationen nutzen, ohne das Modell mit unn√∂tigen Daten zu √ºberfluten.\n","\n","\n","<p><font color='darkblue' size=\"4\">\n","‚ÑπÔ∏è <b>Fazit</b>\n","</font></p>\n","\n","Ein gro√ües Kontextfenster allein l√∂st nicht die Herausforderungen von Chat & Memory. Entscheidend sind clevere Strategien, um Informationen zu selektieren, zusammenzufassen und zielgerichtet einzusetzen.\n"],"metadata":{"id":"ZvY5VNktdgTp"},"id":"ZvY5VNktdgTp"},{"cell_type":"markdown","metadata":{"id":"5cfbf26a"},"source":["# A | Aufgaben\n","---\n"],"id":"5cfbf26a"},{"cell_type":"markdown","source":["\n","\n","Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen.\n"],"metadata":{"id":"pwmSuPsGiQp6"},"id":"pwmSuPsGiQp6"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","KI-gest√ºtztes Notizbuch mit automatischer Kategorisierung\n","</font></p>\n","\n","\n","Entwickeln Sie ein KI-Notizbuch, das Eingaben von Nutzern speichert, automatisch kategorisiert (z. B. \"Technologie\", \"Privat\", \"Aufgaben\") und kontextbezogene Vorschl√§ge liefert. Verwenden Sie dazu ein LangChain Memory-Konzept, z.B. ConversationBufferMemory, f√ºr eine effiziente Verwaltung.\n"],"metadata":{"id":"1UViO3SJiSXy"},"id":"1UViO3SJiSXy"},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Chatbot: Nachrichten- vs. Summary-Speicher\n","</font></p>\n","\n","\n","Erstellen Sie einen einfachen Chatbot mit LangChain, der zwischen Nachrichten- und Summary-Speicher unterscheidet. Analysieren Sie, wie sich die Speichertypen auf die Qualit√§t der Antworten auswirken.\n","\n"],"metadata":{"id":"T6HXRFbXjIVL"},"id":"T6HXRFbXjIVL"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Automatische Protokollerstellung f√ºr Meetings\n","</font></p>\n","\n","\n","Entwickeln Sie ein System, das Gespr√§chsverl√§ufe speichert und automatisch eine zusammenfassende Chat-Notiz erstellt. Nutzen Sie z.B. ConversationSummaryMemory, um die wichtigsten Punkte aus langen Gespr√§chen zu extrahieren.\n","\n"],"metadata":{"id":"9eIl83Z-CqT8"},"id":"9eIl83Z-CqT8"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Virtuelle Assistenten mit eigenem Ged√§chtnis\n","</font></p>\n","\n","\n","Eine Datei wird bereitgestellt, die ein Gespr√§ch zwischen zwei virtuellen Assistenten enth√§lt. Jede Antwort soll aus genau einem Satz bestehen. Die Datei ist folgenderma√üen aufgebaut:\n","\n","|Assistent|Eingabe|\n","|---|---|\n","|botA|Ich hei√üe Max.|\n","|botB|Mein Name ist Emma.|\n","|botA|Ich wohne in Berlin.|\n","|botB|Ich lebe in Hamburg.|\n","|botA|Wo wohne ich und wie hei√üe ich?|\n","|botB|Wo lebe ich?|\n","\n","**Aufgabe**  \n","Schreiben Sie ein Programm, das zwei virtuelle Assistenten simuliert, die jeweils ein eigenes Ged√§chtnis haben. Die Eingaben aus der Datei werden an den jeweiligen Assistenten geschickt, und ihre Antworten sollen in einer neuen Datei gespeichert werden.\n","\n","**Erwartete Ausgabe**  \n","Die erwartete Ausgabe k√∂nnte wie folgt aussehen:\n","\n","|Assistent|Antwort|\n","|---|---|\n","|botA|Hallo Max.|\n","|botB|Hallo Emma.|\n","|botA|Danke f√ºr diese Information.|\n","|botB|Danke f√ºr diese Information.|\n","|botA|Hallo Max, du wohnst in Berlin und dein Name ist Max.|\n","|botB|Hallo Emma, du lebst in Hamburg.|\n","\n","**Hinweise**\n","\n","- Die Antworten m√ºssen nicht exakt mit der Beispielausgabe √ºbereinstimmen, sollten aber sinngem√§√ü √§hnlich sein.\n","- Jeder Assistent hat ein eigenes Ged√§chtnis und sollte sich nur an seine eigenen Informationen erinnern.\n","- Das Programm soll die Antworten in einer neuen Datei im gleichen Tabellenformat speichern."],"metadata":{"id":"CwYae6YZCsto"},"id":"CwYae6YZCsto"}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"colab":{"provenance":[],"collapsed_sections":["fe8a8ba4","4jBnf3n8r2qF","ESERG3-dr7GA","1gue2wy2sNY9","3MF-bxiisUpw","DzD3P1mr28MT","nDONejjp2_sm","0kBaYmRFdI7N","5cfbf26a"],"toc_visible":true},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}
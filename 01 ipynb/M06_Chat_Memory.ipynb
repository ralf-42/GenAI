{"cells":[{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)"],"metadata":{"id":"9JKGDENeDeB0"},"id":"9JKGDENeDeB0"},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b> Chat und Memory </b></font> </br></p>\n","\n","---"],"metadata":{"id":"bxo8g0twDZK5"},"id":"bxo8g0twDZK5"},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","collapsed":true,"id":"dfdhPIzcEYRG","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  Colab-Umfeld</font> </br></p>\n","# Installierte Python Version\n","import sys\n","print(f\"Python Version: \",sys.version)\n","# Installierte LangChain Bibliotheken\n","print()\n","print(\"Installierte LangChain Bibliotheken:\")\n","\n","!pip list | grep '^langchain'\n","# Unterdrückt die \"DeprecationWarning\" von LangChain für die Memory-Funktionden\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")"],"id":"dfdhPIzcEYRG"},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WD3Wwr6sESX8"},"outputs":[],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  SetUp API-Keys (setup_api_keys)</font> </br></p>\n","def setup_api_keys():\n","    \"\"\"Konfiguriert alle benötigten API-Keys aus Google Colab userdata\"\"\"\n","    from google.colab import userdata\n","    import os\n","    from os import environ\n","\n","    # Dictionary der benötigten API-Keys\n","    keys = {\n","        'OPENAI_API_KEY': 'OPENAI_API_KEY',\n","        'HF_TOKEN': 'HF_TOKEN',\n","        # Weitere Keys bei Bedarf\n","    }\n","\n","    # Keys in Umgebungsvariablen setzen\n","    for env_var, key_name in keys.items():\n","        environ[env_var] = userdata.get(key_name)\n","\n","    return {k: environ[k] for k in keys.keys()}\n","\n","# Verwendung\n","all_keys = setup_api_keys()\n","# Bei Bedarf einzelne Keys direkt zugreifen\n","# WEATHER_API_KEY = all_keys['WEATHER_API_KEY']"],"id":"WD3Wwr6sESX8"},{"cell_type":"markdown","source":["**NEW Structure**\n","\n","+ 4. ChatMessageHistory-Implementierungen\n","+ 1. Token-Limit (M06 Abschnitt 3)\n","+ 6. Vektorbasierte Speichersysteme\n","+ 5. Mehrschichtige Memory-Architekturen\n","+ 3. Zusammenfassung (M06 Abschnitt 4)\n","+ del: Erweiterte persistente Speicherung (M06 Abschnitt 5 - Komplexversion)\n","+ 2. Nachrichten-Limit (M06 Abschnitt 2)\n","+ del: Vollständige Konversationsverwaltung (M06 Abschnitt 1)\n","..."],"metadata":{"id":"S6TZE41ImL3M"},"id":"S6TZE41ImL3M"},{"cell_type":"markdown","source":["Manuelle Konversationsverwaltung aus M05 \"history\" übernehmen."],"metadata":{"id":"mYcyugWB93I9"},"id":"mYcyugWB93I9"},{"cell_type":"markdown","source":["# Vereinfachtes Beispiel: Persistenter Chat mit ChatMessageHistory\n","\n","Ich habe das Beispiel deutlich vereinfacht und auf das Wesentliche reduziert, ganz im Stil der kompakteren Beispiele aus M06. Das neue Beispiel konzentriert sich auf die Kernelemente:\n","\n","## Hauptmerkmale des vereinfachten Beispiels\n","\n","- **Minimale Imports**: Nur die notwendigsten Bibliotheken\n","- **Einfachere Datenbank-Einrichtung**: Eine knappe Funktion zum Erstellen der erforderlichen Tabelle\n","- **Reduzierte Funktionalität**: Fokus auf den Chat-Kern ohne erweiterte Session-Management-Funktionen\n","- **Vereinfachte Hauptschleife**: Standardmäßig eine einzelne Sitzung mit einfachem Ein-/Ausgabe-Fluss\n","\n","## Struktur des Codes\n","\n","Der Code folgt weiterhin der klaren Strukturierung aus M06:\n","\n","1. **Importe**: Nur die wichtigsten Bibliotheken für die Funktionalität\n","2. **Konstanten**: Minimale Konfiguration mit Modellname, Datenbankpfad und Systemprompt\n","3. **Datenbank einrichten**: Einfache Funktion zum Erstellen der Datenbanktabelle\n","4. **Chat-Komponenten**: Funktionen zum Abrufen der Chathistorie und Erstellen der Chain\n","5. **Chat-Funktion**: Hauptfunktion für die Interaktion mit dem Benutzer\n","6. **Hauptfunktion**: Einfacher Einstiegspunkt zum Starten des Chats\n","\n","## Kernkonzepte\n","\n","Das vereinfachte Beispiel demonstriert immer noch die wichtigsten Konzepte:\n","\n","1. **Verwendung von SQLChatMessageHistory**: Für die persistente Speicherung von Nachrichten\n","2. **Integration mit RunnableWithMessageHistory**: Für einfache Verknüpfung von Chains mit persistentem Speicher\n","3. **Persistenz über SQLite**: Für zuverlässige Datenspeicherung zwischen Programmausführungen\n","\n","## Wie es funktioniert\n","\n","1. Eine SQLite-Datenbank wird eingerichtet, um Chatnachrichten zu speichern\n","2. Eine eindeutige Session-ID wird generiert, um den Chat zu identifizieren\n","3. Die Chat-Schleife ermöglicht Eingaben und zeigt Antworten an\n","4. Alle Nachrichten werden automatisch in der Datenbank gespeichert\n","5. Die Geschichte bleibt erhalten, auch wenn das Programm neu gestartet wird\n","\n","Dieses vereinfachte Beispiel demonstriert dennoch den wichtigen Kernpunkt: Die einfache Integration von ChatMessageHistory-Implementierungen in LangChain für persistenten Speicher."],"metadata":{"id":"Mqu0e6RAoHUO"},"id":"Mqu0e6RAoHUO"},{"cell_type":"code","source":["#\n","# Beispiel für 1. ChatMessageHistory-Implementierungen\n","#\n","# Abschnitt 1: Importe\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.runnables.history import RunnableWithMessageHistory\n","from langchain_openai import ChatOpenAI\n","from langchain_community.chat_message_histories import SQLChatMessageHistory\n","from IPython.display import display, Markdown\n","import sqlite3\n","import uuid\n","\n","# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","DB_PATH = \"chat_history.db\"\n","DEFAULT_SYSTEM = \"Du bist ein hilfreicher Assistent.\"\n","\n","# Abschnitt 3: Datenbank einrichten\n","def setup_database():\n","    \"\"\"Initialisiert die SQLite-Datenbank für Chat-Historien.\"\"\"\n","    conn = sqlite3.connect(DB_PATH)\n","    cursor = conn.cursor()\n","\n","    # Tabelle erstellen, falls sie nicht existiert\n","    cursor.execute(\"\"\"\n","    CREATE TABLE IF NOT EXISTS message_store (\n","        session_id TEXT,\n","        message BLOB,\n","        index INTEGER\n","    )\n","    \"\"\")\n","\n","    conn.commit()\n","    conn.close()\n","    print(\"Datenbank ist bereit.\")\n","\n","# Abschnitt 4: Chat-Komponenten\n","def get_chat_history(session_id):\n","    \"\"\"Gibt ein ChatMessageHistory-Objekt zurück.\"\"\"\n","    return SQLChatMessageHistory(\n","        session_id=session_id,\n","        connection_string=f\"sqlite:///{DB_PATH}\"\n","    )\n","\n","def create_chat_chain():\n","    \"\"\"Erstellt die Chat-Chain mit History-Unterstützung.\"\"\"\n","    # LLM initialisieren\n","    llm = ChatOpenAI(model=MODEL, temperature=0)\n","\n","    # Prompt mit History-Platzhalter\n","    prompt = ChatPromptTemplate.from_messages([\n","        (\"system\", DEFAULT_SYSTEM),\n","        MessagesPlaceholder(variable_name=\"history\"),\n","        (\"human\", \"{input}\")\n","    ])\n","\n","    # Basis-Chain\n","    chain = prompt | llm\n","\n","    # Chain mit History erweitern\n","    chain_with_history = RunnableWithMessageHistory(\n","        chain,\n","        get_chat_history,\n","        input_messages_key=\"input\",\n","        history_messages_key=\"history\"\n","    )\n","\n","    return chain_with_history\n","\n","# Abschnitt 5: Chat-Funktion\n","def chat():\n","    \"\"\"Führt eine einfache Chat-Konversation mit persistenter Geschichte.\"\"\"\n","    # Setup\n","    setup_database()\n","    chain = create_chat_chain()\n","\n","    # Session-ID generieren\n","    session_id = str(uuid.uuid4())[:8]\n","    print(f\"Chat gestartet. Session-ID: {session_id}\")\n","    print(\"Tippe 'exit' zum Beenden.\")\n","\n","    # Chat-Schleife\n","    while True:\n","        # Benutzereingabe\n","        user_input = input(\"\\nMensch: \")\n","        if user_input.lower() == 'exit':\n","            break\n","\n","        # Antwort mit History generieren\n","        response = chain.invoke(\n","            {\"input\": user_input},\n","            config={\"configurable\": {\"session_id\": session_id}}\n","        )\n","\n","        # Antwort anzeigen\n","        print(f\"\\nKI: {response.content}\")\n","\n","# Abschnitt 6: Hauptfunktion\n","def main():\n","    \"\"\"Startet die Chat-Anwendung.\"\"\"\n","    chat()\n","\n","# Wenn direkt ausgeführt\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"MwEAu4i4nOiz"},"id":"MwEAu4i4nOiz","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Vektorbasierte Speichersysteme - Einfaches Beispiel\n","\n","Dieses Beispiel demonstriert, wie man vektorbasierte Speichersysteme mit LangChain für ein semantisches Chatbot-Gedächtnis implementiert. Im Gegensatz zu einfachen sequentiellen Speichermethoden ermöglicht dieser Ansatz das \"Erinnern\" an frühere Gespräche basierend auf semantischer Ähnlichkeit.\n","\n","## Kernkonzept vektorbasierter Speichersysteme\n","\n","Bei diesem Ansatz werden:\n","\n","1. **Alle Nachrichten in Vektoren umgewandelt** (mittels Embedding-Modellen)\n","2. **In einer Vektordatenbank gespeichert** (hier: Chroma)\n","3. **Semantisch durchsucht** (anhand von Ähnlichkeit zur aktuellen Frage)\n","\n","Statt nur die letzten N Nachrichten zu speichern, kann der Chatbot gezielt relevante Informationen aus der gesamten Konversationshistorie abrufen.\n","\n","## Vorteile dieses Ansatzes\n","\n","- **Semantisches Gedächtnis**: Findet relevante Informationen basierend auf Bedeutung, nicht Position\n","- **Skalierbarkeit**: Funktioniert gut mit großen Konversationshistorien\n","- **Selektiver Abruf**: Speichert die vollständige Konversation, ruft aber nur relevante Teile ab\n","\n","## Wie das Beispiel funktioniert\n","\n","Das Beispiel ist in zwei Phasen aufgeteilt:\n","\n","### Phase 1: Informationen sammeln\n","Der Nutzer teilt verschiedene Informationen über Filme mit, die in der Vektordatenbank gespeichert werden.\n","\n","### Phase 2: Gedächtnis testen\n","Nach Löschen des kurzfristigen Chat-Verlaufs werden Fragen gestellt, die Informationen aus Phase 1 abrufen.\n","\n","Die Demonstration zeigt, wie der Chatbot sich an frühere Informationen \"erinnern\" kann, auch wenn sie nicht mehr im direkten Kontext sind, indem er:\n","1. Die aktuelle Frage in einen Vektor umwandelt\n","2. Ähnliche Vektoren (frühere Konversationsteile) in der Datenbank findet\n","3. Diese relevanten Teile in den Prompt einfügt\n","4. Eine informierte Antwort generiert\n","\n","## Technische Komponenten\n","\n","- **OpenAIEmbeddings**: Wandelt Text in Vektoren um\n","- **Chroma**: Leichtgewichtige Vektordatenbank zur Speicherung\n","- **VectorStoreRetrieverMemory**: LangChain-Komponente zur Integration von Vektorspeichern als Gedächtnis\n","- **Retriever**: Konfiguriert für Ähnlichkeitssuche mit den Top 3 Ergebnissen\n","\n","## Anwendungsbereiche\n","\n","Dieser Ansatz ist besonders nützlich für:\n","- Chatbots mit langen Konversationshistorien\n","- Assistenten, die sich an spezifische Details aus früheren Gesprächen erinnern müssen\n","- Systeme, die große Mengen an Kontextinformationen effizient verwalten müssen\n","\n","Vektorbasierte Speichersysteme stellen eine erhebliche Verbesserung gegenüber einfachen sequentiellen Speichermethoden dar und sind eine zentrale Komponente moderner KI-Assistenten."],"metadata":{"id":"jOIW4whioe_Y"},"id":"jOIW4whioe_Y"},{"cell_type":"code","source":["# Abschnitt 1: Importe\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_community.vectorstores import Chroma\n","from langchain.memory import VectorStoreRetrieverMemory\n","from langchain_core.runnables import RunnablePassthrough\n","from IPython.display import display, Markdown\n","import os\n","import uuid\n","\n","# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","EMBEDDING_MODEL = \"text-embedding-ada-002\"\n","VECTOR_DB_PATH = \"chroma_db\"\n","TEMPERATURE = 0.0\n","\n","SYSTEM_PROMPT = \"\"\"Du bist ein hilfreicher Assistent mit semantischem Gedächtnis.\n","Anders als bei einfachen Chats kannst du vergangene Konversationen durchsuchen,\n","um passende Informationen zu finden - auch wenn sie vor längerer Zeit erwähnt wurden.\n","\n","Wenn du auf frühere Informationen zurückgreifst, erwähne kurz, dass du dich an dieses\n","Thema erinnerst, damit der Nutzer den Kontext besser versteht.\"\"\"\n","\n","# Abschnitt 3: Vektorspeicher einrichten\n","def setup_vector_memory():\n","    \"\"\"Richtet den Vektorspeicher und das Memory ein.\"\"\"\n","    # Embeddings-Modell initialisieren\n","    embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL)\n","\n","    # Chroma-Vektordatenbank initialisieren oder laden\n","    vectorstore = Chroma(\n","        collection_name=\"chat_history\",\n","        embedding_function=embeddings,\n","        persist_directory=VECTOR_DB_PATH\n","    )\n","\n","    # Retriever mit moderierten Parametern konfigurieren\n","    retriever = vectorstore.as_retriever(\n","        search_type=\"similarity\",  # Ähnlichkeitssuche verwenden\n","        search_kwargs={\"k\": 3}     # Top 3 ähnlichste Einträge abrufen\n","    )\n","\n","    # Vektorspeicher-Memory erstellen\n","    memory = VectorStoreRetrieverMemory(\n","        retriever=retriever,\n","        memory_key=\"relevant_history\"\n","    )\n","\n","    return memory, vectorstore\n","\n","# Abschnitt 4: Chat-Komponenten initialisieren\n","def initialize_chat_components(memory):\n","    \"\"\"Initialisiert die LLM-Komponenten und Chain mit Vektorspeicher-Memory.\"\"\"\n","    llm = ChatOpenAI(\n","        model=MODEL,\n","        temperature=TEMPERATURE\n","    )\n","\n","    # Prompt Template mit Platzhalter für relevante Gedächtnisinhalte\n","    prompt = ChatPromptTemplate.from_messages([\n","        (\"system\", SYSTEM_PROMPT),\n","        (\"system\", \"Relevante frühere Konversation:\\n{relevant_history}\"),\n","        MessagesPlaceholder(variable_name=\"chat_history\"),\n","        (\"human\", \"{input}\")\n","    ])\n","\n","    # Eine einfache Liste für den aktuellen Chat-Verlauf\n","    chat_history = []\n","\n","    # Funktion zum Laden des Gedächtnisses und Aktualisieren des Chat-Verlaufs\n","    def get_memory_and_chat_history(input_dict):\n","        # Relevante Erinnerungen abrufen\n","        memory_variables = memory.load_memory_variables({\"prompt\": input_dict[\"input\"]})\n","\n","        # Alles zusammenführen\n","        return {\n","            \"relevant_history\": memory_variables[\"relevant_history\"],\n","            \"chat_history\": input_dict[\"chat_history\"],\n","            \"input\": input_dict[\"input\"]\n","        }\n","\n","    # Chain erstellen\n","    chain = RunnablePassthrough.assign(\n","        context=get_memory_and_chat_history\n","    ) | {\n","        \"relevant_history\": lambda x: x[\"context\"][\"relevant_history\"],\n","        \"chat_history\": lambda x: x[\"chat_history\"],\n","        \"input\": lambda x: x[\"input\"]\n","    } | prompt | llm\n","\n","    return chain, chat_history\n","\n","# Abschnitt 5: Interaktionsfunktion\n","def interact_with_ai(question, chain, memory, chat_history):\n","    \"\"\"Verarbeitet eine Konversation mit Vektorspeicher-Gedächtnis.\"\"\"\n","    # Nutzereingabe anzeigen\n","    display(Markdown(f\"### 🧑‍🦱 Mensch:\"))\n","    display(Markdown(question))\n","\n","    # Antwort mit Vektorspeicher-Gedächtnis generieren\n","    response = chain.invoke({\n","        \"input\": question,\n","        \"chat_history\": chat_history\n","    })\n","\n","    # Antwort anzeigen\n","    display(Markdown(f\"### 🤖 KI:\"))\n","    display(Markdown(response.content))\n","    print()\n","\n","    # Aktualisiere den Chat-Verlauf für den aktuellen Kontext\n","    chat_history.append({\"role\": \"human\", \"content\": question})\n","    chat_history.append({\"role\": \"assistant\", \"content\": response.content})\n","\n","    # In den Vektorspeicher speichern (als Key-Value-Paar für bessere Abrufbarkeit)\n","    memory_key = f\"conversation_{uuid.uuid4()}\"\n","    memory.save_context(\n","        {\"prompt\": question},\n","        {\"response\": response.content}\n","    )\n","\n","    return response.content\n","\n","# Abschnitt 6: Hauptfunktion\n","def main():\n","    \"\"\"Hauptfunktion für die Chat-Anwendung mit Vektorspeicher.\"\"\"\n","    display(Markdown(\"# 🧠 Vektorbasierter Memory-Chat\"))\n","\n","    # Vektorspeicher und Chat-Komponenten einrichten\n","    memory, vectorstore = setup_vector_memory()\n","    chain, chat_history = initialize_chat_components(memory)\n","\n","    # Beispielunterhaltung in zwei Phasen, um den langfristigen Abruf zu demonstrieren\n","    print(\"--- Phase 1: Informationen sammeln ---\")\n","    questions_phase1 = [\n","        \"Mein Lieblingsfilm ist Inception.\",\n","        \"Ich finde die Schauspieler Leonardo DiCaprio und Ellen Page darin großartig.\",\n","        \"Der Regisseur Christopher Nolan hat auch The Dark Knight und Interstellar gedreht.\",\n","        \"Ich mag besonders die Szene mit dem rotierenden Hotelflur.\",\n","        \"Ein anderer Film, den ich empfehlen kann, ist The Matrix.\"\n","    ]\n","\n","    for question in questions_phase1:\n","        interact_with_ai(question, chain, memory, chat_history)\n","\n","    # Chat-History zurücksetzen für Phase 2 (simuliert neue Session)\n","    chat_history.clear()\n","\n","    print(\"\\n--- Phase 2: Gedächtnis testen ---\")\n","    questions_phase2 = [\n","        \"Welcher Film hat einen rotierenden Hotelflur?\",\n","        \"Wer hat den Film Inception gedreht?\",\n","        \"Welche Filme habe ich dir empfohlen?\",\n","        \"Welche Schauspieler mochte ich in Inception?\"\n","    ]\n","\n","    for question in questions_phase2:\n","        interact_with_ai(question, chain, memory, chat_history)\n","\n","    # Speichere die Vektordatenbank für zukünftige Sitzungen\n","    vectorstore.persist()\n","    print(f\"Vektordatenbank wurde unter '{VECTOR_DB_PATH}' gespeichert.\")\n","\n","# Wenn direkt ausgeführt, starte die Hauptfunktion\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"PD2SezZcog5x"},"id":"PD2SezZcog5x","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Mehrschichtige Memory-Architekturen - Code-Beispiel\n","\n","Das folgende Beispiel demonstriert eine mehrschichtige Memory-Architektur mit LangChain, die verschiedene Gedächtnistypen kombiniert, um einen umfassenderen Kontext für den LLM-Assistenten zu schaffen.\n","\n","## Kernkonzept der mehrschichtigen Memory-Architektur\n","\n","Die Implementierung kombiniert drei Gedächtnistypen mit unterschiedlichen Stärken:\n","\n","1. **Kurzzeitgedächtnis** (`ConversationBufferWindowMemory`): Speichert die letzten 3 Nachrichten vollständig, für unmittelbaren Kontext\n","2. **Zusammenfassungsgedächtnis** (`ConversationSummaryMemory`): Komprimiert die gesamte vorherige Konversation in eine Zusammenfassung\n","3. **Entitätsgedächtnis** (`ConversationEntityMemory`): Extrahiert und speichert Informationen über erwähnte Entitäten (Personen, Orte, Konzepte)\n","\n","Diese werden mit `CombinedMemory` zu einer einheitlichen Gedächtnisstruktur zusammengefasst, die alle Vorteile kombiniert.\n","\n","## Vorteile dieses Ansatzes\n","\n","- **Effiziente Token-Nutzung**: Jeder Gedächtnistyp fokussiert sich auf unterschiedliche Aspekte, was den Kontext optimiert\n","- **Langzeitinformationen**: Wichtige Informationen werden auch über lange Konversationen hinweg bewahrt\n","- **Kontextbewusstsein**: Der Assistent kann sowohl auf unmittelbare Nachrichten als auch auf langfristige Informationen zugreifen\n","\n","## Beispielerklärung\n","\n","Der Code demonstriert:\n","\n","1. **Einrichtung der Memory-Schichten** mit unterschiedlichen Parametern für jede Schicht\n","2. **Integration mit LangChain** über ein angepasstes Prompt-Template, das alle Gedächtnisarten einbezieht\n","3. **Konversationsablauf** mit Speicherung und Abruf aus allen Gedächtnisschichten\n","4. **Memory-Inspektion** zur Veranschaulichung des Inhalts der verschiedenen Gedächtnistypen\n","\n","## Beispielunterhaltung\n","\n","Die Beispielunterhaltung zeigt, wie der Assistent Informationen aus verschiedenen Schichten nutzt:\n","- Er merkt sich persönliche Details (Name, Wohnort) mit dem Entitätsgedächtnis\n","- Er behält unmittelbare Konversationselemente mit dem Kurzzeitgedächtnis\n","- Er erfasst übergeordnete Themen mit dem Zusammenfassungsgedächtnis\n","\n","## Anwendungsbereiche\n","\n","Dieser Ansatz ist besonders nützlich für:\n","- Persönliche Assistenten, die sich Nutzerdetails merken müssen\n","- Fachberater, die komplexe Themen diskutieren und dabei Kontext behalten müssen\n","- Informationssysteme, die sowohl schnellen Zugriff auf aktuelle als auch langfristige Informationen benötigen\n","\n","Die mehrschichtige Memory-Architektur stellt einen fortschrittlichen Ansatz dar, der die verschiedenen Speichermethoden aus den vorherigen Modulen zu einer leistungsfähigeren Gesamtlösung kombiniert."],"metadata":{"id":"LVptbXzioAqP"},"id":"LVptbXzioAqP"},{"cell_type":"code","source":["#\n","# Mehrschichtig\n","#\n","# Abschnitt 1: Importe\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain.memory import (\n","    ConversationBufferWindowMemory,\n","    ConversationSummaryMemory,\n","    ConversationEntityMemory,\n","    CombinedMemory\n",")\n","from IPython.display import display, Markdown\n","\n","# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATURE = 0.0\n","\n","SYSTEM_PROMPT = \"\"\"Du bist ein hilfsbereiter Assistent mit Zugriff auf verschiedene Gedächtnisschichten:\n","1. Kurzzeitgedächtnis: Die letzten 3 Nachrichten direkt im Kontext\n","2. Zusammenfassungsgedächtnis: Die Kernthemen der bisherigen Unterhaltung\n","3. Entitätsgedächtnis: Wichtige Informationen über erwähnte Personen, Orte oder Dinge\n","\n","Deine Aufgabe ist es, alle verfügbaren Informationen zu nutzen, um hilfreich zu antworten.\"\"\"\n","\n","# Abschnitt 3: Memory-Schichten einrichten\n","def setup_memory_layers():\n","    \"\"\"Richtet die verschiedenen Memory-Schichten ein.\"\"\"\n","    llm = ChatOpenAI(model=MODEL, temperature=TEMPERATURE)\n","\n","    # Schicht 1: Fenster-basiertes Kurzzeitgedächtnis (letzte 3 Nachrichtenpaare)\n","    buffer_memory = ConversationBufferWindowMemory(\n","        k=3,\n","        return_messages=True,\n","        memory_key=\"buffer_history\",\n","        input_key=\"input\",\n","        output_key=\"output\"\n","    )\n","\n","    # Schicht 2: Zusammenfassungs-Gedächtnis (komprimierte frühere Unterhaltung)\n","    summary_memory = ConversationSummaryMemory(\n","        llm=llm,\n","        memory_key=\"summary_history\",\n","        return_messages=False,\n","        input_key=\"input\",\n","        output_key=\"output\"\n","    )\n","\n","    # Schicht 3: Entitäts-Gedächtnis (Informationen über erwähnte Entitäten)\n","    entity_memory = ConversationEntityMemory(\n","        llm=llm,\n","        memory_key=\"entity_store\",\n","        return_messages=False,\n","        input_key=\"input\",\n","        output_key=\"output\",\n","        k=3  # Limitiert auf die 3 relevantesten Entitäten\n","    )\n","\n","    # Alle Gedächtnisschichten kombinieren\n","    combined_memory = CombinedMemory(\n","        memories=[buffer_memory, summary_memory, entity_memory]\n","    )\n","\n","    return combined_memory\n","\n","# Abschnitt 4: Chat-Komponenten initialisieren\n","def initialize_chat_components(memory):\n","    \"\"\"Initialisiert die LLM-Komponenten und Chain mit kombiniertem Memory.\"\"\"\n","    llm = ChatOpenAI(\n","        model=MODEL,\n","        temperature=TEMPERATURE\n","    )\n","\n","    # Prompt Template mit Platzhaltern für die verschiedenen Gedächtnisarten\n","    prompt = ChatPromptTemplate.from_messages([\n","        (\"system\", SYSTEM_PROMPT),\n","        (\"system\", \"Zusammenfassung der bisherigen Konversation:\\n{summary_history}\"),\n","        (\"system\", \"Wichtige Entitäten:\\n{entity_store}\"),\n","        MessagesPlaceholder(variable_name=\"buffer_history\"),\n","        (\"human\", \"{input}\")\n","    ])\n","\n","    # Funktion, die alle Memory-Variablen in die Chain-Eingabe integriert\n","    def get_memory_and_input(input_dict):\n","        memory_variables = memory.load_memory_variables({\"input\": input_dict[\"input\"]})\n","        merged = {**memory_variables, **input_dict}\n","        return merged\n","\n","    # Chain erstellen\n","    chain = (\n","        RunnablePassthrough.assign(memory_variables=get_memory_and_input)\n","        | RunnablePassthrough.assign(\n","            buffer_history=lambda x: x[\"memory_variables\"][\"buffer_history\"],\n","            summary_history=lambda x: x[\"memory_variables\"][\"summary_history\"],\n","            entity_store=lambda x: x[\"memory_variables\"][\"entity_store\"]\n","        )\n","        | prompt\n","        | llm\n","    )\n","\n","    return chain, memory\n","\n","# Abschnitt 5: Interaktionsfunktion\n","def interact_with_ai(question, chain, memory):\n","    \"\"\"Verarbeitet eine Konversation und gibt die Antwort zurück.\"\"\"\n","    # Nutzereingabe anzeigen\n","    display(Markdown(f\"### 🧑‍🦱 Mensch:\"))\n","    display(Markdown(question))\n","\n","    # Antwort mit mehrschichtigem Memory generieren\n","    response = chain.invoke({\"input\": question})\n","\n","    # Antwort anzeigen\n","    display(Markdown(f\"### 🤖 KI:\"))\n","    display(Markdown(response.content))\n","    print()\n","\n","    # Speichere den Dialog im Memory\n","    memory.save_context({\"input\": question}, {\"output\": response.content})\n","\n","    return response.content\n","\n","# Abschnitt 6: Memory-Inspektion\n","def inspect_memory(memory):\n","    \"\"\"Zeigt den Inhalt der verschiedenen Memory-Schichten an.\"\"\"\n","    memory_vars = memory.load_memory_variables({})\n","\n","    display(Markdown(\"## 🔍 Memory-Inspektion\"))\n","\n","    # Kurzzeitgedächtnis anzeigen\n","    display(Markdown(\"### 📋 Kurzzeitgedächtnis (Letzte Nachrichten):\"))\n","    if \"buffer_history\" in memory_vars and memory_vars[\"buffer_history\"]:\n","        for msg in memory_vars[\"buffer_history\"]:\n","            role = \"🧑‍🦱 Mensch\" if msg.type == \"human\" else \"🤖 KI\"\n","            display(Markdown(f\"**{role}:** {msg.content}\"))\n","    else:\n","        display(Markdown(\"*Noch keine Nachrichten im Kurzzeitgedächtnis.*\"))\n","\n","    # Zusammenfassung anzeigen\n","    display(Markdown(\"### 📝 Zusammenfassungsgedächtnis:\"))\n","    if \"summary_history\" in memory_vars and memory_vars[\"summary_history\"]:\n","        display(Markdown(memory_vars[\"summary_history\"]))\n","    else:\n","        display(Markdown(\"*Noch keine Zusammenfassung vorhanden.*\"))\n","\n","    # Entitäten anzeigen\n","    display(Markdown(\"### 🏷️ Entitätsgedächtnis:\"))\n","    if \"entity_store\" in memory_vars and memory_vars[\"entity_store\"]:\n","        entity_text = memory_vars[\"entity_store\"]\n","        display(Markdown(entity_text))\n","    else:\n","        display(Markdown(\"*Noch keine Entitäten gespeichert.*\"))\n","\n","    print()\n","\n","# Abschnitt 7: Hauptfunktion\n","def main():\n","    \"\"\"Hauptfunktion für die Chat-Anwendung.\"\"\"\n","    display(Markdown(\"# 🧠 Mehrschichtiges Memory-Chat-Beispiel\"))\n","\n","    # Memory-Schichten und Chat-Komponenten einrichten\n","    memory = setup_memory_layers()\n","    chain, memory = initialize_chat_components(memory)\n","\n","    # Beispielunterhaltung\n","    test_questions = [\n","        \"Mein Name ist Max und ich lebe in Berlin.\",\n","        \"Ich interessiere mich für Künstliche Intelligenz und maschinelles Lernen.\",\n","        \"Vor allem neuronale Netze finde ich spannend.\",\n","        \"Ich studiere gerade an der TU Berlin.\",\n","        \"Kannst du mir etwas über Reinforcement Learning erzählen?\",\n","        \"Was sind die Unterschiede zwischen überwachtem und unüberwachtem Lernen?\",\n","        \"Könntest du dich daran erinnern, wo ich studiere und wie ich heiße?\",\n","        \"Welche Themen haben wir bisher besprochen?\"\n","    ]\n","\n","    for i, question in enumerate(test_questions, 1):\n","        print(f\"\\n--- Interaktion {i} ---\")\n","        interact_with_ai(question, chain, memory)\n","\n","        # Nach jeder zweiten Interaktion das Memory inspizieren\n","        if i % 2 == 0:\n","            inspect_memory(memory)\n","\n","# Wenn direkt ausgeführt, starte die Hauptfunktion\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"cD6X4a8Zny8h"},"id":"cD6X4a8Zny8h","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"fe8a8ba4","metadata":{"id":"fe8a8ba4"},"source":["# 1 | Intro\n","---\n"]},{"cell_type":"markdown","source":["\n","\n","Große Sprachmodelle (LLMs) ermöglichen eine natürliche Interaktion ähnlich menschlicher Gespräche. Sie sind in der Lage, auf vorherige Informationen innerhalb eines Dialogs Bezug zu nehmen. In diesem Modul werden wir verschiedene Ansätze zur Verwaltung des Gedächtnisses eines LLMs untersuchen.\n","\n","Da der interne Kontextpuffer eines LLMs begrenzt ist, benötigen wir Strategien, um den Chat-Verlauf zu speichern und effizient zu nutzen. LangChain bietet verschiedene Lösungsansätze, die wir in diesem Modul erkunden werden:\n"],"metadata":{"id":"EmMVp49lhkOh"},"id":"EmMVp49lhkOh"},{"cell_type":"markdown","source":["1. **Persistenter Speicher**: Langfristige Speicherung über Sitzungen hinweg\n","2. **Token-basierte Limitierung**: Berücksichtigung der tatsächlichen Tokenlimits des LLM\n","3. **Zusammenfassung von Konversationen**: Komprimieren längerer Konversationen\n","4. **Begrenzung der Nachrichtenanzahl**: Verwendung eines \"Fensters\" der letzten N Nachrichten\n","5. **Manuelle Konversationsverwaltung**: Einfache Speicherung und Weiterleitung der kompletten Konversationshistorie\n","\n","\n","Jeder dieser Ansätze hat seine eigenen Vor- und Nachteile, die wir im Folgenden genauer untersuchen werden.\n"],"metadata":{"id":"cAOs2SNwhmJ-"},"id":"cAOs2SNwhmJ-"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Message-Typen\n","</font></p>"],"metadata":{"id":"kYpOM3O-BJuO"},"id":"kYpOM3O-BJuO"},{"cell_type":"markdown","source":["Die memory-Module von LangChain erwarten Message-Objekte wie SystemMessage, HumanMessage, AIMessage.\n","\n","Sie helfen LangChain (und dem Modell), den Kontext richtig einzuordnen:    \n","Wer sagt was? Welche Rolle hat diese Nachricht im Dialog?\n","\n","Das ist besonders wichtig, wenn man mit ConversationBufferMemory, ConversationSummaryMemory usw. arbeitest, weil diese listenweise Messages speichern und später wiederverwenden."],"metadata":{"id":"SeApNepSBjqR"},"id":"SeApNepSBjqR"},{"cell_type":"code","source":["# Abschnitt 0: Installation und API-Key\n","!uv pip install --system --prerelease allow -q langchain_community langchain_openai"],"metadata":{"id":"WHJhHEoqkIeS"},"id":"WHJhHEoqkIeS","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K2lC_JK3guaj"},"outputs":[],"source":["from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n","from langchain_openai import ChatOpenAI\n","from IPython.display import display, Markdown"],"id":"K2lC_JK3guaj"},{"cell_type":"code","source":["messages = [\n","    SystemMessage(\n","        content=\"Du bist ein kompetenter und hilfreicher Assistent.\"\n","    ),\n","    HumanMessage(\n","        content=\"Nenne mir die Hauptstadt von Usbekistan?\"\n","    ),\n","]"],"metadata":{"id":"zIOnQp7j9Drq"},"execution_count":null,"outputs":[],"id":"zIOnQp7j9Drq"},{"cell_type":"code","source":["type(messages)"],"metadata":{"id":"6KDVJpDa92h6"},"execution_count":null,"outputs":[],"id":"6KDVJpDa92h6"},{"cell_type":"code","source":["MODEL = 'gpt-4o-mini'\n","TEMPERATURE = 0.0\n","\n","llm = ChatOpenAI(model=MODEL, temperature= TEMPERATURE)\n","response = llm.invoke(messages)"],"metadata":{"id":"z0yFqHlZ9Kts"},"execution_count":null,"outputs":[],"id":"z0yFqHlZ9Kts"},{"cell_type":"code","source":["display(Markdown(\"## 📣 Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response.content))"],"metadata":{"id":"UvDvAWVd9WiF"},"execution_count":null,"outputs":[],"id":"UvDvAWVd9WiF"},{"cell_type":"markdown","metadata":{"id":"7a9a2437"},"source":["# 2 | Persistenter Speicher\n","---\n"],"id":"7a9a2437"},{"cell_type":"markdown","source":["\n","\n","Die Möglichkeit eines ChatBots, sich an vorherige Unterhaltungen zu erinnern, spielt eine zentrale Rolle für eine flüssige und konsistente Kommunikation. Ein **persistenter Speicher** erlaubt es, Gesprächsdaten über mehrere Sitzungen hinweg zu sichern und wiederzuverwenden. Dadurch kann der ChatBot nicht nur den Kontext besser erfassen, sondern auch langfristige Nutzerpräferenzen berücksichtigen.\n","\n","Dieser Abschnitt untersucht verschiedene Speicherlösungen für ChatBots und deren effiziente Integration. Dabei werden unterschiedliche Methoden analysiert – von einfachen Dateisystemen über relationale und NoSQL-Datenbanken bis hin zu spezialisierten Speichern für große Sprachmodelle. Zudem wird aufgezeigt, wie gespeicherte Informationen verwaltet, aktualisiert und geschützt werden, um eine ausgewogene Kombination aus Leistungsfähigkeit, Datenschutz und Benutzerfreundlichkeit zu gewährleisten.\n","\n","Die Implementierung eines dauerhaften Speichers ermöglicht es einem ChatBot, nicht nur die Interaktion mit Nutzern zu verbessern, sondern auch personalisierte, kontextbezogene und langfristig relevante Dialoge zu führen.\n","\n","Für die nachfolgende Version wird die Klasse **BaseMemory** für einen persistenten Chat verwendet. Der Chat-Verlauf wird in einer JSON-Datei gespeichert. Diese Klasse dient als abstrakte Basisklasse, von der spezifische Memory-Implementierungen abgeleitet werden, um Kontexte und Zwischenergebnisse in den Arbeitsabläufen (Chains) zu speichern und wiederzuverwenden.\n"],"metadata":{"id":"qKkboVWsiMaD"},"id":"qKkboVWsiMaD"},{"cell_type":"markdown","source":["## 2.1 | Minimalversion\n"],"metadata":{"id":"ncw0vI8MAJrt"},"id":"ncw0vI8MAJrt"},{"cell_type":"code","source":["import os\n","from openai import OpenAI\n","\n","def chat():\n","    # Initialisierung des OpenAI-Clients\n","    client = OpenAI()\n","    file = \"chat_history.txt\"\n","\n","    print(\"Chatbot gestartet.\")\n","    print(\"Befehle: 'history' zeigt Verlauf, 'exit' beendet Chat\")\n","\n","    # Chat-Kontext initialisieren\n","    messages = [\n","        {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher Assistent. Antworte auf Deutsch und sei präzise.\"}\n","    ]\n","\n","    # Lade bestehende Historie beim Start, falls vorhanden\n","    try:\n","        if os.path.exists(file) and os.path.getsize(file) > 0:\n","            print(\"Lade vorherige Chathistorie...\")\n","            chat_pairs = []\n","            with open(file, 'r', encoding='utf-8') as f:\n","                content = f.read()\n","                segments = content.split(\"\\n\\n\")\n","\n","                for segment in segments:\n","                    if segment.strip() and \"Benutzer: \" in segment and \"KI: \" in segment:\n","                        parts = segment.split(\"KI: \", 1)\n","                        if len(parts) == 2:\n","                            user_part = parts[0].replace(\"Benutzer: \", \"\", 1).strip()\n","                            ai_part = parts[1].strip()\n","                            if user_part and ai_part:\n","                                chat_pairs.append((user_part, ai_part))\n","\n","            # Füge die letzten 5 Austausche zum Kontext hinzu (falls vorhanden)\n","            for user_msg, ai_msg in chat_pairs[-10:]:\n","                messages.append({\"role\": \"user\", \"content\": user_msg})\n","                messages.append({\"role\": \"assistant\", \"content\": ai_msg})\n","\n","            print(f\"{len(chat_pairs)} frühere Unterhaltungen gefunden, die letzten {min(10, len(chat_pairs))} werden im Kontext verwendet.\")\n","    except Exception as e:\n","        print(f\"Fehler beim Laden der Historie: {e}\")\n","\n","    # Endlosschleife für den Chat\n","    while True:\n","        user_input = input(\"\\nBenutzer: \")\n","\n","        if user_input == \"exit\":\n","            print(\"Chat beendet.\")\n","            break\n","        elif user_input == \"history\":\n","            try:\n","                with open(file, 'r', encoding='utf-8') as f:\n","                    print(\"\\n--- Chatverlauf ---\")\n","                    print(f.read())\n","                    print(\"--- Ende des Verlaufs ---\\n\")\n","            except FileNotFoundError:\n","                print(\"Keine Chathistorie gefunden.\")\n","            continue\n","\n","        # Benutzer-Nachricht zum Kontext hinzufügen\n","        messages.append({\"role\": \"user\", \"content\": user_input})\n","\n","        # OpenAI API-Anfrage mit vollem Kontext\n","        try:\n","            response = client.chat.completions.create(\n","                model=\"gpt-4o-mini\",\n","                messages=messages,\n","                max_tokens=1000\n","            )\n","            ai_response = response.choices[0].message.content\n","\n","            # Antwort zum Kontext hinzufügen für zukünftige Anfragen\n","            messages.append({\"role\": \"assistant\", \"content\": ai_response})\n","\n","            # Kontext begrenzen, um Token zu sparen (behält nur die letzten 10 Nachrichten + System-Prompt)\n","            if len(messages) > 11:  # System-Prompt + 10 Nachrichten\n","                messages = [messages[0]] + messages[-10:]\n","\n","        except Exception as e:\n","            ai_response = f\"Fehler: {str(e)}\"\n","\n","        print(f\"KI: {ai_response}\")\n","\n","        # Speichern in einfacher Textdatei\n","        with open(file, 'a', encoding='utf-8') as f:\n","            f.write(f\"Benutzer: {user_input}\\nKI: {ai_response}\\n\\n\")"],"metadata":{"id":"pF-E3tIDAKky"},"execution_count":null,"outputs":[],"id":"pF-E3tIDAKky"},{"cell_type":"code","source":["chat()"],"metadata":{"id":"WdxQR3IsAM0a"},"execution_count":null,"outputs":[],"id":"WdxQR3IsAM0a"},{"cell_type":"markdown","source":["## 2.2 | Komfortversion\n"],"metadata":{"id":"gNaHt9iuAfzs"},"id":"gNaHt9iuAfzs"},{"cell_type":"code","source":["# Abschnitt 1: Importe\n","import json\n","import os\n","from datetime import datetime\n","from pathlib import Path\n","from langchain_core.memory import BaseMemory\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate\n","from langchain_core.messages import SystemMessage\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain.memory import ConversationSummaryBufferMemory\n","from pydantic import BaseModel, Field\n","\n","from IPython.display import display, Markdown"],"metadata":{"id":"XIxfBMsQ87Jc"},"execution_count":null,"outputs":[],"id":"XIxfBMsQ87Jc"},{"cell_type":"code","source":["# Abschnitt 2: Konstanten und Klassen\n","MODEL = \"gpt-4o-mini\"\n","DEFAULT_MEMORY_FILE = \"chat_memory.json\"\n","SYSTEM_PROMPT = \"\"\"Du bist ein kompetenter KI-Assistent mit breitem Fachwissen.\n","\n","Deine Antworten sind:\n","- Klar strukturiert und mit Markdown formatiert\n","- Praxisorientiert und direkt umsetzbar\n","- Basierend auf aktuellem Kenntnisstand\n","- Mit passenden Beispielen versehen\n","- In verständlicher Sprache formuliert\"\"\"\n","\n","class EnhancedJSONMemory(BaseMemory, BaseModel):\n","    \"\"\"Erweiterte Memory-Implementierung mit JSON-Speicherung, Statistiken und Zusammenfassung.\"\"\"\n","    file_path: str = Field(default=DEFAULT_MEMORY_FILE)\n","    summary_memory: object = Field(default=None)\n","    chat_data: dict[str, object] = Field(default_factory=lambda: {\n","        \"conversations\": [],\n","        \"metadata\": {\n","            \"created_at\": datetime.now().isoformat(),\n","            \"last_modified\": datetime.now().isoformat(),\n","            \"total_interactions\": 0,\n","            \"model_name\": MODEL,\n","            \"version\": \"2.0\"\n","        },\n","        \"statistics\": {\n","            \"average_response_length\": 0,\n","            \"total_chars_exchanged\": 0\n","        },\n","        \"summary\": \"\"\n","    })\n","\n","    def __init__(self, **kwargs) -> None:\n","        \"\"\"Initialisiert die Memory-Klasse und lädt existierende Chatverläufe.\"\"\"\n","        super().__init__(**kwargs)\n","        self.summary_memory = ConversationSummaryBufferMemory(\n","            llm=ChatOpenAI(temperature=0, model=MODEL),\n","            max_token_limit=2000,\n","            memory_key=\"summary\",\n","            human_prefix=\"Mensch\",\n","            ai_prefix=\"KI\"\n","        )\n","        self.load_history()\n","\n","    def format_timestamp(self, timestamp_str: str) -> str:\n","        \"\"\"Formatiert einen ISO-Zeitstempel in ein lesbares Format.\"\"\"\n","        return datetime.fromisoformat(timestamp_str).strftime(\"%d.%m.%Y %H:%M:%S\")\n","\n","    def update_statistics(self, user_input: str, response: str) -> None:\n","        \"\"\"Aktualisiert die Chatstatistiken.\"\"\"\n","        stats = self.chat_data[\"statistics\"]\n","        total_responses = len(self.chat_data[\"conversations\"])\n","\n","        if total_responses > 0:\n","            current_total = stats[\"average_response_length\"] * (total_responses - 1)\n","            stats[\"average_response_length\"] = round((current_total + len(response)) / total_responses, 2)\n","\n","        stats[\"total_chars_exchanged\"] += len(user_input) + len(response)\n","\n","    def update_summary(self, user_input: str, response: str) -> None:\n","        \"\"\"Aktualisiert die Konversationszusammenfassung.\"\"\"\n","        self.summary_memory.save_context({\"input\": user_input}, {\"output\": response})\n","        self.chat_data[\"summary\"] = self.summary_memory.load_memory_variables({}).get(\"summary\", \"\")\n","\n","    def save_context(self, inputs: dict[str], outputs: dict[str, str]) -> None:\n","        \"\"\"Speichert den aktuellen Kontext mit erweiterten Metadaten.\"\"\"\n","        self.chat_data[\"conversations\"].append({\n","            \"timestamp\": datetime.now().isoformat(),\n","            \"user_input\": inputs[\"input\"],\n","            \"response\": outputs[\"text\"],\n","            \"interaction_id\": len(self.chat_data[\"conversations\"]) + 1\n","        })\n","        self.chat_data[\"metadata\"][\"last_modified\"] = datetime.now().isoformat()\n","        self.chat_data[\"metadata\"][\"total_interactions\"] += 1\n","\n","        self.update_statistics(inputs[\"input\"], outputs[\"text\"])\n","        self.update_summary(inputs[\"input\"], outputs[\"text\"])\n","        self._save_to_file()\n","\n","    def _save_to_file(self) -> None:\n","        \"\"\"Speichert den Chatverlauf in der JSON-Datei.\"\"\"\n","        try:\n","            with Path(self.file_path).open(\"w\", encoding=\"utf-8\") as file:\n","                json.dump(self.chat_data, file, ensure_ascii=False, indent=2)\n","        except Exception as e:\n","            md_print(f\"⚠️ **Fehler beim Speichern der Historie:** {str(e)}\")\n","\n","    def load_history(self) -> None:\n","        \"\"\"Lädt die Chathistorie aus der JSON-Datei.\"\"\"\n","        if not Path(self.file_path).exists():\n","            return\n","\n","        try:\n","            with Path(self.file_path).open(\"r\", encoding=\"utf-8\") as file:\n","                loaded_data = json.load(file)\n","\n","            if \"metadata\" in loaded_data and \"version\" in loaded_data[\"metadata\"]:\n","                self.chat_data = loaded_data\n","                md_print(\"✅ **Konversationshistorie wurde erfolgreich geladen.**\")\n","\n","                # Lade vorhandene Konversationen in das Summary Memory\n","                for conv in self.chat_data[\"conversations\"]:\n","                    self.summary_memory.save_context(\n","                        {\"input\": conv[\"user_input\"]},\n","                        {\"output\": conv[\"response\"]}\n","                    )\n","            else:\n","                md_print(\"⚠️ **Veraltetes Historienformat erkannt. Erstelle neue Historie.**\")\n","        except json.JSONDecodeError as e:\n","            md_print(f\"⚠️ **Fehler beim Laden der JSON-Datei:** {str(e)}\")\n","\n","    def load_memory_variables(self, inputs: dict[str]) -> dict[str, list[dict[str]]]:\n","        \"\"\"Lädt die Chatvariablen für den Prompt.\"\"\"\n","        return {\"history\": self.chat_data[\"conversations\"], \"summary\": self.chat_data[\"summary\"]}\n","\n","    @property\n","    def memory_variables(self) -> list[str]:\n","        \"\"\"Definiert die verfügbaren Memory-Variablen.\"\"\"\n","        return [\"history\", \"summary\"]\n","\n","    def clear(self) -> None:\n","        \"\"\"Löscht die Chathistorie und speichert den zurückgesetzten Zustand.\"\"\"\n","        # Zurücksetzen des Konversations-Arrays\n","        self.chat_data[\"conversations\"] = []\n","        # Zurücksetzen der Zusammenfassung\n","        self.chat_data[\"summary\"] = \"\"\n","        # Zurücksetzen der Statistiken\n","        self.chat_data[\"statistics\"] = {\n","            \"average_response_length\": 0,\n","            \"total_chars_exchanged\": 0\n","        }\n","        # Metadaten aktualisieren\n","        self.chat_data[\"metadata\"][\"last_modified\"] = datetime.now().isoformat()\n","        self.chat_data[\"metadata\"][\"total_interactions\"] = 0\n","\n","        # Summary Memory zurücksetzen\n","        self.summary_memory.clear()\n","\n","        # Gespeicherte Datei aktualisieren\n","        self._save_to_file()\n","\n","        md_print(\"🧹 **Chathistorie wurde erfolgreich gelöscht.**\")\n","\n","    def show_history(self) -> None:\n","        \"\"\"Zeigt eine detaillierte Konversationshistorie und Zusammenfassung.\"\"\"\n","        if not self.chat_data[\"conversations\"]:\n","            md_print(\"❌ **Keine Konversationshistorie vorhanden.**\")\n","            return\n","\n","        history_markdown = []\n","        history_markdown.append(\"\\n## 📊 Chat-Statistiken:\")\n","        stats = self.chat_data[\"statistics\"]\n","        meta = self.chat_data[\"metadata\"]\n","        history_markdown.append(f\"- **Gesamtinteraktionen:** {meta['total_interactions']}\")\n","        history_markdown.append(f\"- **Durchschnittliche Antwortlänge:** {stats['average_response_length']} Zeichen\")\n","        history_markdown.append(f\"- **Gesamter Zeichenaustausch:** {stats['total_chars_exchanged']} Zeichen\")\n","\n","        if self.chat_data[\"summary\"]:\n","            history_markdown.append(\"\\n## 📝 Zusammenfassung der Konversation:\")\n","            history_markdown.append(self.chat_data[\"summary\"])\n","\n","        history_markdown.append(\"\\n## 📜 Detaillierte Konversationshistorie:\")\n","        for conv in self.chat_data[\"conversations\"]:\n","            history_markdown.append(\"\\n\" + \"=\"*50)\n","            history_markdown.append(f\"### 🔢 Interaktion #{conv['interaction_id']}\")\n","            history_markdown.append(f\"**🕒 Zeitpunkt:** {self.format_timestamp(conv['timestamp'])}\")\n","            history_markdown.append(f\"**🧑‍🦱 Mensch:** {conv['user_input']}\")\n","            history_markdown.append(f\"**🤖 KI:**\\n{conv['response']}\")\n","\n","        md_print(\"\\n\".join(history_markdown))\n"],"metadata":{"id":"CaWtSTHR9kgd"},"execution_count":null,"outputs":[],"id":"CaWtSTHR9kgd"},{"cell_type":"code","source":["# Abschnitt 3: Chat Komponenten\n","# LLM und Memory initialisieren\n","llm = ChatOpenAI(model=MODEL, temperature=0.0)\n","memory = EnhancedJSONMemory()\n","\n","# Prompt Template und Chat Sequence erstellen\n","prompt = ChatPromptTemplate.from_messages([\n","    SystemMessage(content=SYSTEM_PROMPT),\n","    (\"system\", \"Zusammenfassung der bisherigen Konversation:\\n{summary}\\n\"),\n","    (\"system\", \"Bisheriger Chatverlauf:\\n{history}\\n\"),\n","    (\"human\", \"{input}\")\n","])\n","\n","# Hilfsfunktion für Formatierung\n","def format_history(history):\n","    return \"\\n\".join([f\"Mensch: {entry['user_input']}\\nKI: {entry['response']}\" for entry in history])\n","\n","# Chat Sequence erstellen\n","chat_sequence = (\n","    {\n","        \"history\": lambda x: format_history(memory.load_memory_variables({})[\"history\"]),\n","        \"summary\": lambda x: memory.load_memory_variables({})[\"summary\"],\n","        \"input\": lambda x: x[\"input\"]\n","    }\n","    | prompt\n","    | llm\n",")"],"metadata":{"id":"28Qv2Sm59m6l"},"execution_count":null,"outputs":[],"id":"28Qv2Sm59m6l"},{"cell_type":"code","source":["# Abschnitt 4: Funktionen\n","def md_print(text):\n","    \"\"\"Gibt Text als Markdown aus, wenn möglich, sonst als normalen Text.\"\"\"\n","    try:\n","        # Prüfen, ob wir in einem Jupyter Notebook sind\n","        is_notebook = 'ipykernel' in sys.modules\n","        if is_notebook:\n","            display(Markdown(text))\n","        else:\n","            print(text)\n","    except NameError:\n","        # Wenn sys nicht definiert ist, nehmen wir an, dass wir nicht in einem Notebook sind\n","        print(text)\n","\n","def run_chat(chat_sequence, memory) -> None:\n","    \"\"\"Führt die Chat-Schleife aus.\"\"\"\n","    md_print(\"## Chat gestartet\\n\")\n","    md_print(\"**Befehle:**\\n- `history`: Zeigt die Chathistorie\\n- `clear`: Löscht die Chathistorie\\n- `exit` oder `tschüss`: Beendet den Chat\\n\")\n","\n","    # Prüfen, ob wir in einem Jupyter Notebook sind\n","    try:\n","        is_notebook = 'ipykernel' in sys.modules\n","    except NameError:\n","        # sys ist nicht definiert, also sind wir nicht in einem Notebook\n","        is_notebook = False\n","\n","    while True:\n","        # Die Eingabeaufforderung bleibt ein normales print, da input() damit arbeitet\n","        if is_notebook:\n","            from IPython.display import clear_output\n","            # In Notebooks können wir es schöner machen mit garantierter Breitenreduzierung\n","            from ipywidgets import widgets\n","\n","            # Container-Widget mit fester Breite erstellen\n","            container = widgets.HBox(\n","                layout=widgets.Layout(\n","                    width='60%',       # Container auf 60% der verfügbaren Breite beschränken\n","                    margin='0px'\n","                )\n","            )\n","\n","            # Textfeld erstellen (nimmt die volle Breite des Containers ein)\n","            user_input_widget = widgets.Text(\n","                description=\"🧠\",\n","                placeholder=\"Deine Nachricht hier...\",\n","                style={'description_width': '40px'},\n","                layout=widgets.Layout(width='100%')  # Füllt den Container vollständig aus\n","            )\n","\n","            # Das Textfeld in den Container einfügen und anzeigen\n","            container.children = [user_input_widget]\n","            display(container)\n","\n","            # Warten auf Eingabe\n","            user_input = input(\"Eingabe: \")\n","            user_input_widget.value = user_input\n","        else:\n","            user_input = input(\"\\U0001F9B1 Mensch: \")\n","\n","        if user_input.lower() in [\"exit\", \"tschüss\"]:\n","            break\n","        elif user_input.lower() == \"history\":\n","            memory.show_history()\n","            continue\n","        elif user_input.lower() == \"clear\":\n","            memory.clear()\n","            continue\n","\n","        response = chat_sequence.invoke({\"input\": user_input})\n","        memory.save_context({\"input\": user_input}, {\"text\": response.content})\n","\n","        # Markdown-Ausgabe für die Antwort\n","        md_print(f\"\\n## 🤖 KI:\\n{response.content}\\n\")"],"metadata":{"id":"PpFbPKZq9qvH"},"execution_count":null,"outputs":[],"id":"PpFbPKZq9qvH"},{"cell_type":"code","source":["# Abschnitt 5: Hauptfunktion\n","def main() -> None:\n","    \"\"\"Hauptfunktion zum Starten des Chat-Bots.\"\"\"\n","    md_print(\"## 🤖 KI-Chat mit Gedächtnis\")\n","\n","    # sys importieren, wenn noch nicht geschehen\n","    try:\n","        import sys\n","    except ImportError:\n","        pass\n","\n","    # Chat-Komponenten sind bereits global definiert\n","    run_chat(chat_sequence, memory)"],"metadata":{"id":"INS1Ykc89sJF"},"execution_count":null,"outputs":[],"id":"INS1Ykc89sJF"},{"cell_type":"code","source":["# Ausführen, wenn direkt gestartet\n","main()"],"metadata":{"id":"3L89ijxT9XPz"},"execution_count":null,"outputs":[],"id":"3L89ijxT9XPz"},{"cell_type":"markdown","source":["# 3 | Token-Limit\n","---\n"],"metadata":{"id":"7-GWGzh-iBGC"},"id":"7-GWGzh-iBGC"},{"cell_type":"markdown","source":["\n","\n","Die vorherigen Ansätze begrenzten die Anzahl der gespeicherten Nachrichten. In der Praxis ist jedoch das **Token-Limit** des Modells entscheidend. `ConversationTokenBufferMemory` adressiert dieses Problem, indem es den Kontext basierend auf der tatsächlichen Token-Anzahl begrenzt."],"metadata":{"id":"Td23eDsfiCla"},"id":"Td23eDsfiCla"},{"cell_type":"code","source":["# Abschnitt 1: Importe\n","from IPython.display import display, Markdown\n","from langchain.chains import ConversationChain\n","from langchain.memory import ConversationTokenBufferMemory\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import PromptTemplate\n","from langchain.schema import AIMessage"],"metadata":{"id":"iLUCnCgcvKpM"},"execution_count":null,"outputs":[],"id":"iLUCnCgcvKpM"},{"cell_type":"code","source":["# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","MAX_TOKEN_LIMIT = 1024\n","DEFAULT_SYSTEM = \"\"\"\n","Du bist ein kompetenter KI-Assistent mit breitem Fachwissen.\n","\n","Deine Antworten sind:\n","- Klar strukturiert und mit Markdown formatiert\n","- Praxisorientiert und direkt umsetzbar\n","- Basierend auf aktuellem Kenntnisstand\n","- Mit passenden Beispielen versehen\n","- In verständlicher Sprache formuliert\n","\n","Aktuelle Konversation:\n","{history}\n","Mensch: {input}\n","KI:\n","\"\"\""],"metadata":{"id":"0aBGv_RGwSjh"},"execution_count":null,"outputs":[],"id":"0aBGv_RGwSjh"},{"cell_type":"code","source":["# Abschnitt 3: Funktionen definieren\n","def interact_with_ai(question):\n","    \"\"\" Führt eine Interaktion mit der KI durch und verarbeitet die Antwort. \"\"\"\n","    # Zeige die Frage an\n","    display(Markdown(f\"**🧑‍🦱 Mensch:**\"))\n","    display(Markdown(f\"{question}\"))\n","\n","    # KI-Antwort abrufen\n","    response = chain.invoke({\"input\": question})\n","\n","    # Antwort je nach Typ verarbeiten\n","    if isinstance(response, dict) and \"response\" in response:\n","        response_text = response[\"response\"]\n","    elif isinstance(response, AIMessage):\n","        response_text = response.content\n","    else:\n","        response_text = str(response)\n","\n","    # Antwort anzeigen\n","    display(Markdown(f\"**🤖 KI:**\"))\n","    display(Markdown(f\"{response_text}\"))\n","\n","    # Konversation im Memory speichern\n","    memory.save_context({\"input\": question}, {\"output\": response_text})\n","\n","    # Kein return, um doppelte Ausgabe zu vermeiden\n","    return None"],"metadata":{"id":"nUCbVrPlwT8y"},"execution_count":null,"outputs":[],"id":"nUCbVrPlwT8y"},{"cell_type":"code","source":["# Abschnitt 4: LLM und Kette initialisieren\n","llm = ChatOpenAI(\n","    model_name=MODEL,\n","    temperature=0.0\n",")\n","\n","memory = ConversationTokenBufferMemory(\n","    llm=llm,\n","    max_token_limit=MAX_TOKEN_LIMIT,\n","    return_messages=True\n",")\n","\n","# Prompt Template erstellen\n","prompt = PromptTemplate(\n","    input_variables=[\"history\", \"input\"],\n","    template=DEFAULT_SYSTEM\n",")\n","\n","# Conversation Chain erstellen\n","chain = ConversationChain(\n","    llm=llm,\n","    memory=memory,\n","    prompt=prompt,\n","    verbose=False\n",")"],"metadata":{"id":"aCl7mvN2wZCF"},"execution_count":null,"outputs":[],"id":"aCl7mvN2wZCF"},{"cell_type":"code","source":["# Abschnitt 5: Interaktionen mit dem Modell\n","interact_with_ai(\"Mein Name ist Ralf\")\n","interact_with_ai(\"Warum ist der Himmel blau?\")\n","interact_with_ai(\"Und warum ist er manchmal rot?\")\n","interact_with_ai(\"Wie ist mein Name?\")"],"metadata":{"id":"o2pjNJV0wavF"},"execution_count":null,"outputs":[],"id":"o2pjNJV0wavF"},{"cell_type":"markdown","metadata":{"id":"7b95a6bf"},"source":["Gespeicherten Kontext nach Ausführung:"],"id":"7b95a6bf"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9d8ca33b"},"outputs":[],"source":["# Gespeicherter Kontext anzeigen\n","display(Markdown(\"## ✨ Gespeicherter Kontext:\"))\n","display(Markdown(\"---\"))\n","\n","# Lade den Speicher\n","memory_ai = memory.load_memory_variables({})[\"history\"]\n","\n","# Erstelle eine formatierte Markdown-Ausgabe für alle Nachrichten\n","for msg in memory_ai:\n","    markdown_content = f\"**Typ:** {msg.type}\\n\\n**Inhalt:** {msg.content}\"\n","    display(Markdown(markdown_content))"],"id":"9d8ca33b"},{"cell_type":"markdown","metadata":{"id":"4b4bd50e"},"source":["Der Hauptvorteil dieses Ansatzes ist die präzise Kontrolle über die tatsächliche Token-Anzahl, die ans Modell übergeben wird. Statt willkürlich die letzten N Nachrichten zu speichern, behält diese Implementierung so viele Nachrichten wie möglich innerhalb des Token-Limits bei.\n","\n"],"id":"4b4bd50e"},{"cell_type":"code","source":["#\n","# NEW besser als Version oben\n","#\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_openai import ChatOpenAI\n","from langchain_core.messages import HumanMessage, AIMessage\n","\n","# Abschnitt 2: Konstanten definieren (bleibt gleich)\n","MODEL = \"gpt-4o-mini\"\n","MAX_TOKEN_LIMIT = 1024\n","SYSTEM_TEMPLATE = \"\"\"\n","Du bist ein kompetenter KI-Assistent mit breitem Fachwissen.\n","\n","Deine Antworten sind:\n","- Klar strukturiert und mit Markdown formatiert\n","- Praxisorientiert und direkt umsetzbar\n","- Basierend auf aktuellem Kenntnisstand\n","- Mit passenden Beispielen versehen\n","- In verständlicher Sprache formuliert\n","\"\"\"\n","\n","# Abschnitt 4: LLM und Memory initialisieren\n","llm = ChatOpenAI(\n","    model_name=MODEL,\n","    temperature=0.0\n",")\n","\n","# Memory mit MessagesPlaceholder statt ConversationTokenBufferMemory\n","from langchain.memory import ConversationTokenBufferMemory\n","\n","memory = ConversationTokenBufferMemory(\n","    llm=llm,\n","    max_token_limit=MAX_TOKEN_LIMIT,\n","    return_messages=True\n",")\n","\n","# Prompt mit ChatPromptTemplate statt PromptTemplate\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", SYSTEM_TEMPLATE),\n","    MessagesPlaceholder(variable_name=\"history\"),\n","    (\"human\", \"{input}\")\n","])\n","\n","# LCEL Chain statt ConversationChain\n","def load_memory(_):\n","    return memory.chat_memory.messages\n","\n","def save_memory(input_and_output):\n","    user_input = input_and_output[\"input\"]\n","    ai_output = input_and_output[\"output\"]\n","    memory.save_context({\"input\": user_input}, {\"output\": ai_output})\n","    return ai_output\n","\n","# Die LCEL Chain-Definition\n","chain = (\n","    RunnablePassthrough.assign(history=load_memory)\n","    | prompt\n","    | llm\n","    | StrOutputParser()  # Garantiert String-Output\n","    | save_memory\n",")\n","\n","# Vereinfachte interact_with_ai Funktion\n","def interact_with_ai(question):\n","    \"\"\" Führt eine Interaktion mit der KI durch und verarbeitet die Antwort. \"\"\"\n","    # Zeige die Frage an\n","    display(Markdown(f\"**🧑‍🦱 Mensch:**\"))\n","    display(Markdown(f\"{question}\"))\n","\n","    # KI-Antwort abrufen - immer als String dank StrOutputParser\n","    response_text = chain.invoke({\"input\": question})\n","\n","    # Antwort anzeigen\n","    display(Markdown(f\"**🤖 KI:**\"))\n","    display(Markdown(f\"{response_text}\"))\n","\n","    # Memory wird bereits in der Chain gespeichert (save_memory Funktion)\n","\n","    # Kein return, um doppelte Ausgabe zu vermeiden\n","    return None"],"metadata":{"id":"ZkJ9LKYCt-RN"},"id":"ZkJ9LKYCt-RN","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4 | Zusammenfassung\n","---"],"metadata":{"id":"3NVtW3sKiGfB"},"id":"3NVtW3sKiGfB"},{"cell_type":"markdown","source":["\n","\n","\n","Die bisher betrachteten Ansätze haben entweder die vollständige Historie oder einen Teil davon bewahrt. Eine Alternative ist, längere Konversationen zusammenzufassen, um wichtige Informationen zu behalten, ohne die Token-Limits zu überschreiten.\n","\n","LangChain's `ConversationSummaryMemory` speichert und aktualisiert eine Zusammenfassung der Konversation, anstatt ältere Chatverläufe zu vergessen. Dies ermöglicht die Bewahrung wichtiger Informationen und hält den Speicher effizient."],"metadata":{"id":"SkwJX0VPiHpK"},"id":"SkwJX0VPiHpK"},{"cell_type":"code","execution_count":null,"metadata":{"id":"47a38211"},"outputs":[],"source":["# Abschnitt 1: Importe\n","from IPython.display import display, Markdown\n","\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.messages import HumanMessage, AIMessage\n","from langchain_openai import ChatOpenAI\n","from langchain.memory import ConversationSummaryMemory\n","from langchain_core.runnables import RunnablePassthrough\n","\n","# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","\n","conversations = [\n","    \"Mein Name ist Ralf\",\n","    \"Warum ist der Himmel blau?\",\n","    \"Und warum ist er manchmal rot?\",\n","    \"Wie ist mein Name?\"\n","]\n","\n","# Abschnitt 3: Funktionen definieren\n","def interact_with_ai(question):\n","    \"\"\"Führt eine einzelne Interaktion mit der KI durch und verarbeitet die Antwort.\"\"\"\n","    display(Markdown(f\"**🧑‍🦱 Mensch:**\"))\n","    display(Markdown(f\"{question}\"))\n","\n","    response = chain.invoke({\"input\": question})\n","\n","    # Antwort extrahieren\n","    if isinstance(response, dict) and \"response\" in response:\n","        response_text = response[\"response\"]\n","    elif isinstance(response, AIMessage):\n","        response_text = response.content\n","    else:\n","        response_text = str(response)\n","\n","    display(Markdown(f\"**🤖 KI:**\"))\n","    display(Markdown(f\"{response_text}\"))\n","    print()\n","\n","    # Speichere die Konversation im Memory\n","    memory.save_context({\"input\": question}, {\"output\": response_text})\n","\n","    return response_text\n","\n","# Abschnitt 4: Chain und Memory initialisieren\n","# LLM initialisieren\n","llm = ChatOpenAI(\n","    temperature=0,\n","    model_name=MODEL\n",")\n","\n","# Memory initialisieren\n","memory = ConversationSummaryMemory(\n","    llm=llm,\n","    return_messages=True,\n","    max_token_limit=2000\n",")\n","\n","# Prompt Template erstellen\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent. Nutze vorherige Konversationen für kontextbezogene Antworten.\"),\n","    MessagesPlaceholder(variable_name=\"history\"),\n","    (\"human\", \"{input}\")\n","])\n","\n","# Chain erstellen\n","chain = (\n","    RunnablePassthrough.assign(\n","        history=lambda x: memory.load_memory_variables({})[\"history\"]\n","    )\n","    | prompt\n","    | llm\n",")\n","\n","# Abschnitt 5: Nachrichten verarbeiten und Ergebnisse anzeigen\n","for user_input in conversations:\n","    interact_with_ai(user_input)"],"id":"47a38211"},{"cell_type":"markdown","metadata":{"id":"59b1182e"},"source":["Nach der Ausführung können wir die erzeugte Zusammenfassung betrachten:"],"id":"59b1182e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"acff52ee"},"outputs":[],"source":["# Ausgabe der Zusammenfassung mit display(Markdown())\n","display(Markdown(\"## Aktuelle Zusammenfassung:\"))\n","summary_ai = memory.load_memory_variables({})[\"history\"]\n","display(Markdown(f\"{summary_ai[0].content}\"))"],"id":"acff52ee"},{"cell_type":"markdown","metadata":{"id":"289eb07e"},"source":["Dieser Ansatz ist besonders nützlich für längere Konversationen, da er die wichtigsten Informationen extrahiert und kompakt behält, statt nur die letzten N Nachrichten. Dadurch bleibt der Kontext erhalten, ohne das Token-Limit zu sprengen."],"id":"289eb07e"},{"cell_type":"markdown","source":["# 5 | Nachrichten-Limit\n","---\n"],"metadata":{"id":"jUrNYwNah2W0"},"id":"jUrNYwNah2W0"},{"cell_type":"markdown","source":["\n","\n","Manchmal möchten wir die Anzahl der Nachrichten begrenzen, die ein Modell als Kontext erhält, anstatt die komplette Historie zu verwenden. Dies kann sinnvoll sein, wenn:\n","\n","1. Der jüngste Kontext am relevantesten ist\n","2. Wir Tokens sparen wollen\n","3. Wir eine fokussiertere Antwort ohne zu viel Ablenkung durch ältere Themen wünschen\n","\n"],"metadata":{"id":"d523zHw0D2-B"},"id":"d523zHw0D2-B"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","ConversationBufferWindowMemory\n","</font></p>"],"metadata":{"id":"RkAAy6L5h4eL"},"id":"RkAAy6L5h4eL"},{"cell_type":"code","source":["# Abschnitt 0: Installation und API-Key\n","!uv pip install --system --prerelease allow -q langchain_community langchain_openai"],"metadata":{"id":"rG1ndc8NfMea"},"id":"rG1ndc8NfMea","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"cd0358c8","metadata":{"id":"cd0358c8"},"outputs":[],"source":["# Abschnitt 1: Importe\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain.memory import ConversationBufferWindowMemory\n","from langchain.schema import SystemMessage, HumanMessage, AIMessage\n","from IPython.display import display, Markdown\n","from operator import itemgetter"]},{"cell_type":"code","source":["# Abschnitt 2: Konstanten\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATURE = 0.0\n","DEFAULT_SYSTEM = \"Du bist ein hilfsbereiter KI-Assistent.\"\n","MAX_HISTORY = 3  # Anzahl der zu speichernden Nachrichtenpaare\n","\n","TEST_QUESTIONS = [\n","    \"Mein Lieblingsgericht ist Lasagne. Antworte kurz.\",\n","    \"Wie viele Tage hat eine Woche? Antworte kurz.\",\n","    \"Wie viele Tage hat ein Monat? Antworte kurz.\",\n","    \"Wie viele Tage hat ein Jahr? Antworte kurz.\",\n","    \"Was ist mein Lieblingsgericht?\"\n","]"],"metadata":{"id":"O80lYM50qb8l"},"id":"O80lYM50qb8l","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 3: Chat-Komponenten initialisieren\n","llm = ChatOpenAI(temperature=TEMPERATURE, model=MODEL)\n","\n","# Memory mit k=3 für Fenstergröße\n","memory = ConversationBufferWindowMemory(\n","    k=MAX_HISTORY,\n","    return_messages=True,\n","    memory_key=\"history\"\n",")\n","\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", DEFAULT_SYSTEM),\n","    MessagesPlaceholder(variable_name=\"history\"),\n","    (\"human\", \"{input}\")\n","])\n","\n","def get_chat_history(input_dict: dict) -> dict:\n","    \"\"\"Lädt die Chat-Historie aus dem Memory.\"\"\"\n","    memory_vars = memory.load_memory_variables({})\n","    return {\n","        \"history\": memory_vars[\"history\"],\n","        \"input\": input_dict[\"input\"]\n","    }\n","\n","# Chain mit Pipe erstellen\n","chain = get_chat_history | prompt | llm"],"metadata":{"id":"zivpRnk8qdSW"},"id":"zivpRnk8qdSW","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 4: Funktionen definieren\n","def interact_with_ai(question):\n","    \"\"\"Führt eine Interaktion mit der KI durch und verarbeitet die Antwort.\"\"\"\n","    # Ausgabe der Frage\n","    display(Markdown(f\"\\n🧑‍🦱 Mensch:\"))\n","    display(Markdown(question))\n","    display(Markdown(\"\\n🤖 KI: \"))\n","\n","    # KI-Antwort einholen\n","    response = chain.invoke({\"input\": question})\n","\n","    # Antworttext extrahieren\n","    response_text = response.content if isinstance(response, AIMessage) else str(response)\n","\n","    # Formatierte Ausgabe erstellen\n","    display(Markdown(response_text))\n","    print()\n","\n","    # Konversation im Memory speichern\n","    memory.save_context({\"input\": question}, {\"output\": response_text})\n","\n","    return response_text"],"metadata":{"id":"RJaoO8Jyqf9C"},"id":"RJaoO8Jyqf9C","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 5: Hauptprogramm\n","\n","for question in TEST_QUESTIONS:\n","    interact_with_ai(question)"],"metadata":{"id":"JJ83yHdTqh9L"},"id":"JJ83yHdTqh9L","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"6430a41d","metadata":{"id":"6430a41d"},"source":["Nach der Ausführung können wir den gespeicherten Kontext untersuchen:"]},{"cell_type":"code","execution_count":null,"id":"2d0083b7","metadata":{"id":"2d0083b7"},"outputs":[],"source":["print(\"\\nGespeicherter Kontext:\")\n","memory_ai = memory.load_memory_variables({})[\"history\"]\n","for i in range(len(memory_ai)):\n","    print(memory_ai[i].content)"]},{"cell_type":"markdown","id":"083e7fc3","metadata":{"id":"083e7fc3"},"source":["Der Parameter `k` in `ConversationBufferWindowMemory` steuert, wie viele Nachrichten-Paare (Frage und Antwort) dem LLM als Kontext übergeben werden. Mit `k=3` werden nur die letzten drei Konversationsrunden berücksichtigt, auch wenn die gesamte Konversation länger ist.\n","\n"]},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Memory-Funktion mit LCEL\n","</font></p>"],"metadata":{"id":"sWquJY0Dh9P9"},"id":"sWquJY0Dh9P9"},{"cell_type":"markdown","source":["Alternativ können wir auch unsere eigene Fenster-Implementierung erstellen:"],"metadata":{"id":"yuvIAYB9h-ef"},"id":"yuvIAYB9h-ef"},{"cell_type":"code","execution_count":null,"id":"820b1c1a","metadata":{"id":"820b1c1a"},"outputs":[],"source":["# Abschnitt 1: Importe\n","from IPython.display import display, Markdown\n","\n","from langchain_core.runnables import RunnableLambda, RunnableMap\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import PromptTemplate\n","from langchain.schema import AIMessage"]},{"cell_type":"code","source":["# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","MAX_HISTORY = 3  # Maximale Anzahl an gespeicherten Konversationsbeiträgen\n","\n","DEFAULT_SYSTEM = \"\"\"Du bist ein kompetenter KI-Assistent mit breitem Fachwissen.\n","\n","Deine Antworten sind:\n","- Klar strukturiert und mit Markdown formatiert\n","- Praxisorientiert und direkt umsetzbar\n","- Basierend auf aktuellem Kenntnisstand\n","- Mit passenden Beispielen versehen\n","- In verständlicher Sprache formuliert\n","\n","Aktuelle Konversation:\n","{history}\n","Mensch: {input}\n","KI:\"\"\"\n","\n","TEST_QUESTIONS = [\n","    \"Mein Lieblingsgericht ist Lasagne. Antworte kurz.\",\n","    \"Wie viele Tage hat eine Woche? Antworte kurz.\",\n","    \"Wie viele Tage hat ein Monat? Antworte kurz.\",\n","    \"Wie viele Tage hat ein Jahr? Antworte kurz.\",\n","    \"Was ist mein Lieblingsgericht?\"\n","]"],"metadata":{"id":"zfaTDBiDq6je"},"id":"zfaTDBiDq6je","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 3: Funktionen\n","# Speicher für Konversationshistorie manuell verwalten\n","conversation_history: list[str] = []\n","\n","def update_memory(question: str, response: str) -> list[str]:\n","    \"\"\"Fügt neue Konversationsbeiträge zur Historie hinzu.\"\"\"\n","    global conversation_history\n","    conversation_history.append(f\"Mensch: {question}\\nKI: {response}\")\n","\n","    # Begrenze die Historie auf die letzten N Einträge\n","    if len(conversation_history) > MAX_HISTORY:\n","        conversation_history = conversation_history[-MAX_HISTORY:]\n","\n","    return conversation_history\n","\n","def format_history(history: list[str]) -> str:\n","    \"\"\"Formatiert die Historie für die Eingabe an das Modell.\"\"\"\n","    return \"\\n\".join(history)\n","\n","# Interaktionen mit dem Modell\n","def interact_with_ai(question):\n","    \"\"\"Führt eine einzelne Interaktion mit der KI durch.\"\"\"\n","    # Ausgabe der Frage\n","    display(Markdown(f\"\\n🧑‍🦱 Mensch:\"))\n","    display(Markdown(question))\n","    display(Markdown(\"\\n🤖 KI: \"))\n","\n","    # KI-Antwort einholen\n","    response = chain.invoke({\"input\": question})\n","\n","    # Antworttext extrahieren\n","    response_text = response.content if isinstance(response, AIMessage) else str(response)\n","\n","    # Formatierte Ausgabe erstellen\n","    display(Markdown(response_text))\n","    print()\n","\n","    # Speicher aktualisieren\n","    update_memory(question, response_text)\n","\n","    return response_text"],"metadata":{"id":"mKJz2wOcq-wE"},"id":"mKJz2wOcq-wE","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 4: LLM und LCEL-Chain initialisieren\n","llm = ChatOpenAI(\n","    model_name=MODEL,\n","    temperature=0.0\n",")\n","\n","prompt = PromptTemplate(\n","    input_variables=[\"history\", \"input\"],\n","    template=DEFAULT_SYSTEM\n",")\n","\n","# RunnableMap für Input-Transformation\n","prepare_input = RunnableLambda(lambda x: {\n","    \"history\": format_history(conversation_history),\n","    \"input\": x[\"input\"]\n","})\n","\n","# LCEL-Chain erstellen\n","chain = prepare_input | prompt | llm"],"metadata":{"id":"Rr1sdAOjrCf1"},"id":"Rr1sdAOjrCf1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 5: Hauptprogramm\n","\n","for question in TEST_QUESTIONS:\n","    interact_with_ai(question)"],"metadata":{"id":"yHQuvlq4rLnL"},"id":"yHQuvlq4rLnL","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"3e6f7a0f","metadata":{"id":"3e6f7a0f"},"source":["Diese Implementierung nutzt eine einfache Liste, um die letzten N Konversationspaare zu speichern. Sie bietet mehr Kontrolle, erfordert jedoch auch mehr manuellen Code.\n","\n"]},{"cell_type":"markdown","source":["# 6 | Manuelle Verwaltung\n","---\n"],"metadata":{"id":"svg9v5Oahoh2"},"id":"svg9v5Oahoh2"},{"cell_type":"markdown","source":["\n","\n","Im ersten Beispiel werden wir einen Chatbot schrittweise mit einer manuellen Konversationsverwaltung aufbauen. Dieser Ansatz bietet die volle Kontrolle, indem wir die Konversationshistorie explizit verwalten und weitergeben."],"metadata":{"id":"DLm3AoGxjbL_"},"id":"DLm3AoGxjbL_"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2e3ef096"},"outputs":[],"source":["# Abschnitt 1: Importe\n","from IPython.display import display, Markdown\n","\n","from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough, RunnableLambda"],"id":"2e3ef096"},{"cell_type":"code","source":["# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATURE = 0.0\n","MEMORY_WINDOW = 100 # hoher Wert, praktisch keine Beschränkung der Anzahl\n","\n","DEFAULT_SYSTEM = \"\"\"\n","Du bist ein kompetenter KI-Assistent mit breitem Fachwissen.\n","\n","Deine Antworten sind:\n","- Klar strukturiert und mit Markdown formatiert\n","- Praxisorientiert und direkt umsetzbar\n","- Basierend auf aktuellem Kenntnisstand\n","- Mit passenden Beispielen versehen\n","- In verständlicher Sprache formuliert\n","\n","Bei deiner Arbeit:\n","- Analysierst du Fragen sorgfältig\n","- Gibst präzise und relevante Antworten\n","- Erkennst den Kontext der Anfrage\n","- Bietest bei Bedarf weiterführende Informationen\n","- Bleibst sachlich und neutral\n","\n","Formatiere alle Antworten in Markdown für optimale Lesbarkeit.\n","\"\"\"\n","\n","# Testfragen\n","test_questions = [\n","    \"Mein Lieblingsgericht ist Lasagne. Antworte kurz.\",\n","    \"Wie viele Tage hat eine Woche? Antworte kurz.\",\n","    \"Wie viele Tage hat ein Monat? Antworte kurz.\",\n","    \"Wie viele Tage hat ein Jahr? Antworte kurz.\",\n","    \"Was ist mein Lieblingsgericht?\"\n","]"],"metadata":{"id":"5q4rAMhfk-mi"},"execution_count":null,"outputs":[],"id":"5q4rAMhfk-mi"},{"cell_type":"code","source":["# Abschnitt 3: Chat-Komponenten initialisieren\n","llm = ChatOpenAI(\n","    temperature=TEMPERATURE,\n","    model=MODEL\n",")\n","\n","chat_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"human\", \"{question}\")\n","])\n","\n","def create_chain_input(input_dict: dict[str]) -> dict[str]:\n","    \"\"\"Erstellt den Input für die Chain mit History.\"\"\"\n","    return {\n","        \"question\": input_dict[\"question\"],\n","        \"system_prompt\": DEFAULT_SYSTEM,\n","        \"chat_history\": input_dict[\"history\"]\n","    }\n","\n","# LCEL Chain mit History erstellen\n","chat_chain = (\n","    RunnableLambda(create_chain_input)\n","    | chat_prompt\n","    | llm\n","    | StrOutputParser()\n",")"],"metadata":{"id":"j0cLXpxqlAYg"},"execution_count":null,"outputs":[],"id":"j0cLXpxqlAYg"},{"cell_type":"code","source":["# Abschnitt 4: Funktionen definieren\n","def format_history(history: list) -> list:\n","    \"\"\"  Formatiert den Konversationsverlauf. \"\"\"\n","    return history[-MEMORY_WINDOW:]\n","\n","def interact_with_ai(question: str, history: list) -> list:\n","    \"\"\" Verarbeitet eine Konversation mit der KI. \"\"\"\n","    # Nutzereingabe anzeigen\n","    display(Markdown(f\"### 🧑‍🦱 Mensch:\"))\n","    display(Markdown(question))\n","\n","    # Antwort mit History generieren\n","    response = chat_chain.invoke({\n","        \"question\": question,\n","        \"history\": format_history(history)\n","    })\n","\n","    # Antwort anzeigen\n","    display(Markdown(f\"### 🤖 KI:\"))\n","    display(Markdown(response))\n","    print()\n","\n","    # Konversationsverlauf aktualisieren\n","    return history + [\n","        HumanMessage(content=question),\n","        AIMessage(content=response)\n","    ]"],"metadata":{"id":"3_i1y_1DlmpT"},"execution_count":null,"outputs":[],"id":"3_i1y_1DlmpT"},{"cell_type":"code","source":["# Abschnitt 5: Hauptprogramm\n","\n","# Konversationsverlauf initialisieren\n","history = [SystemMessage(content=DEFAULT_SYSTEM)]\n","\n","# Testfragen durchlaufen\n","for question in test_questions:\n","    history = interact_with_ai(question, history)"],"metadata":{"id":"VcpkmeQElFCT"},"execution_count":null,"outputs":[],"id":"VcpkmeQElFCT"},{"cell_type":"code","source":["#\n","# NEW für Kurs-Update - ersetzt die Version oben!\n","#\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.messages import HumanMessage, AIMessage\n","from IPython.display import display, Markdown\n","\n","# Konstanten\n","MODEL = \"gpt-4o-mini\"\n","DEFAULT_SYSTEM = \"Du bist ein freundlicher, sachkundiger AI-Assistent.\"\n","MEMORY_LIMIT = 3  # Begrenzt die History auf die letzten 3 Nachrichtenpaare\n","\n","# Chat-Modell und Prompt in einem Schritt\n","chat_model = ChatOpenAI(temperature=0.7, model=MODEL)\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", DEFAULT_SYSTEM),\n","    MessagesPlaceholder(variable_name=\"history\"),\n","    (\"human\", \"{question}\")\n","])\n","\n","# Kompakte Chat-Chain\n","chat_chain = prompt | chat_model\n","\n","def chat(question, history=None):\n","    \"\"\"Einfache Konversationsfunktion mit Anzeige und begrenzter History-Verwaltung.\"\"\"\n","    if history is None:\n","        history = []\n","\n","    # History auf die letzten MEMORY_LIMIT Nachrichten begrenzen\n","    # Da jedes Paar aus 2 Messages besteht (Human + AI), multiplizieren wir mit 2\n","    if len(history) > MEMORY_LIMIT * 2:\n","        history = history[-(MEMORY_LIMIT * 2):]\n","\n","    # Nutzereingabe anzeigen\n","    display(Markdown(f\"### 🧑‍🦱 Mensch:\\n{question}\"))\n","\n","    # Antwort generieren\n","    response = chat_chain.invoke({\"question\": question, \"history\": history})\n","\n","    # Antwort anzeigen\n","    display(Markdown(f\"### 🤖 KI:\\n{response.content}\"))\n","\n","    # Aktualisierte History zurückgeben\n","    return history + [HumanMessage(content=question), response]\n","\n","# Beispielverwendung\n","history = []\n","for frage in [\"Frage 1\", \"Frage 2\", \"Frage 3\", \"Frage 4\", \"Frage 5\"]:\n","    history = chat(frage, history)\n","    print(f\"History-Länge: {len(history)} Nachrichten\")"],"metadata":{"id":"v3DGl-PQdEuC"},"execution_count":null,"outputs":[],"id":"v3DGl-PQdEuC"},{"cell_type":"markdown","metadata":{"id":"92c7cbac"},"source":["Bei diesem Ansatz speichern wir die gesamte Konversationshistorie in einer Liste und reichen sie bei jeder neuen Anfrage mit. Vorteil ist die volle Kontrolle und der Zugriff auf den gesamten Kontext. Der Nachteil: bei langen Unterhaltungen kann der Kontext die Token-Limits des Modells überschreiten.\n","\n"],"id":"92c7cbac"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Framework-Komponenten\n","</font></p>"],"metadata":{"id":"ZLBcxiHdhux9"},"id":"ZLBcxiHdhux9"},{"cell_type":"markdown","source":["\n","\n","Als Alternative zur manuellen Verwaltung bietet LangChain spezialisierte Komponenten zur Konversationsverwaltung. Diese automatisieren viele der manuellen Schritte und bieten eine strukturiertere Herangehensweise."],"metadata":{"id":"sBGRhPwIhyxq"},"id":"sBGRhPwIhyxq"},{"cell_type":"code","execution_count":null,"metadata":{"id":"7182fbf8"},"outputs":[],"source":["# Abschnitt 1: Importe\n","from IPython.display import display, Markdown\n","from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_community.chat_message_histories import ChatMessageHistory\n","from langchain_core.runnables.history import RunnableWithMessageHistory"],"id":"7182fbf8"},{"cell_type":"code","source":["# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATURE = 0.0\n","DEFAULT_SYSTEM = \"\"\"\n","Du bist ein kompetenter KI-Assistent mit breitem Fachwissen.\n","Deine Antworten sind:\n","- Klar strukturiert und mit Markdown formatiert\n","- Praxisorientiert und direkt umsetzbar\n","- Basierend auf aktuellem Kenntnisstand\n","- Mit passenden Beispielen versehen\n","- In verständlicher Sprache formuliert\n","Bei deiner Arbeit:\n","- Analysierst du Fragen sorgfältig\n","- Gibst präzise und relevante Antworten\n","- Erkennst den Kontext der Anfrage\n","- Bietest bei Bedarf weiterführende Informationen\n","- Bleibst sachlich und neutral\n","Formatiere alle Antworten in Markdown für optimale Lesbarkeit.\n","\"\"\""],"metadata":{"id":"gdq5nmgQm6UR"},"execution_count":null,"outputs":[],"id":"gdq5nmgQm6UR"},{"cell_type":"code","source":["# Abschnitt 3: Historienverwaltung und Chat-Komponenten initialisieren\n","# Dictionary für Sitzungs-Historien\n","session_histories = {}\n","\n","# Hilfsfunktion zur Verwaltung der ChatMessageHistory\n","def get_session_history(session_id):\n","    if session_id not in session_histories:\n","        # Erstelle eine neue History, wenn keine existiert\n","        history = ChatMessageHistory()\n","        # Füge die System-Nachricht zur History hinzu\n","        history.add_message(SystemMessage(content=DEFAULT_SYSTEM))\n","        session_histories[session_id] = history\n","    return session_histories[session_id]\n","\n","# LLM-Instanz initialisieren\n","llm = ChatOpenAI(\n","    temperature=TEMPERATURE,\n","    model=MODEL\n",")\n","\n","# Chat-Prompt mit Platzhalter für die Historie\n","chat_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", DEFAULT_SYSTEM),\n","    MessagesPlaceholder(variable_name=\"history\"),\n","    (\"human\", \"{question}\")\n","])\n","\n","# Basis-Chain ohne Historie erstellen\n","chain = chat_prompt | llm | StrOutputParser()\n","\n","# Chain mit Message-History-Funktionalität erweitern\n","chain_with_history = RunnableWithMessageHistory(\n","    chain,\n","    get_session_history,  # Unsere verbesserte History-Management-Funktion\n","    input_messages_key=\"question\",\n","    history_messages_key=\"history\"\n",")"],"metadata":{"id":"1RSEAlLkm8SI"},"execution_count":null,"outputs":[],"id":"1RSEAlLkm8SI"},{"cell_type":"code","source":["# Abschnitt 4: Funktionen definieren\n","def interact_with_ai(question: str, session_id: str = \"kontinuierliche_session\") -> str:\n","    \"\"\"Verarbeitet eine Konversation und gibt die Antwort zurück.\"\"\"\n","    # Nutzereingabe anzeigen\n","    display(Markdown(f\"### 🧑‍🦱 Mensch:\"))\n","    display(Markdown(question))\n","\n","    # Antwort mit History generieren\n","    response = chain_with_history.invoke(\n","        {\"question\": question},\n","        config={\"configurable\": {\"session_id\": session_id}}\n","    )\n","\n","    # Debug-Ausgabe: Zeige die aktuelle History an\n","    history = session_histories[session_id]\n","\n","    # Antwort anzeigen\n","    display(Markdown(f\"### 🤖 KI:\"))\n","    display(Markdown(response))\n","    print()\n","\n","    return response"],"metadata":{"id":"UFv9qO_um-K5"},"execution_count":null,"outputs":[],"id":"UFv9qO_um-K5"},{"cell_type":"code","source":["# Abschnitt 5: Hauptprogramm\n","# Konstante Session-ID definieren\n","KONSTANTE_SESSION_ID = \"kontinuierliche_session\"\n","\n","# Testfragen\n","test_questions = [\n","    \"Mein Lieblingsgericht ist Lasagne. Antworte kurz.\",\n","    \"Wie viele Tage hat eine Woche? Antworte kurz.\",\n","    \"Wie viele Tage hat ein Monat? Antworte kurz.\",\n","    \"Wie viele Tage hat ein Jahr? Antworte kurz.\",\n","    \"Was ist mein Lieblingsgericht?\"\n","]\n","\n","# Testfragen durchlaufen mit konstanter Session-ID\n","for question in test_questions:\n","    interact_with_ai(question, session_id=KONSTANTE_SESSION_ID)"],"metadata":{"id":"ME-KIsRgnAFc"},"execution_count":null,"outputs":[],"id":"ME-KIsRgnAFc"},{"cell_type":"code","source":["# Optional: Ausgabe der vollständigen Konversation am Ende\n","def zeige_konversation(session_id):\n","    \"\"\" Zeigt die vollständige Konversationshistorie für eine bestimmte Session an. \"\"\"\n","    markdown_text = \"## Vollständige Konversation\\n\"\n","\n","    if session_id in session_histories:\n","        for i, msg in enumerate(session_histories[session_id].messages):\n","            # Bestimme die Rolle der Nachricht\n","            if isinstance(msg, SystemMessage):\n","                rolle = \"⚙️ System\"\n","            elif isinstance(msg, HumanMessage):\n","                rolle = \"🧑‍🦱 Mensch\"\n","            else:\n","                rolle = \"🤖 KI\"\n","\n","            # Kürze den Inhalt, wenn er zu lang ist\n","            inhalt = msg.content\n","            if len(inhalt) > 50:\n","                inhalt = f\"{inhalt[:50]}...\"\n","\n","            # Füge Nachricht zum Markdown-Text hinzu\n","            markdown_text += f\"**{i}.** [{rolle}]: {inhalt}\\n\\n\"\n","    else:\n","        markdown_text += \"*Keine Konversationshistorie für diese Session gefunden.*\\n\\n\"\n","\n","    # Zeige den Markdown-Text an\n","    display(Markdown(markdown_text))\n","\n","# Beispielaufruf\n","zeige_konversation(KONSTANTE_SESSION_ID)"],"metadata":{"id":"5CH5mnNInP2U"},"execution_count":null,"outputs":[],"id":"5CH5mnNInP2U"},{"cell_type":"markdown","metadata":{"id":"01eb41f9"},"source":["Dieser Ansatz nutzt `RunnableWithMessageHistory` und `ChatMessageHistory` von LangChain, um Konversationen zu verwalten. Vorteile sind die einfachere Handhabung und die Möglichkeit, mehrere Konversationen mit unterschiedlichen Session-IDs zu verwalten.\n","\n"],"id":"01eb41f9"},{"cell_type":"markdown","metadata":{"id":"5cfbf26a"},"source":["# A | Aufgaben\n","---\n"],"id":"5cfbf26a"},{"cell_type":"markdown","source":["\n","\n","Die Aufgabestellungen unten bieten Anregungen, Sie können aber auch gerne eine andere Herausforderung angehen.\n"],"metadata":{"id":"pwmSuPsGiQp6"},"id":"pwmSuPsGiQp6"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","KI-gestütztes Notizbuch mit automatischer Kategorisierung\n","</font></p>\n","\n","\n","Entwickeln Sie ein KI-Notizbuch, das Eingaben von Nutzern speichert, automatisch kategorisiert (z. B. \"Technologie\", \"Privat\", \"Aufgaben\") und kontextbezogene Vorschläge liefert. Verwenden Sie dazu ein LangChain Memory-Konzept, z.B. ConversationBufferMemory, für eine effiziente Verwaltung.\n"],"metadata":{"id":"1UViO3SJiSXy"},"id":"1UViO3SJiSXy"},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Chatbot: Nachrichten- vs. Summary-Speicher\n","</font></p>\n","\n","\n","Erstellen Sie einen einfachen Chatbot mit LangChain, der zwischen Nachrichten- und Summary-Speicher unterscheidet. Analysieren Sie, wie sich die Speichertypen auf die Qualität der Antworten auswirken.\n","\n"],"metadata":{"id":"T6HXRFbXjIVL"},"id":"T6HXRFbXjIVL"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Automatische Protokollerstellung für Meetings\n","</font></p>\n","\n","\n","Entwickeln Sie ein System, das Gesprächsverläufe speichert und automatisch eine zusammenfassende Chat-Notiz erstellt. Nutzen Sie z.B. ConversationSummaryMemory, um die wichtigsten Punkte aus langen Gesprächen zu extrahieren.\n","\n"],"metadata":{"id":"9eIl83Z-CqT8"},"id":"9eIl83Z-CqT8"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Virtuelle Assistenten mit eigenem Gedächtnis\n","</font></p>\n","\n","\n","Eine Datei wird bereitgestellt, die ein Gespräch zwischen zwei virtuellen Assistenten enthält. Jede Antwort soll aus genau einem Satz bestehen. Die Datei ist folgendermaßen aufgebaut:\n","\n","|Assistent|Eingabe|\n","|---|---|\n","|botA|Ich heiße Max.|\n","|botB|Mein Name ist Emma.|\n","|botA|Ich wohne in Berlin.|\n","|botB|Ich lebe in Hamburg.|\n","|botA|Wo wohne ich und wie heiße ich?|\n","|botB|Wo lebe ich?|\n","\n","**Aufgabe**  \n","Schreiben Sie ein Programm, das zwei virtuelle Assistenten simuliert, die jeweils ein eigenes Gedächtnis haben. Die Eingaben aus der Datei werden an den jeweiligen Assistenten geschickt, und ihre Antworten sollen in einer neuen Datei gespeichert werden.\n","\n","**Erwartete Ausgabe**  \n","Die erwartete Ausgabe könnte wie folgt aussehen:\n","\n","|Assistent|Antwort|\n","|---|---|\n","|botA|Hallo Max.|\n","|botB|Hallo Emma.|\n","|botA|Danke für diese Information.|\n","|botB|Danke für diese Information.|\n","|botA|Hallo Max, du wohnst in Berlin und dein Name ist Max.|\n","|botB|Hallo Emma, du lebst in Hamburg.|\n","\n","**Hinweise**\n","\n","- Die Antworten müssen nicht exakt mit der Beispielausgabe übereinstimmen, sollten aber sinngemäß ähnlich sein.\n","- Jeder Assistent hat ein eigenes Gedächtnis und sollte sich nur an seine eigenen Informationen erinnern.\n","- Das Programm soll die Antworten in einer neuen Datei im gleichen Tabellenformat speichern."],"metadata":{"id":"CwYae6YZCsto"},"id":"CwYae6YZCsto"}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"colab":{"provenance":[],"collapsed_sections":["ncw0vI8MAJrt","gNaHt9iuAfzs","svg9v5Oahoh2"]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}
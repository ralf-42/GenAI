{"cells":[{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)"],"metadata":{"id":"9JKGDENeDeB0"},"id":"9JKGDENeDeB0"},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b> Chat und Memory </b></font> </br></p>\n","\n","---"],"metadata":{"id":"bxo8g0twDZK5"},"id":"bxo8g0twDZK5"},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","collapsed":true,"id":"dfdhPIzcEYRG","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  Colab-Umfeld</font> </br></p>\n","# Installierte Python Version\n","import sys\n","print(f\"Python Version: \",sys.version)\n","# Installierte LangChain Bibliotheken\n","print()\n","print(\"Installierte LangChain Bibliotheken:\")\n","\n","!pip list | grep '^langchain'\n","# Unterdr√ºckt die \"DeprecationWarning\" von LangChain f√ºr die Memory-Funktionden\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")"],"id":"dfdhPIzcEYRG"},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WD3Wwr6sESX8"},"outputs":[],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  SetUp API-Keys (setup_api_keys)</font> </br></p>\n","def setup_api_keys():\n","    \"\"\"Konfiguriert alle ben√∂tigten API-Keys aus Google Colab userdata\"\"\"\n","    from google.colab import userdata\n","    import os\n","    from os import environ\n","\n","    # Dictionary der ben√∂tigten API-Keys\n","    keys = {\n","        'OPENAI_API_KEY': 'OPENAI_API_KEY',\n","        'HF_TOKEN': 'HF_TOKEN',\n","        # Weitere Keys bei Bedarf\n","    }\n","\n","    # Keys in Umgebungsvariablen setzen\n","    for env_var, key_name in keys.items():\n","        environ[env_var] = userdata.get(key_name)\n","\n","    return {k: environ[k] for k in keys.keys()}\n","\n","# Verwendung\n","all_keys = setup_api_keys()\n","# Bei Bedarf einzelne Keys direkt zugreifen\n","# WEATHER_API_KEY = all_keys['WEATHER_API_KEY']"],"id":"WD3Wwr6sESX8"},{"cell_type":"markdown","id":"fe8a8ba4","metadata":{"id":"fe8a8ba4"},"source":["# 1 | Konversationsverwaltung\n","---\n"]},{"cell_type":"markdown","source":["\n","\n","Gro√üe Sprachmodelle (LLMs) erm√∂glichen eine nat√ºrliche Interaktion √§hnlich menschlicher Gespr√§che. Sie sind in der Lage, auf vorherige Informationen innerhalb eines Dialogs Bezug zu nehmen. In diesem Modul werden wir verschiedene Ans√§tze zur Verwaltung des Ged√§chtnisses eines LLMs untersuchen.\n","\n","Da der interne Kontextpuffer eines LLMs begrenzt ist, ben√∂tigen wir Strategien, um den Chat-Verlauf zu speichern und effizient zu nutzen. LangChain bietet verschiedene L√∂sungsans√§tze, die wir in diesem Modul erkunden werden:\n"],"metadata":{"id":"EmMVp49lhkOh"},"id":"EmMVp49lhkOh"},{"cell_type":"markdown","source":["\n","1. **Manuelle Konversationsverwaltung**: Einfache Speicherung und Weiterleitung der kompletten Konversationshistorie\n","2. **Begrenzung der Nachrichtenanzahl**: Verwendung eines \"Fensters\" der letzten N Nachrichten\n","3. **Token-basierte Limitierung**: Ber√ºcksichtigung der tats√§chlichen Tokenlimits des LLM\n","4. **Zusammenfassung von Konversationen**: Komprimieren l√§ngerer Konversationen\n","5. **Persistenter Speicher**: Langfristige Speicherung √ºber Sitzungen hinweg\n","\n","Jeder dieser Ans√§tze hat seine eigenen Vor- und Nachteile, die wir im Folgenden genauer untersuchen werden.\n"],"metadata":{"id":"cAOs2SNwhmJ-"},"id":"cAOs2SNwhmJ-"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Message-Typen\n","</font></p>"],"metadata":{"id":"kYpOM3O-BJuO"},"id":"kYpOM3O-BJuO"},{"cell_type":"markdown","source":["Die memory-Module von LangChain erwarten Message-Objekte wie SystemMessage, HumanMessage, AIMessage.\n","\n","Sie helfen LangChain (und dem Modell), den Kontext richtig einzuordnen:    \n","Wer sagt was? Welche Rolle hat diese Nachricht im Dialog?\n","\n","Das ist besonders wichtig, wenn man mit ConversationBufferMemory, ConversationSummaryMemory usw. arbeitest, weil diese listenweise Messages speichern und sp√§ter wiederverwenden."],"metadata":{"id":"SeApNepSBjqR"},"id":"SeApNepSBjqR"},{"cell_type":"code","execution_count":null,"metadata":{"id":"K2lC_JK3guaj"},"outputs":[],"source":["from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n","from langchain_openai import ChatOpenAI\n","from IPython.display import display, Markdown"],"id":"K2lC_JK3guaj"},{"cell_type":"code","source":["messages = [\n","    SystemMessage(\n","        content=\"Du bist ein kompetenter und hilfreicher Assistent.\"\n","    ),\n","    HumanMessage(\n","        content=\"Nenne mir die Hauptstadt von Usbekistan?\"\n","    ),\n","]"],"metadata":{"id":"zIOnQp7j9Drq"},"execution_count":null,"outputs":[],"id":"zIOnQp7j9Drq"},{"cell_type":"code","source":["type(messages)"],"metadata":{"id":"6KDVJpDa92h6"},"execution_count":null,"outputs":[],"id":"6KDVJpDa92h6"},{"cell_type":"code","source":["MODEL = 'gpt-4o-mini'\n","TEMPERATURE = 0.0\n","\n","llm = ChatOpenAI(model=MODEL, temperature= TEMPERATURE)\n","response = llm.invoke(messages)"],"metadata":{"id":"z0yFqHlZ9Kts"},"execution_count":null,"outputs":[],"id":"z0yFqHlZ9Kts"},{"cell_type":"code","source":["display(Markdown(\"## üì£ Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response.content))"],"metadata":{"id":"UvDvAWVd9WiF"},"execution_count":null,"outputs":[],"id":"UvDvAWVd9WiF"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Manuelle Konversationsverwaltung\n","</font></p>\n"],"metadata":{"id":"svg9v5Oahoh2"},"id":"svg9v5Oahoh2"},{"cell_type":"markdown","source":["\n","\n","Im ersten Beispiel werden wir einen Chatbot schrittweise mit einer manuellen Konversationsverwaltung aufbauen. Dieser Ansatz bietet die volle Kontrolle, indem wir die Konversationshistorie explizit verwalten und weitergeben."],"metadata":{"id":"DLm3AoGxjbL_"},"id":"DLm3AoGxjbL_"},{"cell_type":"code","source":["# Abschnitt 0: Installation und API-Key\n","!uv pip install --system --prerelease allow -q langchain_community langchain_openai"],"metadata":{"id":"WHJhHEoqkIeS"},"id":"WHJhHEoqkIeS","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"2e3ef096","metadata":{"id":"2e3ef096"},"outputs":[],"source":["# Abschnitt 1: Importe\n","from IPython.display import display, Markdown\n","\n","from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough, RunnableLambda"]},{"cell_type":"code","source":["# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATURE = 0.0\n","MEMORY_WINDOW = 100 # hoher Wert, praktisch keine Beschr√§nkung der Anzahl\n","\n","DEFAULT_SYSTEM = \"\"\"\n","Du bist ein kompetenter KI-Assistent mit breitem Fachwissen.\n","\n","Deine Antworten sind:\n","- Klar strukturiert und mit Markdown formatiert\n","- Praxisorientiert und direkt umsetzbar\n","- Basierend auf aktuellem Kenntnisstand\n","- Mit passenden Beispielen versehen\n","- In verst√§ndlicher Sprache formuliert\n","\n","Bei deiner Arbeit:\n","- Analysierst du Fragen sorgf√§ltig\n","- Gibst pr√§zise und relevante Antworten\n","- Erkennst den Kontext der Anfrage\n","- Bietest bei Bedarf weiterf√ºhrende Informationen\n","- Bleibst sachlich und neutral\n","\n","Formatiere alle Antworten in Markdown f√ºr optimale Lesbarkeit.\n","\"\"\"\n","\n","# Testfragen\n","test_questions = [\n","    \"Mein Lieblingsgericht ist Lasagne. Antworte kurz.\",\n","    \"Wie viele Tage hat eine Woche? Antworte kurz.\",\n","    \"Wie viele Tage hat ein Monat? Antworte kurz.\",\n","    \"Wie viele Tage hat ein Jahr? Antworte kurz.\",\n","    \"Was ist mein Lieblingsgericht?\"\n","]"],"metadata":{"id":"5q4rAMhfk-mi"},"id":"5q4rAMhfk-mi","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 3: Chat-Komponenten initialisieren\n","llm = ChatOpenAI(\n","    temperature=TEMPERATURE,\n","    model=MODEL\n",")\n","\n","chat_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"{system_prompt}\"),\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"human\", \"{question}\")\n","])\n","\n","def create_chain_input(input_dict: dict[str]) -> dict[str]:\n","    \"\"\"Erstellt den Input f√ºr die Chain mit History.\"\"\"\n","    return {\n","        \"question\": input_dict[\"question\"],\n","        \"system_prompt\": DEFAULT_SYSTEM,\n","        \"chat_history\": input_dict[\"history\"]\n","    }\n","\n","# LCEL Chain mit History erstellen\n","chat_chain = (\n","    RunnableLambda(create_chain_input)\n","    | chat_prompt\n","    | llm\n","    | StrOutputParser()\n",")"],"metadata":{"id":"j0cLXpxqlAYg"},"id":"j0cLXpxqlAYg","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 4: Funktionen definieren\n","def format_history(history: list) -> list:\n","    \"\"\"  Formatiert den Konversationsverlauf. \"\"\"\n","    return history[-MEMORY_WINDOW:]\n","\n","def interact_with_ai(question: str, history: list) -> list:\n","    \"\"\" Verarbeitet eine Konversation mit der KI. \"\"\"\n","    # Nutzereingabe anzeigen\n","    display(Markdown(f\"### üßë‚Äçü¶± Mensch:\"))\n","    display(Markdown(question))\n","\n","    # Antwort mit History generieren\n","    response = chat_chain.invoke({\n","        \"question\": question,\n","        \"history\": format_history(history)\n","    })\n","\n","    # Antwort anzeigen\n","    display(Markdown(f\"### ü§ñ KI:\"))\n","    display(Markdown(response))\n","    print()\n","\n","    # Konversationsverlauf aktualisieren\n","    return history + [\n","        HumanMessage(content=question),\n","        AIMessage(content=response)\n","    ]"],"metadata":{"id":"3_i1y_1DlmpT"},"id":"3_i1y_1DlmpT","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 5: Hauptprogramm\n","\n","# Konversationsverlauf initialisieren\n","history = [SystemMessage(content=DEFAULT_SYSTEM)]\n","\n","# Testfragen durchlaufen\n","for question in test_questions:\n","    history = interact_with_ai(question, history)"],"metadata":{"id":"VcpkmeQElFCT"},"id":"VcpkmeQElFCT","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"92c7cbac","metadata":{"id":"92c7cbac"},"source":["Bei diesem Ansatz speichern wir die gesamte Konversationshistorie in einer Liste und reichen sie bei jeder neuen Anfrage mit. Vorteil ist die volle Kontrolle und der Zugriff auf den gesamten Kontext. Der Nachteil: bei langen Unterhaltungen kann der Kontext die Token-Limits des Modells √ºberschreiten.\n","\n"]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Framework-Komponenten\n","</font></p>"],"metadata":{"id":"ZLBcxiHdhux9"},"id":"ZLBcxiHdhux9"},{"cell_type":"markdown","source":["\n","\n","Als Alternative zur manuellen Verwaltung bietet LangChain spezialisierte Komponenten zur Konversationsverwaltung. Diese automatisieren viele der manuellen Schritte und bieten eine strukturiertere Herangehensweise."],"metadata":{"id":"sBGRhPwIhyxq"},"id":"sBGRhPwIhyxq"},{"cell_type":"code","execution_count":null,"id":"7182fbf8","metadata":{"id":"7182fbf8"},"outputs":[],"source":["# Abschnitt 1: Importe\n","from IPython.display import display, Markdown\n","from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_community.chat_message_histories import ChatMessageHistory\n","from langchain_core.runnables.history import RunnableWithMessageHistory"]},{"cell_type":"code","source":["# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATURE = 0.0\n","DEFAULT_SYSTEM = \"\"\"\n","Du bist ein kompetenter KI-Assistent mit breitem Fachwissen.\n","Deine Antworten sind:\n","- Klar strukturiert und mit Markdown formatiert\n","- Praxisorientiert und direkt umsetzbar\n","- Basierend auf aktuellem Kenntnisstand\n","- Mit passenden Beispielen versehen\n","- In verst√§ndlicher Sprache formuliert\n","Bei deiner Arbeit:\n","- Analysierst du Fragen sorgf√§ltig\n","- Gibst pr√§zise und relevante Antworten\n","- Erkennst den Kontext der Anfrage\n","- Bietest bei Bedarf weiterf√ºhrende Informationen\n","- Bleibst sachlich und neutral\n","Formatiere alle Antworten in Markdown f√ºr optimale Lesbarkeit.\n","\"\"\""],"metadata":{"id":"gdq5nmgQm6UR"},"id":"gdq5nmgQm6UR","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 3: Historienverwaltung und Chat-Komponenten initialisieren\n","# Dictionary f√ºr Sitzungs-Historien\n","session_histories = {}\n","\n","# Hilfsfunktion zur Verwaltung der ChatMessageHistory\n","def get_session_history(session_id):\n","    if session_id not in session_histories:\n","        # Erstelle eine neue History, wenn keine existiert\n","        history = ChatMessageHistory()\n","        # F√ºge die System-Nachricht zur History hinzu\n","        history.add_message(SystemMessage(content=DEFAULT_SYSTEM))\n","        session_histories[session_id] = history\n","    return session_histories[session_id]\n","\n","# LLM-Instanz initialisieren\n","llm = ChatOpenAI(\n","    temperature=TEMPERATURE,\n","    model=MODEL\n",")\n","\n","# Chat-Prompt mit Platzhalter f√ºr die Historie\n","chat_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", DEFAULT_SYSTEM),\n","    MessagesPlaceholder(variable_name=\"history\"),\n","    (\"human\", \"{question}\")\n","])\n","\n","# Basis-Chain ohne Historie erstellen\n","chain = chat_prompt | llm | StrOutputParser()\n","\n","# Chain mit Message-History-Funktionalit√§t erweitern\n","chain_with_history = RunnableWithMessageHistory(\n","    chain,\n","    get_session_history,  # Unsere verbesserte History-Management-Funktion\n","    input_messages_key=\"question\",\n","    history_messages_key=\"history\"\n",")"],"metadata":{"id":"1RSEAlLkm8SI"},"id":"1RSEAlLkm8SI","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 4: Funktionen definieren\n","def interact_with_ai(question: str, session_id: str = \"kontinuierliche_session\") -> str:\n","    \"\"\"Verarbeitet eine Konversation und gibt die Antwort zur√ºck.\"\"\"\n","    # Nutzereingabe anzeigen\n","    display(Markdown(f\"### üßë‚Äçü¶± Mensch:\"))\n","    display(Markdown(question))\n","\n","    # Antwort mit History generieren\n","    response = chain_with_history.invoke(\n","        {\"question\": question},\n","        config={\"configurable\": {\"session_id\": session_id}}\n","    )\n","\n","    # Debug-Ausgabe: Zeige die aktuelle History an\n","    history = session_histories[session_id]\n","\n","    # Antwort anzeigen\n","    display(Markdown(f\"### ü§ñ KI:\"))\n","    display(Markdown(response))\n","    print()\n","\n","    return response"],"metadata":{"id":"UFv9qO_um-K5"},"id":"UFv9qO_um-K5","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 5: Hauptprogramm\n","# Konstante Session-ID definieren\n","KONSTANTE_SESSION_ID = \"kontinuierliche_session\"\n","\n","# Testfragen\n","test_questions = [\n","    \"Mein Lieblingsgericht ist Lasagne. Antworte kurz.\",\n","    \"Wie viele Tage hat eine Woche? Antworte kurz.\",\n","    \"Wie viele Tage hat ein Monat? Antworte kurz.\",\n","    \"Wie viele Tage hat ein Jahr? Antworte kurz.\",\n","    \"Was ist mein Lieblingsgericht?\"\n","]\n","\n","# Testfragen durchlaufen mit konstanter Session-ID\n","for question in test_questions:\n","    interact_with_ai(question, session_id=KONSTANTE_SESSION_ID)"],"metadata":{"id":"ME-KIsRgnAFc"},"id":"ME-KIsRgnAFc","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Optional: Ausgabe der vollst√§ndigen Konversation am Ende\n","def zeige_konversation(session_id):\n","    \"\"\" Zeigt die vollst√§ndige Konversationshistorie f√ºr eine bestimmte Session an. \"\"\"\n","    markdown_text = \"## Vollst√§ndige Konversation\\n\"\n","\n","    if session_id in session_histories:\n","        for i, msg in enumerate(session_histories[session_id].messages):\n","            # Bestimme die Rolle der Nachricht\n","            if isinstance(msg, SystemMessage):\n","                rolle = \"‚öôÔ∏è System\"\n","            elif isinstance(msg, HumanMessage):\n","                rolle = \"üßë‚Äçü¶± Mensch\"\n","            else:\n","                rolle = \"ü§ñ KI\"\n","\n","            # K√ºrze den Inhalt, wenn er zu lang ist\n","            inhalt = msg.content\n","            if len(inhalt) > 50:\n","                inhalt = f\"{inhalt[:50]}...\"\n","\n","            # F√ºge Nachricht zum Markdown-Text hinzu\n","            markdown_text += f\"**{i}.** [{rolle}]: {inhalt}\\n\\n\"\n","    else:\n","        markdown_text += \"*Keine Konversationshistorie f√ºr diese Session gefunden.*\\n\\n\"\n","\n","    # Zeige den Markdown-Text an\n","    display(Markdown(markdown_text))\n","\n","# Beispielaufruf\n","zeige_konversation(KONSTANTE_SESSION_ID)"],"metadata":{"id":"5CH5mnNInP2U"},"id":"5CH5mnNInP2U","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"01eb41f9","metadata":{"id":"01eb41f9"},"source":["Dieser Ansatz nutzt `RunnableWithMessageHistory` und `ChatMessageHistory` von LangChain, um Konversationen zu verwalten. Vorteile sind die einfachere Handhabung und die M√∂glichkeit, mehrere Konversationen mit unterschiedlichen Session-IDs zu verwalten.\n","\n"]},{"cell_type":"markdown","source":["# 2 | Nachrichten-Limit\n","---\n"],"metadata":{"id":"jUrNYwNah2W0"},"id":"jUrNYwNah2W0"},{"cell_type":"markdown","source":["\n","\n","Manchmal m√∂chten wir die Anzahl der Nachrichten begrenzen, die ein Modell als Kontext erh√§lt, anstatt die komplette Historie zu verwenden. Dies kann sinnvoll sein, wenn:\n","\n","1. Der j√ºngste Kontext am relevantesten ist\n","2. Wir Tokens sparen wollen\n","3. Wir eine fokussiertere Antwort ohne zu viel Ablenkung durch √§ltere Themen w√ºnschen\n","\n"],"metadata":{"id":"d523zHw0D2-B"},"id":"d523zHw0D2-B"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","ConversationBufferWindowMemory\n","</font></p>"],"metadata":{"id":"RkAAy6L5h4eL"},"id":"RkAAy6L5h4eL"},{"cell_type":"code","execution_count":null,"id":"cd0358c8","metadata":{"id":"cd0358c8"},"outputs":[],"source":["# Abschnitt 1: Importe\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain.memory import ConversationBufferWindowMemory\n","from langchain.schema import SystemMessage, HumanMessage, AIMessage\n","from IPython.display import display, Markdown\n","from operator import itemgetter"]},{"cell_type":"code","source":["# Abschnitt 2: Konstanten\n","MODEL = \"gpt-4o-mini\"\n","TEMPERATURE = 0.0\n","DEFAULT_SYSTEM = \"Du bist ein hilfsbereiter KI-Assistent.\"\n","MAX_HISTORY = 3  # Anzahl der zu speichernden Nachrichtenpaare\n","\n","TEST_QUESTIONS = [\n","    \"Mein Lieblingsgericht ist Lasagne. Antworte kurz.\",\n","    \"Wie viele Tage hat eine Woche? Antworte kurz.\",\n","    \"Wie viele Tage hat ein Monat? Antworte kurz.\",\n","    \"Wie viele Tage hat ein Jahr? Antworte kurz.\",\n","    \"Was ist mein Lieblingsgericht?\"\n","]"],"metadata":{"id":"O80lYM50qb8l"},"id":"O80lYM50qb8l","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 3: Chat-Komponenten initialisieren\n","llm = ChatOpenAI(temperature=TEMPERATURE, model=MODEL)\n","\n","# Memory mit k=3 f√ºr Fenstergr√∂√üe\n","memory = ConversationBufferWindowMemory(\n","    k=MAX_HISTORY,\n","    return_messages=True,\n","    memory_key=\"history\"\n",")\n","\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", DEFAULT_SYSTEM),\n","    MessagesPlaceholder(variable_name=\"history\"),\n","    (\"human\", \"{input}\")\n","])\n","\n","def get_chat_history(input_dict: dict) -> dict:\n","    \"\"\"L√§dt die Chat-Historie aus dem Memory.\"\"\"\n","    memory_vars = memory.load_memory_variables({})\n","    return {\n","        \"history\": memory_vars[\"history\"],\n","        \"input\": input_dict[\"input\"]\n","    }\n","\n","# Chain mit Pipe erstellen\n","chain = get_chat_history | prompt | llm"],"metadata":{"id":"zivpRnk8qdSW"},"id":"zivpRnk8qdSW","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 4: Funktionen definieren\n","def interact_with_ai(question):\n","    \"\"\"F√ºhrt eine Interaktion mit der KI durch und verarbeitet die Antwort.\"\"\"\n","    # Ausgabe der Frage\n","    display(Markdown(f\"\\nüßë‚Äçü¶± Mensch:\"))\n","    display(Markdown(question))\n","    display(Markdown(\"\\nü§ñ KI: \"))\n","\n","    # KI-Antwort einholen\n","    response = chain.invoke({\"input\": question})\n","\n","    # Antworttext extrahieren\n","    response_text = response.content if isinstance(response, AIMessage) else str(response)\n","\n","    # Formatierte Ausgabe erstellen\n","    display(Markdown(response_text))\n","    print()\n","\n","    # Konversation im Memory speichern\n","    memory.save_context({\"input\": question}, {\"output\": response_text})\n","\n","    return response_text"],"metadata":{"id":"RJaoO8Jyqf9C"},"id":"RJaoO8Jyqf9C","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 5: Hauptprogramm\n","\n","for question in TEST_QUESTIONS:\n","    interact_with_ai(question)"],"metadata":{"id":"JJ83yHdTqh9L"},"id":"JJ83yHdTqh9L","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"6430a41d","metadata":{"id":"6430a41d"},"source":["Nach der Ausf√ºhrung k√∂nnen wir den gespeicherten Kontext untersuchen:"]},{"cell_type":"code","execution_count":null,"id":"2d0083b7","metadata":{"id":"2d0083b7"},"outputs":[],"source":["print(\"\\nGespeicherter Kontext:\")\n","memory_ai = memory.load_memory_variables({})[\"history\"]\n","for i in range(len(memory_ai)):\n","    print(memory_ai[i].content)"]},{"cell_type":"markdown","id":"083e7fc3","metadata":{"id":"083e7fc3"},"source":["Der Parameter `k` in `ConversationBufferWindowMemory` steuert, wie viele Nachrichten-Paare (Frage und Antwort) dem LLM als Kontext √ºbergeben werden. Mit `k=3` werden nur die letzten drei Konversationsrunden ber√ºcksichtigt, auch wenn die gesamte Konversation l√§nger ist.\n","\n"]},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Memory-Funktion mit LCEL\n","</font></p>"],"metadata":{"id":"sWquJY0Dh9P9"},"id":"sWquJY0Dh9P9"},{"cell_type":"markdown","source":["Alternativ k√∂nnen wir auch unsere eigene Fenster-Implementierung erstellen:"],"metadata":{"id":"yuvIAYB9h-ef"},"id":"yuvIAYB9h-ef"},{"cell_type":"code","execution_count":null,"id":"820b1c1a","metadata":{"id":"820b1c1a"},"outputs":[],"source":["# Abschnitt 1: Importe\n","from IPython.display import display, Markdown\n","\n","from langchain_core.runnables import RunnableLambda, RunnableMap\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import PromptTemplate\n","from langchain.schema import AIMessage"]},{"cell_type":"code","source":["# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","MAX_HISTORY = 3  # Maximale Anzahl an gespeicherten Konversationsbeitr√§gen\n","\n","DEFAULT_SYSTEM = \"\"\"Du bist ein kompetenter KI-Assistent mit breitem Fachwissen.\n","\n","Deine Antworten sind:\n","- Klar strukturiert und mit Markdown formatiert\n","- Praxisorientiert und direkt umsetzbar\n","- Basierend auf aktuellem Kenntnisstand\n","- Mit passenden Beispielen versehen\n","- In verst√§ndlicher Sprache formuliert\n","\n","Aktuelle Konversation:\n","{history}\n","Mensch: {input}\n","KI:\"\"\"\n","\n","TEST_QUESTIONS = [\n","    \"Mein Lieblingsgericht ist Lasagne. Antworte kurz.\",\n","    \"Wie viele Tage hat eine Woche? Antworte kurz.\",\n","    \"Wie viele Tage hat ein Monat? Antworte kurz.\",\n","    \"Wie viele Tage hat ein Jahr? Antworte kurz.\",\n","    \"Was ist mein Lieblingsgericht?\"\n","]"],"metadata":{"id":"zfaTDBiDq6je"},"id":"zfaTDBiDq6je","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 3: Funktionen\n","# Speicher f√ºr Konversationshistorie manuell verwalten\n","conversation_history: list[str] = []\n","\n","def update_memory(question: str, response: str) -> list[str]:\n","    \"\"\"F√ºgt neue Konversationsbeitr√§ge zur Historie hinzu.\"\"\"\n","    global conversation_history\n","    conversation_history.append(f\"Mensch: {question}\\nKI: {response}\")\n","\n","    # Begrenze die Historie auf die letzten N Eintr√§ge\n","    if len(conversation_history) > MAX_HISTORY:\n","        conversation_history = conversation_history[-MAX_HISTORY:]\n","\n","    return conversation_history\n","\n","def format_history(history: list[str]) -> str:\n","    \"\"\"Formatiert die Historie f√ºr die Eingabe an das Modell.\"\"\"\n","    return \"\\n\".join(history)\n","\n","# Interaktionen mit dem Modell\n","def interact_with_ai(question):\n","    \"\"\"F√ºhrt eine einzelne Interaktion mit der KI durch.\"\"\"\n","    # Ausgabe der Frage\n","    display(Markdown(f\"\\nüßë‚Äçü¶± Mensch:\"))\n","    display(Markdown(question))\n","    display(Markdown(\"\\nü§ñ KI: \"))\n","\n","    # KI-Antwort einholen\n","    response = chain.invoke({\"input\": question})\n","\n","    # Antworttext extrahieren\n","    response_text = response.content if isinstance(response, AIMessage) else str(response)\n","\n","    # Formatierte Ausgabe erstellen\n","    display(Markdown(response_text))\n","    print()\n","\n","    # Speicher aktualisieren\n","    update_memory(question, response_text)\n","\n","    return response_text"],"metadata":{"id":"mKJz2wOcq-wE"},"id":"mKJz2wOcq-wE","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 4: LLM und LCEL-Chain initialisieren\n","llm = ChatOpenAI(\n","    model_name=MODEL,\n","    temperature=0.0\n",")\n","\n","prompt = PromptTemplate(\n","    input_variables=[\"history\", \"input\"],\n","    template=DEFAULT_SYSTEM\n",")\n","\n","# RunnableMap f√ºr Input-Transformation\n","prepare_input = RunnableLambda(lambda x: {\n","    \"history\": format_history(conversation_history),\n","    \"input\": x[\"input\"]\n","})\n","\n","# LCEL-Chain erstellen\n","chain = prepare_input | prompt | llm"],"metadata":{"id":"Rr1sdAOjrCf1"},"id":"Rr1sdAOjrCf1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 5: Hauptprogramm\n","\n","for question in TEST_QUESTIONS:\n","    interact_with_ai(question)"],"metadata":{"id":"yHQuvlq4rLnL"},"id":"yHQuvlq4rLnL","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"3e6f7a0f","metadata":{"id":"3e6f7a0f"},"source":["Diese Implementierung nutzt eine einfache Liste, um die letzten N Konversationspaare zu speichern. Sie bietet mehr Kontrolle, erfordert jedoch auch mehr manuellen Code.\n","\n"]},{"cell_type":"markdown","source":["# 3 | Token-Limit\n","---\n"],"metadata":{"id":"7-GWGzh-iBGC"},"id":"7-GWGzh-iBGC"},{"cell_type":"markdown","source":["\n","\n","Die vorherigen Ans√§tze begrenzten die Anzahl der gespeicherten Nachrichten. In der Praxis ist jedoch das **Token-Limit** des Modells entscheidend. `ConversationTokenBufferMemory` adressiert dieses Problem, indem es den Kontext basierend auf der tats√§chlichen Token-Anzahl begrenzt."],"metadata":{"id":"Td23eDsfiCla"},"id":"Td23eDsfiCla"},{"cell_type":"code","source":["# Abschnitt 1: Importe\n","from IPython.display import display, Markdown\n","from langchain.chains import ConversationChain\n","from langchain.memory import ConversationTokenBufferMemory\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import PromptTemplate\n","from langchain.schema import AIMessage"],"metadata":{"id":"iLUCnCgcvKpM"},"id":"iLUCnCgcvKpM","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","MAX_TOKEN_LIMIT = 1024\n","DEFAULT_SYSTEM = \"\"\"\n","Du bist ein kompetenter KI-Assistent mit breitem Fachwissen.\n","\n","Deine Antworten sind:\n","- Klar strukturiert und mit Markdown formatiert\n","- Praxisorientiert und direkt umsetzbar\n","- Basierend auf aktuellem Kenntnisstand\n","- Mit passenden Beispielen versehen\n","- In verst√§ndlicher Sprache formuliert\n","\n","Aktuelle Konversation:\n","{history}\n","Mensch: {input}\n","KI:\n","\"\"\""],"metadata":{"id":"0aBGv_RGwSjh"},"id":"0aBGv_RGwSjh","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 3: Funktionen definieren\n","def interact_with_ai(question):\n","    \"\"\" F√ºhrt eine Interaktion mit der KI durch und verarbeitet die Antwort. \"\"\"\n","    # Zeige die Frage an\n","    display(Markdown(f\"**üßë‚Äçü¶± Mensch:**\"))\n","    display(Markdown(f\"{question}\"))\n","\n","    # KI-Antwort abrufen\n","    response = chain.invoke({\"input\": question})\n","\n","    # Antwort je nach Typ verarbeiten\n","    if isinstance(response, dict) and \"response\" in response:\n","        response_text = response[\"response\"]\n","    elif isinstance(response, AIMessage):\n","        response_text = response.content\n","    else:\n","        response_text = str(response)\n","\n","    # Antwort anzeigen\n","    display(Markdown(f\"**ü§ñ KI:**\"))\n","    display(Markdown(f\"{response_text}\"))\n","\n","    # Konversation im Memory speichern\n","    memory.save_context({\"input\": question}, {\"output\": response_text})\n","\n","    # Kein return, um doppelte Ausgabe zu vermeiden\n","    return None"],"metadata":{"id":"nUCbVrPlwT8y"},"id":"nUCbVrPlwT8y","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 4: LLM und Kette initialisieren\n","llm = ChatOpenAI(\n","    model_name=MODEL,\n","    temperature=0.0\n",")\n","\n","memory = ConversationTokenBufferMemory(\n","    llm=llm,\n","    max_token_limit=MAX_TOKEN_LIMIT,\n","    return_messages=True\n",")\n","\n","# Prompt Template erstellen\n","prompt = PromptTemplate(\n","    input_variables=[\"history\", \"input\"],\n","    template=DEFAULT_SYSTEM\n",")\n","\n","# Conversation Chain erstellen\n","chain = ConversationChain(\n","    llm=llm,\n","    memory=memory,\n","    prompt=prompt,\n","    verbose=False\n",")"],"metadata":{"id":"aCl7mvN2wZCF"},"id":"aCl7mvN2wZCF","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 5: Interaktionen mit dem Modell\n","interact_with_ai(\"Mein Name ist Ralf\")\n","interact_with_ai(\"Warum ist der Himmel blau?\")\n","interact_with_ai(\"Und warum ist er manchmal rot?\")\n","interact_with_ai(\"Wie ist mein Name?\")"],"metadata":{"id":"o2pjNJV0wavF"},"id":"o2pjNJV0wavF","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"7b95a6bf","metadata":{"id":"7b95a6bf"},"source":["Gespeicherten Kontext nach Ausf√ºhrung:"]},{"cell_type":"code","execution_count":null,"id":"9d8ca33b","metadata":{"id":"9d8ca33b"},"outputs":[],"source":["# Gespeicherter Kontext anzeigen\n","display(Markdown(\"## ‚ú® Gespeicherter Kontext:\"))\n","display(Markdown(\"---\"))\n","\n","# Lade den Speicher\n","memory_ai = memory.load_memory_variables({})[\"history\"]\n","\n","# Erstelle eine formatierte Markdown-Ausgabe f√ºr alle Nachrichten\n","for msg in memory_ai:\n","    markdown_content = f\"**Typ:** {msg.type}\\n\\n**Inhalt:** {msg.content}\"\n","    display(Markdown(markdown_content))"]},{"cell_type":"markdown","id":"4b4bd50e","metadata":{"id":"4b4bd50e"},"source":["Der Hauptvorteil dieses Ansatzes ist die pr√§zise Kontrolle √ºber die tats√§chliche Token-Anzahl, die ans Modell √ºbergeben wird. Statt willk√ºrlich die letzten N Nachrichten zu speichern, beh√§lt diese Implementierung so viele Nachrichten wie m√∂glich innerhalb des Token-Limits bei.\n","\n"]},{"cell_type":"markdown","source":["# 4 | Zusammenfassung\n","---"],"metadata":{"id":"3NVtW3sKiGfB"},"id":"3NVtW3sKiGfB"},{"cell_type":"markdown","source":["\n","\n","\n","Die bisher betrachteten Ans√§tze haben entweder die vollst√§ndige Historie oder einen Teil davon bewahrt. Eine Alternative ist, l√§ngere Konversationen zusammenzufassen, um wichtige Informationen zu behalten, ohne die Token-Limits zu √ºberschreiten.\n","\n","LangChain's `ConversationSummaryMemory` speichert und aktualisiert eine Zusammenfassung der Konversation, anstatt √§ltere Chatverl√§ufe zu vergessen. Dies erm√∂glicht die Bewahrung wichtiger Informationen und h√§lt den Speicher effizient."],"metadata":{"id":"SkwJX0VPiHpK"},"id":"SkwJX0VPiHpK"},{"cell_type":"code","execution_count":null,"id":"47a38211","metadata":{"id":"47a38211"},"outputs":[],"source":["# Abschnitt 1: Importe\n","from IPython.display import display, Markdown\n","\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.messages import HumanMessage, AIMessage\n","from langchain_openai import ChatOpenAI\n","from langchain.memory import ConversationSummaryMemory\n","from langchain_core.runnables import RunnablePassthrough\n","\n","# Abschnitt 2: Konstanten definieren\n","MODEL = \"gpt-4o-mini\"\n","\n","conversations = [\n","    \"Mein Name ist Ralf\",\n","    \"Warum ist der Himmel blau?\",\n","    \"Und warum ist er manchmal rot?\",\n","    \"Wie ist mein Name?\"\n","]\n","\n","# Abschnitt 3: Funktionen definieren\n","def interact_with_ai(question):\n","    \"\"\"F√ºhrt eine einzelne Interaktion mit der KI durch und verarbeitet die Antwort.\"\"\"\n","    display(Markdown(f\"**üßë‚Äçü¶± Mensch:**\"))\n","    display(Markdown(f\"{question}\"))\n","\n","    response = chain.invoke({\"input\": question})\n","\n","    # Antwort extrahieren\n","    if isinstance(response, dict) and \"response\" in response:\n","        response_text = response[\"response\"]\n","    elif isinstance(response, AIMessage):\n","        response_text = response.content\n","    else:\n","        response_text = str(response)\n","\n","    display(Markdown(f\"**ü§ñ KI:**\"))\n","    display(Markdown(f\"{response_text}\"))\n","    print()\n","\n","    # Speichere die Konversation im Memory\n","    memory.save_context({\"input\": question}, {\"output\": response_text})\n","\n","    return response_text\n","\n","# Abschnitt 4: Chain und Memory initialisieren\n","# LLM initialisieren\n","llm = ChatOpenAI(\n","    temperature=0,\n","    model_name=MODEL\n",")\n","\n","# Memory initialisieren\n","memory = ConversationSummaryMemory(\n","    llm=llm,\n","    return_messages=True,\n","    max_token_limit=2000\n",")\n","\n","# Prompt Template erstellen\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent. Nutze vorherige Konversationen f√ºr kontextbezogene Antworten.\"),\n","    MessagesPlaceholder(variable_name=\"history\"),\n","    (\"human\", \"{input}\")\n","])\n","\n","# Chain erstellen\n","chain = (\n","    RunnablePassthrough.assign(\n","        history=lambda x: memory.load_memory_variables({})[\"history\"]\n","    )\n","    | prompt\n","    | llm\n",")\n","\n","# Abschnitt 5: Nachrichten verarbeiten und Ergebnisse anzeigen\n","for user_input in conversations:\n","    interact_with_ai(user_input)"]},{"cell_type":"markdown","id":"59b1182e","metadata":{"id":"59b1182e"},"source":["Nach der Ausf√ºhrung k√∂nnen wir die erzeugte Zusammenfassung betrachten:"]},{"cell_type":"code","execution_count":null,"id":"acff52ee","metadata":{"id":"acff52ee"},"outputs":[],"source":["# Ausgabe der Zusammenfassung mit display(Markdown())\n","display(Markdown(\"## Aktuelle Zusammenfassung:\"))\n","summary_ai = memory.load_memory_variables({})[\"history\"]\n","display(Markdown(f\"{summary_ai[0].content}\"))"]},{"cell_type":"markdown","id":"289eb07e","metadata":{"id":"289eb07e"},"source":["Dieser Ansatz ist besonders n√ºtzlich f√ºr l√§ngere Konversationen, da er die wichtigsten Informationen extrahiert und kompakt beh√§lt, statt nur die letzten N Nachrichten. Dadurch bleibt der Kontext erhalten, ohne das Token-Limit zu sprengen."]},{"cell_type":"markdown","id":"7a9a2437","metadata":{"id":"7a9a2437"},"source":["# 5 | Persistenter Speicher\n","---\n"]},{"cell_type":"markdown","source":["\n","\n","Die M√∂glichkeit eines ChatBots, sich an vorherige Unterhaltungen zu erinnern, spielt eine zentrale Rolle f√ºr eine fl√ºssige und konsistente Kommunikation. Ein **persistenter Speicher** erlaubt es, Gespr√§chsdaten √ºber mehrere Sitzungen hinweg zu sichern und wiederzuverwenden. Dadurch kann der ChatBot nicht nur den Kontext besser erfassen, sondern auch langfristige Nutzerpr√§ferenzen ber√ºcksichtigen.\n","\n","Dieser Abschnitt untersucht verschiedene Speicherl√∂sungen f√ºr ChatBots und deren effiziente Integration. Dabei werden unterschiedliche Methoden analysiert ‚Äì von einfachen Dateisystemen √ºber relationale und NoSQL-Datenbanken bis hin zu spezialisierten Speichern f√ºr gro√üe Sprachmodelle. Zudem wird aufgezeigt, wie gespeicherte Informationen verwaltet, aktualisiert und gesch√ºtzt werden, um eine ausgewogene Kombination aus Leistungsf√§higkeit, Datenschutz und Benutzerfreundlichkeit zu gew√§hrleisten.\n","\n","Die Implementierung eines dauerhaften Speichers erm√∂glicht es einem ChatBot, nicht nur die Interaktion mit Nutzern zu verbessern, sondern auch personalisierte, kontextbezogene und langfristig relevante Dialoge zu f√ºhren.\n","\n","F√ºr die nachfolgende Version wird die Klasse **BaseMemory** f√ºr einen persistenten Chat verwendet. Der Chat-Verlauf wird in einer JSON-Datei gespeichert. Diese Klasse dient als abstrakte Basisklasse, von der spezifische Memory-Implementierungen abgeleitet werden, um Kontexte und Zwischenergebnisse in den Arbeitsabl√§ufen (Chains) zu speichern und wiederzuverwenden.\n"],"metadata":{"id":"qKkboVWsiMaD"},"id":"qKkboVWsiMaD"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Minimalversion mit History als .txt\n","</font></p>"],"metadata":{"id":"ncw0vI8MAJrt"},"id":"ncw0vI8MAJrt"},{"cell_type":"code","source":["import os\n","from openai import OpenAI\n","\n","def chat():\n","    # Initialisierung des OpenAI-Clients\n","    client = OpenAI()\n","    file = \"chat_history.txt\"\n","\n","    print(\"Chatbot gestartet.\")\n","    print(\"Befehle: 'history' zeigt Verlauf, 'exit' beendet Chat\")\n","\n","    # Chat-Kontext initialisieren\n","    messages = [\n","        {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher Assistent. Antworte auf Deutsch und sei pr√§zise.\"}\n","    ]\n","\n","    # Lade bestehende Historie beim Start, falls vorhanden\n","    try:\n","        if os.path.exists(file) and os.path.getsize(file) > 0:\n","            print(\"Lade vorherige Chathistorie...\")\n","            chat_pairs = []\n","            with open(file, 'r', encoding='utf-8') as f:\n","                content = f.read()\n","                segments = content.split(\"\\n\\n\")\n","\n","                for segment in segments:\n","                    if segment.strip() and \"Benutzer: \" in segment and \"KI: \" in segment:\n","                        parts = segment.split(\"KI: \", 1)\n","                        if len(parts) == 2:\n","                            user_part = parts[0].replace(\"Benutzer: \", \"\", 1).strip()\n","                            ai_part = parts[1].strip()\n","                            if user_part and ai_part:\n","                                chat_pairs.append((user_part, ai_part))\n","\n","            # F√ºge die letzten 5 Austausche zum Kontext hinzu (falls vorhanden)\n","            for user_msg, ai_msg in chat_pairs[-10:]:\n","                messages.append({\"role\": \"user\", \"content\": user_msg})\n","                messages.append({\"role\": \"assistant\", \"content\": ai_msg})\n","\n","            print(f\"{len(chat_pairs)} fr√ºhere Unterhaltungen gefunden, die letzten {min(10, len(chat_pairs))} werden im Kontext verwendet.\")\n","    except Exception as e:\n","        print(f\"Fehler beim Laden der Historie: {e}\")\n","\n","    # Endlosschleife f√ºr den Chat\n","    while True:\n","        user_input = input(\"\\nBenutzer: \")\n","\n","        if user_input == \"exit\":\n","            print(\"Chat beendet.\")\n","            break\n","        elif user_input == \"history\":\n","            try:\n","                with open(file, 'r', encoding='utf-8') as f:\n","                    print(\"\\n--- Chatverlauf ---\")\n","                    print(f.read())\n","                    print(\"--- Ende des Verlaufs ---\\n\")\n","            except FileNotFoundError:\n","                print(\"Keine Chathistorie gefunden.\")\n","            continue\n","\n","        # Benutzer-Nachricht zum Kontext hinzuf√ºgen\n","        messages.append({\"role\": \"user\", \"content\": user_input})\n","\n","        # OpenAI API-Anfrage mit vollem Kontext\n","        try:\n","            response = client.chat.completions.create(\n","                model=\"gpt-4o-mini\",\n","                messages=messages,\n","                max_tokens=1000\n","            )\n","            ai_response = response.choices[0].message.content\n","\n","            # Antwort zum Kontext hinzuf√ºgen f√ºr zuk√ºnftige Anfragen\n","            messages.append({\"role\": \"assistant\", \"content\": ai_response})\n","\n","            # Kontext begrenzen, um Token zu sparen (beh√§lt nur die letzten 10 Nachrichten + System-Prompt)\n","            if len(messages) > 11:  # System-Prompt + 10 Nachrichten\n","                messages = [messages[0]] + messages[-10:]\n","\n","        except Exception as e:\n","            ai_response = f\"Fehler: {str(e)}\"\n","\n","        print(f\"KI: {ai_response}\")\n","\n","        # Speichern in einfacher Textdatei\n","        with open(file, 'a', encoding='utf-8') as f:\n","            f.write(f\"Benutzer: {user_input}\\nKI: {ai_response}\\n\\n\")"],"metadata":{"id":"pF-E3tIDAKky"},"id":"pF-E3tIDAKky","execution_count":null,"outputs":[]},{"cell_type":"code","source":["chat()"],"metadata":{"id":"WdxQR3IsAM0a"},"id":"WdxQR3IsAM0a","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Komfortversion mit History als .json\n","</font></p>"],"metadata":{"id":"gNaHt9iuAfzs"},"id":"gNaHt9iuAfzs"},{"cell_type":"code","source":["# Abschnitt 1: Importe\n","import json\n","import os\n","from datetime import datetime\n","from pathlib import Path\n","from langchain_core.memory import BaseMemory\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate\n","from langchain_core.messages import SystemMessage\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain.memory import ConversationSummaryBufferMemory\n","from pydantic import BaseModel, Field\n","\n","from IPython.display import display, Markdown"],"metadata":{"id":"XIxfBMsQ87Jc"},"id":"XIxfBMsQ87Jc","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 2: Konstanten und Klassen\n","MODEL = \"gpt-4o-mini\"\n","DEFAULT_MEMORY_FILE = \"chat_memory.json\"\n","SYSTEM_PROMPT = \"\"\"Du bist ein kompetenter KI-Assistent mit breitem Fachwissen.\n","\n","Deine Antworten sind:\n","- Klar strukturiert und mit Markdown formatiert\n","- Praxisorientiert und direkt umsetzbar\n","- Basierend auf aktuellem Kenntnisstand\n","- Mit passenden Beispielen versehen\n","- In verst√§ndlicher Sprache formuliert\"\"\"\n","\n","class EnhancedJSONMemory(BaseMemory, BaseModel):\n","    \"\"\"Erweiterte Memory-Implementierung mit JSON-Speicherung, Statistiken und Zusammenfassung.\"\"\"\n","    file_path: str = Field(default=DEFAULT_MEMORY_FILE)\n","    summary_memory: object = Field(default=None)\n","    chat_data: dict[str, object] = Field(default_factory=lambda: {\n","        \"conversations\": [],\n","        \"metadata\": {\n","            \"created_at\": datetime.now().isoformat(),\n","            \"last_modified\": datetime.now().isoformat(),\n","            \"total_interactions\": 0,\n","            \"model_name\": MODEL,\n","            \"version\": \"2.0\"\n","        },\n","        \"statistics\": {\n","            \"average_response_length\": 0,\n","            \"total_chars_exchanged\": 0\n","        },\n","        \"summary\": \"\"\n","    })\n","\n","    def __init__(self, **kwargs) -> None:\n","        \"\"\"Initialisiert die Memory-Klasse und l√§dt existierende Chatverl√§ufe.\"\"\"\n","        super().__init__(**kwargs)\n","        self.summary_memory = ConversationSummaryBufferMemory(\n","            llm=ChatOpenAI(temperature=0, model=MODEL),\n","            max_token_limit=2000,\n","            memory_key=\"summary\",\n","            human_prefix=\"Mensch\",\n","            ai_prefix=\"KI\"\n","        )\n","        self.load_history()\n","\n","    def format_timestamp(self, timestamp_str: str) -> str:\n","        \"\"\"Formatiert einen ISO-Zeitstempel in ein lesbares Format.\"\"\"\n","        return datetime.fromisoformat(timestamp_str).strftime(\"%d.%m.%Y %H:%M:%S\")\n","\n","    def update_statistics(self, user_input: str, response: str) -> None:\n","        \"\"\"Aktualisiert die Chatstatistiken.\"\"\"\n","        stats = self.chat_data[\"statistics\"]\n","        total_responses = len(self.chat_data[\"conversations\"])\n","\n","        if total_responses > 0:\n","            current_total = stats[\"average_response_length\"] * (total_responses - 1)\n","            stats[\"average_response_length\"] = round((current_total + len(response)) / total_responses, 2)\n","\n","        stats[\"total_chars_exchanged\"] += len(user_input) + len(response)\n","\n","    def update_summary(self, user_input: str, response: str) -> None:\n","        \"\"\"Aktualisiert die Konversationszusammenfassung.\"\"\"\n","        self.summary_memory.save_context({\"input\": user_input}, {\"output\": response})\n","        self.chat_data[\"summary\"] = self.summary_memory.load_memory_variables({}).get(\"summary\", \"\")\n","\n","    def save_context(self, inputs: dict[str], outputs: dict[str, str]) -> None:\n","        \"\"\"Speichert den aktuellen Kontext mit erweiterten Metadaten.\"\"\"\n","        self.chat_data[\"conversations\"].append({\n","            \"timestamp\": datetime.now().isoformat(),\n","            \"user_input\": inputs[\"input\"],\n","            \"response\": outputs[\"text\"],\n","            \"interaction_id\": len(self.chat_data[\"conversations\"]) + 1\n","        })\n","        self.chat_data[\"metadata\"][\"last_modified\"] = datetime.now().isoformat()\n","        self.chat_data[\"metadata\"][\"total_interactions\"] += 1\n","\n","        self.update_statistics(inputs[\"input\"], outputs[\"text\"])\n","        self.update_summary(inputs[\"input\"], outputs[\"text\"])\n","        self._save_to_file()\n","\n","    def _save_to_file(self) -> None:\n","        \"\"\"Speichert den Chatverlauf in der JSON-Datei.\"\"\"\n","        try:\n","            with Path(self.file_path).open(\"w\", encoding=\"utf-8\") as file:\n","                json.dump(self.chat_data, file, ensure_ascii=False, indent=2)\n","        except Exception as e:\n","            md_print(f\"‚ö†Ô∏è **Fehler beim Speichern der Historie:** {str(e)}\")\n","\n","    def load_history(self) -> None:\n","        \"\"\"L√§dt die Chathistorie aus der JSON-Datei.\"\"\"\n","        if not Path(self.file_path).exists():\n","            return\n","\n","        try:\n","            with Path(self.file_path).open(\"r\", encoding=\"utf-8\") as file:\n","                loaded_data = json.load(file)\n","\n","            if \"metadata\" in loaded_data and \"version\" in loaded_data[\"metadata\"]:\n","                self.chat_data = loaded_data\n","                md_print(\"‚úÖ **Konversationshistorie wurde erfolgreich geladen.**\")\n","\n","                # Lade vorhandene Konversationen in das Summary Memory\n","                for conv in self.chat_data[\"conversations\"]:\n","                    self.summary_memory.save_context(\n","                        {\"input\": conv[\"user_input\"]},\n","                        {\"output\": conv[\"response\"]}\n","                    )\n","            else:\n","                md_print(\"‚ö†Ô∏è **Veraltetes Historienformat erkannt. Erstelle neue Historie.**\")\n","        except json.JSONDecodeError as e:\n","            md_print(f\"‚ö†Ô∏è **Fehler beim Laden der JSON-Datei:** {str(e)}\")\n","\n","    def load_memory_variables(self, inputs: dict[str]) -> dict[str, list[dict[str]]]:\n","        \"\"\"L√§dt die Chatvariablen f√ºr den Prompt.\"\"\"\n","        return {\"history\": self.chat_data[\"conversations\"], \"summary\": self.chat_data[\"summary\"]}\n","\n","    @property\n","    def memory_variables(self) -> list[str]:\n","        \"\"\"Definiert die verf√ºgbaren Memory-Variablen.\"\"\"\n","        return [\"history\", \"summary\"]\n","\n","    def clear(self) -> None:\n","        \"\"\"L√∂scht die Chathistorie und speichert den zur√ºckgesetzten Zustand.\"\"\"\n","        # Zur√ºcksetzen des Konversations-Arrays\n","        self.chat_data[\"conversations\"] = []\n","        # Zur√ºcksetzen der Zusammenfassung\n","        self.chat_data[\"summary\"] = \"\"\n","        # Zur√ºcksetzen der Statistiken\n","        self.chat_data[\"statistics\"] = {\n","            \"average_response_length\": 0,\n","            \"total_chars_exchanged\": 0\n","        }\n","        # Metadaten aktualisieren\n","        self.chat_data[\"metadata\"][\"last_modified\"] = datetime.now().isoformat()\n","        self.chat_data[\"metadata\"][\"total_interactions\"] = 0\n","\n","        # Summary Memory zur√ºcksetzen\n","        self.summary_memory.clear()\n","\n","        # Gespeicherte Datei aktualisieren\n","        self._save_to_file()\n","\n","        md_print(\"üßπ **Chathistorie wurde erfolgreich gel√∂scht.**\")\n","\n","    def show_history(self) -> None:\n","        \"\"\"Zeigt eine detaillierte Konversationshistorie und Zusammenfassung.\"\"\"\n","        if not self.chat_data[\"conversations\"]:\n","            md_print(\"‚ùå **Keine Konversationshistorie vorhanden.**\")\n","            return\n","\n","        history_markdown = []\n","        history_markdown.append(\"\\n## üìä Chat-Statistiken:\")\n","        stats = self.chat_data[\"statistics\"]\n","        meta = self.chat_data[\"metadata\"]\n","        history_markdown.append(f\"- **Gesamtinteraktionen:** {meta['total_interactions']}\")\n","        history_markdown.append(f\"- **Durchschnittliche Antwortl√§nge:** {stats['average_response_length']} Zeichen\")\n","        history_markdown.append(f\"- **Gesamter Zeichenaustausch:** {stats['total_chars_exchanged']} Zeichen\")\n","\n","        if self.chat_data[\"summary\"]:\n","            history_markdown.append(\"\\n## üìù Zusammenfassung der Konversation:\")\n","            history_markdown.append(self.chat_data[\"summary\"])\n","\n","        history_markdown.append(\"\\n## üìú Detaillierte Konversationshistorie:\")\n","        for conv in self.chat_data[\"conversations\"]:\n","            history_markdown.append(\"\\n\" + \"=\"*50)\n","            history_markdown.append(f\"### üî¢ Interaktion #{conv['interaction_id']}\")\n","            history_markdown.append(f\"**üïí Zeitpunkt:** {self.format_timestamp(conv['timestamp'])}\")\n","            history_markdown.append(f\"**üßë‚Äçü¶± Mensch:** {conv['user_input']}\")\n","            history_markdown.append(f\"**ü§ñ KI:**\\n{conv['response']}\")\n","\n","        md_print(\"\\n\".join(history_markdown))\n"],"metadata":{"id":"CaWtSTHR9kgd"},"id":"CaWtSTHR9kgd","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 3: Chat Komponenten\n","# LLM und Memory initialisieren\n","llm = ChatOpenAI(model=MODEL, temperature=0.0)\n","memory = EnhancedJSONMemory()\n","\n","# Prompt Template und Chat Sequence erstellen\n","prompt = ChatPromptTemplate.from_messages([\n","    SystemMessage(content=SYSTEM_PROMPT),\n","    (\"system\", \"Zusammenfassung der bisherigen Konversation:\\n{summary}\\n\"),\n","    (\"system\", \"Bisheriger Chatverlauf:\\n{history}\\n\"),\n","    (\"human\", \"{input}\")\n","])\n","\n","# Hilfsfunktion f√ºr Formatierung\n","def format_history(history):\n","    return \"\\n\".join([f\"Mensch: {entry['user_input']}\\nKI: {entry['response']}\" for entry in history])\n","\n","# Chat Sequence erstellen\n","chat_sequence = (\n","    {\n","        \"history\": lambda x: format_history(memory.load_memory_variables({})[\"history\"]),\n","        \"summary\": lambda x: memory.load_memory_variables({})[\"summary\"],\n","        \"input\": lambda x: x[\"input\"]\n","    }\n","    | prompt\n","    | llm\n",")"],"metadata":{"id":"28Qv2Sm59m6l"},"id":"28Qv2Sm59m6l","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 4: Funktionen\n","def md_print(text):\n","    \"\"\"Gibt Text als Markdown aus, wenn m√∂glich, sonst als normalen Text.\"\"\"\n","    try:\n","        # Pr√ºfen, ob wir in einem Jupyter Notebook sind\n","        is_notebook = 'ipykernel' in sys.modules\n","        if is_notebook:\n","            display(Markdown(text))\n","        else:\n","            print(text)\n","    except NameError:\n","        # Wenn sys nicht definiert ist, nehmen wir an, dass wir nicht in einem Notebook sind\n","        print(text)\n","\n","def run_chat(chat_sequence, memory) -> None:\n","    \"\"\"F√ºhrt die Chat-Schleife aus.\"\"\"\n","    md_print(\"## Chat gestartet\\n\")\n","    md_print(\"**Befehle:**\\n- `history`: Zeigt die Chathistorie\\n- `clear`: L√∂scht die Chathistorie\\n- `exit` oder `tsch√ºss`: Beendet den Chat\\n\")\n","\n","    # Pr√ºfen, ob wir in einem Jupyter Notebook sind\n","    try:\n","        is_notebook = 'ipykernel' in sys.modules\n","    except NameError:\n","        # sys ist nicht definiert, also sind wir nicht in einem Notebook\n","        is_notebook = False\n","\n","    while True:\n","        # Die Eingabeaufforderung bleibt ein normales print, da input() damit arbeitet\n","        if is_notebook:\n","            from IPython.display import clear_output\n","            # In Notebooks k√∂nnen wir es sch√∂ner machen mit garantierter Breitenreduzierung\n","            from ipywidgets import widgets\n","\n","            # Container-Widget mit fester Breite erstellen\n","            container = widgets.HBox(\n","                layout=widgets.Layout(\n","                    width='60%',       # Container auf 60% der verf√ºgbaren Breite beschr√§nken\n","                    margin='0px'\n","                )\n","            )\n","\n","            # Textfeld erstellen (nimmt die volle Breite des Containers ein)\n","            user_input_widget = widgets.Text(\n","                description=\"üß†\",\n","                placeholder=\"Deine Nachricht hier...\",\n","                style={'description_width': '40px'},\n","                layout=widgets.Layout(width='100%')  # F√ºllt den Container vollst√§ndig aus\n","            )\n","\n","            # Das Textfeld in den Container einf√ºgen und anzeigen\n","            container.children = [user_input_widget]\n","            display(container)\n","\n","            # Warten auf Eingabe\n","            user_input = input(\"Eingabe: \")\n","            user_input_widget.value = user_input\n","        else:\n","            user_input = input(\"\\U0001F9B1 Mensch: \")\n","\n","        if user_input.lower() in [\"exit\", \"tsch√ºss\"]:\n","            break\n","        elif user_input.lower() == \"history\":\n","            memory.show_history()\n","            continue\n","        elif user_input.lower() == \"clear\":\n","            memory.clear()\n","            continue\n","\n","        response = chat_sequence.invoke({\"input\": user_input})\n","        memory.save_context({\"input\": user_input}, {\"text\": response.content})\n","\n","        # Markdown-Ausgabe f√ºr die Antwort\n","        md_print(f\"\\n## ü§ñ KI:\\n{response.content}\\n\")"],"metadata":{"id":"PpFbPKZq9qvH"},"id":"PpFbPKZq9qvH","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Abschnitt 5: Hauptfunktion\n","def main() -> None:\n","    \"\"\"Hauptfunktion zum Starten des Chat-Bots.\"\"\"\n","    md_print(\"## ü§ñ KI-Chat mit Ged√§chtnis\")\n","\n","    # sys importieren, wenn noch nicht geschehen\n","    try:\n","        import sys\n","    except ImportError:\n","        pass\n","\n","    # Chat-Komponenten sind bereits global definiert\n","    run_chat(chat_sequence, memory)"],"metadata":{"id":"INS1Ykc89sJF"},"id":"INS1Ykc89sJF","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ausf√ºhren, wenn direkt gestartet\n","main()"],"metadata":{"id":"3L89ijxT9XPz"},"id":"3L89ijxT9XPz","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"5cfbf26a","metadata":{"id":"5cfbf26a"},"source":["# A | Aufgaben\n","---\n"]},{"cell_type":"markdown","source":["\n","\n","Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen.\n"],"metadata":{"id":"pwmSuPsGiQp6"},"id":"pwmSuPsGiQp6"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","KI-gest√ºtztes Notizbuch mit automatischer Kategorisierung\n","</font></p>\n","\n","\n","Entwickeln Sie ein KI-Notizbuch, das Eingaben von Nutzern speichert, automatisch kategorisiert (z. B. \"Technologie\", \"Privat\", \"Aufgaben\") und kontextbezogene Vorschl√§ge liefert. Verwenden Sie dazu ein LangChain Memory-Konzept, z.B. ConversationBufferMemory, f√ºr eine effiziente Verwaltung.\n"],"metadata":{"id":"1UViO3SJiSXy"},"id":"1UViO3SJiSXy"},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Chatbot: Nachrichten- vs. Summary-Speicher\n","</font></p>\n","\n","\n","Erstellen Sie einen einfachen Chatbot mit LangChain, der zwischen Nachrichten- und Summary-Speicher unterscheidet. Analysieren Sie, wie sich die Speichertypen auf die Qualit√§t der Antworten auswirken.\n","\n"],"metadata":{"id":"T6HXRFbXjIVL"},"id":"T6HXRFbXjIVL"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Automatische Protokollerstellung f√ºr Meetings\n","</font></p>\n","\n","\n","Entwickeln Sie ein System, das Gespr√§chsverl√§ufe speichert und automatisch eine zusammenfassende Chat-Notiz erstellt. Nutzen Sie z.B. ConversationSummaryMemory, um die wichtigsten Punkte aus langen Gespr√§chen zu extrahieren.\n","\n"],"metadata":{"id":"9eIl83Z-CqT8"},"id":"9eIl83Z-CqT8"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Virtuelle Assistenten mit eigenem Ged√§chtnis\n","</font></p>\n","\n","\n","Eine Datei wird bereitgestellt, die ein Gespr√§ch zwischen zwei virtuellen Assistenten enth√§lt. Jede Antwort soll aus genau einem Satz bestehen. Die Datei ist folgenderma√üen aufgebaut:\n","\n","|Assistent|Eingabe|\n","|---|---|\n","|botA|Ich hei√üe Max.|\n","|botB|Mein Name ist Emma.|\n","|botA|Ich wohne in Berlin.|\n","|botB|Ich lebe in Hamburg.|\n","|botA|Wo wohne ich und wie hei√üe ich?|\n","|botB|Wo lebe ich?|\n","\n","**Aufgabe**  \n","Schreiben Sie ein Programm, das zwei virtuelle Assistenten simuliert, die jeweils ein eigenes Ged√§chtnis haben. Die Eingaben aus der Datei werden an den jeweiligen Assistenten geschickt, und ihre Antworten sollen in einer neuen Datei gespeichert werden.\n","\n","**Erwartete Ausgabe**  \n","Die erwartete Ausgabe k√∂nnte wie folgt aussehen:\n","\n","|Assistent|Antwort|\n","|---|---|\n","|botA|Hallo Max.|\n","|botB|Hallo Emma.|\n","|botA|Danke f√ºr diese Information.|\n","|botB|Danke f√ºr diese Information.|\n","|botA|Hallo Max, du wohnst in Berlin und dein Name ist Max.|\n","|botB|Hallo Emma, du lebst in Hamburg.|\n","\n","**Hinweise**\n","\n","- Die Antworten m√ºssen nicht exakt mit der Beispielausgabe √ºbereinstimmen, sollten aber sinngem√§√ü √§hnlich sein.\n","- Jeder Assistent hat ein eigenes Ged√§chtnis und sollte sich nur an seine eigenen Informationen erinnern.\n","- Das Programm soll die Antworten in einer neuen Datei im gleichen Tabellenformat speichern."],"metadata":{"id":"CwYae6YZCsto"},"id":"CwYae6YZCsto"}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"colab":{"provenance":[],"collapsed_sections":["fe8a8ba4","jUrNYwNah2W0","7-GWGzh-iBGC","3NVtW3sKiGfB","7a9a2437","5cfbf26a"],"toc_visible":true},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}
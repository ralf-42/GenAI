{"cells":[{"cell_type":"markdown","id":"37rMyCQz2JNw","metadata":{"id":"37rMyCQz2JNw"},"source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)"]},{"cell_type":"markdown","id":"R5CfUEMJdvFQ","metadata":{"id":"R5CfUEMJdvFQ"},"source":["<p><font size=\"5\" color='grey'> <b>\n","LangChain 101\n","</b></font> </br></p>\n","\n","---"]},{"cell_type":"code","source":["#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n","!uv pip install --system -q git+https://github.com/ralf-42/Python_Modules\n","from genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages\n","setup_api_keys(['OPENAI_API_KEY', 'HF_TOKEN'], create_globals=False)\n","print()\n","check_environment()\n","print()\n","get_ipinfo()\n","# Bei Bedarf: Trennen zwischen Installationsname () und Importname (f√ºr Python) beide Angaben in Klammern\n","# install_packages([('markitdown[all]', 'markitdown'), 'langchain_chroma', ]"],"metadata":{"id":"PnT8m8TTUhF5"},"id":"PnT8m8TTUhF5","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"91ee89a4-477b-41ce-a511-aadd4c2fd547","metadata":{"id":"91ee89a4-477b-41ce-a511-aadd4c2fd547"},"source":["# 1 | Was ist LangChain?\n","---"]},{"cell_type":"markdown","id":"b5004f8c","metadata":{"id":"b5004f8c"},"source":["LangChain ist ein Framework zur **Entwicklung von Anwendungen mit gro√üen Sprachmodellen** (LLMs). Es vereinfacht die Integration von LLMs in eigene Anwendungen durch:\n","\n","- Modulare Komponenten f√ºr verschiedene Aufgaben\n","- Vorgefertigte Ketten (Chains) f√ºr komplexe Workflows\n","- Einfache Integration externer Datenquellen\n","- Werkzeuge f√ºr das Speichern von Konversationskontexten\n","\n"]},{"cell_type":"markdown","source":["<img src=\"https://python.langchain.com/img/brand/wordmark.png\" class=\"logo\" width=\"360\"/>\n","\n"],"metadata":{"id":"WgQFFaXrcx4E"},"id":"WgQFFaXrcx4E"},{"cell_type":"markdown","id":"tl73bT-kKkia","metadata":{"id":"tl73bT-kKkia"},"source":["[Einf√ºhrung](https://python.langchain.com/docs/introduction/)   \n","[Konzepte](https://python.langchain.com/docs/concepts/)   \n","[API-References](https://python.langchain.com/api_reference/index.html)   \n","[Integrations](https://python.langchain.com/docs/integrations/providers/)\n","\n","---\n","\n","[Tutorials](https://python.langchain.com/docs/tutorials/)   \n","[How-to-Guides](https://python.langchain.com/docs/how_to/)   "]},{"cell_type":"markdown","id":"fGV8FwGmIwmR","metadata":{"id":"fGV8FwGmIwmR"},"source":["# 2 | √úberblick Konzepte\n","---\n","\n","\n"]},{"cell_type":"markdown","source":["Die Darstellung veranschaulicht das Grundprinzip von LangChain: die Verkettung verschiedener Konzepte zu einem durchg√§ngigen Verarbeitungsprozess, der Eingaben in strukturierte Ausgaben umwandelt."],"metadata":{"id":"nZ2ZMEd3ulKZ"},"id":"nZ2ZMEd3ulKZ"},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/langchain_prozess_01.png)"],"metadata":{"id":"KdBKNv3gzJAq"},"id":"KdBKNv3gzJAq"},{"cell_type":"markdown","source":["Konzepte von LangChain sind grundlegende Bausteine und Prinzipien. Hier sind einige der wichtigsten Konzepte:\n","\n","+ Nachrichten (**Messages**): Kommunikationseinheiten in Chat-Modellen f√ºr Ein- und Ausgabe.\n","+ Prompt-Vorlagen (**Prompt template**s): Komponenten zur Erstellung strukturierter Prompts f√ºr LLMs.\n","+ Chat-Modelle (**Chat models**): LLMs, die √ºber eine Chat-API verf√ºgbar sind und Sequenzen von Nachrichten verarbeiten.\n","+ Chains (**Chains**): Verkn√ºpfungen mehrerer LLMs oder anderer Komponenten f√ºr komplexere Anwendungen.\n","+ Strukturierte Ausgabe (**Structured output**): Technik, um Chat-Modelle in strukturierten Formaten antworten zu lassen.\n","+ Chat-Verlauf (**Chat history**): Eine Sequenz von Nachrichten, die eine Konversation darstellt.\n","+ Retrieval Augmented Generation (**RAG**): Technik zur Verbesserung von Sprachmodellen durch Kombination mit externen Wissensbasen.\n","+ Retriever (**Retriever**): Komponenten, die relevante Dokumente aus einer Wissensbasis abrufen.\n","+ Agenten (**Agents**): Nutzen Sprachmodelle, um Aktionssequenzen auszuw√§hlen und mit externen Ressourcen zu interagieren.\n","+ Tools (**Tools**): Funktionen mit definierten Schemata f√ºr Name, Beschreibung und Argumente.\n"],"metadata":{"id":"cCuow5DOEsID"},"id":"cCuow5DOEsID"},{"cell_type":"markdown","id":"EouwC_wdZb-c","metadata":{"id":"EouwC_wdZb-c"},"source":["# 3 | Nachrichten-Typen\n","---"]},{"cell_type":"markdown","id":"yS2mM2J1aXlY","metadata":{"id":"yS2mM2J1aXlY"},"source":["Beim Einsatz von Large Language Models (LLMs) wie GPT gibt es drei Hauptarten von Nachrichten (Messages), die die Interaktion mit dem Modell steuern. Diese Messages definieren, wie das Modell reagiert, welche Eingaben es erh√§lt und welche Ausgaben es generiert.\n","\n"]},{"cell_type":"markdown","id":"sGqRXptbbQ2a","metadata":{"id":"sGqRXptbbQ2a"},"source":["**System Message:**   \n","Die `System Message` dient dazu, das **Verhalten des LLMs** zu steuern. Sie ist eine nicht sichtbare Nachricht, die dem Modell Anweisungen gibt, wie es sich verhalten soll. Dabei kann sie die Pers√∂nlichkeit, den Tonfall und die Einschr√§nkungen des Modells beeinflussen. Die Systemnachricht funktioniert wie eine Art \"Rollenspiel-Anweisung\", um das Modell in eine gew√ºnschte Rolle zu versetzen.\n","\n","*Merkmale:*\n","- Definiert, wie das Modell reagieren soll\n","- Legt Pers√∂nlichkeit, Verhalten und Einschr√§nkungen fest\n","- Wird zu Beginn einer Sitzung gesetzt und bleibt bestehen\n","- Nicht sichtbar f√ºr den Benutzer\n","\n","*Beispiel einer System Message:*\n","```json\n","{\n","  \"role\": \"system\",\n","  \"content\": \"Du bist ein KI-Assistent, der pr√§zise und informative Antworten in einem professionellen Ton liefert.\"\n","}\n","```\n","Dieses Beispiel weist das Modell an, in einem professionellen Tonfall kurze und informative Antworten zu geben.\n","\n","---\n","\n"]},{"cell_type":"markdown","id":"upsIzpcqbbP7","metadata":{"id":"upsIzpcqbbP7"},"source":["**User Message**   \n","Die `User Message` ist die eigentliche **Eingabe des Benutzer**s. Sie kann eine Frage, eine Aufforderung, kontextrelevanten Inhalt oder ein Befehl sein. Das Modell nutzt diese Nachricht als Ausgangspunkt f√ºr die Generierung einer Antwort.\n","\n","*Merkmale:*\n","- Direkte Eingabe des Benutzers\n","- Kann eine Frage, Aufforderung, kontextrelevanten Inhalt oder einen Befehl enthalten\n","- Basis f√ºr die Antwort des Modells\n","\n","*Beispiel einer User Message:*\n","```json\n","{\n","  \"role\": \"user\",\n","  \"content\": \"Was sind die wichtigsten Unterschiede zwischen KI und maschinellem Lernen?\"\n","}\n","```\n","Hier fragt der Benutzer nach den Unterschieden zwischen KI und maschinellem Lernen.\n","\n","---\n"]},{"cell_type":"markdown","id":"CozrrtYXbhZb","metadata":{"id":"CozrrtYXbhZb"},"source":["**AI Message**   \n","Die `AI Message` ist die **Antwort, die das Modell generiert**. Sie basiert auf der User Message und den Anweisungen aus der System Message. Die AI Message kann verschiedene Eigenschaften haben, ist aber haupts√§chlich auf den Inhalt fokussiert.\n","\n","*Merkmale:*\n","- Antwort des Modells auf die Benutzeranfrage\n","- Kann verschiedene Eigenschaften haben (z. B. L√§nge, Stil)\n","- Haupts√§chlich inhaltlich relevant\n","\n","*Beispiel einer AI Message:*\n","```json\n","{\n","  \"role\": \"assistant\",\n","  \"content\": \"K√ºnstliche Intelligenz (KI) ist ein weites Feld, das verschiedene Teilgebiete umfasst, darunter auch das maschinelle Lernen (ML). ML konzentriert sich auf die Erstellung von Modellen, die aus Daten lernen und Vorhersagen treffen.\"\n","}\n","```\n","Hier gibt das Modell eine inhaltliche Antwort auf die Frage des Benutzers.\n","\n","---\n","\n"]},{"cell_type":"markdown","id":"PiHnhKHVbofP","metadata":{"id":"PiHnhKHVbofP"},"source":["**Kurz gefasst**   \n","\n","+  **System Message** legt die Regeln und das Verhalten des Modells fest.\n","+ **User Message** stellt eine Anfrage oder einen Befehl.\n","+ **AI Message** gibt die generierte Antwort basierend auf den vorherigen Nachrichten.\n","\n","\n"]},{"cell_type":"markdown","id":"-JwJD0rLIyT8","metadata":{"id":"-JwJD0rLIyT8"},"source":["# 4 | Prompts\n","---"]},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/langchain_prozess_02.png)"],"metadata":{"id":"j9stNgKo93Eg"},"id":"j9stNgKo93Eg"},{"cell_type":"markdown","source":["\n","\n","Prompts sind die Eingaben bzw. Auftr√§ge an das LLM. LangChain bietet verschiedene Template-Systeme. Ein **Template** ist eine **Vorlage** ‚Äì genauer gesagt ein Text mit **Platzhaltern**, die sp√§ter durch echte Werte ersetzt werden.\n","\n","\n","Ein **PromptTemplate** ist eine Vorlage f√ºr einen Eingabetext, der an ein Sprachmodell geschickt wird.\n","Er enth√§lt Platzhalter `{placeholder}`, die sp√§ter mit Nutzereingaben oder Kontext bef√ºllt werden.\n","\n","**Templates machen die KI-Prompts:**\n","\n","+ Wiederverwendbar (z.‚ÄØB. f√ºr viele Themen oder Fragen)\n","\n","+ Strukturiert (z.‚ÄØB. mit Rollen und Anweisungen)\n","\n","+ Dynamisch erweiterbar (mit Memory, Tools, etc.)"],"metadata":{"id":"QMVO4fzPFZ88"},"id":"QMVO4fzPFZ88"},{"cell_type":"markdown","source":["## 4.1 | Modellbildung\n"],"metadata":{"id":"BMPwDNTCCOfx"},"id":"BMPwDNTCCOfx"},{"cell_type":"code","execution_count":null,"id":"e1926d03","metadata":{"id":"e1926d03"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n","from langchain.prompts import PromptTemplate, ChatPromptTemplate\n","from langchain.prompts.few_shot import FewShotPromptTemplate"]},{"cell_type":"code","source":["model_name = \"gpt-4o-mini\"\n","temperature = 0.0"],"metadata":{"id":"RG-y_bG8HOOi"},"id":"RG-y_bG8HOOi","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Standardwerte f√ºr den Baustein ChatOpenAI:**  \n","\n","| Parameter                                     | Kurzbeschreibung                                                                        |\n","| --------------------------------------------- | --------------------------------------------------------------------------------------- |\n","| **model_name: 'gpt-3.5-turbo'**               | Bezeichnet das zu verwendende Sprachmodell (z. B. GPT-Version)                          |\n","| **temperature: 0.7**                          | Steuert die Kreativit√§t der Antworten (niedriger = pr√§ziser, h√∂her = freier)            |\n","| **max_tokens: None**                          | Kein festes Token-Limit; Ausgabe kann beliebig lang sein (bis zum Modellmaximum)        |\n","| **max_retries: 6**                            | Anzahl automatischer Wiederholungsversuche bei API-Fehlern                              |\n","| **n: 1**                                      | Gibt an, wie viele Antwortvarianten pro Anfrage generiert werden                        |\n","| **streaming: False**                          | Wenn *True*, werden Tokens beim Generieren gestreamt (z. B. f√ºr Live-Ausgabe)           |\n","| **verbose: False**                            | Wenn *True*, werden zus√§tzliche Debug-Informationen in der Konsole ausgegeben           |\n","| **openai_api_key: env('OPENAI_API_KEY')**     | Zugriffsschl√ºssel wird aus der Umgebungsvariable **OPENAI_API_KEY** gelesen             |\n","\n","\n","\n"],"metadata":{"id":"Jj3MV7C5T6rx"},"id":"Jj3MV7C5T6rx"},{"cell_type":"code","source":["# Modell definieren\n","llm = ChatOpenAI(model=model_name, temperature=temperature)"],"metadata":{"id":"yd3MebxgGYgy"},"id":"yd3MebxgGYgy","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"HqTUeMW20084","metadata":{"id":"HqTUeMW20084"},"source":["## 4.2 | Simple Prompt (ohne Rollen)\n"]},{"cell_type":"markdown","source":["<p><font color='darkblue' size=\"4\">\n","<b>‚ùóHinweis:</b>\n","</font></p>\n","Ohne Memory-Management hat das Modell ein kurzes Ged√§chnis.\n"],"metadata":{"id":"5JF1_AvYNYY_"},"id":"5JF1_AvYNYY_"},{"cell_type":"markdown","source":["**Prompt wird als Zeichenkette √ºbergeben:**"],"metadata":{"id":"1azC75R5JpWC"},"id":"1azC75R5JpWC"},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(\"Was ist Generative KI?\")"],"metadata":{"id":"jPizca8FJV5b"},"id":"jPizca8FJV5b","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Antwort\n","type(response)"],"metadata":{"id":"EHjmjLgW3ENA"},"id":"EHjmjLgW3ENA","execution_count":null,"outputs":[]},{"cell_type":"code","source":["response"],"metadata":{"id":"4ZYngACo4RWN"},"id":"4ZYngACo4RWN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["for r in response:\n","    print(r)"],"metadata":{"id":"jveXqUjbpo0s"},"id":"jveXqUjbpo0s","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"cOJGeiLGJb-g"},"id":"cOJGeiLGJb-g","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(\"Was war meine letzte Frage?\")"],"metadata":{"id":"9fayTzIrNN2b"},"id":"9fayTzIrNN2b","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"IahQJ0mFNUUV"},"id":"IahQJ0mFNUUV","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Prompt wird als formatierte String-Variable √ºbergeben:**"],"metadata":{"id":"aKRhzB0oJnUG"},"id":"aKRhzB0oJnUG"},{"cell_type":"code","source":["# Pure Python L√∂sung f√ºr simple prompt mit template\n","thema = \"Machine Learning\"\n","prompt = f\"Erkl√§re {thema} in einfachen Worten.\""],"metadata":{"id":"8s5cQY2oHUvW"},"id":"8s5cQY2oHUvW","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(prompt)"],"metadata":{"id":"plYGfaoLJ8DI"},"execution_count":null,"outputs":[],"id":"plYGfaoLJ8DI"},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"SLX0zaONJ8DJ"},"execution_count":null,"outputs":[],"id":"SLX0zaONJ8DJ"},{"cell_type":"markdown","source":["**Prompt wird als template √ºbergeben:**"],"metadata":{"id":"n-VVZEHDKSIw"},"id":"n-VVZEHDKSIw"},{"cell_type":"code","source":["# Einfaches Template mit Platzhaltern\n","simple_prompt_template = PromptTemplate.from_template(\n","    \"Erkl√§re {thema} in einfachen Worten.\"\n",")"],"metadata":{"id":"koh0-_3r_HxG"},"id":"koh0-_3r_HxG","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prompt-Erstellung\n","thema = \"Generative KI\"\n","prompt = simple_prompt_template.format(thema=thema)\n","prompt"],"metadata":{"id":"nFrCjL6j_xo1"},"id":"nFrCjL6j_xo1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(prompt)"],"metadata":{"id":"-of5FbM02qfZ"},"id":"-of5FbM02qfZ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"lmEE-3NDAE15"},"id":"lmEE-3NDAE15","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Vorteil PromptTemplate vs f-string**"],"metadata":{"id":"VxlzHPsbJENM"},"id":"VxlzHPsbJENM"},{"cell_type":"markdown","source":["| Kriterium                        | `f-string`                | `PromptTemplate`                         |\n","| -------------------------------- | ------------------------- | ---------------------------------------- |\n","| üß± Basisfunktion                 | String zusammenbauen      | String-Vorlage mit Platzhaltern          |\n","| üîÅ Wiederverwendbarkeit          | manuell                   | sehr gut (Template + `.format()`)        |\n","| üîå Integration mit LangChain     | ‚ùå Nur als fertiger Text   | ‚úÖ Nahtlos (in Chains, mit LLMs, etc.)    |\n","| üì¶ Kombinierbar mit Tools        | ‚ùå Nein                    | ‚úÖ Ja (Memory, OutputParser, Agents)      |\n","| üß† Erweiterbar (z.‚ÄØB. Beispiele) | ‚ùå M√ºhsam                  | ‚úÖ z.‚ÄØB. `FewShotPromptTemplate`          |\n","| üïµÔ∏è Nachvollziehbarkeit          | schwer bei gro√üen Prompts | gut dokumentierbar & testbar             |\n","| üë• Rollensteuerung               | ‚ùå Nur manuell             | ‚úÖ mit `ChatPromptTemplate`               |\n","| üß™ Validierung                   | ‚ùå keine                   | ‚úÖ Platzhalterpr√ºfung (`input_variables`) |\n"],"metadata":{"id":"pWT8Q3AkJCZe"},"id":"pWT8Q3AkJCZe"},{"cell_type":"markdown","id":"8akKmCCc06nm","metadata":{"id":"8akKmCCc06nm"},"source":["## 4.3 | Chat Prompt (mit Rollen) üìå"]},{"cell_type":"markdown","source":["Das **ChatPromptTemplate** ist eine spezielle Prompt-Vorlage f√ºr Chatmodelle (z.‚ÄØB. GPT-3.5, GPT-4), die **mehrere Rollen und Nachrichten** unterst√ºtzt ‚Äì also genau das, was Chat-Modelle eigentlich brauchen.\n","\n","Es geh√∂rt zur LangChain-Bibliothek und baut auf dem Prinzip auf:\n","\n","\n","**Wer sagt was? ‚Üí system, user, assistant, etc.**"],"metadata":{"id":"SVoHwxtrMM_j"},"id":"SVoHwxtrMM_j"},{"cell_type":"code","execution_count":null,"id":"iW9URvON0IXr","metadata":{"id":"iW9URvON0IXr"},"outputs":[],"source":["# prompt-template als tuple\n","chat_template = ChatPromptTemplate([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"human\", \"Erkl√§re mir {thema}\"),\n","])"]},{"cell_type":"markdown","source":["**... oder ...**"],"metadata":{"id":"6FrHMy1PYH0S"},"id":"6FrHMy1PYH0S"},{"cell_type":"code","source":["# prompt-template als dict\n","chat_template = ChatPromptTemplate([\n","    {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher und humorvoller Assistent.\"},\n","    {\"role\": \"human\", \"content\": \"Erkl√§re mir {thema}\"},\n","])"],"metadata":{"id":"cGkROwH9YEyv"},"id":"cGkROwH9YEyv","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**... oder ...**"],"metadata":{"id":"qxuO76Z9YpDE"},"id":"qxuO76Z9YpDE"},{"cell_type":"code","source":["# Prompt-Template als Nachrichtenobjekte\n","chat_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"human\", \"Erkl√§re mir {thema}\")\n","])"],"metadata":{"id":"w3OFTq0XYlgl"},"id":"w3OFTq0XYlgl","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Thema definieren\n","thema = \"Machine Learning\"\n","\n","# Prompt formatieren - hier die richtige Methode verwenden\n","prompt = chat_template.format_messages(thema=thema)"],"metadata":{"id":"wEp6OiL4AhFa"},"id":"wEp6OiL4AhFa","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ausgabe der formatierten Nachrichten\n","for message in prompt:\n","    print(f\"Role: {message.type}, Content: {message.content}\")"],"metadata":{"id":"G6d2eTb2Aiv7"},"id":"G6d2eTb2Aiv7","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aufrufen der Kette mit Eingaben\n","response = llm.invoke(prompt)"],"metadata":{"id":"zYC8V9dzAkXR"},"id":"zYC8V9dzAkXR","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"6PtZzYUSC7-F"},"id":"6PtZzYUSC7-F","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Vorteile gegen√ºber `PromptTemplate`**"],"metadata":{"id":"I8EIgo8DMaYO"},"id":"I8EIgo8DMaYO"},{"cell_type":"markdown","source":["| Vorteil                      | `PromptTemplate`     | `ChatPromptTemplate`            |\n","| ---------------------------- | -------------------- | ------------------------------- |\n","| üöª Rollenstruktur            | ‚ùå Nur einfacher Text | ‚úÖ Klar: `system`, `user`, `ai`  |\n","| üìö Konversationsaufbau       | ‚ùå Nicht geeignet     | ‚úÖ Ideal f√ºr Multi-Turn-Dialoge  |\n","| üîÑ Kombinierbar mit `Memory` | ‚ùå Nein               | ‚úÖ Ja, via `MessagesPlaceholder` |\n","| üîå F√ºr Chatmodelle wie GPT   | ‚ö†Ô∏è Nur als Flie√ütext | ‚úÖ Nativ & strukturiert          |\n","| üîç Klarheit & Wartbarkeit    | ‚ö†Ô∏è Nur Inline-Text   | ‚úÖ Saubere Message-Trennung      |\n","| üß™ Prompt-Testbarkeit        | ‚ùå Eingeschr√§nkt      | ‚úÖ Besser strukturiert           |\n"],"metadata":{"id":"2uSD48umMXI7"},"id":"2uSD48umMXI7"},{"cell_type":"markdown","id":"Ps344Ejw0-ih","metadata":{"id":"Ps344Ejw0-ih"},"source":["## 4.4 | Few-Shot-Learning Prompt\n"]},{"cell_type":"markdown","source":["Few-Shot-Prompting ist eine Technik, bei der man einem KI-Modell einige Beispiele (sogenannte \"Shots\") vorlegt, bevor es eine neue Aufgabe l√∂sen soll. Das Modell kann aus diesen Beispielen das Muster erkennen und auf neue, √§hnliche F√§lle anwenden."],"metadata":{"id":"LKY3IaJSDLvt"},"id":"LKY3IaJSDLvt"},{"cell_type":"code","execution_count":null,"id":"cEopgOiS0KU_","metadata":{"id":"cEopgOiS0KU_"},"outputs":[],"source":["examples = [\n","    {\"frage\": \"Was ist Python?\", \"antwort\": \"Eine interpretierende Programmiersprache.\"},\n","    {\"frage\": \"Was ist Java?\", \"antwort\": \"Eine objektorientierte Sprache.\"}\n","]"]},{"cell_type":"code","source":["# Few-Shot-Learning als f-String\n","few_shot_prompt = f\"\"\"\n","Du bist ein Assistent, der Fragen zu Programmiersprachen beantwortet. Halte deine Antworten kurz und pr√§zise.\n","\n","Beispiel 1:\n","Frage: {examples[0]['frage']}\n","Antwort: {examples[0]['antwort']}\n","\n","Beispiel 2:\n","Frage: {examples[1]['frage']}\n","Antwort: {examples[1]['antwort']}\n","\n","Beantworte nun die folgende Frage im gleichen Stil:\n","Frage: {{frage}}\n","Antwort:\n","\"\"\""],"metadata":{"id":"0pn8fDgQgka-"},"id":"0pn8fDgQgka-","execution_count":null,"outputs":[]},{"cell_type":"code","source":["frage = \"Was ist Rust?\"\n","prompt = few_shot_prompt.format(frage=frage)"],"metadata":{"id":"dDq-RkD4iVex"},"id":"dDq-RkD4iVex","execution_count":null,"outputs":[]},{"cell_type":"code","source":["response = llm.invoke(prompt)"],"metadata":{"id":"2Evh3l-3rOHz"},"id":"2Evh3l-3rOHz","execution_count":null,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"P0OKaz-RrSg8"},"id":"P0OKaz-RrSg8","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**... oder ...**"],"metadata":{"id":"NIS2M0Dbhvun"},"id":"NIS2M0Dbhvun"},{"cell_type":"markdown","source":["`FewShotPromptTemplate` ist eine Prompt-Vorlage, die automatisch mehrere Beispiele einf√ºgt, bevor das eigentliche Nutzer-Input kommt."],"metadata":{"id":"NnBh2dz8M2fi"},"id":"NnBh2dz8M2fi"},{"cell_type":"code","source":["# Prompt f√ºr einzelne Beispiele\n","example_prompt = PromptTemplate.from_template(\n","    \"Frage: {frage}\\nAntwort: {antwort}\"\n",")"],"metadata":{"id":"fHEYrEPcDD9S"},"id":"fHEYrEPcDD9S","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Few-Shot-Prompt\n","few_shot_prompt = FewShotPromptTemplate(\n","    examples=examples,\n","    example_prompt=example_prompt,\n","    prefix=\"Du bist ein Assistent, der Fragen zu Programmiersprachen beantwortet. Halte deine Antworten kurz und pr√§zise:\",\n","    suffix=\"Frage: {frage}\\nAntwort:\",\n","    input_variables=[\"frage\"]\n",")\n","\n","prompt = few_shot_prompt.format(frage=\"Was ist Go?\")"],"metadata":{"id":"BpIbLhIhDcUp"},"id":"BpIbLhIhDcUp","execution_count":null,"outputs":[]},{"cell_type":"code","source":["response = llm.invoke(prompt)"],"metadata":{"id":"KZPoIO6gDo23"},"id":"KZPoIO6gDo23","execution_count":null,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"Yr0YleeiD_x7"},"id":"Yr0YleeiD_x7","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Vorteile eines `FewShotPromptTemplate`**"],"metadata":{"id":"XFbved9KM_WJ"},"id":"XFbved9KM_WJ"},{"cell_type":"markdown","source":["| Vorteil                      | Beschreibung                                                      |\n","| ---------------------------- | ----------------------------------------------------------------- |\n","| üß† Lerneffekt f√ºr das Modell | Das Modell erkennt aus Beispielen, wie es antworten soll          |\n","| üß∞ Flexibilit√§t              | Beispiele k√∂nnen dynamisch gesetzt oder generiert werden          |\n","| üß± Struktur & Konsistenz     | Einheitlicher Aufbau aller Beispiele √ºber `example_prompt`        |\n","| ü™Ñ Kein Training n√∂tig       | Kein Fine-Tuning ‚Äì nur durch Prompting                            |\n","| üõ† Kombinierbar              | Kann mit Templates, Tools, Chains, OutputParser kombiniert werden |\n"],"metadata":{"id":"EWjuAd2_M-E7"},"id":"EWjuAd2_M-E7"},{"cell_type":"markdown","id":"8c859b9f","metadata":{"id":"8c859b9f"},"source":["# 5 | Modelle\n","---"]},{"cell_type":"markdown","source":["LangChain ist mit zahlreichen LLMs ‚Äì sowohl eines als auch verschiedener Anbieter ‚Äì kompatibel und bietet vielseitige Konfigurationsoptionen f√ºr individuelle Anwendungsf√§lle."],"metadata":{"id":"5y6J-chfEOPH"},"id":"5y6J-chfEOPH"},{"cell_type":"markdown","source":["**Beispiel `gpt-4o-mini` und `o3-mini`**"],"metadata":{"id":"Oc0UPT5eTyTR"},"id":"Oc0UPT5eTyTR"},{"cell_type":"markdown","source":["+ gpt-4o-mini ist ein kompaktes, multimodales KI-Modell von OpenAI, das Text- und Bildeingaben verarbeiten kann und besonders f√ºr schnelle, allt√§gliche Aufgaben sowie kleinere Anwendungen optimiert wurde\n","\n","+ o3-mini ist ein spezialisiertes KI-Modell, das vor allem f√ºr komplexe Wissensverarbeitung, anspruchsvolle Denkaufgaben und professionelle Programmierprojekte entwickelt wurde"],"metadata":{"id":"f8yl7OApTvcQ"},"id":"f8yl7OApTvcQ"},{"cell_type":"markdown","source":["**Modellparameter:**"],"metadata":{"id":"fxdaHMICUDPT"},"id":"fxdaHMICUDPT"},{"cell_type":"markdown","source":["| **Parameter**       | **`gpt-4o`**     | **`gpt-4o-mini`** | **`o3-mini`**    | **Erkl√§rung**                                                               |\n","| ------------------- | ---------------- | ----------------- | ---------------- | --------------------------------------------------------------------------- |\n","| `model_name`        | ‚úÖ                | ‚úÖ                 | ‚úÖ                | Modellbezeichnung, z.‚ÄØB. `\"gpt-4o\"`                                         |\n","| `temperature`       | ‚úÖ                | ‚úÖ                 | ‚ùå                | Kreativit√§t/Zufall (0.0 = deterministisch, 1.0 = kreativ)                   |\n","| `top_p`             | ‚úÖ                | ‚úÖ                 | ‚ùå                | Nucleus Sampling (Alternative zu `temperature`)                             |\n","| `max_tokens`        | ‚úÖ                | ‚úÖ                 | ‚úÖ                | Maximale Anzahl an generierten Tokens                                       |\n","| `frequency_penalty` | ‚úÖ                | ‚úÖ                 | ‚ùå                | Bestraft Wiederholungen im Output                                           |\n","| `presence_penalty`  | ‚úÖ                | ‚úÖ                 | ‚ùå                | F√∂rdert neue Inhalte statt Wiederholungen                                   |\n","| `streaming`         | ‚úÖ                | ‚úÖ                 | ‚úÖ                | Ausgabe erfolgt tokenweise in Echtzeit (Streaming API)                      |\n","| `seed`              | ‚úÖ *(via kwargs)* | ‚úÖ *(via kwargs)*  | ‚úÖ *(via kwargs)* | Fixiert Zufallsverhalten f√ºr reproduzierbare Ergebnisse                     |\n","| `response_format`   | ‚úÖ *(via kwargs)* | ‚úÖ *(via kwargs)*  | ‚úÖ *(via kwargs)* | Gibt Ausgabeformat wie `\"text\"` oder `\"json\"` vor                           |\n","| `logit_bias`        | ‚úÖ *(via kwargs)* | ‚úÖ *(via kwargs)*  | ‚úÖ *(via kwargs)* | Gewichtung einzelner Tokens zur Output-Steuerung                            |\n","| `tool_choice`       | ‚úÖ *(via kwargs)* | ‚úÖ *(via kwargs)*  | ‚úÖ *(via kwargs)* | Auswahl eines bestimmten Tools beim Function Calling                        |\n","| `reasoning_effort`  | ‚ùå                | ‚ùå                 | ‚úÖ                | o3-spezifisch: `\"low\"`, `\"medium\"`, `\"high\"` zur Steuerung des Denkaufwands |\n","\n"],"metadata":{"id":"viwfwnIETILD"},"id":"viwfwnIETILD"},{"cell_type":"code","source":["# Modelle konfigurieren\n","\n","# Chat-Modell\n","chat_model = ChatOpenAI(\n","    model_name=\"gpt-4o-mini\",\n","    temperature=0.9\n",")\n","\n","# Reasoning-Modell\n","resoning_model = ChatOpenAI(\n","    model_name=\"o3-mini\",\n",")"],"metadata":{"id":"aDGqhlxoENvs"},"id":"aDGqhlxoENvs","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Gemeinsamer simple prompt\n","prompt = \"Was ist der Unterschied zwischen Prompting und Retrieval-Augmented-Generation?\""],"metadata":{"id":"PywoNdLrPCfO"},"id":"PywoNdLrPCfO","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aufruf\n","response = chat_model.invoke(prompt)"],"metadata":{"id":"iriw2JZPOazT"},"id":"iriw2JZPOazT","execution_count":null,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Chat-Model:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"9OlEy8jQPck1"},"id":"9OlEy8jQPck1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aufruf\n","response = resoning_model.invoke(prompt)"],"metadata":{"id":"_Znkwm2SPOrX"},"id":"_Znkwm2SPOrX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Reasoning-Model:\")\n","mprint(\"---\")\n","mprint(response.content)"],"metadata":{"id":"N0hcQkSfQyVK"},"id":"N0hcQkSfQyVK","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**üìä Vergleich: Chat-Modell vs. Reasoning-Modell**"],"metadata":{"id":"YJljcFw4Rz6W"},"id":"YJljcFw4Rz6W"},{"cell_type":"markdown","source":["| **Aspekt**                      | **Chat-Modell**                                                           | **Reasoning-Modell**                                                                                |\n","| ------------------------------- | ------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------- |\n","| **Stil**                        | Erkl√§rend, klassisch strukturiert mit Flie√ütext                           | Punktuell, technisch und systematisch                                                               |\n","| **Formatierung**                | Flie√ütext mit Abs√§tzen und Beispielen                                     | Aufz√§hlungen (‚Ä¢) und strukturierte Unterpunkte                                                      |\n","| **Definition Prompting**        | Kurze Definition als Text, danach Beispiel                                | Technisch klar: ‚Äûparametrisches Wissen‚Äú, Rolle der Prompt-Formulierung betont                       |\n","| **Definition RAG**              | Flie√ütext-Erkl√§rung mit Ablaufbeschreibung und Beispiel                   | Detaillierte Schritte: Abruf, Kontextintegration, Vorteile bzgl. Aktualit√§t und Pr√§zision erl√§utert |\n","| **Beispiele**                   | Ja ‚Äì einfache, praxisnahe Beispiele (z.‚ÄØB. Klimawandel, Quantencomputing) | Nein ‚Äì keine konkreten Beispiele, aber kontextbezogene Beschreibung                                 |\n","| **Vergleich Prompting vs. RAG** | Abstrakte Gegen√ºberstellung in einem abschlie√üenden Absatz                | Direkter Vergleich im letzten Abschnitt mit Betonung der Wissensquellen                             |\n","| **Technische Tiefe**            | Mittel ‚Äì f√ºr Einsteiger\\:innen gut verst√§ndlich                           | Hoch ‚Äì Begriffe wie ‚Äûparametrisches Wissen‚Äú, Fokus auf Architektur                                  |\n","| **Zielgruppe**                  | Allgemeines Publikum, Einstieg in das Thema                               | Fortgeschrittene Nutzer\\:innen, Technik-affine Leserschaft                                          |\n","| **St√§rken des Modells**         | Klare Erkl√§rung, gute didaktische Struktur                                | Pr√§zise Begriffsabgrenzung, hohe Informationsdichte                                                 |\n","\n"],"metadata":{"id":"OzPIlXnqR2Fq"},"id":"OzPIlXnqR2Fq"},{"cell_type":"markdown","source":["**üìä Vergleich: Chat-Modelle**"],"metadata":{"id":"GO40ayL6plxO"},"id":"GO40ayL6plxO"},{"cell_type":"markdown","source":["| Model              | Creator    | Open?  | API Name(s) (falls verf√ºgbar)                            | Context Window             |\n","| ------------------ | ---------- | ------ | -------------------------------------------------------- | -------------------------- |\n","| **GPT-5**          | OpenAI     | Closed | `gpt-5`, `gpt-5-mini`, `gpt-5-nano`, `gpt-5-chat-latest` | 400K Tokens (~296 Seiten)  |\n","| **GPT-4.1**        | OpenAI     | Closed | `gpt-4.1`, `gpt-4.1-mini`, `gpt-4.1-nano`                | 1M Tokens (~741 Seiten)    |\n","| **GPT-4o**         | OpenAI     | Closed | `gpt-4o`, `gpt-4o-mini`                                  | 128K Tokens (~95 Seiten)   |\n","| **GPT-4 Turbo**    | OpenAI     | Closed | `gpt-4-turbo`, `gpt-4-turbo-2024-04-09`                  | 128K Tokens (~95 Seiten)   |\n","| **gpt-oss-120b**   | OpenAI     | Open   | ‚Äî                                                        | 131K Tokens (~97 Seiten)   |\n","| **gpt-oss-20b**    | OpenAI     | Open   | ‚Äî                                                        | 131K Tokens (~97 Seiten)   |\n","| **Claude 4**       | Anthropic  | Closed | `claude-sonnet-4-20250514`, `claude-opus-4-20250514`     | 200K Tokens (~148 Seiten)  |\n","| **Gemini 2.5 Pro** | Google     | Closed | `gemini-2.5-pro`, `gemini-2.5-pro-exp-03-25`             | 1M Tokens (~741 Seiten)    |\n","| **Llama 3.1 405B** | Meta       | Open   | ‚Äî                                                        | 512K Tokens (~379 Seiten)  |\n","| **DeepSeek-V3**    | DeepSeek   | Open   | ‚Äî                                                        | 128K Tokens (~95 Seiten)   |\n","| **Mistral 7B**     | Mistral.AI | Open   | ‚Äî                                                        | 32K Tokens (~24 Seiten)    |\n","\n","<br>\n","\n","*Umrechnung Token/Seiten:  ~1.350 Tokens ‚âà 1 Seite deutschsprachiger Text*"],"metadata":{"id":"RpcsYzLApH7v"},"id":"RpcsYzLApH7v"},{"cell_type":"markdown","id":"c314ca71","metadata":{"id":"c314ca71"},"source":["# 6 | Chains\n","---"]},{"cell_type":"markdown","source":["Chains verbinden mehrere Komponenten zu einer Verarbeitungskette. LangChain Expression Language (LCEL) ist das **neue Programmiermodell in LangChain v0.3+**, das die Entwicklung von LLM-Anwendungen vereinfacht.\n"],"metadata":{"id":"cEEMxhSBHrXA"},"id":"cEEMxhSBHrXA"},{"cell_type":"markdown","source":["\n","**Hier die Kernpunkte:**\n","\n","1. Was ist LCEL?\n","```python\n","# LCEL nutzt den Pipe-Operator (|) f√ºr klare Verkettungen\n","chain = prompt | model | parser\n","```\n","- Eine deklarative Sprache zum Verketten von LangChain-Komponenten\n","- Erm√∂glicht linearen Datenfluss zwischen Komponenten\n","- Basiert auf dem Pipe-Operator (|) f√ºr intuitive Verbindungen\n","\n","2. Warum LCEL nutzen?\n","- Bessere Lesbarkeit des Codes\n","- Einfachere Wartung und Debugging\n","- Verbesserte Performance durch optimierte Ausf√ºhrung\n","- Bessere Typsicherheit und Fehlererkennung\n","- Unterst√ºtzt modernes Streaming und Async-Operationen\n","\n","3. Praktisches Beispiel:"],"metadata":{"id":"ltTQItwXH6w8"},"id":"ltTQItwXH6w8"},{"cell_type":"markdown","source":["## 6.1 | Simple Chain üìå\n"],"metadata":{"id":"BndZGbyWKs1C"},"id":"BndZGbyWKs1C"},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers.string import StrOutputParser\n","from langchain_openai import ChatOpenAI\n","\n","# 1. Prompt-Template\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"human\", \"{input}\")\n","])\n","\n","# 2. Modell\n","model_name = \"gpt-4o-mini\"\n","temperature = 0\n","\n","llm = ChatOpenAI(\n","    model_name=model_name,\n","    temperature=temperature\n",")\n","\n","# 3. Parser\n","parser = StrOutputParser()\n","\n","#  4. Einfache LCEL-Kette\n","chain = prompt | llm | parser\n","\n","# 5. Ausf√ºhrung\n","response = chain.invoke({\"input\": \"Erkl√§re LangChain Expression Language.\"})"],"metadata":{"id":"PsWtjWROIW9d"},"id":"PsWtjWROIW9d","execution_count":null,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response)"],"metadata":{"id":"VWYAG9aWJpZd"},"id":"VWYAG9aWJpZd","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","LCEL ist der empfohlene Weg f√ºr alle neuen LangChain-Projekte, da es die Entwicklung vereinfacht und zukunftssicher macht."],"metadata":{"id":"49Wu_XreIZY2"},"id":"49Wu_XreIZY2"},{"cell_type":"markdown","id":"Qy35G1-FvrIR","metadata":{"id":"Qy35G1-FvrIR"},"source":["## 6.2 | Sequential Chains\n"]},{"cell_type":"markdown","id":"AWhQwgMbxitn","metadata":{"id":"AWhQwgMbxitn"},"source":["Sequentielle Chain: Linear, ein Input wird schrittweise verarbeitet"]},{"cell_type":"code","execution_count":null,"id":"ceqbCIOCuqng","metadata":{"id":"ceqbCIOCuqng"},"outputs":[],"source":["# Prompts\n","zusammenfassen_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Fasse den folgenden Text pr√§gnant zusammen.\"),\n","    (\"human\", \"{text}\")\n","])\n","\n","uebersetzen_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"√úbersetze den folgenden Text ins Deutsche.\"),\n","    (\"human\", \"{text}\")\n","])"]},{"cell_type":"code","execution_count":null,"id":"kddZQ3rMv0Hw","metadata":{"id":"kddZQ3rMv0Hw"},"outputs":[],"source":["# Verarbeitungskette\n","\n","# Erste Chain: Zusammenfassung\n","summarization_chain = (\n","    zusammenfassen_template\n","    | chat_model\n","    | (lambda output: {\"text\": output})  # Notwendig, f√ºr String-> Dict, wird von translation_chain erwartet\n",")\n","\n","# Zweite Chain: Translation/√úbersetzung\n","translation_chain = (\n","    uebersetzen_template\n","    | chat_model\n",")\n","\n","# Sequential Chain: Zusammenfassen und dann √úbersetzen\n","sequential_chain = summarization_chain | translation_chain | parser"]},{"cell_type":"code","execution_count":null,"id":"ymYXf4pKwSee","metadata":{"id":"ymYXf4pKwSee"},"outputs":[],"source":["# Input Text\n","user_input = \"\"\"\n","Machine Learning is a specialized branch of artificial intelligence (AI) dedicated to the development of algorithms and systems that can automatically learn from data and past experiences. Instead of following rigid, pre-defined instructions, machine learning models identify patterns, adapt their behavior, and improve their performance over time based on the information they are exposed to. This ability enables them to make predictions, recognize complex relationships, and solve problems in dynamic environments. Machine learning techniques are widely applied across various fields, including healthcare, finance, transportation, and entertainment, driving innovations such as personalized recommendations, autonomous vehicles, and intelligent diagnostics.\n","\"\"\""]},{"cell_type":"code","source":["# Aufruf der Kette\n","result = sequential_chain.invoke({\"text\": user_input})"],"metadata":{"id":"WU8Bah_MNFSS"},"id":"WU8Bah_MNFSS","execution_count":null,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(result)"],"metadata":{"id":"bipXv_2DMdCN"},"id":"bipXv_2DMdCN","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"R23C1DI1wi1X","metadata":{"id":"R23C1DI1wi1X"},"source":["## 6.3 | Q&A-Chain\n"]},{"cell_type":"markdown","id":"-pshkn53xc7T","metadata":{"id":"-pshkn53xc7T"},"source":["QA Chain: Kontextbasiert, kombiniert mehrere Inputs f√ºr eine einzelne Verarbeitung"]},{"cell_type":"code","execution_count":null,"id":"3I8ucKu7wAoo","metadata":{"id":"3I8ucKu7wAoo"},"outputs":[],"source":["# Q&A Prompt\n","qa_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Beantworte die Frage basierend auf dem gegebenen Kontext.\"),\n","    (\"human\", \"\"\"\n","    Kontext: {context}\n","\n","    Frage: {question}\n","    \"\"\")\n","])"]},{"cell_type":"code","execution_count":null,"id":"9iqmLBYSwpiX","metadata":{"id":"9iqmLBYSwpiX"},"outputs":[],"source":["# Verkettung\n","qa_chain = qa_template | chat_model | parser"]},{"cell_type":"code","execution_count":null,"id":"R0lVOJaSw2a9","metadata":{"id":"R0lVOJaSw2a9"},"outputs":[],"source":["# Inhalte\n","context = \"\"\"\n","Python ist eine beliebte Programmiersprache f√ºr Machine Learning.\n","Sie bietet viele Bibliotheken wie TensorFlow und PyTorch.\n","\"\"\"\n","question = \"Welche ML-Bibliotheken gibt es f√ºr Python?\""]},{"cell_type":"code","source":["# Aufruf\n","result = qa_chain.invoke({\n","    \"context\": context,\n","    \"question\": question\n","})"],"metadata":{"id":"hM1ADYiFNZ9X"},"id":"hM1ADYiFNZ9X","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"bxFOmczGyPPN","metadata":{"id":"bxFOmczGyPPN"},"outputs":[],"source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(result)"]},{"cell_type":"markdown","source":["# 7 | OutputParser üìå\n","---"],"metadata":{"id":"LVqVL9CWRXU7"},"id":"LVqVL9CWRXU7"},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/langchain_prozess_03.png)"],"metadata":{"id":"erhTv-_o97q9"},"id":"erhTv-_o97q9"},{"cell_type":"markdown","source":["Ein OutputParser ist ein Konzept in LangChain, das hilft, die Antworten eines Modells richtig weiterzuverarbeiten. Es nimmt die **rohe Ausgabe** (zum Beispiel einen langen Text) und formt sie in ein **bestimmtes Format**, das sp√§ter in der Anwendung leichter benutzt werden kann."],"metadata":{"id":"EVBcCwJERfaj"},"id":"EVBcCwJERfaj"},{"cell_type":"code","source":["from langchain_core.output_parsers.string import StrOutputParser\n","\n","# Einfacher Prompt ohne explizite Formatvorgabe\n","prompt_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent. Formatiere Deine Antwort im json-Format mit den Infos zu Name und Alter.\"),\n","    (\"human\", \"{user_input}\")\n","])\n","\n","\n","model_name = \"gpt-4o-mini\"\n","temperature\t= 0\n","\n","# Modell\n","llm = ChatOpenAI(model=model_name, temperature=temperature)\n","\n","# Struktur der Antwort definieren\n","parser = StrOutputParser()\n","\n","# LCEL-Chain\n","chain = prompt | llm | parser"],"metadata":{"id":"QKtYSBLhVlts"},"execution_count":null,"outputs":[],"id":"QKtYSBLhVlts"},{"cell_type":"code","source":["# Ausf√ºhren\n","response = chain.invoke({\"input\": \"Bitte gibt den Namen einer Person und das Alter\"})"],"metadata":{"id":"mxndm0mLw4d1"},"id":"mxndm0mLw4d1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ausgabe\n","mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response)"],"metadata":{"id":"BGceDe7Kw6ot"},"id":"BGceDe7Kw6ot","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 8 | Runnables\n","---\n"],"metadata":{"id":"KRqrnpMx3Hwf"},"id":"KRqrnpMx3Hwf"},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/langchain_prozess_01.png)"],"metadata":{"id":"LFOITVoEzpkm"},"id":"LFOITVoEzpkm"},{"cell_type":"markdown","source":["Ein **Runnable** ist ein elementarer Baustein in LangChain. Er nimmt eine Eingabe und liefert eine Ausgabe - wie eine Funktion mit klaren Regeln. Man kann sich Konzepte wie Prompts, LLMs oder Parser als Runnables vorstellen. Runnable kann man als **Oberbegriff** f√ºr Prompts, LLMs, Parser und √§hnliche Komponenten verwenden.\n","\n","\n","\n","In der Grafik sieht man die **drei wichtigsten Runnables**: Prompt-Template, LLM und Parser, die zusammen eine Kette bilden.\n","\n","In LangChain sind die wichtigsten Komponenten als Runnables verf√ºgbar:\n","\n","+ Prompts/Templates (der erste Baustein in der Kette)\n","+ LLMs (das Herzst√ºck in der Mitte)\n","+ Output-Parser (verwandelt die LLM-Antwort in ein nutzbares Format)\n","+ Komplette Chains (die ganze Verarbeitungskette)\n","+ Tools (f√ºr spezielle Aufgaben)\n","+ Retriever (holen zus√§tzliche Informationen)\n","\n","\n","Vorteile von Runnables:\n","\n","+ Gleiche Bedienung: Alle Runnables (Prompt, LLM, Parser) funktionieren nach demselben Prinzip\n","+ Einfaches Verbinden: Man kann sie unkompliziert zu einer Kette zusammenf√ºgen - genau wie im Bild gezeigt\n","+ Anpassungsf√§hig: Sie funktionieren sowohl einzeln als auch in der Gruppe\n","Schrittweise Ausgabe: Ergebnisse k√∂nnen st√ºckweise weitergegeben werden"],"metadata":{"id":"DnUDR_983M4l"},"id":"DnUDR_983M4l"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Benutzerdefinierte Runnables erstellen\n","</font></p>"],"metadata":{"id":"WWJ7U1iJ6b8Q"},"id":"WWJ7U1iJ6b8Q"},{"cell_type":"markdown","source":["Man kann auch eigene Runnables erstellen, indem man eine Klasse definiert, die das Runnable-Interface implementiert oder eine Funktion mit `RunnableLambda` umsetzt:\n"],"metadata":{"id":"PAvbE_TR6g87"},"id":"PAvbE_TR6g87"},{"cell_type":"code","source":["from langchain_core.runnables import RunnableLambda\n","from langchain_core.output_parsers.string import StrOutputParser\n","\n","# 1. Mit RunnableLambda eine einfache Transformation erstellen\n","def text_verdoppeln(eingabe):\n","    return f\"{eingabe} <br> <br>  {eingabe}\"\n","\n","verdoppler = RunnableLambda(text_verdoppeln)\n","\n","# Verwendung des eigenen Runnables\n","verdoppler.invoke(\"Dies ist ein Text.\")"],"metadata":{"id":"_0wEf_hL6peM"},"id":"_0wEf_hL6peM","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Einfacher Prompt ohne explizite Formatvorgabe\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent. Antworte kurz.\"),\n","    (\"human\", \"Eingabe: {input}\")\n","])\n","\n","# In eine Kette einbauen - Korrektur der Pipe-Operatoren\n","chain_mit_verdoppler = prompt | llm | parser | verdoppler\n","\n","# Test der Kette\n","response = chain_mit_verdoppler.invoke({\"input\": \"Wer war Einstein?\"})"],"metadata":{"id":"NvZdOf1961xb"},"id":"NvZdOf1961xb","execution_count":null,"outputs":[]},{"cell_type":"code","source":["mprint(\"## üì£ Model response:\")\n","mprint(\"---\")\n","mprint(response)"],"metadata":{"id":"_elA0imx2HZA"},"id":"_elA0imx2HZA","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Runnable-Methoden\n","</font></p>\n","\n","Jedes Runnable unterst√ºtzt standardm√§√üig folgende Methoden:\n","\n","1. **invoke()**: F√ºr einzelne, synchrone Anfragen\n","2. **batch()**: Verarbeitet mehrere Eingaben parallel\n","3. **stream()**: Gibt Teilergebnisse zur√ºck, sobald sie verf√ºgbar sind\n","4. **ainvoke()**: Asynchrone Version von invoke\n","5. **abatch()**: Asynchrone Version von batch\n","6. **astream()**: Asynchrone Version von stream\n","\n"],"metadata":{"id":"CjFvHqWA3S2V"},"id":"CjFvHqWA3S2V"},{"cell_type":"markdown","source":["Hier ein einfaches Beispiel mit den verschiedenen Aufrufmethoden:\n"],"metadata":{"id":"a9zHZAMP3bCL"},"id":"a9zHZAMP3bCL"},{"cell_type":"code","source":["from langchain.prompts import ChatPromptTemplate\n","from langchain_openai import ChatOpenAI\n","from langchain.schema import StrOutputParser\n","\n","# Einfaches Template erstellen\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"human\", \"{input}\")\n","])\n","\n","# Runnable-Kette erstellen\n","chain = prompt | llm | parser"],"metadata":{"id":"kQTKHK2D4DBb"},"id":"kQTKHK2D4DBb","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. invoke - Einzelne Anfrage\n","response = chain.invoke({\"input\": \"Was ist ein Runnable in LangChain?\"})\n","\n","mprint(\"## üì£ Invoke response:\")\n","mprint(\"---\")\n","mprint(response)"],"metadata":{"id":"TVWtHSYw5Jla"},"id":"TVWtHSYw5Jla","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. batch - Mehrere Anfragen parallel\n","response = chain.batch([\n","    {\"input\": \"Erkl√§re den Begriff LCEL kurz.\"},\n","    {\"input\": \"Was sind die Hauptvorteile von Runnables?\"}\n","])\n","mprint(\"##  üì£ Batch response::\")\n","mprint(\"---\")\n","for i, ergebnis in enumerate(response, 1):\n","    mprint(f\"Anfrage {i}: {response[:50]}...\\n\")"],"metadata":{"id":"UAet4KSq5K2D"},"id":"UAet4KSq5K2D","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. stream - Schrittweise Ausgabe\n","mprint(\"##  üì£ Stream response:\")\n","mprint(\"---\")\n","for chunk in chain.stream({\"input\": \"Erkl√§re die Abk√ºrzung LCEL kurz.\"}):\n","    print(chunk, end=\" ‚û° \")  # Zeichen trennt die via stream gelieferten Ergebnisse/Ausgaben\n","print(\"\\n\")"],"metadata":{"id":"VJn4e_IX5L-b"},"id":"VJn4e_IX5L-b","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"F_5gn03b46rC","metadata":{"id":"F_5gn03b46rC"},"source":["# A | Aufgabe\n","---"]},{"cell_type":"markdown","id":"36LyGMmo5jb3","metadata":{"id":"36LyGMmo5jb3"},"source":["Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen."]},{"cell_type":"markdown","id":"Je-szbGU7IJ-","metadata":{"id":"Je-szbGU7IJ-"},"source":["\n","<p><font color='black' size=\"5\">\n","Erstellen einer LangChain-Kette zur Textanalyse\n","</font></p>"]},{"cell_type":"markdown","id":"kevePvA-7PzW","metadata":{"id":"kevePvA-7PzW"},"source":["Entwickeln Sie eine Verarbeitungskette, die einen Text analysiert und verschiedene Informationen dar√ºber extrahiert.\n","\n","**Schritte:**\n","1. Erstelle ein ChatPromptTemplate, das ein LLM anweist, einen Text zu analysieren\n","2. Die Analyse soll folgende Aspekte umfassen:\n","   - Hauptthema des Textes\n","   - Tonalit√§t (formal, informell, etc.)\n","   - Schl√ºsselw√∂rter (5-10)\n","   - Kurze Zusammenfassung (max. 3 S√§tze)\n","3. Formatiere die Ausgabe strukturiert mit Markdown-√úberschriften\n","4. Teste die Kette mit mindestens zwei verschiedenen Texten\n","\n","\n"]},{"cell_type":"markdown","id":"6oBvtYws7qpB","metadata":{"id":"6oBvtYws7qpB"},"source":["<p><font color='black' size=\"5\">\n","Few-Shot-Learning f√ºr Textklassifikation\n","</font></p>"]},{"cell_type":"markdown","id":"gI7alQcE79ER","metadata":{"id":"gI7alQcE79ER"},"source":["Erstellen Sie ein System, das mithilfe von Few-Shot-Learning Texte in vorgegebene Kategorien klassifiziert.\n","\n","**Schritte:**\n","1. Definiere 3-5 Kategorien f√ºr die Klassifikation (z.B. Sport, Politik, Technologie, Kultur)\n","2. Erstelle einen FewShotPromptTemplate mit Beispielen f√ºr jede Kategorie\n","3. Entwickle eine Chain, die neue Texte klassifiziert\n","4. Implementiere eine Funktion, die neben der Kategorie auch eine Begr√ºndung f√ºr die Einordnung liefert\n","5. Teste das System mit verschiedenen Texten, die nicht in den Beispielen vorkommen\n"]},{"cell_type":"markdown","id":"l35x2g1e8fa9","metadata":{"id":"l35x2g1e8fa9"},"source":["<p><font color='black' size=\"5\">\n","Q&A-System mit Sequential Chain\n","</font></p>"]},{"cell_type":"markdown","id":"qHFRGWmP8gga","metadata":{"id":"qHFRGWmP8gga"},"source":["Entwickeln Sie ein System, das Fragen zu einem gegebenen Kontext beantwortet, aber zuerst den Kontext zusammenfasst und dann die Frage beantwortet.\n","\n","\n","**Schritte:**\n","\n","+ Erstellen Sie zwei Templates:\n","    + Ein Template zur Zusammenfassung des Kontexts\n","    + Ein Template zur Beantwortung einer Frage basierend auf der Zusammenfassung\n","+ Verbinden Sie die Templates in einer sequentiellen Kette\n","+ Implementieren Sie einen Mechanismus, der die Zusammenfassung und die Antwort getrennt zur√ºckgibt\n","+ Nutzen Sie LCEL (LangChain Expression Language) f√ºr die Verkettung\n","+ Testen Sie das System mit verschiedenen Kontexten und Fragen"]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["91ee89a4-477b-41ce-a511-aadd4c2fd547","fGV8FwGmIwmR","EouwC_wdZb-c","-JwJD0rLIyT8","BMPwDNTCCOfx","HqTUeMW20084","8akKmCCc06nm","Ps344Ejw0-ih","8c859b9f","c314ca71","BndZGbyWKs1C","Qy35G1-FvrIR","R23C1DI1wi1X","LVqVL9CWRXU7","KRqrnpMx3Hwf","F_5gn03b46rC"],"toc_visible":true},"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":5}
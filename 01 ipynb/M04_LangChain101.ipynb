{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37rMyCQz2JNw",
   "metadata": {
    "id": "37rMyCQz2JNw"
   },
   "source": [
    "<p><font size=\"7\" color='grey'> <b>\n",
    "Anwendung Generativer KI\n",
    "</b></font> </br></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R5CfUEMJdvFQ",
   "metadata": {
    "id": "R5CfUEMJdvFQ"
   },
   "source": [
    "<p><font size=\"6\" color='grey'> <b>\n",
    "LangChain 101\n",
    "</b></font> </br></p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdhPIzcEYRG",
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "dfdhPIzcEYRG",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "#@markdown   <p><font size=\"4\" color='green'>  Colab-Umfeld</font> </br></p>\n",
    "# Installierte Python Version\n",
    "import sys\n",
    "print(f\"Python Version: \",sys.version)\n",
    "# Installierte LangChain Bibliotheken\n",
    "print()\n",
    "print(\"Installierte LangChain Bibliotheken:\")\n",
    "!pip list | grep '^langchain'\n",
    "# Unterdr√ºckt die \"DeprecationWarning\" von LangChain f√ºr die Memory-Funktionden\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WD3Wwr6sESX8",
   "metadata": {
    "cellView": "form",
    "id": "WD3Wwr6sESX8"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "#@markdown   <p><font size=\"4\" color='green'>  SetUp API-Keys (setup_api_keys)</font> </br></p>\n",
    "def setup_api_keys():\n",
    "    \"\"\"Konfiguriert alle ben√∂tigten API-Keys aus Google Colab userdata\"\"\"\n",
    "    from google.colab import userdata\n",
    "    import os\n",
    "    from os import environ\n",
    "\n",
    "    # Dictionary der ben√∂tigten API-Keys\n",
    "    keys = {\n",
    "        'OPENAI_API_KEY': 'OPENAI_API_KEY',\n",
    "        'HF_TOKEN': 'HF_TOKEN',\n",
    "        # Weitere Keys bei Bedarf\n",
    "    }\n",
    "\n",
    "    # Keys in Umgebungsvariablen setzen\n",
    "    for env_var, key_name in keys.items():\n",
    "        environ[env_var] = userdata.get(key_name)\n",
    "\n",
    "    return {k: environ[k] for k in keys.keys()}\n",
    "\n",
    "# Verwendung\n",
    "all_keys = setup_api_keys()\n",
    "# Bei Bedarf einzelne Keys direkt zugreifen\n",
    "# WEATHER_API_KEY = all_keys['WEATHER_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PHDCKRbtgVXy",
   "metadata": {
    "id": "PHDCKRbtgVXy"
   },
   "source": [
    "# **1 | Was ist LangChain?**\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ee89a4-477b-41ce-a511-aadd4c2fd547",
   "metadata": {},
   "source": [
    "# 1 | Was ist LangChain?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5004f8c",
   "metadata": {
    "id": "b5004f8c"
   },
   "source": [
    "LangChain ist ein Framework zur **Entwicklung von Anwendungen mit gro√üen Sprachmodellen** (LLMs). Es vereinfacht die Integration von LLMs in eigene Anwendungen durch:\n",
    "\n",
    "- Modulare Komponenten f√ºr verschiedene Aufgaben\n",
    "- Vorgefertigte Ketten (Chains) f√ºr komplexe Workflows\n",
    "- Einfache Integration externer Datenquellen\n",
    "- Werkzeuge f√ºr das Speichern von Konversationskontexten\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tl73bT-kKkia",
   "metadata": {
    "id": "tl73bT-kKkia"
   },
   "source": [
    "[Einf√ºhrung](https://python.langchain.com/docs/introduction/)   \n",
    "[Konzepte](https://python.langchain.com/docs/concepts/)   \n",
    "[API-References](https://python.langchain.com/api_reference/index.html)   \n",
    "[Integrations](https://python.langchain.com/docs/integrations/providers/)\n",
    "\n",
    "---\n",
    "\n",
    "[Tutorials](https://python.langchain.com/docs/tutorials/)   \n",
    "[How-to-Guides](https://python.langchain.com/docs/how_to/)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JrgQ46eLphEv",
   "metadata": {
    "id": "JrgQ46eLphEv"
   },
   "outputs": [],
   "source": [
    "!uv pip install --system -q langchain_community langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GpYUCQJUpqbe",
   "metadata": {
    "id": "GpYUCQJUpqbe"
   },
   "outputs": [],
   "source": [
    "from os import environ\n",
    "from google.colab import userdata\n",
    "environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fGV8FwGmIwmR",
   "metadata": {
    "id": "fGV8FwGmIwmR"
   },
   "source": [
    "# **2 | Kernkonzepte**\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EouwC_wdZb-c",
   "metadata": {
    "id": "EouwC_wdZb-c"
   },
   "source": [
    "## 2.1 Message-Typen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yS2mM2J1aXlY",
   "metadata": {
    "id": "yS2mM2J1aXlY"
   },
   "source": [
    "Beim Einsatz von Large Language Models (LLMs) wie GPT gibt es drei Hauptarten von Nachrichten (Messages), die die Interaktion mit dem Modell steuern. Diese Messages definieren, wie das Modell reagiert, welche Eingaben es erh√§lt und welche Ausgaben es generiert.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sGqRXptbbQ2a",
   "metadata": {
    "id": "sGqRXptbbQ2a"
   },
   "source": [
    "**System Message:**   \n",
    "Die `System Message` dient dazu, das Verhalten des LLMs zu steuern. Sie ist eine nicht sichtbare Nachricht, die dem Modell Anweisungen gibt, wie es sich verhalten soll. Dabei kann sie die Pers√∂nlichkeit, den Tonfall und die Einschr√§nkungen des Modells beeinflussen. Die Systemnachricht funktioniert wie eine Art \"Rollenspiel-Anweisung\", um das Modell in eine gew√ºnschte Rolle zu versetzen.\n",
    "\n",
    "*Merkmale:*\n",
    "- Definiert, wie das Modell reagieren soll\n",
    "- Legt Pers√∂nlichkeit, Verhalten und Einschr√§nkungen fest\n",
    "- Wird zu Beginn einer Sitzung gesetzt und bleibt bestehen\n",
    "- Nicht sichtbar f√ºr den Benutzer\n",
    "\n",
    "*Beispiel einer System Message:*\n",
    "```json\n",
    "{\n",
    "  \"role\": \"system\",\n",
    "  \"content\": \"You are an AI assistant that provides concise and informative answers in a professional tone.\"\n",
    "}\n",
    "```\n",
    "Dieses Beispiel weist das Modell an, in einem professionellen Tonfall kurze und informative Antworten zu geben.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upsIzpcqbbP7",
   "metadata": {
    "id": "upsIzpcqbbP7"
   },
   "source": [
    "**User Message**   \n",
    "Die `User Message` ist die eigentliche Eingabe des Benutzers. Sie kann eine Frage, eine Aufforderung oder ein Befehl sein. Das Modell nutzt diese Nachricht als Ausgangspunkt f√ºr die Generierung einer Antwort.\n",
    "\n",
    "*Merkmale:*\n",
    "- Direkte Eingabe des Benutzers\n",
    "- Kann eine Frage, Anfrage oder einen Befehl enthalten\n",
    "- Basis f√ºr die Antwort des Modells\n",
    "\n",
    "*Beispiel einer User Message:*\n",
    "```json\n",
    "{\n",
    "  \"role\": \"user\",\n",
    "  \"content\": \"What are the key differences between AI and machine learning?\"\n",
    "}\n",
    "```\n",
    "Hier fragt der Benutzer nach den Unterschieden zwischen KI und maschinellem Lernen.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CozrrtYXbhZb",
   "metadata": {
    "id": "CozrrtYXbhZb"
   },
   "source": [
    "**AI Message**   \n",
    "Die `AI Message` ist die Antwort, die das Modell generiert. Sie basiert auf der User Message und den Anweisungen aus der System Message. Die AI Message kann verschiedene Eigenschaften haben, ist aber haupts√§chlich auf den Inhalt fokussiert.\n",
    "\n",
    "*Merkmale:*\n",
    "- Antwort des Modells auf die Benutzeranfrage\n",
    "- Kann verschiedene Eigenschaften haben (z. B. L√§nge, Stil)\n",
    "- Haupts√§chlich inhaltlich relevant\n",
    "\n",
    "*Beispiel einer AI Message:*\n",
    "```json\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"content\": \"Artificial Intelligence (AI) is a broad field that includes various subfields, one of which is Machine Learning (ML). ML focuses on creating models that learn from data and make predictions.\"\n",
    "}\n",
    "```\n",
    "Hier gibt das Modell eine inhaltliche Antwort auf die Frage des Benutzers.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PiHnhKHVbofP",
   "metadata": {
    "id": "PiHnhKHVbofP"
   },
   "source": [
    "**Zusammenhang**   \n",
    "Die drei Message-Typen arbeiten zusammen, um eine sinnvolle Interaktion mit dem Modell zu erm√∂glichen:\n",
    "\n",
    "+  **System Message** legt die Regeln und das Verhalten des Modells fest.\n",
    "+ **User Message** stellt eine Anfrage oder einen Befehl.\n",
    "+ **AI Message** gibt die generierte Antwort basierend auf den vorherigen Nachrichten.\n",
    "\n",
    "**Beispiel f√ºr eine vollst√§ndige Konversation:**    \n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a helpful AI assistant that provides educational answers in a friendly tone.\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Can you explain what Large Language Models are?\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"Sure! Large Language Models (LLMs) are advanced AI systems trained on vast amounts of text data. They can understand and generate human-like text, making them useful for various applications like chatbots, content creation, and more.\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "In diesem Beispiel wurde das Modell so eingestellt, dass es in einem freundlichen Tonfall antwortet, dann erh√§lt es eine Benutzeranfrage und gibt eine passende Antwort.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L_RH5TC7yqOR",
   "metadata": {
    "id": "L_RH5TC7yqOR"
   },
   "outputs": [],
   "source": [
    "!uv pip install --system -q langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K2lC_JK3guaj",
   "metadata": {
    "id": "K2lC_JK3guaj"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "MODEL = 'gpt-4o-mini'\n",
    "TEMPERATURE = 0.0\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"Du bist ein kompetenter und hilfreicher Assistent.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Nenne mir die Hauptstadt von Usbekistan?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "# Initialisieren Sie das OpenAI LLM mit Ihrem API-Key\n",
    "llm = ChatOpenAI(\n",
    "  model=MODEL,\n",
    "  temperature= TEMPERATURE,\n",
    ")\n",
    "\n",
    "display(Markdown(\"## üì£ Model response:\"))\n",
    "display(Markdown(\"---\"))\n",
    "output = llm.invoke(messages)\n",
    "display(Markdown(output.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2vJuMVHZ1jJ8",
   "metadata": {
    "id": "2vJuMVHZ1jJ8"
   },
   "outputs": [],
   "source": [
    "messages.append(output)\n",
    "messages.append(HumanMessage(content=\"Bis Du sicher? Ich glaube, sie wurde k√ºrzlich umbenannt.\"))\n",
    "\n",
    "for message in messages:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A8Z7DZRH5TFr",
   "metadata": {
    "id": "A8Z7DZRH5TFr"
   },
   "source": [
    "Wir k√∂nnen das Konversationsarray an das Modell √ºbermitteln und seine neueste Antwort anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9qFxkEmAz1w1",
   "metadata": {
    "id": "9qFxkEmAz1w1"
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"## üì£ Model response:\"))\n",
    "display(Markdown(\"---\"))\n",
    "output = llm.invoke(messages)\n",
    "display(Markdown(output.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Iw3Ovw5mcFcY",
   "metadata": {
    "id": "Iw3Ovw5mcFcY"
   },
   "source": [
    "**Fazit**    \n",
    "Die System-, User- und AI-Message sind essenzielle Bestandteile jeder Interaktion mit einem LLM. W√§hrend die System Message das Verhalten vorgibt, stellt die User Message die eigentliche Anfrage dar, auf die das Modell mit einer AI Message antwortet. Dieses Konzept ist zentral f√ºr den Einsatz von LLMs in Chatbots, KI-gest√ºtzten Assistenten und anderen interaktiven Anwendungen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-JwJD0rLIyT8",
   "metadata": {
    "id": "-JwJD0rLIyT8"
   },
   "source": [
    "## 2.2 Prompts\n",
    "\n",
    "Prompts sind die Eingaben bzw. Auftr√§ge an das LLM. LangChain bietet verschiedene Template-Systeme:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HqTUeMW20084",
   "metadata": {
    "id": "HqTUeMW20084"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Simple Prompt\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1926d03",
   "metadata": {
    "id": "e1926d03"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "# 1. Einfaches Template mit Platzhaltern\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"thema\"],\n",
    "    template=\"Erkl√§re {thema} in einfachen Worten.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C92-v-s70Q3k",
   "metadata": {
    "id": "C92-v-s70Q3k"
   },
   "outputs": [],
   "source": [
    "print(template.format(thema=\"Machine Learning\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_o2EkUkN3hEX",
   "metadata": {
    "id": "_o2EkUkN3hEX"
   },
   "source": [
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Eingabeaufforderungsvorlagen\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gPvElGbrdOHq",
   "metadata": {
    "id": "gPvElGbrdOHq"
   },
   "source": [
    "\n",
    "\n",
    "Mit LangChain k√∂nnen Sie Operationsketten erstellen, die normalerweise als Teil einer LLM-f√§higen Anwendung ausgef√ºhrt werden. Eine dieser Operationen ist eine Eingabeaufforderungsvorlage, mit der Sie Text in eine zuvor erstellte Eingabeaufforderung einf√ºgen k√∂nnen. In diesem Beispiel erstellen wir eine Eingabeaufforderungsvorlage, die das Modell auffordert, einen zuf√§lligen Titel f√ºr einen Blogbeitrag zu erstellen.\n",
    "\n",
    "```\n",
    "Gibt nur den Titel eines Blog-Beitragsartikels zum Thema {topic} in {language} zur√ºck.\n",
    "```\n",
    "\n",
    "Um dieses Ziel zu erreichen, verwenden wir ein **PromptTemplate**-Objekt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L-g3zOXBmulc",
   "metadata": {
    "id": "L-g3zOXBmulc"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "topic = \"Haustiere f√ºr Datenwissenschaftler\"\n",
    "language = \"german\"\n",
    "\n",
    "# H√∂here Temperaturen f√ºr mehr Kreativit√§t\n",
    "llm = ChatOpenAI(model=MODEL, temperature=0.7)\n",
    "\n",
    "# Definieren der Eingabeaufforderungsvorlage\n",
    "title_template = PromptTemplate(\n",
    "    input_variables=['topic', 'language'],\n",
    "    template='Gibt nur den Titel eines Blog-Beitragsartikels zum Thema {topic} in {language} zur√ºck.'\n",
    ")\n",
    "\n",
    "# Verkettung\n",
    "title_chain = title_template | llm\n",
    "\n",
    "# Aufrufen der Kette mit Eingaben\n",
    "response = title_chain.invoke({'topic': topic, 'language': language})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8akKmCCc06nm",
   "metadata": {
    "id": "8akKmCCc06nm"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Chat Prompt\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iW9URvON0IXr",
   "metadata": {
    "id": "iW9URvON0IXr"
   },
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein hilfreicher Assistent.\"),\n",
    "    (\"human\", \"Erkl√§re mir {thema}\"),\n",
    "    (\"assistant\", \"Ich erkl√§re dir {thema} gerne.\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jwrybdk_0agA",
   "metadata": {
    "id": "jwrybdk_0agA"
   },
   "outputs": [],
   "source": [
    "print(chat_template.format(thema=\"Machine Learning\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ps344Ejw0-ih",
   "metadata": {
    "id": "Ps344Ejw0-ih"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Few-Shot-Learning Prompt\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cEopgOiS0KU_",
   "metadata": {
    "id": "cEopgOiS0KU_"
   },
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"frage\": \"Was ist Python?\", \"antwort\": \"Eine Programmiersprache.\"},\n",
    "    {\"frage\": \"Was ist Java?\", \"antwort\": \"Eine objektorientierte Sprache.\"}\n",
    "]\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=PromptTemplate(\n",
    "        input_variables=[\"frage\", \"antwort\"],\n",
    "        template=\"Frage: {frage}\\nAntwort: {antwort}\"\n",
    "    ),\n",
    "    prefix=\"Beantworte die Frage basierend auf den Beispielen:\",\n",
    "    suffix=\"Frage: {input}\\nAntwort:\",\n",
    "    input_variables=[\"input\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OdMhepTp0fJU",
   "metadata": {
    "id": "OdMhepTp0fJU"
   },
   "outputs": [],
   "source": [
    "print(few_shot_prompt.format(input=\"Was ist Cobol?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c859b9f",
   "metadata": {
    "id": "8c859b9f"
   },
   "source": [
    "## 2.3 Modelle\n",
    "\n",
    "LangChain unterst√ºtzt verschiedene LLMs und bietet flexible Konfigurationsm√∂glichkeiten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e20e1cb",
   "metadata": {
    "id": "9e20e1cb"
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Text-Completion-Modell mit Parametern\n",
    "llm = OpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-instruct\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "# Chat-Modell f√ºr Konversationen\n",
    "chat_model = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c314ca71",
   "metadata": {
    "id": "c314ca71"
   },
   "source": [
    "## 2.4 LCEL aka Chains\n",
    "\n",
    "Chains verbinden mehrere Komponenten zu einer Verarbeitungskette:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y8RgQ-PZ7dii",
   "metadata": {
    "id": "y8RgQ-PZ7dii"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "LangChain Expression Language (LCEL)\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T6dXYZWP7pyy",
   "metadata": {
    "id": "T6dXYZWP7pyy"
   },
   "source": [
    "LangChain Expression Language (LCEL) ist das **neue Programmiermodell in LangChain v0.3+**, das die Entwicklung von LLM-Anwendungen vereinfacht.\n",
    "\n",
    "Hier die Kernpunkte:\n",
    "\n",
    "1. Was ist LCEL?\n",
    "```python\n",
    "# LCEL nutzt den Pipe-Operator (|) f√ºr klare Verkettungen\n",
    "chain = prompt | model | output_parser\n",
    "```\n",
    "- Eine deklarative Sprache zum Verketten von LangChain-Komponenten\n",
    "- Erm√∂glicht linearen Datenfluss zwischen Komponenten\n",
    "- Basiert auf dem Pipe-Operator (|) f√ºr intuitive Verbindungen\n",
    "\n",
    "2. Warum LCEL nutzen?\n",
    "- Bessere Lesbarkeit des Codes\n",
    "- Einfachere Wartung und Debugging\n",
    "- Verbesserte Performance durch optimierte Ausf√ºhrung\n",
    "- Bessere Typsicherheit und Fehlererkennung\n",
    "- Unterst√ºtzt modernes Streaming und Async-Operationen\n",
    "\n",
    "3. Praktisches Beispiel:\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "# Einfache LCEL-Kette\n",
    "chain = (\n",
    "    ChatPromptTemplate.from_messages([(\"human\", \"{input}\")])\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# Ausf√ºhrung\n",
    "result = chain.invoke({\"input\": \"Erkl√§re LCEL\"})\n",
    "```\n",
    "\n",
    "LCEL ist der empfohlene Weg f√ºr alle neuen LangChain-Projekte, da es die Entwicklung vereinfacht und zukunftssicher macht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_04hK8NK-P0f",
   "metadata": {
    "id": "_04hK8NK-P0f"
   },
   "source": [
    "F√ºr komplexere Anwendungen mit umfangreichem Zustandsmanagement, Verzweigungen oder mehreren Agenten wird die Verwendung von LangGraph empfohlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c889d",
   "metadata": {
    "id": "a05c889d"
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Konstanten\n",
    "MODEL=\"gpt-4o-mini\",\n",
    "TEMPERATUR=0.7\n",
    "\n",
    "# Modell-Konfiguration\n",
    "chat_model = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GxL5B1OdvF-v",
   "metadata": {
    "id": "GxL5B1OdvF-v"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Simple Chain\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8w1pX9ehummD",
   "metadata": {
    "id": "8w1pX9ehummD"
   },
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein hilfreicher Assistent.\"),\n",
    "    (\"human\", \"{eingabe}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aXL5zK8ZvS5N",
   "metadata": {
    "id": "aXL5zK8ZvS5N"
   },
   "outputs": [],
   "source": [
    "# Verarbeitungskette\n",
    "simple_chain = (\n",
    "    prompt_template\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XOjiVTuHu_4-",
   "metadata": {
    "id": "XOjiVTuHu_4-"
   },
   "outputs": [],
   "source": [
    "# Aufruf\n",
    "result = simple_chain.invoke({\"eingabe\": \"Was ist Machine Learning?\"})\n",
    "print(\"Simple Chain Ergebnis:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qy35G1-FvrIR",
   "metadata": {
    "id": "Qy35G1-FvrIR"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Sequential Chains\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AWhQwgMbxitn",
   "metadata": {
    "id": "AWhQwgMbxitn"
   },
   "source": [
    "Sequentielle Chain: Linear, ein Input wird schrittweise verarbeitet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceqbCIOCuqng",
   "metadata": {
    "id": "ceqbCIOCuqng"
   },
   "outputs": [],
   "source": [
    "# Prompts\n",
    "zusammenfassen_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Fasse den folgenden Text pr√§gnant zusammen.\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "uebersetzen_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"√úbersetze den folgenden Text ins Deutsche.\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kddZQ3rMv0Hw",
   "metadata": {
    "id": "kddZQ3rMv0Hw"
   },
   "outputs": [],
   "source": [
    "# Verarbeitungskette\n",
    "sequential_chain = (\n",
    "    zusammenfassen_template\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: {\"text\": x})  # Ausgabe der ersten Chain als Eingabe f√ºr die zweite\n",
    "    | uebersetzen_template\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ymYXf4pKwSee",
   "metadata": {
    "id": "ymYXf4pKwSee"
   },
   "outputs": [],
   "source": [
    "# Aufruf\n",
    "text = \"\"\"\n",
    "Machine Learning is a subset of artificial intelligence that focuses on developing\n",
    "systems that can learn and improve from experience without being explicitly programmed.\n",
    "\"\"\"\n",
    "seq_result = sequential_chain.invoke({\"text\": text})\n",
    "print(\"\\nSequential Chain Ergebnis:\", seq_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R23C1DI1wi1X",
   "metadata": {
    "id": "R23C1DI1wi1X"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Q&A-Chain\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-pshkn53xc7T",
   "metadata": {
    "id": "-pshkn53xc7T"
   },
   "source": [
    "QA Chain: Kontextbasiert, kombiniert mehrere Inputs f√ºr eine einzelne Verarbeitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3I8ucKu7wAoo",
   "metadata": {
    "id": "3I8ucKu7wAoo"
   },
   "outputs": [],
   "source": [
    "# Prompt\n",
    "qa_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Beantworte die Frage basierend auf dem gegebenen Kontext.\"),\n",
    "    (\"human\", \"\"\"\n",
    "    Kontext: {context}\n",
    "\n",
    "    Frage: {question}\n",
    "    \"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9iqmLBYSwpiX",
   "metadata": {
    "id": "9iqmLBYSwpiX"
   },
   "outputs": [],
   "source": [
    "# Verkettung\n",
    "qa_chain = (\n",
    "    qa_template\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R0lVOJaSw2a9",
   "metadata": {
    "id": "R0lVOJaSw2a9"
   },
   "outputs": [],
   "source": [
    "# Aufruf\n",
    "context = \"\"\"\n",
    "Python ist eine beliebte Programmiersprache f√ºr Machine Learning.\n",
    "Sie bietet viele Bibliotheken wie TensorFlow und PyTorch.\n",
    "\"\"\"\n",
    "question = \"Welche ML-Bibliotheken gibt es f√ºr Python?\"\n",
    "qa_result = qa_chain.invoke({\n",
    "    \"context\": context,\n",
    "    \"question\": question\n",
    "})\n",
    "print(\"\\nQA Chain Ergebnis:\", qa_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bxFOmczGyPPN",
   "metadata": {
    "id": "bxFOmczGyPPN"
   },
   "outputs": [],
   "source": [
    "# Formatierte Ausgabe\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(qa_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iZ5UJro5DCdR",
   "metadata": {
    "id": "iZ5UJro5DCdR"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "invoke - batch - stream\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tYRFZ2K0DKHb",
   "metadata": {
    "id": "tYRFZ2K0DKHb"
   },
   "source": [
    "Aufruf der Verarbeitungskette erfolgt √ºber:   \n",
    "+ invoke(): F√ºr einzelne, einfache Anfragen\n",
    "+ batch(): Wenn mehrere Anfragen parallel verarbeitet werden sollen\n",
    "+ stream(): F√ºr lange Antworten oder bessere Nutzererfahrung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7xBrIqZQ5et1",
   "metadata": {
    "id": "7xBrIqZQ5et1"
   },
   "outputs": [],
   "source": [
    "template = PromptTemplate(\n",
    "    input_variables=[\"thema\"],\n",
    "    template=\"Erkl√§re {thema} in einfachen Worten.\"\n",
    ")\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein hilfreicher Assistent.\"),\n",
    "    (\"human\", \"{eingabe}\")\n",
    "])\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# LCEL-Verkettung mit Pipe-Operator\n",
    "chain = prompt_template | chat_model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7z1qJcbIDhL-",
   "metadata": {
    "id": "7z1qJcbIDhL-"
   },
   "outputs": [],
   "source": [
    "# ---invoke--- #\n",
    "result_with_invoke = chain.invoke(\"hello world!\")\n",
    "print(result_with_invoke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aithABrSEAf2",
   "metadata": {
    "id": "aithABrSEAf2"
   },
   "outputs": [],
   "source": [
    "# ---batch--- #\n",
    "result_with_batch = chain.batch([\"hello\", \"world\", \"!\"])\n",
    "print(result_with_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QjVNjfUXEBuA",
   "metadata": {
    "id": "QjVNjfUXEBuA"
   },
   "outputs": [],
   "source": [
    "# ---stream--- #\n",
    "for chunk in chain.stream(\"Karl ist 22 Jahre alt.\"):\n",
    "    print(chunk, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HBZ_LQtwyGgc",
   "metadata": {
    "id": "HBZ_LQtwyGgc"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Vergleich Sequential vs Q&A\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XYFeZlgvx-xM",
   "metadata": {
    "id": "XYFeZlgvx-xM"
   },
   "source": [
    "| Aspekt | Sequentielle Chain | QA Chain |\n",
    "|--------|-------------------|-----------|\n",
    "| Eingabe | Einzelner Text | Kontext + Frage |\n",
    "| Verarbeitung | Schrittweise | Parallel |\n",
    "| Ausgabe | Transformierter Text | Kontextbezogene Antwort |\n",
    "| Anwendung | Textverarbeitung | Informationsextraktion |\n",
    "| Flexibilit√§t | Fest definiert | Kontextabh√§ngig |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LrfdbCP92lZt",
   "metadata": {
    "id": "LrfdbCP92lZt"
   },
   "source": [
    "Verschoben aus M01 - redundant?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IOpAcj-byBPg",
   "metadata": {
    "id": "IOpAcj-byBPg"
   },
   "source": [
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Erstellen einer einfache sequentielle Kette\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HFgf9flrdPwq",
   "metadata": {
    "id": "HFgf9flrdPwq"
   },
   "source": [
    "\n",
    "\n",
    "Wir werden nun LangChain verwenden, um mehrere LLM-Aufrufe mithilfe der Klasse **SimpleSequentialChain** zu einer l√§ngeren Kette zusammenzufassen. Wir werden zwei kleinere Ketten verwenden, um einen Titel und einen Textk√∂rper f√ºr einen Blogbeitrag zu erstellen. Wir beginnen mit der Definition der beiden Eingabeaufforderungen, die wir zum Erstellen dieses Blogbeitrags verwenden werden. Beachten Sie auch, dass wir verlangen, dass das LLM [markdown](https://en.wikipedia.org/wiki/Markdown) verwendet, um den eigentlichen Blogbeitrag zu generieren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ELS-9JD3Sao6",
   "metadata": {
    "id": "ELS-9JD3Sao6"
   },
   "outputs": [],
   "source": [
    "# Erstellen Sie die beiden Eingabeaufforderungsvorlagen\n",
    "title_template = PromptTemplate( input_variables = ['topic'], template = 'Gibt nur den Titel eines Blog-Beitragsartikels zu {topic} in Deutsch' )\n",
    "article_template = PromptTemplate( input_variables = ['title'], template = 'Schreibe einen Blog-Beitrag {title}, das Format ist Markdown.' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k3vy7jf3-_xl",
   "metadata": {
    "id": "k3vy7jf3-_xl"
   },
   "source": [
    "Wir erstellen die erste Kette, um den zuf√§lligen Titel zu generieren. Hier k√∂nnen die Benutzer das Thema angeben. Wir verwenden eine h√∂here Temperatur, um die Kreativit√§t des Titels zu steigern. Wir verwenden auch ein einfacheres Modell, um die Kosten f√ºr die relativ einfache Aufgabe der Titelauswahl zu minimieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_KV8UJUDyP8M",
   "metadata": {
    "id": "_KV8UJUDyP8M"
   },
   "outputs": [],
   "source": [
    "MODEL = 'gpt-4o-mini'\n",
    "\n",
    "# Erstellen Sie eine Kette, um einen Zufallsgenerator zu erzeugen.\n",
    "llm = ChatOpenAI(model=MODEL, temperature=0.7)\n",
    "title_chain = title_template | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cO0ielMf_xRa",
   "metadata": {
    "id": "cO0ielMf_xRa"
   },
   "source": [
    "Als n√§chstes verfassen wir den eigentlichen Blogbeitrag. Wir verwenden eine niedrigere Temperatur, um die Kreativit√§t zu verringern und das LLM dazu zu bringen, sich an sachliche Informationen zu halten und Halluzinationen zu vermeiden. Wir verwenden auch ein komplexeres Modell, um einen besseren Artikel zu erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G7OCQCggyHlB",
   "metadata": {
    "id": "G7OCQCggyHlB"
   },
   "outputs": [],
   "source": [
    "MODEL2 = 'gpt-4o'\n",
    "\n",
    "# Erstellen der Artikelkette\n",
    "llm2 = ChatOpenAI(model=MODEL2, temperature=0.1)\n",
    "article_chain = article_template | llm2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6rftFvMyBa4-",
   "metadata": {
    "id": "6rftFvMyBa4-"
   },
   "source": [
    "Nun kombinieren wir diese beiden Ketten zu einer. Die Eingabe f√ºr die erste Kette ist das ausgew√§hlte Thema. Die erste Kette gibt dann den Titel an die zweite Kette aus, die wiederum den eigentlichen Artikel ausgibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ibHBt4eyKZt",
   "metadata": {
    "id": "2ibHBt4eyKZt"
   },
   "outputs": [],
   "source": [
    "# Erstellen einer vollst√§ndige Kette, um einen neuen Blogbeitrag zu erstellen\n",
    "complete_chain = title_chain | article_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V_7P-n_DBvpD",
   "metadata": {
    "id": "V_7P-n_DBvpD"
   },
   "source": [
    "Wir k√∂nnen nun den fertigen Artikel anzeigen. In diesem Fall haben wir einen Artikel zum Thema ‚ÄûArchitecture‚Äú angefordert und den Markdown des fertigen Artikels angezeigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mwro_kfKXvml",
   "metadata": {
    "id": "mwro_kfKXvml"
   },
   "outputs": [],
   "source": [
    "article = complete_chain.invoke('Architecture')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h1pN8CbXCE6F",
   "metadata": {
    "id": "h1pN8CbXCE6F"
   },
   "source": [
    "Die eigentliche Anzeige des Markdowns wird durch diesen Code √ºbernommen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vt5uGJawuru_",
   "metadata": {
    "id": "Vt5uGJawuru_"
   },
   "outputs": [],
   "source": [
    "Markdown(article.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VlARJEowAAIR",
   "metadata": {
    "id": "VlARJEowAAIR"
   },
   "source": [
    "# **3 | √úberblick LangChain Konzepte**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g5B7bWJKBB-C",
   "metadata": {
    "id": "g5B7bWJKBB-C"
   },
   "source": [
    "Konzepte von LangChain sind grundlegende Bausteine und Prinzipien, die das Framework ausmachen und seine Funktionsweise erm√∂glichen. Hier sind einige der wichtigsten Konzepte:\n",
    "\n",
    "+ Chat-Modelle (**Chat models**): LLMs, die √ºber eine Chat-API verf√ºgbar sind und Sequenzen von Nachrichten verarbeiten1.\n",
    "+ Nachrichten (**Messages**): Kommunikationseinheiten in Chat-Modellen f√ºr Ein- und Ausgabe1.\n",
    "+ Chat-Verlauf (**Chat history**): Eine Sequenz von Nachrichten, die eine Konversation darstellt.\n",
    "+ Tools (**Tools**): Funktionen mit definierten Schemata f√ºr Name, Beschreibung und Argumente.\n",
    "+ Strukturierte Ausgabe (**Structured output**): Technik, um Chat-Modelle in strukturierten Formaten antworten zu lassen.\n",
    "+ Retrieval Augmented Generation (**RAG**): Technik zur Verbesserung von Sprachmodellen durch Kombination mit externen Wissensbasen.\n",
    "+ Prompt-Vorlagen (**Prompt template**s): Komponenten zur Erstellung strukturierter Prompts f√ºr LLMs.\n",
    "+ Chains (**Chains**): Verkn√ºpfungen mehrerer LLMs oder anderer Komponenten f√ºr komplexere Anwendungen.\n",
    "+ Agenten (**Agents**): Nutzen Sprachmodelle, um Aktionssequenzen auszuw√§hlen und mit externen Ressourcen zu interagieren.\n",
    "+ Retriever (**Retriever**): Komponenten, die relevante Dokumente aus einer Wissensbasis abrufen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a96d3df",
   "metadata": {
    "id": "9a96d3df"
   },
   "source": [
    "# **4 | Best Practices**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cKCGHGvuI9mI",
   "metadata": {
    "id": "cKCGHGvuI9mI"
   },
   "source": [
    "\n",
    "\n",
    "1. **Modularit√§t**:\n",
    "   - Baue Anwendungen aus kleinen, wiederverwendbaren Komponenten\n",
    "   - Teste einzelne Komponenten separat\n",
    "   - Nutze Vererbung f√ºr eigene Implementierungen\n",
    "\n",
    "2. **Fehlerbehandlung**:\n",
    "   - Implementiere Retry-Mechanismen f√ºr API-Aufrufe\n",
    "   - Validiere Eingaben vor der Verarbeitung\n",
    "   - Logge wichtige Zwischenschritte\n",
    "\n",
    "3. **Performance**:\n",
    "   - Nutze Caching f√ºr Embeddings und h√§ufige Anfragen\n",
    "   - Optimiere Chunk-Gr√∂√üen f√ºr deine Anwendung\n",
    "   - Verwende Batch-Verarbeitung wo m√∂glich\n",
    "\n",
    "4. **Sicherheit und Kosten**:\n",
    "   - Setze Limits f√ºr Token und Anfragen\n",
    "   - √úberwache API-Kosten\n",
    "   - Implementiere Rate-Limiting\n",
    "\n",
    "5. **Testing**:\n",
    "   - Schreibe Unit-Tests f√ºr Chains und Parser\n",
    "   - Teste mit verschiedenen Eingabeformaten\n",
    "   - Simuliere API-Aufrufe in Tests\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7158dc5",
   "metadata": {
    "id": "d7158dc5"
   },
   "source": [
    "# **5 | Weiterf√ºhrende Ressourcen**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ayc326rsfyJ6",
   "metadata": {
    "id": "ayc326rsfyJ6"
   },
   "source": [
    "\n",
    "\n",
    "- [Offizielle LangChain Dokumentation](https://python.langchain.com/docs/)\n",
    "- [LangChain GitHub Repository](https://github.com/langchain-ai/langchain)\n",
    "- [LangChain Cookbook mit Beispielen](https://github.com/langchain-ai/langchain/tree/master/cookbook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F_5gn03b46rC",
   "metadata": {
    "id": "F_5gn03b46rC"
   },
   "source": [
    "# **A | Aufgabe**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36LyGMmo5jb3",
   "metadata": {
    "id": "36LyGMmo5jb3"
   },
   "source": [
    "Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Je-szbGU7IJ-",
   "metadata": {
    "id": "Je-szbGU7IJ-"
   },
   "source": [
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Erstellen einer LangChain-Kette zur Textanalyse\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kevePvA-7PzW",
   "metadata": {
    "id": "kevePvA-7PzW"
   },
   "source": [
    "Entwickeln Sie eine Verarbeitungskette, die einen Text analysiert und verschiedene Informationen dar√ºber extrahiert.\n",
    "\n",
    "**Schritte:**\n",
    "1. Erstelle ein ChatPromptTemplate, das ein LLM anweist, einen Text zu analysieren\n",
    "2. Die Analyse soll folgende Aspekte umfassen:\n",
    "   - Hauptthema des Textes\n",
    "   - Tonalit√§t (formal, informell, etc.)\n",
    "   - Schl√ºsselw√∂rter (5-10)\n",
    "   - Kurze Zusammenfassung (max. 3 S√§tze)\n",
    "3. Formatiere die Ausgabe strukturiert mit Markdown-√úberschriften\n",
    "4. Teste die Kette mit mindestens zwei verschiedenen Texten\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6oBvtYws7qpB",
   "metadata": {
    "id": "6oBvtYws7qpB"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Few-Shot-Learning f√ºr Textklassifikation\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gI7alQcE79ER",
   "metadata": {
    "id": "gI7alQcE79ER"
   },
   "source": [
    "Erstellen Sie ein System, das mithilfe von Few-Shot-Learning Texte in vorgegebene Kategorien klassifiziert.\n",
    "\n",
    "**Schritte:**\n",
    "1. Definiere 3-5 Kategorien f√ºr die Klassifikation (z.B. Sport, Politik, Technologie, Kultur)\n",
    "2. Erstelle einen FewShotPromptTemplate mit Beispielen f√ºr jede Kategorie\n",
    "3. Entwickle eine Chain, die neue Texte klassifiziert\n",
    "4. Implementiere eine Funktion, die neben der Kategorie auch eine Begr√ºndung f√ºr die Einordnung liefert\n",
    "5. Teste das System mit verschiedenen Texten, die nicht in den Beispielen vorkommen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l35x2g1e8fa9",
   "metadata": {
    "id": "l35x2g1e8fa9"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Q&A-System mit Sequential Chain\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qHFRGWmP8gga",
   "metadata": {
    "id": "qHFRGWmP8gga"
   },
   "source": [
    "Entwickeln Sie ein System, das Fragen zu einem gegebenen Kontext beantwortet, aber zuerst den Kontext zusammenfasst und dann die Frage beantwortet.\n",
    "\n",
    "\n",
    "**Schritte:**\n",
    "\n",
    "+ Erstellen Sie zwei Templates:\n",
    "    + Ein Template zur Zusammenfassung des Kontexts\n",
    "    + Ein Template zur Beantwortung einer Frage basierend auf der Zusammenfassung\n",
    "+ Verbinden Sie die Templates in einer sequentiellen Kette\n",
    "+ Implementieren Sie einen Mechanismus, der die Zusammenfassung und die Antwort getrennt zur√ºckgibt\n",
    "+ Nutzen Sie LCEL (LangChain Expression Language) f√ºr die Verkettung\n",
    "+ Testen Sie das System mit verschiedenen Kontexten und Fragen"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "PHDCKRbtgVXy",
    "fGV8FwGmIwmR",
    "EouwC_wdZb-c",
    "-JwJD0rLIyT8",
    "8c859b9f",
    "c314ca71",
    "VlARJEowAAIR",
    "9a96d3df",
    "d7158dc5",
    "F_5gn03b46rC"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

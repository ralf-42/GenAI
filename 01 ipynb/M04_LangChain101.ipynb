{"cells":[{"cell_type":"markdown","id":"37rMyCQz2JNw","metadata":{"id":"37rMyCQz2JNw"},"source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)"]},{"cell_type":"markdown","id":"R5CfUEMJdvFQ","metadata":{"id":"R5CfUEMJdvFQ"},"source":["<p><font size=\"5\" color='grey'> <b>\n","LangChain 101\n","</b></font> </br></p>\n","\n","---"]},{"cell_type":"code","execution_count":1,"id":"dfdhPIzcEYRG","metadata":{"cellView":"form","collapsed":true,"id":"dfdhPIzcEYRG","jupyter":{"outputs_hidden":true},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747679535797,"user_tz":-120,"elapsed":5582,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"6091b4bc-6101-4c83-b399-3c5548599787"},"outputs":[{"output_type":"stream","name":"stdout","text":["Python Version:  3.11.12 (main, Apr  9 2025, 08:55:54) [GCC 11.4.0]\n","\n","Installierte LangChain Bibliotheken:\n","langchain                             0.3.25\n","langchain-core                        0.3.59\n","langchain-text-splitters              0.3.8\n"]}],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  Colab-Umfeld</font> </br></p>\n","# Installierte Python Version\n","import sys\n","print(f\"Python Version: \",sys.version)\n","# Installierte LangChain Bibliotheken\n","print()\n","print(\"Installierte LangChain Bibliotheken:\")\n","\n","!pip list | grep '^langchain'\n","# Unterdrückt die \"DeprecationWarning\" von LangChain für die Memory-Funktionden\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")"]},{"cell_type":"code","execution_count":2,"id":"WD3Wwr6sESX8","metadata":{"cellView":"form","id":"WD3Wwr6sESX8","executionInfo":{"status":"ok","timestamp":1747679537748,"user_tz":-120,"elapsed":1947,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  SetUp API-Keys (setup_api_keys)</font> </br></p>\n","def setup_api_keys():\n","    \"\"\"Konfiguriert alle benötigten API-Keys aus Google Colab userdata\"\"\"\n","    from google.colab import userdata\n","    import os\n","    from os import environ\n","\n","    # Dictionary der benötigten API-Keys\n","    keys = {\n","        'OPENAI_API_KEY': 'OPENAI_API_KEY',\n","        'HF_TOKEN': 'HF_TOKEN',\n","        # Weitere Keys bei Bedarf\n","    }\n","\n","    # Keys in Umgebungsvariablen setzen\n","    for env_var, key_name in keys.items():\n","        environ[env_var] = userdata.get(key_name)\n","\n","    return {k: environ[k] for k in keys.keys()}\n","\n","# Verwendung\n","all_keys = setup_api_keys()\n","# Bei Bedarf einzelne Keys direkt zugreifen\n","# WEATHER_API_KEY = all_keys['WEATHER_API_KEY']"]},{"cell_type":"markdown","id":"91ee89a4-477b-41ce-a511-aadd4c2fd547","metadata":{"id":"91ee89a4-477b-41ce-a511-aadd4c2fd547"},"source":["# 1 | Was ist LangChain?\n","---"]},{"cell_type":"markdown","id":"b5004f8c","metadata":{"id":"b5004f8c"},"source":["LangChain ist ein Framework zur **Entwicklung von Anwendungen mit großen Sprachmodellen** (LLMs). Es vereinfacht die Integration von LLMs in eigene Anwendungen durch:\n","\n","- Modulare Komponenten für verschiedene Aufgaben\n","- Vorgefertigte Ketten (Chains) für komplexe Workflows\n","- Einfache Integration externer Datenquellen\n","- Werkzeuge für das Speichern von Konversationskontexten\n","\n"]},{"cell_type":"markdown","id":"tl73bT-kKkia","metadata":{"id":"tl73bT-kKkia"},"source":["[Einführung](https://python.langchain.com/docs/introduction/)   \n","[Konzepte](https://python.langchain.com/docs/concepts/)   \n","[API-References](https://python.langchain.com/api_reference/index.html)   \n","[Integrations](https://python.langchain.com/docs/integrations/providers/)\n","\n","---\n","\n","[Tutorials](https://python.langchain.com/docs/tutorials/)   \n","[How-to-Guides](https://python.langchain.com/docs/how_to/)   "]},{"cell_type":"code","execution_count":3,"id":"JrgQ46eLphEv","metadata":{"id":"JrgQ46eLphEv","executionInfo":{"status":"ok","timestamp":1747679541848,"user_tz":-120,"elapsed":4051,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["!uv pip install --system --prerelease allow -q langchain_community langchain_openai"]},{"cell_type":"markdown","id":"fGV8FwGmIwmR","metadata":{"id":"fGV8FwGmIwmR"},"source":["# 2 | Kernkonzepte\n","---\n","\n","\n"]},{"cell_type":"markdown","source":["Diese einfache Darstellung veranschaulicht das Grundprinzip von LangChain: die Verkettung verschiedener Konzepte zu einem durchgängigen Verarbeitungsprozess, der Eingaben in strukturierte Ausgaben umwandelt."],"metadata":{"id":"nZ2ZMEd3ulKZ"},"id":"nZ2ZMEd3ulKZ"},{"cell_type":"markdown","source":["<img src=\"https://raw.githubusercontent.com/ralf-42/Image/main/langchain_prozess.png\" width=\"207\" alt=\"Avatar\">"],"metadata":{"id":"KdBKNv3gzJAq"},"id":"KdBKNv3gzJAq"},{"cell_type":"markdown","id":"EouwC_wdZb-c","metadata":{"id":"EouwC_wdZb-c"},"source":["## 2.1 Message-Typen"]},{"cell_type":"markdown","id":"yS2mM2J1aXlY","metadata":{"id":"yS2mM2J1aXlY"},"source":["Beim Einsatz von Large Language Models (LLMs) wie GPT gibt es drei Hauptarten von Nachrichten (Messages), die die Interaktion mit dem Modell steuern. Diese Messages definieren, wie das Modell reagiert, welche Eingaben es erhält und welche Ausgaben es generiert.\n","\n"]},{"cell_type":"markdown","id":"sGqRXptbbQ2a","metadata":{"id":"sGqRXptbbQ2a"},"source":["**System Message:**   \n","Die `System Message` dient dazu, das Verhalten des LLMs zu steuern. Sie ist eine nicht sichtbare Nachricht, die dem Modell Anweisungen gibt, wie es sich verhalten soll. Dabei kann sie die Persönlichkeit, den Tonfall und die Einschränkungen des Modells beeinflussen. Die Systemnachricht funktioniert wie eine Art \"Rollenspiel-Anweisung\", um das Modell in eine gewünschte Rolle zu versetzen.\n","\n","*Merkmale:*\n","- Definiert, wie das Modell reagieren soll\n","- Legt Persönlichkeit, Verhalten und Einschränkungen fest\n","- Wird zu Beginn einer Sitzung gesetzt und bleibt bestehen\n","- Nicht sichtbar für den Benutzer\n","\n","*Beispiel einer System Message:*\n","```json\n","{\n","  \"role\": \"system\",\n","  \"content\": \"Du bist ein KI-Assistent, der präzise und informative Antworten in einem professionellen Ton liefert.\"\n","}\n","```\n","Dieses Beispiel weist das Modell an, in einem professionellen Tonfall kurze und informative Antworten zu geben.\n","\n","---\n","\n"]},{"cell_type":"markdown","id":"upsIzpcqbbP7","metadata":{"id":"upsIzpcqbbP7"},"source":["**User Message**   \n","Die `User Message` ist die eigentliche Eingabe des Benutzers. Sie kann eine Frage, eine Aufforderung, kontextrelevanten Inhalt oder ein Befehl sein. Das Modell nutzt diese Nachricht als Ausgangspunkt für die Generierung einer Antwort.\n","\n","*Merkmale:*\n","- Direkte Eingabe des Benutzers\n","- Kann eine Frage, Aufforderung, kontextrelevanten Inhalt oder einen Befehl enthalten\n","- Basis für die Antwort des Modells\n","\n","*Beispiel einer User Message:*\n","```json\n","{\n","  \"role\": \"user\",\n","  \"content\": \"Was sind die wichtigsten Unterschiede zwischen KI und maschinellem Lernen?\"\n","}\n","```\n","Hier fragt der Benutzer nach den Unterschieden zwischen KI und maschinellem Lernen.\n","\n","---\n"]},{"cell_type":"markdown","id":"CozrrtYXbhZb","metadata":{"id":"CozrrtYXbhZb"},"source":["**AI Message**   \n","Die `AI Message` ist die Antwort, die das Modell generiert. Sie basiert auf der User Message und den Anweisungen aus der System Message. Die AI Message kann verschiedene Eigenschaften haben, ist aber hauptsächlich auf den Inhalt fokussiert.\n","\n","*Merkmale:*\n","- Antwort des Modells auf die Benutzeranfrage\n","- Kann verschiedene Eigenschaften haben (z. B. Länge, Stil)\n","- Hauptsächlich inhaltlich relevant\n","\n","*Beispiel einer AI Message:*\n","```json\n","{\n","  \"role\": \"assistant\",\n","  \"content\": \"Künstliche Intelligenz (KI) ist ein weites Feld, das verschiedene Teilgebiete umfasst, darunter auch das maschinelle Lernen (ML). ML konzentriert sich auf die Erstellung von Modellen, die aus Daten lernen und Vorhersagen treffen.\"\n","}\n","```\n","Hier gibt das Modell eine inhaltliche Antwort auf die Frage des Benutzers.\n","\n","---\n","\n"]},{"cell_type":"markdown","id":"PiHnhKHVbofP","metadata":{"id":"PiHnhKHVbofP"},"source":["**Zusammenhang**   \n","Die drei Message-Typen arbeiten zusammen, um eine sinnvolle Interaktion mit dem Modell zu ermöglichen:\n","\n","+  **System Message** legt die Regeln und das Verhalten des Modells fest.\n","+ **User Message** stellt eine Anfrage oder einen Befehl.\n","+ **AI Message** gibt die generierte Antwort basierend auf den vorherigen Nachrichten.\n","\n","**Beispiel für eine vollständige Konversation:**    \n","```json\n","[\n","  {\n","    \"role\": \"system\",\n","    \"content\": \"Sie sind ein KI-Assistent, der präzise und informative Antworten in einem professionellen Ton liefert.\"\n","  },\n","  {\n","    \"role\": \"user\",\n","    \"content\": \"Kannst der erklären, was große Sprachtmodelle sind?\"\n","  },\n","  {\n","    \"role\": \"assistant\",\n","    \"content\": \"Sicher! Large Language Models (LLMs) sind fortschrittliche KI-Systeme, die mit riesigen Textdatenmengen trainiert werden. Sie können menschenähnliche Texte verstehen und generieren und sind daher für verschiedene Anwendungen wie Chatbots, Content-Erstellung und mehr nützlich.\"\n","  }\n","]\n","```\n","In diesem Beispiel wurde das Modell so eingestellt, dass es in einem freundlichen Tonfall antwortet, dann erhält es eine Benutzeranfrage und gibt eine passende Antwort.\n","\n"]},{"cell_type":"markdown","id":"Iw3Ovw5mcFcY","metadata":{"id":"Iw3Ovw5mcFcY"},"source":["**Fazit**    \n","Die System-, User- und AI-Message sind essenzielle Bestandteile für die Interaktion mit einem LLM. Während die System Message das Verhalten vorgibt, stellt die User Message die eigentliche Anfrage dar, auf die das Modell mit einer AI Message antwortet. Dieses Konzept ist zentral für den Einsatz von LLMs in Chatbots, KI-gestützten Assistenten und anderen interaktiven Anwendungen.\n","\n","\n","Bei einem `einfachen` Dialog kann man jedoch auf diese Trennung verzichten,\n","\n"]},{"cell_type":"markdown","id":"-JwJD0rLIyT8","metadata":{"id":"-JwJD0rLIyT8"},"source":["## 2.2 Prompts"]},{"cell_type":"markdown","source":["\n","\n","Prompts sind die Eingaben bzw. Aufträge an das LLM. LangChain bietet verschiedene Template-Systeme. Ein **Template** ist eine **Vorlage** – genauer gesagt ein Text mit **Platzhaltern**, die später durch echte Werte ersetzt werden.\n","\n","\n","Ein **PromptTemplate** ist eine Vorlage für einen Eingabetext, der an ein Sprachmodell geschickt wird.\n","Er enthält Platzhalter ({}), die später mit Nutzereingaben oder Kontext befüllt werden.\n","\n","**Templates machen die KI-Prompts:**\n","\n","+ Wiederverwendbar (z. B. für viele Themen oder Fragen)\n","\n","+ Strukturiert (z. B. mit Rollen und Anweisungen)\n","\n","+ Dynamisch erweiterbar (mit Memory, Tools, etc.)"],"metadata":{"id":"QMVO4fzPFZ88"},"id":"QMVO4fzPFZ88"},{"cell_type":"markdown","id":"HqTUeMW20084","metadata":{"id":"HqTUeMW20084"},"source":["<p><font color='black' size=\"5\">\n","Simple Prompt (ohne Rollen)\n","</font></p>"]},{"cell_type":"code","execution_count":4,"id":"e1926d03","metadata":{"id":"e1926d03","executionInfo":{"status":"ok","timestamp":1747679549039,"user_tz":-120,"elapsed":7188,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","from langchain.prompts import PromptTemplate, ChatPromptTemplate\n","from langchain.prompts.few_shot import FewShotPromptTemplate\n","from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n","\n","from IPython.display import display, Markdown"]},{"cell_type":"code","source":["MODEL = \"gpt-4o-mini\"\n","TEMPERATUR = 0.0"],"metadata":{"id":"RG-y_bG8HOOi","executionInfo":{"status":"ok","timestamp":1747679549074,"user_tz":-120,"elapsed":31,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"RG-y_bG8HOOi","execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Modell definieren\n","llm = ChatOpenAI(model=MODEL, temperature=TEMPERATUR)"],"metadata":{"id":"yd3MebxgGYgy","executionInfo":{"status":"ok","timestamp":1747679553892,"user_tz":-120,"elapsed":4816,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"yd3MebxgGYgy","execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["**Prompt wird als String übergeben:**"],"metadata":{"id":"1azC75R5JpWC"},"id":"1azC75R5JpWC"},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(\"Was ist Generative KI?\")"],"metadata":{"id":"jPizca8FJV5b","executionInfo":{"status":"ok","timestamp":1747679560613,"user_tz":-120,"elapsed":6686,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"jPizca8FJV5b","execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Ausgabe\n","display(Markdown(\"## 📣 Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":365},"id":"cOJGeiLGJb-g","executionInfo":{"status":"ok","timestamp":1747679560748,"user_tz":-120,"elapsed":83,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"8d42aeff-098c-4f33-c1f4-7e590f2c9241"},"id":"cOJGeiLGJb-g","execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## 📣 Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Generative KI, oder generative künstliche Intelligenz, bezieht sich auf eine Klasse von Algorithmen und Modellen, die in der Lage sind, neue Inhalte zu erzeugen. Diese Inhalte können in verschiedenen Formen vorliegen, darunter Texte, Bilder, Musik, Videos und mehr. Generative KI nutzt häufig Techniken des maschinellen Lernens, insbesondere neuronale Netzwerke, um Muster in bestehenden Daten zu erkennen und darauf basierend neue, ähnliche Daten zu generieren.\n\nEin bekanntes Beispiel für generative KI sind Sprachmodelle wie GPT (Generative Pre-trained Transformer), die in der Lage sind, menschenähnliche Texte zu schreiben, Fragen zu beantworten oder Geschichten zu erzählen. In der Bildgenerierung gibt es Modelle wie DALL-E oder Midjourney, die aus Textbeschreibungen Bilder erstellen können.\n\nGenerative KI hat viele Anwendungen, darunter:\n\n1. **Kreatives Schreiben**: Unterstützung bei der Erstellung von Geschichten, Artikeln oder Gedichten.\n2. **Bild- und Kunstgenerierung**: Erzeugung von Kunstwerken oder Designideen.\n3. **Musikkomposition**: Erstellung neuer Musikstücke oder Melodien.\n4. **Spieleentwicklung**: Generierung von Inhalten wie Levels oder Charakteren.\n5. **Simulationen**: Erzeugung realistischer Daten für Trainings- und Testzwecke in verschiedenen Bereichen.\n\nDie Technologie hat das Potenzial, kreative Prozesse zu revolutionieren, wirft jedoch auch ethische und rechtliche Fragen auf, insbesondere in Bezug auf Urheberrecht, Fälschungen und die Verbreitung von Fehlinformationen."},"metadata":{}}]},{"cell_type":"markdown","source":["**Prompt wird als variabler String übergeben:**"],"metadata":{"id":"aKRhzB0oJnUG"},"id":"aKRhzB0oJnUG"},{"cell_type":"code","source":["# Pure Python Lösung für simple prompt mit template\n","thema = \"Machine Learning\"\n","prompt = f\"Erkläre {thema} in einfachen Worten.\""],"metadata":{"id":"8s5cQY2oHUvW","executionInfo":{"status":"ok","timestamp":1747679560749,"user_tz":-120,"elapsed":71,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"8s5cQY2oHUvW","execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(prompt)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1747679563730,"user_tz":-120,"elapsed":3045,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"id":"plYGfaoLJ8DI"},"execution_count":10,"outputs":[],"id":"plYGfaoLJ8DI"},{"cell_type":"code","source":["# Ausgabe\n","display(Markdown(\"## 📣 Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":222},"executionInfo":{"status":"ok","timestamp":1747679563828,"user_tz":-120,"elapsed":96,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"73dfeedd-8c34-48aa-fd52-596411a56e61","id":"SLX0zaONJ8DJ"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## 📣 Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine Learning, oder maschinelles Lernen, ist ein Teilbereich der Künstlichen Intelligenz, der es Computern ermöglicht, aus Daten zu lernen und Muster zu erkennen, ohne dass sie dafür explizit programmiert werden müssen. \n\nStell dir vor, du hast viele Bilder von Katzen und Hunden. Anstatt einem Computer genau zu sagen, wie eine Katze oder ein Hund aussieht, zeigst du ihm einfach viele Beispiele. Der Computer analysiert diese Bilder und lernt, die Unterschiede zwischen Katzen und Hunden zu erkennen. \n\nWenn du ihm dann ein neues Bild zeigst, kann er basierend auf dem, was er gelernt hat, vorhersagen, ob es sich um eine Katze oder einen Hund handelt. \n\nZusammengefasst: Machine Learning ist wie das Lernen eines Menschen – je mehr Informationen und Erfahrungen er hat, desto besser kann er Entscheidungen treffen oder Vorhersagen machen."},"metadata":{}}],"id":"SLX0zaONJ8DJ"},{"cell_type":"markdown","source":["**Prompt wird als template übergeben:**"],"metadata":{"id":"n-VVZEHDKSIw"},"id":"n-VVZEHDKSIw"},{"cell_type":"code","source":["# 1. Einfaches Template mit Platzhaltern\n","simple_prompt_template = PromptTemplate.from_template(\n","    \"Erkläre {thema} in einfachen Worten.\"\n",")"],"metadata":{"id":"koh0-_3r_HxG","executionInfo":{"status":"ok","timestamp":1747679563831,"user_tz":-120,"elapsed":90,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"koh0-_3r_HxG","execution_count":12,"outputs":[]},{"cell_type":"code","source":["thema = \"Generative KI\"\n","prompt = simple_prompt_template.format(thema=thema)\n","response = llm.invoke(prompt)"],"metadata":{"id":"nFrCjL6j_xo1","executionInfo":{"status":"ok","timestamp":1747679566447,"user_tz":-120,"elapsed":2572,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"nFrCjL6j_xo1","execution_count":13,"outputs":[]},{"cell_type":"code","source":["display(Markdown(\"## 📣 Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response.content))"],"metadata":{"id":"lmEE-3NDAE15","colab":{"base_uri":"https://localhost:8080/","height":181},"executionInfo":{"status":"ok","timestamp":1747679566454,"user_tz":-120,"elapsed":4,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"a5a273b8-6f94-4fce-ec48-c647c7ca5ac2"},"id":"lmEE-3NDAE15","execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## 📣 Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Generative KI ist eine Art von künstlicher Intelligenz, die in der Lage ist, neue Inhalte zu erstellen. Das können Texte, Bilder, Musik oder sogar Videos sein. Sie funktioniert, indem sie aus großen Mengen von Daten lernt, wie bestimmte Inhalte aussehen oder klingen. \n\nStell dir vor, du hast viele Bilder von Katzen und Hunden. Eine generative KI kann dann lernen, wie diese Tiere aussehen, und neue Bilder von Katzen oder Hunden erstellen, die es so noch nicht gibt. \n\nIm Grunde genommen ist generative KI wie ein kreativer Computer, der neue Ideen und Inhalte basierend auf dem, was er gelernt hat, entwickeln kann."},"metadata":{}}]},{"cell_type":"markdown","source":["**Vorteil PromptTemplate vs f-string**"],"metadata":{"id":"VxlzHPsbJENM"},"id":"VxlzHPsbJENM"},{"cell_type":"markdown","source":["| Kriterium                        | `f-string`                | `PromptTemplate`                         |\n","| -------------------------------- | ------------------------- | ---------------------------------------- |\n","| 🧱 Basisfunktion                 | String zusammenbauen      | String-Vorlage mit Platzhaltern          |\n","| 🔁 Wiederverwendbarkeit          | manuell                   | sehr gut (Template + `.format()`)        |\n","| 🔌 Integration mit LangChain     | ❌ Nur als fertiger Text   | ✅ Nahtlos (in Chains, mit LLMs, etc.)    |\n","| 📦 Kombinierbar mit Tools        | ❌ Nein                    | ✅ Ja (Memory, OutputParser, Agents)      |\n","| 🧠 Erweiterbar (z. B. Beispiele) | ❌ Mühsam                  | ✅ z. B. `FewShotPromptTemplate`          |\n","| 🕵️ Nachvollziehbarkeit          | schwer bei großen Prompts | gut dokumentierbar & testbar             |\n","| 👥 Rollensteuerung               | ❌ Nur manuell             | ✅ mit `ChatPromptTemplate`               |\n","| 🧪 Validierung                   | ❌ keine                   | ✅ Platzhalterprüfung (`input_variables`) |\n"],"metadata":{"id":"pWT8Q3AkJCZe"},"id":"pWT8Q3AkJCZe"},{"cell_type":"markdown","id":"8akKmCCc06nm","metadata":{"id":"8akKmCCc06nm"},"source":["<p><font color='black' size=\"5\">\n","Chat Prompt (mit Rollen)\n","</font></p>"]},{"cell_type":"markdown","source":["Das **ChatPromptTemplate** ist eine spezielle Prompt-Vorlage für Chatmodelle (z. B. GPT-3.5, GPT-4), die **mehrere Rollen und Nachrichten** unterstützt – also genau das, was Chat-Modelle eigentlich brauchen.\n","\n","Es gehört zur LangChain-Bibliothek und baut auf dem Prinzip auf:\n","\n","\n","**Wer sagt was? → system, user, assistant, etc.**"],"metadata":{"id":"SVoHwxtrMM_j"},"id":"SVoHwxtrMM_j"},{"cell_type":"code","execution_count":15,"id":"iW9URvON0IXr","metadata":{"id":"iW9URvON0IXr","executionInfo":{"status":"ok","timestamp":1747679566546,"user_tz":-120,"elapsed":90,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# prompt-template als tuple\n","chat_template = ChatPromptTemplate([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"user\", \"Erkläre mir {thema}\"),\n","])"]},{"cell_type":"markdown","source":["**... oder ...**"],"metadata":{"id":"6FrHMy1PYH0S"},"id":"6FrHMy1PYH0S"},{"cell_type":"code","source":["# prompt-template als dict\n","chat_template = ChatPromptTemplate([\n","    {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher und humorvoller Assistent.\"},\n","    {\"role\": \"user\", \"content\": \"Erkläre mir {thema}\"},\n","])"],"metadata":{"id":"cGkROwH9YEyv","executionInfo":{"status":"ok","timestamp":1747679566550,"user_tz":-120,"elapsed":18,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"cGkROwH9YEyv","execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["**... oder ...**"],"metadata":{"id":"qxuO76Z9YpDE"},"id":"qxuO76Z9YpDE"},{"cell_type":"code","source":["# prompt-template als Nachrichtenobjekte\n","chat_template = ChatPromptTemplate([\n","    SystemMessage(content=\"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    HumanMessage(content=\"Erkläre mir {thema}\"),\n","])"],"metadata":{"id":"w3OFTq0XYlgl","executionInfo":{"status":"ok","timestamp":1747679566551,"user_tz":-120,"elapsed":14,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"w3OFTq0XYlgl","execution_count":17,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"jwrybdk_0agA","metadata":{"id":"jwrybdk_0agA","executionInfo":{"status":"aborted","timestamp":1747679600102,"user_tz":-120,"elapsed":11624,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["thema = \"Machine Learning\"\n","prompt = chat_template.format_messages(thema=thema)"]},{"cell_type":"code","source":["for message in prompt:\n","    print(f\"Role: {message.type}, Content: {message.content}\")"],"metadata":{"id":"HnGeULGPd9hO","executionInfo":{"status":"aborted","timestamp":1747679600106,"user_tz":-120,"elapsed":7765,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"HnGeULGPd9hO","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aufrufen der Kette mit Eingaben\n","response = llm.invoke(prompt)"],"metadata":{"id":"ZLeSLoy_CLpI","executionInfo":{"status":"ok","timestamp":1747679567443,"user_tz":-120,"elapsed":875,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"ZLeSLoy_CLpI","execution_count":20,"outputs":[]},{"cell_type":"code","source":["display(Markdown(\"## 📣 Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response.content))"],"metadata":{"id":"6irKh0y0C4nu","colab":{"base_uri":"https://localhost:8080/","height":99},"executionInfo":{"status":"ok","timestamp":1747679567521,"user_tz":-120,"elapsed":76,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"07000182-6954-44a5-b949-f01470f93780"},"id":"6irKh0y0C4nu","execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## 📣 Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Natürlich! Ich helfe dir gerne. Welches Thema möchtest du erklärt bekommen?"},"metadata":{}}]},{"cell_type":"markdown","source":["**Vorteile gegenüber `PromptTemplate`**"],"metadata":{"id":"I8EIgo8DMaYO"},"id":"I8EIgo8DMaYO"},{"cell_type":"markdown","source":["| Vorteil                      | `PromptTemplate`     | `ChatPromptTemplate`            |\n","| ---------------------------- | -------------------- | ------------------------------- |\n","| 🚻 Rollenstruktur            | ❌ Nur einfacher Text | ✅ Klar: `system`, `user`, `ai`  |\n","| 📚 Konversationsaufbau       | ❌ Nicht geeignet     | ✅ Ideal für Multi-Turn-Dialoge  |\n","| 🔄 Kombinierbar mit `Memory` | ❌ Nein               | ✅ Ja, via `MessagesPlaceholder` |\n","| 🔌 Für Chatmodelle wie GPT   | ⚠️ Nur als Fließtext | ✅ Nativ & strukturiert          |\n","| 🔍 Klarheit & Wartbarkeit    | ⚠️ Nur Inline-Text   | ✅ Saubere Message-Trennung      |\n","| 🧪 Prompt-Testbarkeit        | ❌ Eingeschränkt      | ✅ Besser strukturiert           |\n"],"metadata":{"id":"2uSD48umMXI7"},"id":"2uSD48umMXI7"},{"cell_type":"markdown","id":"Ps344Ejw0-ih","metadata":{"id":"Ps344Ejw0-ih"},"source":["<p><font color='black' size=\"5\">\n","Few-Shot-Learning Prompt\n","</font></p>"]},{"cell_type":"markdown","source":["`FewShotPromptTemplate` ist eine Prompt-Vorlage, die automatisch mehrere Beispiele einfügt, bevor das eigentliche Nutzer-Input kommt."],"metadata":{"id":"NnBh2dz8M2fi"},"id":"NnBh2dz8M2fi"},{"cell_type":"code","execution_count":22,"id":"cEopgOiS0KU_","metadata":{"id":"cEopgOiS0KU_","executionInfo":{"status":"ok","timestamp":1747679567532,"user_tz":-120,"elapsed":7,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["examples = [\n","    {\"frage\": \"Was ist Python?\", \"antwort\": \"Eine interpretierende Programmiersprache.\"},\n","    {\"frage\": \"Was ist Java?\", \"antwort\": \"Eine objektorientierte Sprache.\"}\n","]"]},{"cell_type":"code","source":["# Few-Shot-Learning als String\n","few_shot_prompt = f\"\"\"\n","Du bist ein Assistent, der Fragen zu Programmiersprachen beantwortet. Halte deine Antworten kurz und präzise.\n","\n","Beispiel 1:\n","Frage: {examples[0]['frage']}\n","Antwort: {examples[0]['antwort']}\n","\n","Beispiel 2:\n","Frage: {examples[1]['frage']}\n","Antwort: {examples[1]['antwort']}\n","\n","Beantworte nun die folgende Frage im gleichen Stil:\n","Frage: {{frage}}\n","Antwort:\n","\"\"\""],"metadata":{"id":"0pn8fDgQgka-","executionInfo":{"status":"ok","timestamp":1747679567534,"user_tz":-120,"elapsed":6,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"0pn8fDgQgka-","execution_count":23,"outputs":[]},{"cell_type":"code","source":["frage = \"Was ist JavaScript?\"\n","prompt = few_shot_prompt.format(frage=frage)"],"metadata":{"id":"dDq-RkD4iVex","executionInfo":{"status":"ok","timestamp":1747679567536,"user_tz":-120,"elapsed":1,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"dDq-RkD4iVex","execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["**... oder ...**"],"metadata":{"id":"NIS2M0Dbhvun"},"id":"NIS2M0Dbhvun"},{"cell_type":"code","source":["# Prompt für einzelne Beispiele\n","example_prompt = PromptTemplate.from_template(\n","    \"Frage: {frage}\\nAntwort: {antwort}\"\n",")"],"metadata":{"id":"fHEYrEPcDD9S","executionInfo":{"status":"ok","timestamp":1747679567541,"user_tz":-120,"elapsed":2,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"fHEYrEPcDD9S","execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Few-Shot-Prompt\n","few_shot_prompt = FewShotPromptTemplate(\n","    examples=examples,\n","    example_prompt=example_prompt,\n","    prefix=\"Du bist ein Assistent, der Fragen zu Programmiersprachen beantwortet. Halte deine Antworten kurz und präzise:\",\n","    suffix=\"Frage: {input}\\nAntwort:\",\n","    input_variables=[\"input\"]\n",")\n","\n","prompt = few_shot_prompt.format(input=\"Was ist JavaScript?\")"],"metadata":{"id":"BpIbLhIhDcUp","executionInfo":{"status":"ok","timestamp":1747679567569,"user_tz":-120,"elapsed":1,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"BpIbLhIhDcUp","execution_count":26,"outputs":[]},{"cell_type":"code","source":["# geringere Temperaturen für mehr Sachlichkeit\n","llm = ChatOpenAI(model=MODEL, temperature=0.2)\n","response = llm.invoke(prompt)"],"metadata":{"id":"KZPoIO6gDo23","executionInfo":{"status":"ok","timestamp":1747679568277,"user_tz":-120,"elapsed":692,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"KZPoIO6gDo23","execution_count":27,"outputs":[]},{"cell_type":"code","source":["display(Markdown(\"## 📣 Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response.content))"],"metadata":{"id":"Yr0YleeiD_x7","colab":{"base_uri":"https://localhost:8080/","height":99},"executionInfo":{"status":"ok","timestamp":1747679568317,"user_tz":-120,"elapsed":32,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"03b1c9e4-23af-455f-9216-d5ab489ac4d3"},"id":"Yr0YleeiD_x7","execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## 📣 Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Eine Skriptsprache für die Webentwicklung."},"metadata":{}}]},{"cell_type":"markdown","source":["**Vorteile eines `FewShotPromptTemplate`**"],"metadata":{"id":"XFbved9KM_WJ"},"id":"XFbved9KM_WJ"},{"cell_type":"markdown","source":["| Vorteil                      | Beschreibung                                                      |\n","| ---------------------------- | ----------------------------------------------------------------- |\n","| 🧠 Lerneffekt für das Modell | Das Modell erkennt aus Beispielen, wie es antworten soll          |\n","| 🧰 Flexibilität              | Beispiele können dynamisch gesetzt oder generiert werden          |\n","| 🧱 Struktur & Konsistenz     | Einheitlicher Aufbau aller Beispiele über `example_prompt`        |\n","| 🪄 Kein Training nötig       | Kein Fine-Tuning – nur durch Prompting                            |\n","| 🛠 Kombinierbar              | Kann mit Templates, Tools, Chains, OutputParser kombiniert werden |\n"],"metadata":{"id":"EWjuAd2_M-E7"},"id":"EWjuAd2_M-E7"},{"cell_type":"markdown","id":"8c859b9f","metadata":{"id":"8c859b9f"},"source":["## 2.3 Modelle\n","\n","LangChain unterstützt verschiedene LLMs und bietet flexible Konfigurationsmöglichkeiten:"]},{"cell_type":"markdown","source":["**Beispiel `gpt-4o-mini` und `o3-mini`**"],"metadata":{"id":"Oc0UPT5eTyTR"},"id":"Oc0UPT5eTyTR"},{"cell_type":"markdown","source":["+ gpt-4o-mini ist ein kompaktes, multimodales KI-Modell von OpenAI, das Text- und Bildeingaben verarbeiten kann und besonders für schnelle, alltägliche Aufgaben sowie kleinere Anwendungen optimiert wurde\n","\n","+ o3-mini ist ein spezialisiertes KI-Modell, das vor allem für komplexe Wissensverarbeitung, anspruchsvolle Denkaufgaben und professionelle Programmierprojekte entwickelt wurde"],"metadata":{"id":"f8yl7OApTvcQ"},"id":"f8yl7OApTvcQ"},{"cell_type":"markdown","source":["**Modellparameter:**"],"metadata":{"id":"fxdaHMICUDPT"},"id":"fxdaHMICUDPT"},{"cell_type":"markdown","source":["| **Parameter**       | **`gpt-4o-mini`**  | **`o3-mini`**  | **Erklärung**                                                                |\n","| ------------------- | ------------------- | ----------------- | ---------------------------------------------------------------------------- |\n","| `model_name`        | ✅                   | ✅                 | Modellbezeichnung z. B. `\"gpt-4o-mini\"` oder `\"o3-mini\"`                     |\n","| `temperature`       | ✅                   | ❌                 | Kreativität/Zufall in der Antwort (0.0 = deterministisch, 1.0 = kreativ)     |\n","| `top_p`             | ✅                   | ❌                 | Nucleus Sampling (alternative zu `temperature`)                              |\n","| `max_tokens`        | ✅                   | ✅                 | Maximale Länge der generierten Antwort                                       |\n","| `frequency_penalty` | ✅                   | ❌                 | Bestraft Wiederholungen im Output                                            |\n","| `presence_penalty`  | ✅                   | ❌                 | Fördert neue Inhalte statt Wiederholungen                                    |\n","| `streaming`         | ✅                   | ✅                 | Tokenweise Live-Ausgabe des Texts (Streaming API)                            |\n","| `seed`              | ✅ *(via kwargs)*    | ✅ *(via kwargs)*  | Fixiert Zufallselemente (Reproduzierbarkeit)                                 |\n","| `response_format`   | ✅ *(via kwargs)*    | ✅ *(via kwargs)*  | Format wie `\"text\"` oder `\"json\"`                                            |\n","| `logit_bias`        | ✅ *(via kwargs)*    | ✅ *(via kwargs)*  | Gewichtung bestimmter Tokens                                                 |\n","| `tool_choice`       | ✅ *(via kwargs)*    | ✅ *(via kwargs)*  | Tool-Auswahl für Function Calling                                            |\n","| `reasoning_effort`  | ❌                   | ✅                 | Speziell für o3-Modelle: `\"low\"`, `\"medium\"`, `\"high\"` – steuert Denkaufwand |\n"],"metadata":{"id":"viwfwnIETILD"},"id":"viwfwnIETILD"},{"cell_type":"code","source":["# Modelle konfigurieren\n","\n","# Chat-Modell\n","chat_model = ChatOpenAI(\n","    model_name=\"gpt-4o-mini\",\n","    temperature=0.9\n",")\n","\n","# Reasoning-Modell\n","resoning_model = ChatOpenAI(\n","    model_name=\"o3-mini\",\n",")"],"metadata":{"id":"aDGqhlxoENvs","executionInfo":{"status":"ok","timestamp":1747679568590,"user_tz":-120,"elapsed":271,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"aDGqhlxoENvs","execution_count":29,"outputs":[]},{"cell_type":"code","source":["# Gemeinsamer simple prompt\n","prompt = \"Was ist der Unterschied zwischen Klassifikation und Regression im Machine Learning?\""],"metadata":{"id":"PywoNdLrPCfO","executionInfo":{"status":"ok","timestamp":1747679568772,"user_tz":-120,"elapsed":118,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"PywoNdLrPCfO","execution_count":30,"outputs":[]},{"cell_type":"code","source":["# Aufruf\n","result = chat_model.invoke(prompt)"],"metadata":{"id":"iriw2JZPOazT","executionInfo":{"status":"ok","timestamp":1747679573896,"user_tz":-120,"elapsed":5101,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"iriw2JZPOazT","execution_count":31,"outputs":[]},{"cell_type":"code","source":["display(Markdown(\"## 📣 Chat-Model:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":523},"id":"9OlEy8jQPck1","executionInfo":{"status":"ok","timestamp":1747679573929,"user_tz":-120,"elapsed":34,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"8e5a80f9-17b7-482b-8df9-89d020fd38c8"},"id":"9OlEy8jQPck1","execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## 📣 Chat-Model:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Klassifikation und Regression sind zwei grundlegende Arten von Aufgaben im Machine Learning, die sich in ihrem Ziel und ihrer Herangehensweise unterscheiden:\n\n### Klassifikation\n- **Ziel**: Bei der Klassifikation geht es darum, Datenpunkte in vordefinierte Kategorien oder Klassen einzuordnen. \n- **Ausgabe**: Die Ausgabe ist diskret, d.h. die Modelle liefern eine Klassenzugehörigkeit (z.B. \"Ja\" oder \"Nein\", \"Hund\" oder \"Katze\").\n- **Beispiele**: \n  - E-Mail-Spam-Filter (Spam oder Nicht-Spam)\n  - Bildklassifikation (Identifizierung von Objekten in Bildern)\n  - Sentiment-Analyse (positiv, neutral, negativ)\n\n### Regression\n- **Ziel**: In der Regression hingegen wird versucht, eine kontinuierliche Ausgabe vorherzusagen. Es geht darum, quantitative Werte zu schätzen.\n- **Ausgabe**: Die Ausgabe ist kontinuierlich, d.h. die Modelle liefern numerische Werte (z.B. Preise, Temperaturen).\n- **Beispiele**:\n  - Vorhersage von Hauspreisen basierend auf verschiedenen Merkmalen (z.B. Größe, Lage)\n  - Wettervorhersage (Temperatur, Niederschlag)\n  - Umsatzprognose für ein Unternehmen\n\n### Zusammenfassung\n- Klassifikation: Diskrete Ausgaben, Zuordnung zu Klassen.\n- Regression: Kontinuierliche Ausgaben, Vorhersage von numerischen Werten.\n\nBeide Ansätze verwenden oft ähnliche Algorithmen und Techniken, aber die Wahl zwischen Klassifikation und Regression hängt vom spezifischen Anwendungsfall und den Anforderungen an die Vorhersage ab."},"metadata":{}}]},{"cell_type":"code","source":["# Aufruf\n","result = resoning_model.invoke(prompt)"],"metadata":{"id":"_Znkwm2SPOrX","executionInfo":{"status":"ok","timestamp":1747679582867,"user_tz":-120,"elapsed":8936,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"_Znkwm2SPOrX","execution_count":33,"outputs":[]},{"cell_type":"code","source":["display(Markdown(\"## 📣 Reasoning-Model:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"id":"N0hcQkSfQyVK","executionInfo":{"status":"error","timestamp":1747679614421,"user_tz":-120,"elapsed":46,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"fb6708b7-155a-498f-845a-4114143d46ac"},"id":"N0hcQkSfQyVK","execution_count":50,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## 📣 Reasoning-Model:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"error","ename":"AttributeError","evalue":"'dict' object has no attribute 'content'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-cbeb4fc5cc2e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"## 📣 Reasoning-Model:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'content'"]}]},{"cell_type":"markdown","source":["**📊 Vergleich: Chat-Modell vs. Reasoning-Modell**"],"metadata":{"id":"YJljcFw4Rz6W"},"id":"YJljcFw4Rz6W"},{"cell_type":"markdown","source":["| Kriterium                 | 📣 **Chat-Modell**                                                                                             | 🧠 **Reasoning-Modell**                                                                                      |\n","| ------------------------- | -------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |\n","| **Stil**                  | Locker, freundlich, erklärend                                                                                  | Präzise, sachlich, eher technisch                                                                            |\n","| **Struktur**              | Abschnitte mit Überschriften, Beispiele, klare Gliederung in <br>*Klassifikation*, *Regression*, *Zusammenfassung* | Punktweise Gliederung mit technischer Terminologie, inkl. **Fehlermetriken** und **Entscheidungsstrategien** |\n","| **Zielgruppe**            | Eher auf Einsteiger\\*innen oder allgemeines Publikum ausgerichtet                                              | Fachlich versierte Nutzer\\*innen, z. B. Data Scientists oder Studierende in der Informatik                   |\n","| **Didaktik / Lesbarkeit** | Sehr zugänglich, mit einfachen Beispielen und fließendem Text                                                  | Kompakter, mehr Fachbegriffe, eher für „analytisches Nachschlagen“                                           |\n","| **Inhalte**               | Fokus auf Zweck und typische Beispiele                                                                         | Zusätzliche Tiefe: **Evaluationsmetriken**, **Softmax-Funktion**, **Bestimmtheitsmaß**                       |\n","| **Kreativität**           | Höher – wirkt wie ein Tutor, der mündlich erklärt                                                              | Geringer – wirkt wie ein Lehrbuch oder Fachaufsatz                                                           |\n","| **Temperatur-Effekt**     | spürbar → mehr Sprachvielfalt, z. B. beim Übergang zur Zusammenfassung                                         | gering → klare, determiniert wirkende Formulierungen                                                         |\n"],"metadata":{"id":"OzPIlXnqR2Fq"},"id":"OzPIlXnqR2Fq"},{"cell_type":"markdown","id":"c314ca71","metadata":{"id":"c314ca71"},"source":["## 2.4 Chains (LCEL)"]},{"cell_type":"markdown","source":["Chains verbinden mehrere Komponenten zu einer Verarbeitungskette. LangChain Expression Language (LCEL) ist das **neue Programmiermodell in LangChain v0.3+**, das die Entwicklung von LLM-Anwendungen vereinfacht.\n"],"metadata":{"id":"cEEMxhSBHrXA"},"id":"cEEMxhSBHrXA"},{"cell_type":"markdown","source":["\n","**Hier die Kernpunkte:**\n","\n","1. Was ist LCEL?\n","```python\n","# LCEL nutzt den Pipe-Operator (|) für klare Verkettungen\n","chain = prompt | model | output_parser\n","```\n","- Eine deklarative Sprache zum Verketten von LangChain-Komponenten\n","- Ermöglicht linearen Datenfluss zwischen Komponenten\n","- Basiert auf dem Pipe-Operator (|) für intuitive Verbindungen\n","\n","2. Warum LCEL nutzen?\n","- Bessere Lesbarkeit des Codes\n","- Einfachere Wartung und Debugging\n","- Verbesserte Performance durch optimierte Ausführung\n","- Bessere Typsicherheit und Fehlererkennung\n","- Unterstützt modernes Streaming und Async-Operationen\n","\n","3. Praktisches Beispiel:"],"metadata":{"id":"ltTQItwXH6w8"},"id":"ltTQItwXH6w8"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Simple Chain\n","</font></p>"],"metadata":{"id":"BndZGbyWKs1C"},"id":"BndZGbyWKs1C"},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain_openai import ChatOpenAI\n","\n","# Prompt-Template\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"user\", \"{input}\")\n","])\n","\n","#  Einfache LCEL-Kette\n","chain = prompt | chat_model\n","\n","# Ausführung\n","result = chain.invoke({\"input\": \"Erkläre LangChain Expression Language.\"})"],"metadata":{"id":"PsWtjWROIW9d","executionInfo":{"status":"ok","timestamp":1747679589414,"user_tz":-120,"elapsed":6473,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"PsWtjWROIW9d","execution_count":35,"outputs":[]},{"cell_type":"code","source":["display(Markdown(\"## 📣 Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(result.content))"],"metadata":{"id":"VWYAG9aWJpZd","colab":{"base_uri":"https://localhost:8080/","height":418},"executionInfo":{"status":"ok","timestamp":1747679589450,"user_tz":-120,"elapsed":6,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"75e972d0-e0f6-4db8-94b6-71f72adb5274"},"id":"VWYAG9aWJpZd","execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## 📣 Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"LangChain Expression Language (LCEL) ist eine spezialisierte Sprache, die in der LangChain-Bibliothek verwendet wird, um komplexe logische Ausdrücke zu formulieren und zu evaluieren. LangChain selbst ist eine Framework-Umgebung, die Entwicklern hilft, mit Sprachmodellen zu arbeiten und diese in verschiedene Anwendungen zu integrieren, sei es zur Automatisierung, Datenverarbeitung oder zur Entwicklung von Chatbots.\n\nHier sind einige wesentliche Punkte zu LCEL:\n\n1. **Einfachheit**: Die Sprache wurde so gestaltet, dass sie einfach zu lernen und zu verwenden ist. Auch Entwickler, die mit Programmierung nicht ganz so vertraut sind, können sie nutzen.\n\n2. **Flexibilität**: LCEL ermöglicht es, komplexe Bedingungen und logische Ausdrücke zu formulieren, die spezifische Antworten oder Aktionen auslösen können, abhängig von den Eingabedaten.\n\n3. **Integration**: Die Sprache ist nahtlos in die LangChain-Umgebung integriert, sodass Entwicklern die Möglichkeit geboten wird, Sprachmodelle effektiv zu steuern und deren Outputs zu verarbeiten.\n\n4. **Beispiele**: Mit LCEL können Entwickler Bedingungen wie \"Wenn die Benutzeranfrage das Wort 'Hilfe' enthält, dann antworte mit den verfügbaren Optionen\" formulieren.\n\n5. **Anwendungsfälle**: LCEL kann in verschiedenen Anwendungen eingesetzt werden, von der Bearbeitung natürlicher Sprache über die Automatisierung von Geschäftsvorgängen bis hin zur Entwicklung von interaktiven Chatbots.\n\nWenn du mehr über bestimmte Aspekte oder Anwendungsbeispiele von LangChain Expression Language wissen möchtest, lass es mich wissen! Vielleicht kann ich auch einen Witz über Programmiersprachen einbauen – Programmierer haben schließlich auch manchmal Schwierigkeiten, ihre „Sprache“ zu finden! 😄"},"metadata":{}}]},{"cell_type":"markdown","source":["\n","LCEL ist der empfohlene Weg für alle neuen LangChain-Projekte, da es die Entwicklung vereinfacht und zukunftssicher macht."],"metadata":{"id":"49Wu_XreIZY2"},"id":"49Wu_XreIZY2"},{"cell_type":"markdown","id":"Qy35G1-FvrIR","metadata":{"id":"Qy35G1-FvrIR"},"source":["<p><font color='black' size=\"5\">\n","Sequential Chains\n","</font></p>"]},{"cell_type":"markdown","id":"AWhQwgMbxitn","metadata":{"id":"AWhQwgMbxitn"},"source":["Sequentielle Chain: Linear, ein Input wird schrittweise verarbeitet"]},{"cell_type":"code","execution_count":37,"id":"ceqbCIOCuqng","metadata":{"id":"ceqbCIOCuqng","executionInfo":{"status":"ok","timestamp":1747679589455,"user_tz":-120,"elapsed":3,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Prompts\n","zusammenfassen_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Fasse den folgenden Text prägnant zusammen.\"),\n","    (\"user\", \"{text}\")\n","])\n","\n","uebersetzen_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Übersetze den folgenden Text ins Deutsche.\"),\n","    (\"user\", \"{text}\")\n","])"]},{"cell_type":"code","execution_count":38,"id":"kddZQ3rMv0Hw","metadata":{"id":"kddZQ3rMv0Hw","executionInfo":{"status":"ok","timestamp":1747679589462,"user_tz":-120,"elapsed":2,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Verarbeitungskette\n","\n","# Erste Verarbeitungskette: Zusammenfassung\n","summarization_chain = (\n","    zusammenfassen_template\n","    | chat_model\n","    | (lambda output: {\"text\": output})  # Notwendig, für String-> Dict, wird von translation_chain erwartet\n",")\n","\n","# Zweite Verarbeitungskette: Translation/Übersetzung\n","translation_chain = (\n","    uebersetzen_template\n","    | chat_model\n",")\n","\n","# Gesamte Kette: Zusammenfassen und dann Übersetzen\n","sequential_chain = summarization_chain | translation_chain"]},{"cell_type":"code","execution_count":39,"id":"ymYXf4pKwSee","metadata":{"id":"ymYXf4pKwSee","executionInfo":{"status":"ok","timestamp":1747679589468,"user_tz":-120,"elapsed":2,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Input Text\n","text = \"\"\"\n","Machine Learning is a specialized branch of artificial intelligence (AI) dedicated to the development of algorithms and systems that can automatically learn from data and past experiences. Instead of following rigid, pre-defined instructions, machine learning models identify patterns, adapt their behavior, and improve their performance over time based on the information they are exposed to. This ability enables them to make predictions, recognize complex relationships, and solve problems in dynamic environments. Machine learning techniques are widely applied across various fields, including healthcare, finance, transportation, and entertainment, driving innovations such as personalized recommendations, autonomous vehicles, and intelligent diagnostics.\n","\"\"\""]},{"cell_type":"code","source":["# Aufruf der Kette\n","result = sequential_chain.invoke({\"text\": text})"],"metadata":{"id":"WU8Bah_MNFSS","executionInfo":{"status":"ok","timestamp":1747679592525,"user_tz":-120,"elapsed":3054,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"WU8Bah_MNFSS","execution_count":40,"outputs":[]},{"cell_type":"code","source":["display(Markdown(\"## 📣 Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(result.content))"],"metadata":{"id":"bipXv_2DMdCN","colab":{"base_uri":"https://localhost:8080/","height":134},"executionInfo":{"status":"ok","timestamp":1747679592562,"user_tz":-120,"elapsed":39,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"b094d4de-1f9a-4d4c-db14-3f4e59d32f0f"},"id":"bipXv_2DMdCN","execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## 📣 Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Maschinenlernen ist ein Bereich der künstlichen Intelligenz, der Algorithmen entwickelt, die aus Daten und Erfahrungen lernen können. Anstatt festen Anweisungen zu folgen, erkennen diese Modelle Muster, passen ihr Verhalten an und verbessern sich über die Zeit. Sie finden Anwendung in vielen Bereichen wie Gesundheitswesen, Finanzen, Verkehr und Unterhaltung und ermöglichen Innovationen wie personalisierte Empfehlungen und autonome Fahrzeuge."},"metadata":{}}]},{"cell_type":"markdown","id":"R23C1DI1wi1X","metadata":{"id":"R23C1DI1wi1X"},"source":["<p><font color='black' size=\"5\">\n","Q&A-Chain\n","</font></p>"]},{"cell_type":"markdown","id":"-pshkn53xc7T","metadata":{"id":"-pshkn53xc7T"},"source":["QA Chain: Kontextbasiert, kombiniert mehrere Inputs für eine einzelne Verarbeitung"]},{"cell_type":"code","execution_count":42,"id":"3I8ucKu7wAoo","metadata":{"id":"3I8ucKu7wAoo","executionInfo":{"status":"ok","timestamp":1747679592567,"user_tz":-120,"elapsed":2,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Q&A Prompt\n","qa_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Beantworte die Frage basierend auf dem gegebenen Kontext.\"),\n","    (\"user\", \"\"\"\n","    Kontext: {context}\n","\n","    Frage: {question}\n","    \"\"\")\n","])"]},{"cell_type":"code","execution_count":43,"id":"9iqmLBYSwpiX","metadata":{"id":"9iqmLBYSwpiX","executionInfo":{"status":"ok","timestamp":1747679592571,"user_tz":-120,"elapsed":2,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Verkettung\n","qa_chain = qa_template | chat_model"]},{"cell_type":"code","execution_count":44,"id":"R0lVOJaSw2a9","metadata":{"id":"R0lVOJaSw2a9","executionInfo":{"status":"ok","timestamp":1747679592576,"user_tz":-120,"elapsed":2,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Inhalte\n","context = \"\"\"\n","Python ist eine beliebte Programmiersprache für Machine Learning.\n","Sie bietet viele Bibliotheken wie TensorFlow und PyTorch.\n","\"\"\"\n","question = \"Welche ML-Bibliotheken gibt es für Python?\""]},{"cell_type":"code","source":["# Aufruf\n","result = qa_chain.invoke({\n","    \"context\": context,\n","    \"question\": question\n","})"],"metadata":{"id":"hM1ADYiFNZ9X","executionInfo":{"status":"ok","timestamp":1747679596883,"user_tz":-120,"elapsed":4304,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"hM1ADYiFNZ9X","execution_count":45,"outputs":[]},{"cell_type":"code","execution_count":46,"id":"bxFOmczGyPPN","metadata":{"id":"bxFOmczGyPPN","colab":{"base_uri":"https://localhost:8080/","height":266},"executionInfo":{"status":"ok","timestamp":1747679596913,"user_tz":-120,"elapsed":8,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"354c50da-b4d3-46c5-ca21-12cca50f4563"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## 📣 Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Für Python gibt es mehrere beliebte Bibliotheken für Machine Learning, darunter:\n\n1. **TensorFlow** - Eine weit verbreitete Bibliothek für maschinelles Lernen und tiefes Lernen.\n2. **PyTorch** - Beliebt für seine Flexibilität und Benutzerfreundlichkeit, insbesondere in der Forschung.\n3. **scikit-learn** - Eine Bibliothek für traditionelle Machine Learning-Algorithmen und -Techniken.\n4. **Keras** - Eine benutzerfreundliche API, die über TensorFlow aufgebaut ist und das schnelle Prototyping von Deep-Learning-Modellen ermöglicht.\n5. **XGBoost** - Eine leistungsstarke Bibliothek für Gradient Boosting, die häufig für Wettbewerbe im Bereich Machine Learning verwendet wird.\n6. **LightGBM** - Eine weitere Gradient-Boosting-Bibliothek, die für ihre Geschwindigkeit und Effizienz bekannt ist.\n7. **CatBoost** - Eine Gradient-Boosting-Bibliothek, die speziell für kategoriale Daten entwickelt wurde.\n\nDiese Bibliotheken bieten eine Vielzahl von Funktionen und Werkzeugen, um unterschiedliche Anforderungen im Bereich Machine Learning zu erfüllen."},"metadata":{}}],"source":["display(Markdown(\"## 📣 Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(result.content))"]},{"cell_type":"markdown","source":["## 2.5 OutputParser"],"metadata":{"id":"LVqVL9CWRXU7"},"id":"LVqVL9CWRXU7"},{"cell_type":"markdown","source":["Ein OutputParser ist ein Konzept in LangChain, das hilft, die Antworten eines Modells richtig weiterzuverarbeiten. Es nimmt die **rohe Ausgabe** (zum Beispiel einen langen Text) und formt sie in ein **bestimmtes Format**, das später in der Anwendung leichter benutzt werden kann."],"metadata":{"id":"EVBcCwJERfaj"},"id":"EVBcCwJERfaj"},{"cell_type":"code","source":["from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n","\n","# Einfacher Prompt ohne explizite Formatvorgabe\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent. Antworte bitte im JSON-Format mit den Feldern 'name' und 'alter'.\"),\n","    (\"user\", \"Bitte gib Name und Alter einer Person an.\")\n","])\n","\n","# Struktur der Antwort definieren\n","response_schemas = [\n","    ResponseSchema(name=\"name\", description=\"Der vollständige Name der Person.\"),\n","    ResponseSchema(name=\"alter\", description=\"Das Alter der Person in Jahren.\")\n","]\n","\n","# StructuredOutputParser erstellen\n","output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n","\n","\n","# Modell\n","chat_model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n","\n","# LCEL-Chain\n","chain = prompt | chat_model | output_parser\n","\n","# Ausführen\n","result = chain.invoke({\"input\": \"\"})\n","\n","print(result)"],"metadata":{"id":"QKtYSBLhVlts","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747679599737,"user_tz":-120,"elapsed":2820,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"27a03dfd-6252-4ca3-ddea-99ec98b8e66b"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["{'name': 'Max Mustermann', 'alter': 30}\n"]}],"id":"QKtYSBLhVlts"},{"cell_type":"markdown","source":["## 2.6 Runnables\n"],"metadata":{"id":"KRqrnpMx3Hwf"},"id":"KRqrnpMx3Hwf"},{"cell_type":"markdown","source":["<img src=\"https://raw.githubusercontent.com/ralf-42/Image/main/langchain_prozess.png\" width=\"207\" alt=\"Avatar\">"],"metadata":{"id":"LFOITVoEzpkm"},"id":"LFOITVoEzpkm"},{"cell_type":"markdown","source":["In der oben gezeigten Grafik sieht man die drei wichtigsten Runnables: Prompt, LLM und Parser, die zusammen eine Kette bilden.\n","\n","Vorteile von Runnables:\n","\n","+ Gleiche Bedienung: Alle Runnables (Prompt, LLM, Parser) funktionieren nach demselben Prinzip\n","+ Einfaches Verbinden: Man kann sie unkompliziert zu einer Kette zusammenfügen - genau wie im Bild gezeigt\n","+ Anpassungsfähig: Sie funktionieren sowohl einzeln als auch in der Gruppe\n","Schrittweise Ausgabe: Ergebnisse können stückweise weitergegeben werden\n","\n","In LangChain sind die wichtigsten Komponenten als Runnables verfügbar:\n","\n","+ Prompts/Templates (der erste Baustein in der Kette)\n","+ LLMs (das Herzstück in der Mitte)\n","+ Output-Parser (verwandelt die LLM-Antwort in ein nutzbares Format)\n","+ Komplette Chains (die ganze Verarbeitungskette)\n","+ Tools (für spezielle Aufgaben)\n","+ Retriever (holen zusätzliche Informationen)\n"],"metadata":{"id":"DnUDR_983M4l"},"id":"DnUDR_983M4l"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Benutzerdefinierte Runnables erstellen\n","</font></p>"],"metadata":{"id":"WWJ7U1iJ6b8Q"},"id":"WWJ7U1iJ6b8Q"},{"cell_type":"markdown","source":["Man kann auch eigene Runnables erstellen, indem man eine Klasse definiert, die das Runnable-Interface implementiert oder eine Funktion mit `RunnableLambda` wrapped:\n"],"metadata":{"id":"PAvbE_TR6g87"},"id":"PAvbE_TR6g87"},{"cell_type":"code","source":["from langchain_core.runnables import RunnableLambda\n","from langchain_core.output_parsers.string import StrOutputParser\n","\n","# 1. Mit RunnableLambda eine einfache Transformation erstellen\n","def text_verdoppeln(eingabe):\n","    return f\"{eingabe} <br> <br>  {eingabe}\"\n","\n","verdoppler = RunnableLambda(text_verdoppeln)\n","\n","# Verwendung des eigenen Runnables\n","verdoppler.invoke(\"Text\")"],"metadata":{"id":"_0wEf_hL6peM","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1747680851643,"user_tz":-120,"elapsed":34,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"4b7526fe-449e-45db-d110-7ec3b4c707b1"},"id":"_0wEf_hL6peM","execution_count":92,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Text <br> <br>  Text'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":92}]},{"cell_type":"code","source":["# Einfacher Prompt ohne explizite Formatvorgabe\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent. Antworte kurz.\"),\n","    (\"user\", \"Eingabe: {eingabe}\")\n","])\n","\n","# In eine Kette einbauen - Korrektur der Pipe-Operatoren\n","runnable_chain_mit_verdoppler = prompt | chat_model | StrOutputParser() | verdoppler\n","\n","# Test der Kette\n","result = runnable_chain_mit_verdoppler.invoke({\"eingabe\": \"Wer war Einstein?\"})"],"metadata":{"id":"NvZdOf1961xb","executionInfo":{"status":"ok","timestamp":1747680855310,"user_tz":-120,"elapsed":1045,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"NvZdOf1961xb","execution_count":93,"outputs":[]},{"cell_type":"code","source":["display(Markdown(\"## 📣 Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(result))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":169},"id":"_elA0imx2HZA","executionInfo":{"status":"ok","timestamp":1747680856075,"user_tz":-120,"elapsed":83,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"904a431f-a6aa-4bc8-f2b0-693e0bcf76c2"},"id":"_elA0imx2HZA","execution_count":94,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## 📣 Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Albert Einstein war ein theoretischer Physiker, der für seine Relativitätstheorie bekannt ist. Er wurde 1879 in Deutschland geboren und gilt als einer der bedeutendsten Wissenschaftler des 20. Jahrhunderts.  <br> <br>  Albert Einstein war ein theoretischer Physiker, der für seine Relativitätstheorie bekannt ist. Er wurde 1879 in Deutschland geboren und gilt als einer der bedeutendsten Wissenschaftler des 20. Jahrhunderts. "},"metadata":{}}]},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Runnable-Methoden\n","</font></p>\n","\n","Jedes Runnable unterstützt standardmäßig folgende Methoden:\n","\n","1. **invoke()**: Für einzelne, synchrone Anfragen\n","2. **batch()**: Verarbeitet mehrere Eingaben parallel\n","3. **stream()**: Gibt Teilergebnisse zurück, sobald sie verfügbar sind\n","4. **ainvoke()**: Asynchrone Version von invoke\n","5. **abatch()**: Asynchrone Version von batch\n","6. **astream()**: Asynchrone Version von stream\n","\n"],"metadata":{"id":"CjFvHqWA3S2V"},"id":"CjFvHqWA3S2V"},{"cell_type":"markdown","source":["Hier ein einfaches Beispiel mit den verschiedenen Aufrufmethoden:\n"],"metadata":{"id":"a9zHZAMP3bCL"},"id":"a9zHZAMP3bCL"},{"cell_type":"code","source":["from langchain.prompts import ChatPromptTemplate\n","from langchain_openai import ChatOpenAI\n","from langchain.schema import StrOutputParser\n","\n","# Einfaches Template erstellen\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"user\", \"{eingabe}\")\n","])\n","\n","# Modell definieren\n","model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n","\n","# Output-Parser\n","parser = StrOutputParser()\n","\n","# Runnable-Kette erstellen\n","runnable_chain = prompt | model | parser"],"metadata":{"id":"kQTKHK2D4DBb","executionInfo":{"status":"ok","timestamp":1747681111851,"user_tz":-120,"elapsed":156,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"kQTKHK2D4DBb","execution_count":95,"outputs":[]},{"cell_type":"code","source":["# 1. invoke - Einzelne Anfrage\n","ergebnis = runnable_chain.invoke({\"eingabe\": \"Was ist ein Runnable in LangChain?\"})\n","print(f\"INVOKE-ERGEBNIS:\\n{ergebnis}\\n\")"],"metadata":{"id":"TVWtHSYw5Jla","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747681138788,"user_tz":-120,"elapsed":3228,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"9746bd0a-e643-4617-a715-76273d9de222"},"id":"TVWtHSYw5Jla","execution_count":96,"outputs":[{"output_type":"stream","name":"stdout","text":["INVOKE-ERGEBNIS:\n","In LangChain bezieht sich ein \"Runnable\" auf ein Konzept, das es ermöglicht, verschiedene Komponenten oder Funktionen in einer Kette von Operationen zu kombinieren und auszuführen. Es handelt sich dabei um eine abstrakte Schnittstelle, die es Entwicklern ermöglicht, ihre Logik modular und wiederverwendbar zu gestalten.\n","\n","Ein Runnable kann verschiedene Aufgaben ausführen, wie z.B. Datenverarbeitung, API-Aufrufe oder die Interaktion mit Modellen. Die Idee ist, dass man verschiedene Runnables miteinander verknüpfen kann, um komplexe Workflows zu erstellen, ohne sich um die Details der Implementierung jeder einzelnen Komponente kümmern zu müssen.\n","\n","Das macht es einfacher, verschiedene Teile einer Anwendung zu testen, zu debuggen und zu optimieren. Wenn du also das nächste Mal einen Runnable in LangChain siehst, denk daran: Es ist wie ein Schweizer Taschenmesser für deine Programmierlogik – vielseitig und immer bereit, dir zu helfen! 🛠️😄\n","\n"]}]},{"cell_type":"code","source":["# 2. batch - Mehrere Anfragen parallel\n","ergebnisse = runnable_chain.batch([\n","    {\"eingabe\": \"Erkläre den Begriff LCEL kurz.\"},\n","    {\"eingabe\": \"Was sind die Hauptvorteile von Runnables?\"}\n","])\n","print(\"BATCH-ERGEBNISSE:\")\n","for i, ergebnis in enumerate(ergebnisse, 1):\n","    print(f\"Anfrage {i}: {ergebnis[:50]}...\\n\")"],"metadata":{"id":"UAet4KSq5K2D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747681168285,"user_tz":-120,"elapsed":7221,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"b787ea24-715c-41da-bb93-a514489e4ec0"},"id":"UAet4KSq5K2D","execution_count":97,"outputs":[{"output_type":"stream","name":"stdout","text":["BATCH-ERGEBNISSE:\n","Anfrage 1: LCEL steht für \"Low Carbon Energy Lab\" und bezieht...\n","\n","Anfrage 2: Runnables in Java sind eine praktische Möglichkeit...\n","\n"]}]},{"cell_type":"code","source":["# 3. stream - Schrittweise Ausgabe\n","print(\"STREAM-ERGEBNIS:\")\n","for chunk in runnable_chain.stream({\"eingabe\": \"Zähle von 1 bis 5 auf. Mache eine Pause von 3 Sekunden nach jedem Schritt.\"}):\n","    print(chunk, end=\"🫗\")\n","print(\"\\n\")"],"metadata":{"id":"VJn4e_IX5L-b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747681300595,"user_tz":-120,"elapsed":1908,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"b480a958-5cb6-4495-ccd9-6bde6d53a7b6"},"id":"VJn4e_IX5L-b","execution_count":101,"outputs":[{"output_type":"stream","name":"stdout","text":["STREAM-ERGEBNIS:\n","🫗Natürlich🫗!🫗 Hier🫗 ist🫗 die🫗 Z🫗ähl🫗ung🫗 von🫗 🫗1🫗 bis🫗 🫗5🫗 mit🫗 Pa🫗usen🫗:\n","\n","🫗1🫗...🫗 (🫗Pause🫗 von🫗 🫗3🫗 Sekunden🫗)🫗  \n","🫗2🫗...🫗 (🫗Pause🫗 von🫗 🫗3🫗 Sekunden🫗)🫗  \n","🫗3🫗...🫗 (🫗Pause🫗 von🫗 🫗3🫗 Sekunden🫗)🫗  \n","🫗4🫗...🫗 (🫗Pause🫗 von🫗 🫗3🫗 Sekunden🫗)🫗  \n","🫗5🫗...🫗 \n","\n","🫗Und🫗 voilà🫗!🫗 Du🫗 hast🫗 bis🫗 🫗5🫗 gez🫗ählt🫗!🫗 🎉🫗 Wenn🫗 du🫗 noch🫗 mehr🫗 zählen🫗 oder🫗 etwas🫗 anderes🫗 machen🫗 möchtest🫗,🫗 sag🫗 einfach🫗 Bes🫗cheid🫗!🫗🫗\n","\n"]}]},{"cell_type":"markdown","id":"VlARJEowAAIR","metadata":{"id":"VlARJEowAAIR"},"source":["# 3 | Überblick LangChain Konzepte\n","---"]},{"cell_type":"markdown","id":"g5B7bWJKBB-C","metadata":{"id":"g5B7bWJKBB-C"},"source":["Konzepte von LangChain sind grundlegende Bausteine und Prinzipien, die das Framework ausmachen und seine Funktionsweise ermöglichen. Hier sind einige der wichtigsten Konzepte:\n","\n","+ Chat-Modelle (**Chat models**): LLMs, die über eine Chat-API verfügbar sind und Sequenzen von Nachrichten verarbeiten1.\n","+ Nachrichten (**Messages**): Kommunikationseinheiten in Chat-Modellen für Ein- und Ausgabe1.\n","+ Chat-Verlauf (**Chat history**): Eine Sequenz von Nachrichten, die eine Konversation darstellt.\n","+ Tools (**Tools**): Funktionen mit definierten Schemata für Name, Beschreibung und Argumente.\n","+ Strukturierte Ausgabe (**Structured output**): Technik, um Chat-Modelle in strukturierten Formaten antworten zu lassen.\n","+ Retrieval Augmented Generation (**RAG**): Technik zur Verbesserung von Sprachmodellen durch Kombination mit externen Wissensbasen.\n","+ Prompt-Vorlagen (**Prompt template**s): Komponenten zur Erstellung strukturierter Prompts für LLMs.\n","+ Chains (**Chains**): Verknüpfungen mehrerer LLMs oder anderer Komponenten für komplexere Anwendungen.\n","+ Agenten (**Agents**): Nutzen Sprachmodelle, um Aktionssequenzen auszuwählen und mit externen Ressourcen zu interagieren.\n","+ Retriever (**Retriever**): Komponenten, die relevante Dokumente aus einer Wissensbasis abrufen."]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Anbieterbezogene APIs in LangChain\n","</font></p>"],"metadata":{"id":"Gm6A7xSEzNVv"},"id":"Gm6A7xSEzNVv"},{"cell_type":"markdown","source":["| API-Name in LangChain | Anbieter             | Bemerkung                                 |\n","|-----------------------|----------------------|-------------------------------------------|\n","| `ChatOpenAI`           | OpenAI               | Chat-Modelle (`gpt-3.5-turbo`, `gpt-4`, ...) |\n","| `OpenAI`               | OpenAI               | Text Completion (`text-davinci-003`, alt)  |\n","| `ChatAnthropic`        | Anthropic            | Claude-Modelle (`Claude 3`, etc.)          |\n","| `ChatGoogleGenerativeAI` | Google (Gemini)      | Gemini-Modelle (`gemini-1.5-pro`, etc.)    |\n","| `ChatCohere`           | Cohere               | Command-Modelle                           |\n","| `ChatMistralAI`        | Mistral              | Mistral-Modelle (`Mistral 7B`, Mixtral etc.) |\n","| `ChatFireworks`        | Fireworks AI         | Zugriff auf viele Open-Weight-Modelle      |\n","| `ChatAzureOpenAI`      | Azure (Microsoft)    | OpenAI-Modelle über Azure API              |\n","| `BedrockLLM`           | AWS Bedrock          | Zugang zu Claude, Titan, Jurassic, etc.    |"],"metadata":{"id":"MjHiOl3dzLyc"},"id":"MjHiOl3dzLyc"},{"cell_type":"markdown","id":"F_5gn03b46rC","metadata":{"id":"F_5gn03b46rC"},"source":["# A | Aufgabe\n","---"]},{"cell_type":"markdown","id":"36LyGMmo5jb3","metadata":{"id":"36LyGMmo5jb3"},"source":["Die Aufgabestellungen unten bieten Anregungen, Sie können aber auch gerne eine andere Herausforderung angehen."]},{"cell_type":"markdown","id":"Je-szbGU7IJ-","metadata":{"id":"Je-szbGU7IJ-"},"source":["\n","<p><font color='black' size=\"5\">\n","Erstellen einer LangChain-Kette zur Textanalyse\n","</font></p>"]},{"cell_type":"markdown","id":"kevePvA-7PzW","metadata":{"id":"kevePvA-7PzW"},"source":["Entwickeln Sie eine Verarbeitungskette, die einen Text analysiert und verschiedene Informationen darüber extrahiert.\n","\n","**Schritte:**\n","1. Erstelle ein ChatPromptTemplate, das ein LLM anweist, einen Text zu analysieren\n","2. Die Analyse soll folgende Aspekte umfassen:\n","   - Hauptthema des Textes\n","   - Tonalität (formal, informell, etc.)\n","   - Schlüsselwörter (5-10)\n","   - Kurze Zusammenfassung (max. 3 Sätze)\n","3. Formatiere die Ausgabe strukturiert mit Markdown-Überschriften\n","4. Teste die Kette mit mindestens zwei verschiedenen Texten\n","\n","\n"]},{"cell_type":"markdown","id":"6oBvtYws7qpB","metadata":{"id":"6oBvtYws7qpB"},"source":["<p><font color='black' size=\"5\">\n","Few-Shot-Learning für Textklassifikation\n","</font></p>"]},{"cell_type":"markdown","id":"gI7alQcE79ER","metadata":{"id":"gI7alQcE79ER"},"source":["Erstellen Sie ein System, das mithilfe von Few-Shot-Learning Texte in vorgegebene Kategorien klassifiziert.\n","\n","**Schritte:**\n","1. Definiere 3-5 Kategorien für die Klassifikation (z.B. Sport, Politik, Technologie, Kultur)\n","2. Erstelle einen FewShotPromptTemplate mit Beispielen für jede Kategorie\n","3. Entwickle eine Chain, die neue Texte klassifiziert\n","4. Implementiere eine Funktion, die neben der Kategorie auch eine Begründung für die Einordnung liefert\n","5. Teste das System mit verschiedenen Texten, die nicht in den Beispielen vorkommen\n"]},{"cell_type":"markdown","id":"l35x2g1e8fa9","metadata":{"id":"l35x2g1e8fa9"},"source":["<p><font color='black' size=\"5\">\n","Q&A-System mit Sequential Chain\n","</font></p>"]},{"cell_type":"markdown","id":"qHFRGWmP8gga","metadata":{"id":"qHFRGWmP8gga"},"source":["Entwickeln Sie ein System, das Fragen zu einem gegebenen Kontext beantwortet, aber zuerst den Kontext zusammenfasst und dann die Frage beantwortet.\n","\n","\n","**Schritte:**\n","\n","+ Erstellen Sie zwei Templates:\n","    + Ein Template zur Zusammenfassung des Kontexts\n","    + Ein Template zur Beantwortung einer Frage basierend auf der Zusammenfassung\n","+ Verbinden Sie die Templates in einer sequentiellen Kette\n","+ Implementieren Sie einen Mechanismus, der die Zusammenfassung und die Antwort getrennt zurückgibt\n","+ Nutzen Sie LCEL (LangChain Expression Language) für die Verkettung\n","+ Testen Sie das System mit verschiedenen Kontexten und Fragen"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":5}
{"cells":[{"cell_type":"markdown","id":"37rMyCQz2JNw","metadata":{"id":"37rMyCQz2JNw"},"source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)"]},{"cell_type":"markdown","id":"R5CfUEMJdvFQ","metadata":{"id":"R5CfUEMJdvFQ"},"source":["<p><font size=\"5\" color='grey'> <b>\n","LangChain 101\n","</b></font> </br></p>\n","\n","---"]},{"cell_type":"code","execution_count":null,"id":"dfdhPIzcEYRG","metadata":{"cellView":"form","collapsed":true,"id":"dfdhPIzcEYRG","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  Colab-Umfeld</font> </br></p>\n","# Installierte Python Version\n","import sys\n","print(f\"Python Version: \",sys.version)\n","# Installierte LangChain Bibliotheken\n","print()\n","print(\"Installierte LangChain Bibliotheken:\")\n","\n","!pip list | grep '^langchain'\n","# Unterdr√ºckt die \"DeprecationWarning\" von LangChain f√ºr die Memory-Funktionden\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")"]},{"cell_type":"code","execution_count":null,"id":"WD3Wwr6sESX8","metadata":{"cellView":"form","id":"WD3Wwr6sESX8"},"outputs":[],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  SetUp API-Keys (setup_api_keys)</font> </br></p>\n","def setup_api_keys():\n","    \"\"\"Konfiguriert alle ben√∂tigten API-Keys aus Google Colab userdata\"\"\"\n","    from google.colab import userdata\n","    import os\n","    from os import environ\n","\n","    # Dictionary der ben√∂tigten API-Keys\n","    keys = {\n","        'OPENAI_API_KEY': 'OPENAI_API_KEY',\n","        'HF_TOKEN': 'HF_TOKEN',\n","        # Weitere Keys bei Bedarf\n","    }\n","\n","    # Keys in Umgebungsvariablen setzen\n","    for env_var, key_name in keys.items():\n","        environ[env_var] = userdata.get(key_name)\n","\n","    return {k: environ[k] for k in keys.keys()}\n","\n","# Verwendung\n","all_keys = setup_api_keys()\n","# Bei Bedarf einzelne Keys direkt zugreifen\n","# WEATHER_API_KEY = all_keys['WEATHER_API_KEY']"]},{"cell_type":"markdown","id":"91ee89a4-477b-41ce-a511-aadd4c2fd547","metadata":{"id":"91ee89a4-477b-41ce-a511-aadd4c2fd547"},"source":["# 1 | Was ist LangChain?\n","---"]},{"cell_type":"markdown","id":"b5004f8c","metadata":{"id":"b5004f8c"},"source":["LangChain ist ein Framework zur **Entwicklung von Anwendungen mit gro√üen Sprachmodellen** (LLMs). Es vereinfacht die Integration von LLMs in eigene Anwendungen durch:\n","\n","- Modulare Komponenten f√ºr verschiedene Aufgaben\n","- Vorgefertigte Ketten (Chains) f√ºr komplexe Workflows\n","- Einfache Integration externer Datenquellen\n","- Werkzeuge f√ºr das Speichern von Konversationskontexten\n","\n"]},{"cell_type":"markdown","id":"tl73bT-kKkia","metadata":{"id":"tl73bT-kKkia"},"source":["[Einf√ºhrung](https://python.langchain.com/docs/introduction/)   \n","[Konzepte](https://python.langchain.com/docs/concepts/)   \n","[API-References](https://python.langchain.com/api_reference/index.html)   \n","[Integrations](https://python.langchain.com/docs/integrations/providers/)\n","\n","---\n","\n","[Tutorials](https://python.langchain.com/docs/tutorials/)   \n","[How-to-Guides](https://python.langchain.com/docs/how_to/)   "]},{"cell_type":"code","execution_count":null,"id":"JrgQ46eLphEv","metadata":{"id":"JrgQ46eLphEv"},"outputs":[],"source":["!uv pip install --system --prerelease allow -q langchain_community langchain_openai"]},{"cell_type":"markdown","id":"fGV8FwGmIwmR","metadata":{"id":"fGV8FwGmIwmR"},"source":["# 2 | Kernkonzepte\n","---\n","\n","\n"]},{"cell_type":"markdown","id":"EouwC_wdZb-c","metadata":{"id":"EouwC_wdZb-c"},"source":["## 2.1 Message-Typen"]},{"cell_type":"markdown","id":"yS2mM2J1aXlY","metadata":{"id":"yS2mM2J1aXlY"},"source":["Beim Einsatz von Large Language Models (LLMs) wie GPT gibt es drei Hauptarten von Nachrichten (Messages), die die Interaktion mit dem Modell steuern. Diese Messages definieren, wie das Modell reagiert, welche Eingaben es erh√§lt und welche Ausgaben es generiert.\n","\n"]},{"cell_type":"markdown","id":"sGqRXptbbQ2a","metadata":{"id":"sGqRXptbbQ2a"},"source":["**System Message:**   \n","Die `System Message` dient dazu, das Verhalten des LLMs zu steuern. Sie ist eine nicht sichtbare Nachricht, die dem Modell Anweisungen gibt, wie es sich verhalten soll. Dabei kann sie die Pers√∂nlichkeit, den Tonfall und die Einschr√§nkungen des Modells beeinflussen. Die Systemnachricht funktioniert wie eine Art \"Rollenspiel-Anweisung\", um das Modell in eine gew√ºnschte Rolle zu versetzen.\n","\n","*Merkmale:*\n","- Definiert, wie das Modell reagieren soll\n","- Legt Pers√∂nlichkeit, Verhalten und Einschr√§nkungen fest\n","- Wird zu Beginn einer Sitzung gesetzt und bleibt bestehen\n","- Nicht sichtbar f√ºr den Benutzer\n","\n","*Beispiel einer System Message:*\n","```json\n","{\n","  \"role\": \"system\",\n","  \"content\": \"Du bist ein KI-Assistent, der pr√§zise und informative Antworten in einem professionellen Ton liefert.\"\n","}\n","```\n","Dieses Beispiel weist das Modell an, in einem professionellen Tonfall kurze und informative Antworten zu geben.\n","\n","---\n","\n"]},{"cell_type":"markdown","id":"upsIzpcqbbP7","metadata":{"id":"upsIzpcqbbP7"},"source":["**User Message**   \n","Die `User Message` ist die eigentliche Eingabe des Benutzers. Sie kann eine Frage, eine Aufforderung, kontextrelevanten Inhalt oder ein Befehl sein. Das Modell nutzt diese Nachricht als Ausgangspunkt f√ºr die Generierung einer Antwort.\n","\n","*Merkmale:*\n","- Direkte Eingabe des Benutzers\n","- Kann eine Frage, Aufforderung, kontextrelevanten Inhalt oder einen Befehl enthalten\n","- Basis f√ºr die Antwort des Modells\n","\n","*Beispiel einer User Message:*\n","```json\n","{\n","  \"role\": \"user\",\n","  \"content\": \"Was sind die wichtigsten Unterschiede zwischen KI und maschinellem Lernen?\"\n","}\n","```\n","Hier fragt der Benutzer nach den Unterschieden zwischen KI und maschinellem Lernen.\n","\n","---\n"]},{"cell_type":"markdown","id":"CozrrtYXbhZb","metadata":{"id":"CozrrtYXbhZb"},"source":["**AI Message**   \n","Die `AI Message` ist die Antwort, die das Modell generiert. Sie basiert auf der User Message und den Anweisungen aus der System Message. Die AI Message kann verschiedene Eigenschaften haben, ist aber haupts√§chlich auf den Inhalt fokussiert.\n","\n","*Merkmale:*\n","- Antwort des Modells auf die Benutzeranfrage\n","- Kann verschiedene Eigenschaften haben (z. B. L√§nge, Stil)\n","- Haupts√§chlich inhaltlich relevant\n","\n","*Beispiel einer AI Message:*\n","```json\n","{\n","  \"role\": \"assistant\",\n","  \"content\": \"K√ºnstliche Intelligenz (KI) ist ein weites Feld, das verschiedene Teilgebiete umfasst, darunter auch das maschinelle Lernen (ML). ML konzentriert sich auf die Erstellung von Modellen, die aus Daten lernen und Vorhersagen treffen.\"\n","}\n","```\n","Hier gibt das Modell eine inhaltliche Antwort auf die Frage des Benutzers.\n","\n","---\n","\n"]},{"cell_type":"markdown","id":"PiHnhKHVbofP","metadata":{"id":"PiHnhKHVbofP"},"source":["**Zusammenhang**   \n","Die drei Message-Typen arbeiten zusammen, um eine sinnvolle Interaktion mit dem Modell zu erm√∂glichen:\n","\n","+  **System Message** legt die Regeln und das Verhalten des Modells fest.\n","+ **User Message** stellt eine Anfrage oder einen Befehl.\n","+ **AI Message** gibt die generierte Antwort basierend auf den vorherigen Nachrichten.\n","\n","**Beispiel f√ºr eine vollst√§ndige Konversation:**    \n","```json\n","[\n","  {\n","    \"role\": \"system\",\n","    \"content\": \"Sie sind ein KI-Assistent, der pr√§zise und informative Antworten in einem professionellen Ton liefert.\"\n","  },\n","  {\n","    \"role\": \"user\",\n","    \"content\": \"Kannst der erkl√§ren, was gro√üe Sprachtmodelle sind?\"\n","  },\n","  {\n","    \"role\": \"assistant\",\n","    \"content\": \"Sicher! Large Language Models (LLMs) sind fortschrittliche KI-Systeme, die mit riesigen Textdatenmengen trainiert werden. Sie k√∂nnen menschen√§hnliche Texte verstehen und generieren und sind daher f√ºr verschiedene Anwendungen wie Chatbots, Content-Erstellung und mehr n√ºtzlich.\"\n","  }\n","]\n","```\n","In diesem Beispiel wurde das Modell so eingestellt, dass es in einem freundlichen Tonfall antwortet, dann erh√§lt es eine Benutzeranfrage und gibt eine passende Antwort.\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"id":"K2lC_JK3guaj","metadata":{"id":"K2lC_JK3guaj"},"outputs":[],"source":["from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n","from langchain_openai import ChatOpenAI\n","from IPython.display import display, Markdown"]},{"cell_type":"code","source":["messages = [\n","    SystemMessage(\n","        content=\"Du bist ein kompetenter und hilfreicher Assistent.\"\n","    ),\n","    HumanMessage(\n","        content=\"Nenne mir die Hauptstadt von Usbekistan?\"\n","    ),\n","]"],"metadata":{"id":"zIOnQp7j9Drq"},"id":"zIOnQp7j9Drq","execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(messages)"],"metadata":{"id":"6KDVJpDa92h6"},"id":"6KDVJpDa92h6","execution_count":null,"outputs":[]},{"cell_type":"code","source":["MODEL = 'gpt-4o-mini'\n","TEMPERATURE = 0.0\n","\n","llm = ChatOpenAI(model=MODEL, temperature= TEMPERATURE)\n","response = llm.invoke(messages)"],"metadata":{"id":"z0yFqHlZ9Kts"},"id":"z0yFqHlZ9Kts","execution_count":null,"outputs":[]},{"cell_type":"code","source":["display(Markdown(\"## üì£ Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response.content))"],"metadata":{"id":"UvDvAWVd9WiF"},"id":"UvDvAWVd9WiF","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Die Liste `messages` wird um den  der letzen Abfrage und eine Nachfrage erg√§nzt"],"metadata":{"id":"TPlBQklE-MnL"},"id":"TPlBQklE-MnL"},{"cell_type":"code","execution_count":null,"id":"2vJuMVHZ1jJ8","metadata":{"id":"2vJuMVHZ1jJ8"},"outputs":[],"source":["messages.append(response)\n","messages.append(HumanMessage(content=\"Bis Du sicher? Ich glaube, sie wurde k√ºrzlich umbenannt.\"))"]},{"cell_type":"code","source":["for i, message in enumerate(messages, 1):\n","    print(f\"{i}. {message}\")"],"metadata":{"id":"eMKmypC59kIR"},"id":"eMKmypC59kIR","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"A8Z7DZRH5TFr","metadata":{"id":"A8Z7DZRH5TFr"},"source":["Die Konversationsliste `messages` wird wieder an das Modell √ºbergeben."]},{"cell_type":"code","execution_count":null,"id":"9qFxkEmAz1w1","metadata":{"id":"9qFxkEmAz1w1"},"outputs":[],"source":["response = llm.invoke(messages)"]},{"cell_type":"code","source":["display(Markdown(\"## üì£ Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response.content))"],"metadata":{"id":"aBbh-siY-jU8"},"id":"aBbh-siY-jU8","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"Iw3Ovw5mcFcY","metadata":{"id":"Iw3Ovw5mcFcY"},"source":["**Fazit**    \n","Die System-, User- und AI-Message sind essenzielle Bestandteile jeder Interaktion mit einem LLM. W√§hrend die System Message das Verhalten vorgibt, stellt die User Message die eigentliche Anfrage dar, auf die das Modell mit einer AI Message antwortet. Dieses Konzept ist zentral f√ºr den Einsatz von LLMs in Chatbots, KI-gest√ºtzten Assistenten und anderen interaktiven Anwendungen.\n","\n"]},{"cell_type":"markdown","id":"-JwJD0rLIyT8","metadata":{"id":"-JwJD0rLIyT8"},"source":["## 2.2 Prompts\n","\n","Prompts sind die Eingaben bzw. Auftr√§ge an das LLM. LangChain bietet verschiedene Template-Systeme:"]},{"cell_type":"markdown","id":"HqTUeMW20084","metadata":{"id":"HqTUeMW20084"},"source":["<p><font color='black' size=\"5\">\n","Simple Prompt\n","</font></p>"]},{"cell_type":"code","execution_count":null,"id":"e1926d03","metadata":{"id":"e1926d03"},"outputs":[],"source":["from langchain.prompts import PromptTemplate, ChatPromptTemplate\n","from langchain.prompts.few_shot import FewShotPromptTemplate"]},{"cell_type":"code","source":["# 1. Einfaches Template mit Platzhaltern\n","simple_explanation_prompt = PromptTemplate.from_template(\n","    \"Erkl√§re {thema} in einfachen Worten.\"\n",")"],"metadata":{"id":"koh0-_3r_HxG"},"id":"koh0-_3r_HxG","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"C92-v-s70Q3k","metadata":{"id":"C92-v-s70Q3k"},"outputs":[],"source":["print(simple_explanation_prompt.format(thema=\"Machine Learning\"))"]},{"cell_type":"code","source":["thema = \"Generative KI\"\n","prompt = simple_explanation_prompt.format(thema=thema)\n","response = llm.invoke(prompt)"],"metadata":{"id":"nFrCjL6j_xo1"},"id":"nFrCjL6j_xo1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["display(Markdown(\"## üì£ Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response.content))"],"metadata":{"id":"lmEE-3NDAE15"},"id":"lmEE-3NDAE15","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"_o2EkUkN3hEX","metadata":{"id":"_o2EkUkN3hEX"},"source":["\n","<p><font color='black' size=\"5\">\n","Eingabeaufforderungsvorlagen\n","</font></p>"]},{"cell_type":"markdown","id":"gPvElGbrdOHq","metadata":{"id":"gPvElGbrdOHq"},"source":["\n","\n","Mit LangChain kann man Operationsketten erstellen, die als Teil einer LLM-f√§higen Anwendung ausgef√ºhrt werden. Eine dieser Operationen ist eine Eingabeaufforderungsvorlage, mit der man Text in eine zuvor erstellte Eingabeaufforderung einf√ºgen k√∂nnen. In diesem Beispiel wird eine Eingabeaufforderungsvorlage erstellt, die das Modell auffordert, einen zuf√§lligen Titel f√ºr einen Blogbeitrag zu erstellen.\n","\n","```\n","Gibt nur den Titel eines Blog-Beitragsartikels zum Thema {topic} in {language} zur√ºck.\n","```\n","\n","Um dieses Ziel zu erreichen, verwenden wir ein **PromptTemplate**-Objekt."]},{"cell_type":"code","execution_count":null,"id":"L-g3zOXBmulc","metadata":{"id":"L-g3zOXBmulc"},"outputs":[],"source":["topic = \"Haustiere f√ºr Datenwissenschaftler\"\n","language = \"german\""]},{"cell_type":"code","source":["blog_title_prompt = PromptTemplate.from_template(\n","    \"Gib nur den Titel eines Blogbeitrags zum Thema {topic} in {language} zur√ºck.\"\n",")"],"metadata":{"id":"cxGzHqueA3vx"},"id":"cxGzHqueA3vx","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# H√∂here Temperaturen f√ºr mehr Kreativit√§t\n","llm = ChatOpenAI(model=MODEL, temperature=0.7)\n","\n","# Verkettung von Prompt und LLM -  Erl√§uterungen zu Verkettungen siehe weiter unten\n","title_chain = blog_title_prompt | llm\n","\n","# Aufrufen der Kette mit Eingaben\n","response = title_chain.invoke({'topic': topic, 'language': language})"],"metadata":{"id":"D1xptkQOAxwY"},"id":"D1xptkQOAxwY","execution_count":null,"outputs":[]},{"cell_type":"code","source":["display(Markdown(\"## üì£ Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response.content))"],"metadata":{"id":"283uv5DxBd3R"},"id":"283uv5DxBd3R","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"8akKmCCc06nm","metadata":{"id":"8akKmCCc06nm"},"source":["<p><font color='black' size=\"5\">\n","Chat Prompt\n","</font></p>"]},{"cell_type":"code","execution_count":null,"id":"iW9URvON0IXr","metadata":{"id":"iW9URvON0IXr"},"outputs":[],"source":["chat_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"human\", \"Erkl√§re mir {thema}\"),\n","    (\"assistant\", \"Ich erkl√§re dir {thema} gerne.\")\n","])"]},{"cell_type":"code","execution_count":null,"id":"jwrybdk_0agA","metadata":{"id":"jwrybdk_0agA"},"outputs":[],"source":["print(chat_template.format(thema=\"Machine Learning\"))"]},{"cell_type":"code","source":["# Verkettung von Prompt und LLM\n","chat_chain = chat_template | llm\n","\n","# Aufrufen der Kette mit Eingaben\n","thema = \"Machine Learning\"\n","response = chat_chain.invoke({'thema': thema})"],"metadata":{"id":"ZLeSLoy_CLpI"},"id":"ZLeSLoy_CLpI","execution_count":null,"outputs":[]},{"cell_type":"code","source":["display(Markdown(\"## üì£ Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response.content))"],"metadata":{"id":"6irKh0y0C4nu"},"id":"6irKh0y0C4nu","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"Ps344Ejw0-ih","metadata":{"id":"Ps344Ejw0-ih"},"source":["<p><font color='black' size=\"5\">\n","Few-Shot-Learning Prompt\n","</font></p>"]},{"cell_type":"code","execution_count":null,"id":"cEopgOiS0KU_","metadata":{"id":"cEopgOiS0KU_"},"outputs":[],"source":["examples = [\n","    {\"frage\": \"Was ist Python?\", \"antwort\": \"Eine interpretierende Programmiersprache.\"},\n","    {\"frage\": \"Was ist Java?\", \"antwort\": \"Eine objektorientierte Sprache.\"}\n","]"]},{"cell_type":"code","source":["# Prompt f√ºr einzelne Beispiele\n","example_prompt = PromptTemplate.from_template(\n","    \"Frage: {frage}\\nAntwort: {antwort}\"\n",")"],"metadata":{"id":"fHEYrEPcDD9S"},"id":"fHEYrEPcDD9S","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Few-Shot-Prompt\n","few_shot_prompt = FewShotPromptTemplate(\n","    examples=examples,\n","    example_prompt=example_prompt,\n","    prefix=\"Beantworte die Fragen basierend auf den Beispielen:\",\n","    suffix=\"Frage: {input}\\nAntwort:\",\n","    input_variables=[\"input\"]\n",")"],"metadata":{"id":"BpIbLhIhDcUp"},"id":"BpIbLhIhDcUp","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"OdMhepTp0fJU","metadata":{"id":"OdMhepTp0fJU"},"outputs":[],"source":["print(few_shot_prompt.format(input=\"Was ist Cobol?\"))"]},{"cell_type":"code","source":["# geringere Temperaturen f√ºr mehr Sachlichkeit\n","llm = ChatOpenAI(model=MODEL, temperature=0.2)\n","\n","# Verkettung von Prompt und LLM\n","few_shot_chain = few_shot_prompt | llm\n","\n","# Aufrufen der Kette mit Eingaben\n","response = few_shot_chain.invoke({'input': \"Was ist Cobol\"})"],"metadata":{"id":"KZPoIO6gDo23"},"id":"KZPoIO6gDo23","execution_count":null,"outputs":[]},{"cell_type":"code","source":["display(Markdown(\"## üì£ Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response.content))"],"metadata":{"id":"Yr0YleeiD_x7"},"id":"Yr0YleeiD_x7","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"8c859b9f","metadata":{"id":"8c859b9f"},"source":["## 2.3 Modelle\n","\n","LangChain unterst√ºtzt verschiedene LLMs und bietet flexible Konfigurationsm√∂glichkeiten:"]},{"cell_type":"code","execution_count":null,"id":"9e20e1cb","metadata":{"id":"9e20e1cb"},"outputs":[],"source":["from langchain_openai import ChatOpenAI, OpenAI"]},{"cell_type":"code","source":["# Text-Completion-Modell (z. B. f√ºr Aufgaben wie Zusammenfassungen)\n","completion_model = OpenAI(\n","    model_name=\"gpt-3.5-turbo-instruct\",\n","    temperature=0.7,\n","    max_tokens=500\n",")\n","\n","# Chat-Modell f√ºr interaktive Konversationen\n","chat_model = ChatOpenAI(\n","    model_name=\"gpt-4o-mini\",\n","    temperature=0.9\n",")"],"metadata":{"id":"aDGqhlxoENvs"},"id":"aDGqhlxoENvs","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Vergleich ChatOpenAI vs OpenAI\n","</font></p>"],"metadata":{"id":"l4imGktVzcap"},"id":"l4imGktVzcap"},{"cell_type":"markdown","source":["| Aspekt                 | `ChatOpenAI`                                           | `OpenAI`                                              |\n","|-------------------------|--------------------------------------------------------|-------------------------------------------------------|\n","| Typ                     | Chat-Model Wrapper                                     | Text-Completion Wrapper                               |\n","| Hauptzweck              | Gespr√§chsbasierte Interaktion (multi-turn Dialoge)     | Klassische Textvervollst√§ndigung (prompt ‚Üí completion) |\n","| Basis-Modelle           | GPT-Modelle mit Chat-API (`gpt-3.5-turbo`, `gpt-4`, etc.) | GPT-Modelle √ºber Completion-API (`text-davinci-003`, etc.) |\n","| Eingabeformat           | Nachrichtenstruktur (`messages`)                      | Flacher Textprompt (`prompt`)                         |\n","| R√ºckgabeformat          | Strukturiert (Nachricht/Text, Rolleninformationen)     | Einfacher Text (reiner Completion-Text)               |\n","| Kosten                  | G√ºnstiger (bei GPT-3.5, GPT-4 Turbo)                   | Teurer (bei vergleichbarem Modell `text-davinci-003`)  |\n","| Zukunftsperspektive     | **Empfohlen f√ºr neue Projekte**                        | Veraltet, wird seltener empfohlen                     |\n","| Zus√§tzliche Funktionen  | Bessere Steuerung (z.B. Systemnachrichten, Tool-Use)    | Klassische Vervollst√§ndigung ohne Rollen oder Kontext  |"],"metadata":{"id":"wzA3AF28ycyx"},"id":"wzA3AF28ycyx"},{"cell_type":"markdown","source":["## 2.4 OutputParser"],"metadata":{"id":"LVqVL9CWRXU7"},"id":"LVqVL9CWRXU7"},{"cell_type":"markdown","source":["Ein OutputParser ist ein Konzept in LangChain, das hilft, die Antworten eines Modells richtig weiterzuverarbeiten. Es nimmt die rohe Ausgabe (zum Beispiel einen langen Text) und formt sie in ein bestimmtes Format, das sp√§ter in der Anwendung leichter benutzt werden kann."],"metadata":{"id":"EVBcCwJERfaj"},"id":"EVBcCwJERfaj"},{"cell_type":"code","source":["from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n","\n","# Einfacher Prompt ohne explizite Formatvorgabe\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent. Antworte bitte im JSON-Format mit den Feldern 'name' und 'alter'.\"),\n","    (\"human\", \"Bitte gib Name und Alter einer Person an.\")\n","])\n","\n","# Modell\n","chat_model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n","\n","# Struktur der Antwort definieren\n","response_schemas = [\n","    ResponseSchema(name=\"name\", description=\"Der vollst√§ndige Name der Person.\"),\n","    ResponseSchema(name=\"alter\", description=\"Das Alter der Person in Jahren.\")\n","]\n","\n","# StructuredOutputParser erstellen\n","output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n","\n","# LCEL-Chain\n","chain = prompt | chat_model | output_parser\n","\n","# Ausf√ºhren\n","response = chain.invoke({\"input\": \"\"})\n","\n","print(response)"],"metadata":{"id":"QKtYSBLhVlts"},"id":"QKtYSBLhVlts","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"c314ca71","metadata":{"id":"c314ca71"},"source":["## 2.5 Chains (LCEL)"]},{"cell_type":"markdown","source":["Chains verbinden mehrere Komponenten zu einer Verarbeitungskette. LangChain Expression Language (LCEL) ist das **neue Programmiermodell in LangChain v0.3+**, das die Entwicklung von LLM-Anwendungen vereinfacht.\n"],"metadata":{"id":"cEEMxhSBHrXA"},"id":"cEEMxhSBHrXA"},{"cell_type":"markdown","source":["\n","**Hier die Kernpunkte:**\n","\n","1. Was ist LCEL?\n","```python\n","# LCEL nutzt den Pipe-Operator (|) f√ºr klare Verkettungen\n","chain = prompt | model | output_parser\n","```\n","- Eine deklarative Sprache zum Verketten von LangChain-Komponenten\n","- Erm√∂glicht linearen Datenfluss zwischen Komponenten\n","- Basiert auf dem Pipe-Operator (|) f√ºr intuitive Verbindungen\n","\n","2. Warum LCEL nutzen?\n","- Bessere Lesbarkeit des Codes\n","- Einfachere Wartung und Debugging\n","- Verbesserte Performance durch optimierte Ausf√ºhrung\n","- Bessere Typsicherheit und Fehlererkennung\n","- Unterst√ºtzt modernes Streaming und Async-Operationen\n","\n","3. Praktisches Beispiel:"],"metadata":{"id":"ltTQItwXH6w8"},"id":"ltTQItwXH6w8"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Simple Chain\n","</font></p>"],"metadata":{"id":"BndZGbyWKs1C"},"id":"BndZGbyWKs1C"},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# Prompt-Template\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"human\", \"{input}\")\n","])\n","\n","#  Einfache LCEL-Kette\n","chain = prompt | chat_model | StrOutputParser()\n","\n","# Ausf√ºhrung\n","response = chain.invoke({\"input\": \"Erkl√§re LangChain Expression Language.\"})"],"metadata":{"id":"PsWtjWROIW9d"},"id":"PsWtjWROIW9d","execution_count":null,"outputs":[]},{"cell_type":"code","source":["display(Markdown(\"## üì£ Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response))"],"metadata":{"id":"VWYAG9aWJpZd"},"id":"VWYAG9aWJpZd","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","LCEL ist der empfohlene Weg f√ºr alle neuen LangChain-Projekte, da es die Entwicklung vereinfacht und zukunftssicher macht."],"metadata":{"id":"49Wu_XreIZY2"},"id":"49Wu_XreIZY2"},{"cell_type":"markdown","id":"Qy35G1-FvrIR","metadata":{"id":"Qy35G1-FvrIR"},"source":["<p><font color='black' size=\"5\">\n","Sequential Chains\n","</font></p>"]},{"cell_type":"markdown","id":"AWhQwgMbxitn","metadata":{"id":"AWhQwgMbxitn"},"source":["Sequentielle Chain: Linear, ein Input wird schrittweise verarbeitet"]},{"cell_type":"code","execution_count":null,"id":"ceqbCIOCuqng","metadata":{"id":"ceqbCIOCuqng"},"outputs":[],"source":["# Prompts\n","zusammenfassen_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Fasse den folgenden Text pr√§gnant zusammen.\"),\n","    (\"human\", \"{text}\")\n","])\n","\n","uebersetzen_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"√úbersetze den folgenden Text ins Deutsche.\"),\n","    (\"human\", \"{text}\")\n","])"]},{"cell_type":"code","execution_count":null,"id":"kddZQ3rMv0Hw","metadata":{"id":"kddZQ3rMv0Hw"},"outputs":[],"source":["# Verarbeitungskette\n","\n","# Erste Verarbeitungskette: Zusammenfassung\n","summarization_chain = (\n","    zusammenfassen_template\n","    | chat_model\n","    | StrOutputParser()\n","    | (lambda output: {\"text\": output})  # Notwendig, f√ºr String-> Dict, wird von chat_model erwartet\n",")\n","\n","# Zweite Verarbeitungskette: √úbersetzung\n","translation_chain = (\n","    uebersetzen_template\n","    | chat_model\n","    | StrOutputParser()\n",")\n","\n","# Gesamte Kette: Zusammenfassen und dann √úbersetzen\n","sequential_chain = summarization_chain | translation_chain"]},{"cell_type":"code","execution_count":null,"id":"ymYXf4pKwSee","metadata":{"id":"ymYXf4pKwSee"},"outputs":[],"source":["# Input Text\n","text = \"\"\"\n","Machine Learning is a specialized branch of artificial intelligence (AI) dedicated to the development of algorithms and systems that can automatically learn from data and past experiences. Instead of following rigid, pre-defined instructions, machine learning models identify patterns, adapt their behavior, and improve their performance over time based on the information they are exposed to. This ability enables them to make predictions, recognize complex relationships, and solve problems in dynamic environments. Machine learning techniques are widely applied across various fields, including healthcare, finance, transportation, and entertainment, driving innovations such as personalized recommendations, autonomous vehicles, and intelligent diagnostics.\n","\"\"\""]},{"cell_type":"code","source":["# Aufruf der Kette\n","response = sequential_chain.invoke({\"text\": text})"],"metadata":{"id":"WU8Bah_MNFSS"},"id":"WU8Bah_MNFSS","execution_count":null,"outputs":[]},{"cell_type":"code","source":["display(Markdown(\"## üì£ Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response))"],"metadata":{"id":"bipXv_2DMdCN"},"id":"bipXv_2DMdCN","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"R23C1DI1wi1X","metadata":{"id":"R23C1DI1wi1X"},"source":["<p><font color='black' size=\"5\">\n","Q&A-Chain\n","</font></p>"]},{"cell_type":"markdown","id":"-pshkn53xc7T","metadata":{"id":"-pshkn53xc7T"},"source":["QA Chain: Kontextbasiert, kombiniert mehrere Inputs f√ºr eine einzelne Verarbeitung"]},{"cell_type":"code","execution_count":null,"id":"3I8ucKu7wAoo","metadata":{"id":"3I8ucKu7wAoo"},"outputs":[],"source":["# Q&A Prompt\n","qa_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Beantworte die Frage basierend auf dem gegebenen Kontext.\"),\n","    (\"human\", \"\"\"\n","    Kontext: {context}\n","\n","    Frage: {question}\n","    \"\"\")\n","])"]},{"cell_type":"code","execution_count":null,"id":"9iqmLBYSwpiX","metadata":{"id":"9iqmLBYSwpiX"},"outputs":[],"source":["# Verkettung\n","qa_chain = qa_template | chat_model | StrOutputParser()"]},{"cell_type":"code","execution_count":null,"id":"R0lVOJaSw2a9","metadata":{"id":"R0lVOJaSw2a9"},"outputs":[],"source":["# Inhalte\n","context = \"\"\"\n","Python ist eine beliebte Programmiersprache f√ºr Machine Learning.\n","Sie bietet viele Bibliotheken wie TensorFlow und PyTorch.\n","\"\"\"\n","question = \"Welche ML-Bibliotheken gibt es f√ºr Python?\""]},{"cell_type":"code","source":["# Aufruf\n","response = qa_chain.invoke({\n","    \"context\": context,\n","    \"question\": question\n","})"],"metadata":{"id":"hM1ADYiFNZ9X"},"id":"hM1ADYiFNZ9X","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"bxFOmczGyPPN","metadata":{"id":"bxFOmczGyPPN"},"outputs":[],"source":["display(Markdown(\"## üì£ Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response))"]},{"cell_type":"markdown","source":["## 2.6 Runnables\n"],"metadata":{"id":"KRqrnpMx3Hwf"},"id":"KRqrnpMx3Hwf"},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Was sind Runnables?\n","</font></p>\n","\n","Runnables sind das Fundament der modernen LangChain-Architektur (ab Version 0.2+). Sie stellen eine einheitliche Schnittstelle f√ºr alle Komponenten dar, die Eingaben verarbeiten und Ausgaben erzeugen k√∂nnen. Runnables k√∂nnen wie Funktionen aufgerufen werden, bieten jedoch zus√§tzliche Vorteile:\n","\n","- **Einheitliches Interface**: Alle Runnables teilen dieselben grundlegenden Methoden (invoke, batch, stream, etc.)\n","- **Kompositionsf√§higkeit**: Runnables k√∂nnen einfach miteinander kombiniert werden (LCEL nutzt dieses Konzept)\n","- **Flexibilit√§t**: Sie unterst√ºtzen synchrone und asynchrone Operationen\n","- **Streaming**: Einfache Unterst√ºtzung f√ºr die schrittweise Ausgabe\n","\n","Fast alle wichtigen Komponenten in modernem LangChain sind Runnables, darunter:\n","- Prompts (PromptTemplate, ChatPromptTemplate)\n","- Modelle (ChatOpenAI, OpenAI)\n","- OutputParser\n","- Chains\n","- Tools\n","- Retriever\n"],"metadata":{"id":"DnUDR_983M4l"},"id":"DnUDR_983M4l"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Eigene Runnables erstellen\n","</font></p>"],"metadata":{"id":"WWJ7U1iJ6b8Q"},"id":"WWJ7U1iJ6b8Q"},{"cell_type":"markdown","source":["Man kann auch eigene Runnables erstellen, indem man eine Klasse definiert, die das Runnable-Interface implementiert oder eine Funktion mit `RunnableLambda` wrapped:\n"],"metadata":{"id":"PAvbE_TR6g87"},"id":"PAvbE_TR6g87"},{"cell_type":"code","source":["from langchain_core.runnables import RunnableLambda\n","\n","# 1. Mit RunnableLambda eine einfache Transformation erstellen\n","def text_verdoppeln(eingabe):\n","    return f\"{eingabe} - {eingabe}\"\n","\n","verdoppler = RunnableLambda(text_verdoppeln)\n","\n","# Verwendung des eigenen Runnables\n","print(verdoppler.invoke(\"Hallo\"))  # Ausgabe: Hallo-Hallo"],"metadata":{"id":"_0wEf_hL6peM"},"id":"_0wEf_hL6peM","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. In eine Kette einbauen\n","runnable_chain_mit_verdoppler = prompt | model | parser | verdoppler\n","\n","# Test der Kette\n","ergebnis = runnable_chain_mit_verdoppler.invoke({\"eingabe\": \"Nenne ein kurzes Wort.\"})\n","print(f\"Ergebnis mit Verdoppler: {ergebnis}\")"],"metadata":{"id":"NvZdOf1961xb"},"id":"NvZdOf1961xb","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","LCEL und Runnables\n","</font></p>\n","\n","Die LangChain Expression Language (LCEL) basiert vollst√§ndig auf dem Runnable-Konzept. Der Pipe-Operator (`|`) verbindet Runnables miteinander, indem er die Ausgabe eines Runnables zur Eingabe des n√§chsten macht. Jede LCEL-Kette ist selbst ein Runnable.\n"],"metadata":{"id":"h2nh4GAe3dNp"},"id":"h2nh4GAe3dNp"},{"cell_type":"code","source":["# Beispiel einer LCEL-Kette mit verschiedenen Runnable-Typen\n","from langchain_core.runnables import RunnableParallel\n","\n","# Eine Runnable-Komponente, die parallele Berechnungen durchf√ºhrt\n","parallel_runnable = RunnableParallel(\n","    zusammenfassung=prompt | model | parser,\n","    wortanzahl=lambda eingabe: len(eingabe[\"eingabe\"].split())\n",")\n","\n","# Verwenden der parallelen Komponente\n","ergebnis = parallel_runnable.invoke({\"eingabe\": \"Wie funktionieren Runnables in LangChain?\"})\n","print(\"\\nPARALLELE VERARBEITUNG:\")\n","print(f\"Wortanzahl: {ergebnis['wortanzahl']}\")\n","print(f\"Zusammenfassung: {ergebnis['zusammenfassung'][:50]}...\")"],"metadata":{"id":"1VAUDcSv5r4n"},"id":"1VAUDcSv5r4n","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Runnable-Methoden\n","</font></p>\n","\n","Jedes Runnable unterst√ºtzt standardm√§√üig folgende Methoden:\n","\n","1. **invoke()**: F√ºr einzelne, synchrone Anfragen\n","2. **batch()**: Verarbeitet mehrere Eingaben parallel\n","3. **stream()**: Gibt Teilergebnisse zur√ºck, sobald sie verf√ºgbar sind\n","4. **ainvoke()**: Asynchrone Version von invoke\n","5. **abatch()**: Asynchrone Version von batch\n","6. **astream()**: Asynchrone Version von stream\n","\n"],"metadata":{"id":"CjFvHqWA3S2V"},"id":"CjFvHqWA3S2V"},{"cell_type":"markdown","source":["Hier ein einfaches Beispiel mit den verschiedenen Aufrufmethoden:\n"],"metadata":{"id":"a9zHZAMP3bCL"},"id":"a9zHZAMP3bCL"},{"cell_type":"code","source":["from langchain.prompts import ChatPromptTemplate\n","from langchain_openai import ChatOpenAI\n","from langchain.schema import StrOutputParser\n","\n","# Einfaches Template erstellen\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"human\", \"{eingabe}\")\n","])\n","\n","# Modell definieren\n","model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n","\n","# Output-Parser\n","parser = StrOutputParser()\n","\n","# Runnable-Kette erstellen\n","runnable_chain = prompt | model | parser"],"metadata":{"id":"kQTKHK2D4DBb"},"id":"kQTKHK2D4DBb","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. invoke - Einzelne Anfrage\n","ergebnis = runnable_chain.invoke({\"eingabe\": \"Was ist ein Runnable in LangChain?\"})\n","print(f\"INVOKE-ERGEBNIS:\\n{ergebnis}\\n\")"],"metadata":{"id":"TVWtHSYw5Jla"},"id":"TVWtHSYw5Jla","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. batch - Mehrere Anfragen parallel\n","ergebnisse = runnable_chain.batch([\n","    {\"eingabe\": \"Erkl√§re den Begriff LCEL kurz.\"},\n","    {\"eingabe\": \"Was sind die Hauptvorteile von Runnables?\"}\n","])\n","print(\"BATCH-ERGEBNISSE:\")\n","for i, ergebnis in enumerate(ergebnisse, 1):\n","    print(f\"Anfrage {i}: {ergebnis[:50]}...\\n\")"],"metadata":{"id":"UAet4KSq5K2D"},"id":"UAet4KSq5K2D","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. stream - Schrittweise Ausgabe\n","print(\"STREAM-ERGEBNIS:\")\n","for chunk in runnable_chain.stream({\"eingabe\": \"Z√§hle von 1 bis 5 auf. Mache eine Pause von 2 Sekunden nach jedem Schritt.\"}):\n","    print(chunk, end=\"\")\n","print(\"\\n\")"],"metadata":{"id":"VJn4e_IX5L-b"},"id":"VJn4e_IX5L-b","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"VlARJEowAAIR","metadata":{"id":"VlARJEowAAIR"},"source":["# 3 | √úberblick LangChain Konzepte\n","---"]},{"cell_type":"markdown","id":"g5B7bWJKBB-C","metadata":{"id":"g5B7bWJKBB-C"},"source":["Konzepte von LangChain sind grundlegende Bausteine und Prinzipien, die das Framework ausmachen und seine Funktionsweise erm√∂glichen. Hier sind einige der wichtigsten Konzepte:\n","\n","+ Chat-Modelle (**Chat models**): LLMs, die √ºber eine Chat-API verf√ºgbar sind und Sequenzen von Nachrichten verarbeiten1.\n","+ Nachrichten (**Messages**): Kommunikationseinheiten in Chat-Modellen f√ºr Ein- und Ausgabe1.\n","+ Chat-Verlauf (**Chat history**): Eine Sequenz von Nachrichten, die eine Konversation darstellt.\n","+ Tools (**Tools**): Funktionen mit definierten Schemata f√ºr Name, Beschreibung und Argumente.\n","+ Strukturierte Ausgabe (**Structured output**): Technik, um Chat-Modelle in strukturierten Formaten antworten zu lassen.\n","+ Retrieval Augmented Generation (**RAG**): Technik zur Verbesserung von Sprachmodellen durch Kombination mit externen Wissensbasen.\n","+ Prompt-Vorlagen (**Prompt template**s): Komponenten zur Erstellung strukturierter Prompts f√ºr LLMs.\n","+ Chains (**Chains**): Verkn√ºpfungen mehrerer LLMs oder anderer Komponenten f√ºr komplexere Anwendungen.\n","+ Agenten (**Agents**): Nutzen Sprachmodelle, um Aktionssequenzen auszuw√§hlen und mit externen Ressourcen zu interagieren.\n","+ Retriever (**Retriever**): Komponenten, die relevante Dokumente aus einer Wissensbasis abrufen."]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Anbieterbezogene APIs in LangChain\n","</font></p>"],"metadata":{"id":"Gm6A7xSEzNVv"},"id":"Gm6A7xSEzNVv"},{"cell_type":"markdown","source":["| API-Name in LangChain | Anbieter             | Bemerkung                                 |\n","|-----------------------|----------------------|-------------------------------------------|\n","| `ChatOpenAI`           | OpenAI               | Chat-Modelle (`gpt-3.5-turbo`, `gpt-4`, ...) |\n","| `OpenAI`               | OpenAI               | Text Completion (`text-davinci-003`, alt)  |\n","| `ChatAnthropic`        | Anthropic            | Claude-Modelle (`Claude 3`, etc.)          |\n","| `ChatGoogleGenerativeAI` | Google (Gemini)      | Gemini-Modelle (`gemini-1.5-pro`, etc.)    |\n","| `ChatCohere`           | Cohere               | Command-Modelle                           |\n","| `ChatMistralAI`        | Mistral              | Mistral-Modelle (`Mistral 7B`, Mixtral etc.) |\n","| `ChatFireworks`        | Fireworks AI         | Zugriff auf viele Open-Weight-Modelle      |\n","| `ChatAzureOpenAI`      | Azure (Microsoft)    | OpenAI-Modelle √ºber Azure API              |\n","| `BedrockLLM`           | AWS Bedrock          | Zugang zu Claude, Titan, Jurassic, etc.    |"],"metadata":{"id":"MjHiOl3dzLyc"},"id":"MjHiOl3dzLyc"},{"cell_type":"markdown","id":"F_5gn03b46rC","metadata":{"id":"F_5gn03b46rC"},"source":["# A | Aufgabe\n","---"]},{"cell_type":"markdown","id":"36LyGMmo5jb3","metadata":{"id":"36LyGMmo5jb3"},"source":["Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen."]},{"cell_type":"markdown","id":"Je-szbGU7IJ-","metadata":{"id":"Je-szbGU7IJ-"},"source":["\n","<p><font color='black' size=\"5\">\n","Erstellen einer LangChain-Kette zur Textanalyse\n","</font></p>"]},{"cell_type":"markdown","id":"kevePvA-7PzW","metadata":{"id":"kevePvA-7PzW"},"source":["Entwickeln Sie eine Verarbeitungskette, die einen Text analysiert und verschiedene Informationen dar√ºber extrahiert.\n","\n","**Schritte:**\n","1. Erstelle ein ChatPromptTemplate, das ein LLM anweist, einen Text zu analysieren\n","2. Die Analyse soll folgende Aspekte umfassen:\n","   - Hauptthema des Textes\n","   - Tonalit√§t (formal, informell, etc.)\n","   - Schl√ºsselw√∂rter (5-10)\n","   - Kurze Zusammenfassung (max. 3 S√§tze)\n","3. Formatiere die Ausgabe strukturiert mit Markdown-√úberschriften\n","4. Teste die Kette mit mindestens zwei verschiedenen Texten\n","\n","\n"]},{"cell_type":"markdown","id":"6oBvtYws7qpB","metadata":{"id":"6oBvtYws7qpB"},"source":["<p><font color='black' size=\"5\">\n","Few-Shot-Learning f√ºr Textklassifikation\n","</font></p>"]},{"cell_type":"markdown","id":"gI7alQcE79ER","metadata":{"id":"gI7alQcE79ER"},"source":["Erstellen Sie ein System, das mithilfe von Few-Shot-Learning Texte in vorgegebene Kategorien klassifiziert.\n","\n","**Schritte:**\n","1. Definiere 3-5 Kategorien f√ºr die Klassifikation (z.B. Sport, Politik, Technologie, Kultur)\n","2. Erstelle einen FewShotPromptTemplate mit Beispielen f√ºr jede Kategorie\n","3. Entwickle eine Chain, die neue Texte klassifiziert\n","4. Implementiere eine Funktion, die neben der Kategorie auch eine Begr√ºndung f√ºr die Einordnung liefert\n","5. Teste das System mit verschiedenen Texten, die nicht in den Beispielen vorkommen\n"]},{"cell_type":"markdown","id":"l35x2g1e8fa9","metadata":{"id":"l35x2g1e8fa9"},"source":["<p><font color='black' size=\"5\">\n","Q&A-System mit Sequential Chain\n","</font></p>"]},{"cell_type":"markdown","id":"qHFRGWmP8gga","metadata":{"id":"qHFRGWmP8gga"},"source":["Entwickeln Sie ein System, das Fragen zu einem gegebenen Kontext beantwortet, aber zuerst den Kontext zusammenfasst und dann die Frage beantwortet.\n","\n","\n","**Schritte:**\n","\n","+ Erstellen Sie zwei Templates:\n","    + Ein Template zur Zusammenfassung des Kontexts\n","    + Ein Template zur Beantwortung einer Frage basierend auf der Zusammenfassung\n","+ Verbinden Sie die Templates in einer sequentiellen Kette\n","+ Implementieren Sie einen Mechanismus, der die Zusammenfassung und die Antwort getrennt zur√ºckgibt\n","+ Nutzen Sie LCEL (LangChain Expression Language) f√ºr die Verkettung\n","+ Testen Sie das System mit verschiedenen Kontexten und Fragen"]}],"metadata":{"colab":{"collapsed_sections":["91ee89a4-477b-41ce-a511-aadd4c2fd547","fGV8FwGmIwmR","EouwC_wdZb-c","-JwJD0rLIyT8","8c859b9f","LVqVL9CWRXU7","c314ca71","KRqrnpMx3Hwf","VlARJEowAAIR","F_5gn03b46rC"],"provenance":[],"toc_visible":true},"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":5}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37rMyCQz2JNw",
   "metadata": {
    "id": "37rMyCQz2JNw"
   },
   "source": [
    "<p><font size=\"7\" color='grey'> <b>\n",
    "Anwendung Generativer KI\n",
    "</b></font> </br></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R5CfUEMJdvFQ",
   "metadata": {
    "id": "R5CfUEMJdvFQ"
   },
   "source": [
    "<p><font size=\"6\" color='grey'> <b>\n",
    "LangChain 101\n",
    "</b></font> </br></p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdhPIzcEYRG",
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "dfdhPIzcEYRG",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "#@markdown   <p><font size=\"4\" color='green'>  Colab-Umfeld</font> </br></p>\n",
    "# Installierte Python Version\n",
    "import sys\n",
    "print(f\"Python Version: \",sys.version)\n",
    "# Installierte LangChain Bibliotheken\n",
    "print()\n",
    "print(\"Installierte LangChain Bibliotheken:\")\n",
    "!pip list | grep '^langchain'\n",
    "# Unterdrückt die \"DeprecationWarning\" von LangChain für die Memory-Funktionden\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WD3Wwr6sESX8",
   "metadata": {
    "cellView": "form",
    "id": "WD3Wwr6sESX8"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "#@markdown   <p><font size=\"4\" color='green'>  SetUp API-Keys (setup_api_keys)</font> </br></p>\n",
    "def setup_api_keys():\n",
    "    \"\"\"Konfiguriert alle benötigten API-Keys aus Google Colab userdata\"\"\"\n",
    "    from google.colab import userdata\n",
    "    import os\n",
    "    from os import environ\n",
    "\n",
    "    # Dictionary der benötigten API-Keys\n",
    "    keys = {\n",
    "        'OPENAI_API_KEY': 'OPENAI_API_KEY',\n",
    "        'HF_TOKEN': 'HF_TOKEN',\n",
    "        # Weitere Keys bei Bedarf\n",
    "    }\n",
    "\n",
    "    # Keys in Umgebungsvariablen setzen\n",
    "    for env_var, key_name in keys.items():\n",
    "        environ[env_var] = userdata.get(key_name)\n",
    "\n",
    "    return {k: environ[k] for k in keys.keys()}\n",
    "\n",
    "# Verwendung\n",
    "all_keys = setup_api_keys()\n",
    "# Bei Bedarf einzelne Keys direkt zugreifen\n",
    "# WEATHER_API_KEY = all_keys['WEATHER_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PHDCKRbtgVXy",
   "metadata": {
    "id": "PHDCKRbtgVXy"
   },
   "source": [
    "# **1 | Was ist LangChain?**\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ee89a4-477b-41ce-a511-aadd4c2fd547",
   "metadata": {},
   "source": [
    "# 1 | Was ist LangChain?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5004f8c",
   "metadata": {
    "id": "b5004f8c"
   },
   "source": [
    "LangChain ist ein Framework zur **Entwicklung von Anwendungen mit großen Sprachmodellen** (LLMs). Es vereinfacht die Integration von LLMs in eigene Anwendungen durch:\n",
    "\n",
    "- Modulare Komponenten für verschiedene Aufgaben\n",
    "- Vorgefertigte Ketten (Chains) für komplexe Workflows\n",
    "- Einfache Integration externer Datenquellen\n",
    "- Werkzeuge für das Speichern von Konversationskontexten\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tl73bT-kKkia",
   "metadata": {
    "id": "tl73bT-kKkia"
   },
   "source": [
    "[Einführung](https://python.langchain.com/docs/introduction/)   \n",
    "[Konzepte](https://python.langchain.com/docs/concepts/)   \n",
    "[API-References](https://python.langchain.com/api_reference/index.html)   \n",
    "[Integrations](https://python.langchain.com/docs/integrations/providers/)\n",
    "\n",
    "---\n",
    "\n",
    "[Tutorials](https://python.langchain.com/docs/tutorials/)   \n",
    "[How-to-Guides](https://python.langchain.com/docs/how_to/)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JrgQ46eLphEv",
   "metadata": {
    "id": "JrgQ46eLphEv"
   },
   "outputs": [],
   "source": [
    "!uv pip install --system -q langchain_community langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GpYUCQJUpqbe",
   "metadata": {
    "id": "GpYUCQJUpqbe"
   },
   "outputs": [],
   "source": [
    "from os import environ\n",
    "from google.colab import userdata\n",
    "environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fGV8FwGmIwmR",
   "metadata": {
    "id": "fGV8FwGmIwmR"
   },
   "source": [
    "# **2 | Kernkonzepte**\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EouwC_wdZb-c",
   "metadata": {
    "id": "EouwC_wdZb-c"
   },
   "source": [
    "## 2.1 Message-Typen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yS2mM2J1aXlY",
   "metadata": {
    "id": "yS2mM2J1aXlY"
   },
   "source": [
    "Beim Einsatz von Large Language Models (LLMs) wie GPT gibt es drei Hauptarten von Nachrichten (Messages), die die Interaktion mit dem Modell steuern. Diese Messages definieren, wie das Modell reagiert, welche Eingaben es erhält und welche Ausgaben es generiert.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sGqRXptbbQ2a",
   "metadata": {
    "id": "sGqRXptbbQ2a"
   },
   "source": [
    "**System Message:**   \n",
    "Die `System Message` dient dazu, das Verhalten des LLMs zu steuern. Sie ist eine nicht sichtbare Nachricht, die dem Modell Anweisungen gibt, wie es sich verhalten soll. Dabei kann sie die Persönlichkeit, den Tonfall und die Einschränkungen des Modells beeinflussen. Die Systemnachricht funktioniert wie eine Art \"Rollenspiel-Anweisung\", um das Modell in eine gewünschte Rolle zu versetzen.\n",
    "\n",
    "*Merkmale:*\n",
    "- Definiert, wie das Modell reagieren soll\n",
    "- Legt Persönlichkeit, Verhalten und Einschränkungen fest\n",
    "- Wird zu Beginn einer Sitzung gesetzt und bleibt bestehen\n",
    "- Nicht sichtbar für den Benutzer\n",
    "\n",
    "*Beispiel einer System Message:*\n",
    "```json\n",
    "{\n",
    "  \"role\": \"system\",\n",
    "  \"content\": \"You are an AI assistant that provides concise and informative answers in a professional tone.\"\n",
    "}\n",
    "```\n",
    "Dieses Beispiel weist das Modell an, in einem professionellen Tonfall kurze und informative Antworten zu geben.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upsIzpcqbbP7",
   "metadata": {
    "id": "upsIzpcqbbP7"
   },
   "source": [
    "**User Message**   \n",
    "Die `User Message` ist die eigentliche Eingabe des Benutzers. Sie kann eine Frage, eine Aufforderung oder ein Befehl sein. Das Modell nutzt diese Nachricht als Ausgangspunkt für die Generierung einer Antwort.\n",
    "\n",
    "*Merkmale:*\n",
    "- Direkte Eingabe des Benutzers\n",
    "- Kann eine Frage, Anfrage oder einen Befehl enthalten\n",
    "- Basis für die Antwort des Modells\n",
    "\n",
    "*Beispiel einer User Message:*\n",
    "```json\n",
    "{\n",
    "  \"role\": \"user\",\n",
    "  \"content\": \"What are the key differences between AI and machine learning?\"\n",
    "}\n",
    "```\n",
    "Hier fragt der Benutzer nach den Unterschieden zwischen KI und maschinellem Lernen.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CozrrtYXbhZb",
   "metadata": {
    "id": "CozrrtYXbhZb"
   },
   "source": [
    "**AI Message**   \n",
    "Die `AI Message` ist die Antwort, die das Modell generiert. Sie basiert auf der User Message und den Anweisungen aus der System Message. Die AI Message kann verschiedene Eigenschaften haben, ist aber hauptsächlich auf den Inhalt fokussiert.\n",
    "\n",
    "*Merkmale:*\n",
    "- Antwort des Modells auf die Benutzeranfrage\n",
    "- Kann verschiedene Eigenschaften haben (z. B. Länge, Stil)\n",
    "- Hauptsächlich inhaltlich relevant\n",
    "\n",
    "*Beispiel einer AI Message:*\n",
    "```json\n",
    "{\n",
    "  \"role\": \"assistant\",\n",
    "  \"content\": \"Artificial Intelligence (AI) is a broad field that includes various subfields, one of which is Machine Learning (ML). ML focuses on creating models that learn from data and make predictions.\"\n",
    "}\n",
    "```\n",
    "Hier gibt das Modell eine inhaltliche Antwort auf die Frage des Benutzers.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PiHnhKHVbofP",
   "metadata": {
    "id": "PiHnhKHVbofP"
   },
   "source": [
    "**Zusammenhang**   \n",
    "Die drei Message-Typen arbeiten zusammen, um eine sinnvolle Interaktion mit dem Modell zu ermöglichen:\n",
    "\n",
    "+  **System Message** legt die Regeln und das Verhalten des Modells fest.\n",
    "+ **User Message** stellt eine Anfrage oder einen Befehl.\n",
    "+ **AI Message** gibt die generierte Antwort basierend auf den vorherigen Nachrichten.\n",
    "\n",
    "**Beispiel für eine vollständige Konversation:**    \n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a helpful AI assistant that provides educational answers in a friendly tone.\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Can you explain what Large Language Models are?\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"Sure! Large Language Models (LLMs) are advanced AI systems trained on vast amounts of text data. They can understand and generate human-like text, making them useful for various applications like chatbots, content creation, and more.\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "In diesem Beispiel wurde das Modell so eingestellt, dass es in einem freundlichen Tonfall antwortet, dann erhält es eine Benutzeranfrage und gibt eine passende Antwort.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L_RH5TC7yqOR",
   "metadata": {
    "id": "L_RH5TC7yqOR"
   },
   "outputs": [],
   "source": [
    "!uv pip install --system -q langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K2lC_JK3guaj",
   "metadata": {
    "id": "K2lC_JK3guaj"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "MODEL = 'gpt-4o-mini'\n",
    "TEMPERATURE = 0.0\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"Du bist ein kompetenter und hilfreicher Assistent.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Nenne mir die Hauptstadt von Usbekistan?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "# Initialisieren Sie das OpenAI LLM mit Ihrem API-Key\n",
    "llm = ChatOpenAI(\n",
    "  model=MODEL,\n",
    "  temperature= TEMPERATURE,\n",
    ")\n",
    "\n",
    "display(Markdown(\"## 📣 Model response:\"))\n",
    "display(Markdown(\"---\"))\n",
    "output = llm.invoke(messages)\n",
    "display(Markdown(output.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2vJuMVHZ1jJ8",
   "metadata": {
    "id": "2vJuMVHZ1jJ8"
   },
   "outputs": [],
   "source": [
    "messages.append(output)\n",
    "messages.append(HumanMessage(content=\"Bis Du sicher? Ich glaube, sie wurde kürzlich umbenannt.\"))\n",
    "\n",
    "for message in messages:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A8Z7DZRH5TFr",
   "metadata": {
    "id": "A8Z7DZRH5TFr"
   },
   "source": [
    "Wir können das Konversationsarray an das Modell übermitteln und seine neueste Antwort anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9qFxkEmAz1w1",
   "metadata": {
    "id": "9qFxkEmAz1w1"
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"## 📣 Model response:\"))\n",
    "display(Markdown(\"---\"))\n",
    "output = llm.invoke(messages)\n",
    "display(Markdown(output.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Iw3Ovw5mcFcY",
   "metadata": {
    "id": "Iw3Ovw5mcFcY"
   },
   "source": [
    "**Fazit**    \n",
    "Die System-, User- und AI-Message sind essenzielle Bestandteile jeder Interaktion mit einem LLM. Während die System Message das Verhalten vorgibt, stellt die User Message die eigentliche Anfrage dar, auf die das Modell mit einer AI Message antwortet. Dieses Konzept ist zentral für den Einsatz von LLMs in Chatbots, KI-gestützten Assistenten und anderen interaktiven Anwendungen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-JwJD0rLIyT8",
   "metadata": {
    "id": "-JwJD0rLIyT8"
   },
   "source": [
    "## 2.2 Prompts\n",
    "\n",
    "Prompts sind die Eingaben bzw. Aufträge an das LLM. LangChain bietet verschiedene Template-Systeme:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HqTUeMW20084",
   "metadata": {
    "id": "HqTUeMW20084"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Simple Prompt\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1926d03",
   "metadata": {
    "id": "e1926d03"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "# 1. Einfaches Template mit Platzhaltern\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"thema\"],\n",
    "    template=\"Erkläre {thema} in einfachen Worten.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C92-v-s70Q3k",
   "metadata": {
    "id": "C92-v-s70Q3k"
   },
   "outputs": [],
   "source": [
    "print(template.format(thema=\"Machine Learning\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_o2EkUkN3hEX",
   "metadata": {
    "id": "_o2EkUkN3hEX"
   },
   "source": [
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Eingabeaufforderungsvorlagen\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gPvElGbrdOHq",
   "metadata": {
    "id": "gPvElGbrdOHq"
   },
   "source": [
    "\n",
    "\n",
    "Mit LangChain können Sie Operationsketten erstellen, die normalerweise als Teil einer LLM-fähigen Anwendung ausgeführt werden. Eine dieser Operationen ist eine Eingabeaufforderungsvorlage, mit der Sie Text in eine zuvor erstellte Eingabeaufforderung einfügen können. In diesem Beispiel erstellen wir eine Eingabeaufforderungsvorlage, die das Modell auffordert, einen zufälligen Titel für einen Blogbeitrag zu erstellen.\n",
    "\n",
    "```\n",
    "Gibt nur den Titel eines Blog-Beitragsartikels zum Thema {topic} in {language} zurück.\n",
    "```\n",
    "\n",
    "Um dieses Ziel zu erreichen, verwenden wir ein **PromptTemplate**-Objekt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L-g3zOXBmulc",
   "metadata": {
    "id": "L-g3zOXBmulc"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "topic = \"Haustiere für Datenwissenschaftler\"\n",
    "language = \"german\"\n",
    "\n",
    "# Höhere Temperaturen für mehr Kreativität\n",
    "llm = ChatOpenAI(model=MODEL, temperature=0.7)\n",
    "\n",
    "# Definieren der Eingabeaufforderungsvorlage\n",
    "title_template = PromptTemplate(\n",
    "    input_variables=['topic', 'language'],\n",
    "    template='Gibt nur den Titel eines Blog-Beitragsartikels zum Thema {topic} in {language} zurück.'\n",
    ")\n",
    "\n",
    "# Verkettung\n",
    "title_chain = title_template | llm\n",
    "\n",
    "# Aufrufen der Kette mit Eingaben\n",
    "response = title_chain.invoke({'topic': topic, 'language': language})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8akKmCCc06nm",
   "metadata": {
    "id": "8akKmCCc06nm"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Chat Prompt\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iW9URvON0IXr",
   "metadata": {
    "id": "iW9URvON0IXr"
   },
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein hilfreicher Assistent.\"),\n",
    "    (\"human\", \"Erkläre mir {thema}\"),\n",
    "    (\"assistant\", \"Ich erkläre dir {thema} gerne.\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jwrybdk_0agA",
   "metadata": {
    "id": "jwrybdk_0agA"
   },
   "outputs": [],
   "source": [
    "print(chat_template.format(thema=\"Machine Learning\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ps344Ejw0-ih",
   "metadata": {
    "id": "Ps344Ejw0-ih"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Few-Shot-Learning Prompt\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cEopgOiS0KU_",
   "metadata": {
    "id": "cEopgOiS0KU_"
   },
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"frage\": \"Was ist Python?\", \"antwort\": \"Eine Programmiersprache.\"},\n",
    "    {\"frage\": \"Was ist Java?\", \"antwort\": \"Eine objektorientierte Sprache.\"}\n",
    "]\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=PromptTemplate(\n",
    "        input_variables=[\"frage\", \"antwort\"],\n",
    "        template=\"Frage: {frage}\\nAntwort: {antwort}\"\n",
    "    ),\n",
    "    prefix=\"Beantworte die Frage basierend auf den Beispielen:\",\n",
    "    suffix=\"Frage: {input}\\nAntwort:\",\n",
    "    input_variables=[\"input\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OdMhepTp0fJU",
   "metadata": {
    "id": "OdMhepTp0fJU"
   },
   "outputs": [],
   "source": [
    "print(few_shot_prompt.format(input=\"Was ist Cobol?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c859b9f",
   "metadata": {
    "id": "8c859b9f"
   },
   "source": [
    "## 2.3 Modelle\n",
    "\n",
    "LangChain unterstützt verschiedene LLMs und bietet flexible Konfigurationsmöglichkeiten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e20e1cb",
   "metadata": {
    "id": "9e20e1cb"
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Text-Completion-Modell mit Parametern\n",
    "llm = OpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-instruct\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "# Chat-Modell für Konversationen\n",
    "chat_model = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c314ca71",
   "metadata": {
    "id": "c314ca71"
   },
   "source": [
    "## 2.4 LCEL aka Chains\n",
    "\n",
    "Chains verbinden mehrere Komponenten zu einer Verarbeitungskette:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y8RgQ-PZ7dii",
   "metadata": {
    "id": "y8RgQ-PZ7dii"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "LangChain Expression Language (LCEL)\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T6dXYZWP7pyy",
   "metadata": {
    "id": "T6dXYZWP7pyy"
   },
   "source": [
    "LangChain Expression Language (LCEL) ist das **neue Programmiermodell in LangChain v0.3+**, das die Entwicklung von LLM-Anwendungen vereinfacht.\n",
    "\n",
    "Hier die Kernpunkte:\n",
    "\n",
    "1. Was ist LCEL?\n",
    "```python\n",
    "# LCEL nutzt den Pipe-Operator (|) für klare Verkettungen\n",
    "chain = prompt | model | output_parser\n",
    "```\n",
    "- Eine deklarative Sprache zum Verketten von LangChain-Komponenten\n",
    "- Ermöglicht linearen Datenfluss zwischen Komponenten\n",
    "- Basiert auf dem Pipe-Operator (|) für intuitive Verbindungen\n",
    "\n",
    "2. Warum LCEL nutzen?\n",
    "- Bessere Lesbarkeit des Codes\n",
    "- Einfachere Wartung und Debugging\n",
    "- Verbesserte Performance durch optimierte Ausführung\n",
    "- Bessere Typsicherheit und Fehlererkennung\n",
    "- Unterstützt modernes Streaming und Async-Operationen\n",
    "\n",
    "3. Praktisches Beispiel:\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "# Einfache LCEL-Kette\n",
    "chain = (\n",
    "    ChatPromptTemplate.from_messages([(\"human\", \"{input}\")])\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# Ausführung\n",
    "result = chain.invoke({\"input\": \"Erkläre LCEL\"})\n",
    "```\n",
    "\n",
    "LCEL ist der empfohlene Weg für alle neuen LangChain-Projekte, da es die Entwicklung vereinfacht und zukunftssicher macht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_04hK8NK-P0f",
   "metadata": {
    "id": "_04hK8NK-P0f"
   },
   "source": [
    "Für komplexere Anwendungen mit umfangreichem Zustandsmanagement, Verzweigungen oder mehreren Agenten wird die Verwendung von LangGraph empfohlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c889d",
   "metadata": {
    "id": "a05c889d"
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Konstanten\n",
    "MODEL=\"gpt-4o-mini\",\n",
    "TEMPERATUR=0.7\n",
    "\n",
    "# Modell-Konfiguration\n",
    "chat_model = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GxL5B1OdvF-v",
   "metadata": {
    "id": "GxL5B1OdvF-v"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Simple Chain\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8w1pX9ehummD",
   "metadata": {
    "id": "8w1pX9ehummD"
   },
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein hilfreicher Assistent.\"),\n",
    "    (\"human\", \"{eingabe}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aXL5zK8ZvS5N",
   "metadata": {
    "id": "aXL5zK8ZvS5N"
   },
   "outputs": [],
   "source": [
    "# Verarbeitungskette\n",
    "simple_chain = (\n",
    "    prompt_template\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XOjiVTuHu_4-",
   "metadata": {
    "id": "XOjiVTuHu_4-"
   },
   "outputs": [],
   "source": [
    "# Aufruf\n",
    "result = simple_chain.invoke({\"eingabe\": \"Was ist Machine Learning?\"})\n",
    "print(\"Simple Chain Ergebnis:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qy35G1-FvrIR",
   "metadata": {
    "id": "Qy35G1-FvrIR"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Sequential Chains\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AWhQwgMbxitn",
   "metadata": {
    "id": "AWhQwgMbxitn"
   },
   "source": [
    "Sequentielle Chain: Linear, ein Input wird schrittweise verarbeitet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceqbCIOCuqng",
   "metadata": {
    "id": "ceqbCIOCuqng"
   },
   "outputs": [],
   "source": [
    "# Prompts\n",
    "zusammenfassen_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Fasse den folgenden Text prägnant zusammen.\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])\n",
    "\n",
    "uebersetzen_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Übersetze den folgenden Text ins Deutsche.\"),\n",
    "    (\"human\", \"{text}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kddZQ3rMv0Hw",
   "metadata": {
    "id": "kddZQ3rMv0Hw"
   },
   "outputs": [],
   "source": [
    "# Verarbeitungskette\n",
    "sequential_chain = (\n",
    "    zusammenfassen_template\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: {\"text\": x})  # Ausgabe der ersten Chain als Eingabe für die zweite\n",
    "    | uebersetzen_template\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ymYXf4pKwSee",
   "metadata": {
    "id": "ymYXf4pKwSee"
   },
   "outputs": [],
   "source": [
    "# Aufruf\n",
    "text = \"\"\"\n",
    "Machine Learning is a subset of artificial intelligence that focuses on developing\n",
    "systems that can learn and improve from experience without being explicitly programmed.\n",
    "\"\"\"\n",
    "seq_result = sequential_chain.invoke({\"text\": text})\n",
    "print(\"\\nSequential Chain Ergebnis:\", seq_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R23C1DI1wi1X",
   "metadata": {
    "id": "R23C1DI1wi1X"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Q&A-Chain\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-pshkn53xc7T",
   "metadata": {
    "id": "-pshkn53xc7T"
   },
   "source": [
    "QA Chain: Kontextbasiert, kombiniert mehrere Inputs für eine einzelne Verarbeitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3I8ucKu7wAoo",
   "metadata": {
    "id": "3I8ucKu7wAoo"
   },
   "outputs": [],
   "source": [
    "# Prompt\n",
    "qa_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Beantworte die Frage basierend auf dem gegebenen Kontext.\"),\n",
    "    (\"human\", \"\"\"\n",
    "    Kontext: {context}\n",
    "\n",
    "    Frage: {question}\n",
    "    \"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9iqmLBYSwpiX",
   "metadata": {
    "id": "9iqmLBYSwpiX"
   },
   "outputs": [],
   "source": [
    "# Verkettung\n",
    "qa_chain = (\n",
    "    qa_template\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R0lVOJaSw2a9",
   "metadata": {
    "id": "R0lVOJaSw2a9"
   },
   "outputs": [],
   "source": [
    "# Aufruf\n",
    "context = \"\"\"\n",
    "Python ist eine beliebte Programmiersprache für Machine Learning.\n",
    "Sie bietet viele Bibliotheken wie TensorFlow und PyTorch.\n",
    "\"\"\"\n",
    "question = \"Welche ML-Bibliotheken gibt es für Python?\"\n",
    "qa_result = qa_chain.invoke({\n",
    "    \"context\": context,\n",
    "    \"question\": question\n",
    "})\n",
    "print(\"\\nQA Chain Ergebnis:\", qa_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bxFOmczGyPPN",
   "metadata": {
    "id": "bxFOmczGyPPN"
   },
   "outputs": [],
   "source": [
    "# Formatierte Ausgabe\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(qa_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iZ5UJro5DCdR",
   "metadata": {
    "id": "iZ5UJro5DCdR"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "invoke - batch - stream\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tYRFZ2K0DKHb",
   "metadata": {
    "id": "tYRFZ2K0DKHb"
   },
   "source": [
    "Aufruf der Verarbeitungskette erfolgt über:   \n",
    "+ invoke(): Für einzelne, einfache Anfragen\n",
    "+ batch(): Wenn mehrere Anfragen parallel verarbeitet werden sollen\n",
    "+ stream(): Für lange Antworten oder bessere Nutzererfahrung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7xBrIqZQ5et1",
   "metadata": {
    "id": "7xBrIqZQ5et1"
   },
   "outputs": [],
   "source": [
    "template = PromptTemplate(\n",
    "    input_variables=[\"thema\"],\n",
    "    template=\"Erkläre {thema} in einfachen Worten.\"\n",
    ")\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Du bist ein hilfreicher Assistent.\"),\n",
    "    (\"human\", \"{eingabe}\")\n",
    "])\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# LCEL-Verkettung mit Pipe-Operator\n",
    "chain = prompt_template | chat_model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7z1qJcbIDhL-",
   "metadata": {
    "id": "7z1qJcbIDhL-"
   },
   "outputs": [],
   "source": [
    "# ---invoke--- #\n",
    "result_with_invoke = chain.invoke(\"hello world!\")\n",
    "print(result_with_invoke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aithABrSEAf2",
   "metadata": {
    "id": "aithABrSEAf2"
   },
   "outputs": [],
   "source": [
    "# ---batch--- #\n",
    "result_with_batch = chain.batch([\"hello\", \"world\", \"!\"])\n",
    "print(result_with_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QjVNjfUXEBuA",
   "metadata": {
    "id": "QjVNjfUXEBuA"
   },
   "outputs": [],
   "source": [
    "# ---stream--- #\n",
    "for chunk in chain.stream(\"Karl ist 22 Jahre alt.\"):\n",
    "    print(chunk, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HBZ_LQtwyGgc",
   "metadata": {
    "id": "HBZ_LQtwyGgc"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Vergleich Sequential vs Q&A\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XYFeZlgvx-xM",
   "metadata": {
    "id": "XYFeZlgvx-xM"
   },
   "source": [
    "| Aspekt | Sequentielle Chain | QA Chain |\n",
    "|--------|-------------------|-----------|\n",
    "| Eingabe | Einzelner Text | Kontext + Frage |\n",
    "| Verarbeitung | Schrittweise | Parallel |\n",
    "| Ausgabe | Transformierter Text | Kontextbezogene Antwort |\n",
    "| Anwendung | Textverarbeitung | Informationsextraktion |\n",
    "| Flexibilität | Fest definiert | Kontextabhängig |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LrfdbCP92lZt",
   "metadata": {
    "id": "LrfdbCP92lZt"
   },
   "source": [
    "Verschoben aus M01 - redundant?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IOpAcj-byBPg",
   "metadata": {
    "id": "IOpAcj-byBPg"
   },
   "source": [
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Erstellen einer einfache sequentielle Kette\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HFgf9flrdPwq",
   "metadata": {
    "id": "HFgf9flrdPwq"
   },
   "source": [
    "\n",
    "\n",
    "Wir werden nun LangChain verwenden, um mehrere LLM-Aufrufe mithilfe der Klasse **SimpleSequentialChain** zu einer längeren Kette zusammenzufassen. Wir werden zwei kleinere Ketten verwenden, um einen Titel und einen Textkörper für einen Blogbeitrag zu erstellen. Wir beginnen mit der Definition der beiden Eingabeaufforderungen, die wir zum Erstellen dieses Blogbeitrags verwenden werden. Beachten Sie auch, dass wir verlangen, dass das LLM [markdown](https://en.wikipedia.org/wiki/Markdown) verwendet, um den eigentlichen Blogbeitrag zu generieren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ELS-9JD3Sao6",
   "metadata": {
    "id": "ELS-9JD3Sao6"
   },
   "outputs": [],
   "source": [
    "# Erstellen Sie die beiden Eingabeaufforderungsvorlagen\n",
    "title_template = PromptTemplate( input_variables = ['topic'], template = 'Gibt nur den Titel eines Blog-Beitragsartikels zu {topic} in Deutsch' )\n",
    "article_template = PromptTemplate( input_variables = ['title'], template = 'Schreibe einen Blog-Beitrag {title}, das Format ist Markdown.' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k3vy7jf3-_xl",
   "metadata": {
    "id": "k3vy7jf3-_xl"
   },
   "source": [
    "Wir erstellen die erste Kette, um den zufälligen Titel zu generieren. Hier können die Benutzer das Thema angeben. Wir verwenden eine höhere Temperatur, um die Kreativität des Titels zu steigern. Wir verwenden auch ein einfacheres Modell, um die Kosten für die relativ einfache Aufgabe der Titelauswahl zu minimieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_KV8UJUDyP8M",
   "metadata": {
    "id": "_KV8UJUDyP8M"
   },
   "outputs": [],
   "source": [
    "MODEL = 'gpt-4o-mini'\n",
    "\n",
    "# Erstellen Sie eine Kette, um einen Zufallsgenerator zu erzeugen.\n",
    "llm = ChatOpenAI(model=MODEL, temperature=0.7)\n",
    "title_chain = title_template | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cO0ielMf_xRa",
   "metadata": {
    "id": "cO0ielMf_xRa"
   },
   "source": [
    "Als nächstes verfassen wir den eigentlichen Blogbeitrag. Wir verwenden eine niedrigere Temperatur, um die Kreativität zu verringern und das LLM dazu zu bringen, sich an sachliche Informationen zu halten und Halluzinationen zu vermeiden. Wir verwenden auch ein komplexeres Modell, um einen besseren Artikel zu erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G7OCQCggyHlB",
   "metadata": {
    "id": "G7OCQCggyHlB"
   },
   "outputs": [],
   "source": [
    "MODEL2 = 'gpt-4o'\n",
    "\n",
    "# Erstellen der Artikelkette\n",
    "llm2 = ChatOpenAI(model=MODEL2, temperature=0.1)\n",
    "article_chain = article_template | llm2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6rftFvMyBa4-",
   "metadata": {
    "id": "6rftFvMyBa4-"
   },
   "source": [
    "Nun kombinieren wir diese beiden Ketten zu einer. Die Eingabe für die erste Kette ist das ausgewählte Thema. Die erste Kette gibt dann den Titel an die zweite Kette aus, die wiederum den eigentlichen Artikel ausgibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ibHBt4eyKZt",
   "metadata": {
    "id": "2ibHBt4eyKZt"
   },
   "outputs": [],
   "source": [
    "# Erstellen einer vollständige Kette, um einen neuen Blogbeitrag zu erstellen\n",
    "complete_chain = title_chain | article_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V_7P-n_DBvpD",
   "metadata": {
    "id": "V_7P-n_DBvpD"
   },
   "source": [
    "Wir können nun den fertigen Artikel anzeigen. In diesem Fall haben wir einen Artikel zum Thema „Architecture“ angefordert und den Markdown des fertigen Artikels angezeigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mwro_kfKXvml",
   "metadata": {
    "id": "mwro_kfKXvml"
   },
   "outputs": [],
   "source": [
    "article = complete_chain.invoke('Architecture')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h1pN8CbXCE6F",
   "metadata": {
    "id": "h1pN8CbXCE6F"
   },
   "source": [
    "Die eigentliche Anzeige des Markdowns wird durch diesen Code übernommen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vt5uGJawuru_",
   "metadata": {
    "id": "Vt5uGJawuru_"
   },
   "outputs": [],
   "source": [
    "Markdown(article.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VlARJEowAAIR",
   "metadata": {
    "id": "VlARJEowAAIR"
   },
   "source": [
    "# **3 | Überblick LangChain Konzepte**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g5B7bWJKBB-C",
   "metadata": {
    "id": "g5B7bWJKBB-C"
   },
   "source": [
    "Konzepte von LangChain sind grundlegende Bausteine und Prinzipien, die das Framework ausmachen und seine Funktionsweise ermöglichen. Hier sind einige der wichtigsten Konzepte:\n",
    "\n",
    "+ Chat-Modelle (**Chat models**): LLMs, die über eine Chat-API verfügbar sind und Sequenzen von Nachrichten verarbeiten1.\n",
    "+ Nachrichten (**Messages**): Kommunikationseinheiten in Chat-Modellen für Ein- und Ausgabe1.\n",
    "+ Chat-Verlauf (**Chat history**): Eine Sequenz von Nachrichten, die eine Konversation darstellt.\n",
    "+ Tools (**Tools**): Funktionen mit definierten Schemata für Name, Beschreibung und Argumente.\n",
    "+ Strukturierte Ausgabe (**Structured output**): Technik, um Chat-Modelle in strukturierten Formaten antworten zu lassen.\n",
    "+ Retrieval Augmented Generation (**RAG**): Technik zur Verbesserung von Sprachmodellen durch Kombination mit externen Wissensbasen.\n",
    "+ Prompt-Vorlagen (**Prompt template**s): Komponenten zur Erstellung strukturierter Prompts für LLMs.\n",
    "+ Chains (**Chains**): Verknüpfungen mehrerer LLMs oder anderer Komponenten für komplexere Anwendungen.\n",
    "+ Agenten (**Agents**): Nutzen Sprachmodelle, um Aktionssequenzen auszuwählen und mit externen Ressourcen zu interagieren.\n",
    "+ Retriever (**Retriever**): Komponenten, die relevante Dokumente aus einer Wissensbasis abrufen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a96d3df",
   "metadata": {
    "id": "9a96d3df"
   },
   "source": [
    "# **4 | Best Practices**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cKCGHGvuI9mI",
   "metadata": {
    "id": "cKCGHGvuI9mI"
   },
   "source": [
    "\n",
    "\n",
    "1. **Modularität**:\n",
    "   - Baue Anwendungen aus kleinen, wiederverwendbaren Komponenten\n",
    "   - Teste einzelne Komponenten separat\n",
    "   - Nutze Vererbung für eigene Implementierungen\n",
    "\n",
    "2. **Fehlerbehandlung**:\n",
    "   - Implementiere Retry-Mechanismen für API-Aufrufe\n",
    "   - Validiere Eingaben vor der Verarbeitung\n",
    "   - Logge wichtige Zwischenschritte\n",
    "\n",
    "3. **Performance**:\n",
    "   - Nutze Caching für Embeddings und häufige Anfragen\n",
    "   - Optimiere Chunk-Größen für deine Anwendung\n",
    "   - Verwende Batch-Verarbeitung wo möglich\n",
    "\n",
    "4. **Sicherheit und Kosten**:\n",
    "   - Setze Limits für Token und Anfragen\n",
    "   - Überwache API-Kosten\n",
    "   - Implementiere Rate-Limiting\n",
    "\n",
    "5. **Testing**:\n",
    "   - Schreibe Unit-Tests für Chains und Parser\n",
    "   - Teste mit verschiedenen Eingabeformaten\n",
    "   - Simuliere API-Aufrufe in Tests\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7158dc5",
   "metadata": {
    "id": "d7158dc5"
   },
   "source": [
    "# **5 | Weiterführende Ressourcen**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ayc326rsfyJ6",
   "metadata": {
    "id": "ayc326rsfyJ6"
   },
   "source": [
    "\n",
    "\n",
    "- [Offizielle LangChain Dokumentation](https://python.langchain.com/docs/)\n",
    "- [LangChain GitHub Repository](https://github.com/langchain-ai/langchain)\n",
    "- [LangChain Cookbook mit Beispielen](https://github.com/langchain-ai/langchain/tree/master/cookbook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F_5gn03b46rC",
   "metadata": {
    "id": "F_5gn03b46rC"
   },
   "source": [
    "# **A | Aufgabe**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36LyGMmo5jb3",
   "metadata": {
    "id": "36LyGMmo5jb3"
   },
   "source": [
    "Die Aufgabestellungen unten bieten Anregungen, Sie können aber auch gerne eine andere Herausforderung angehen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Je-szbGU7IJ-",
   "metadata": {
    "id": "Je-szbGU7IJ-"
   },
   "source": [
    "\n",
    "<p><font color='black' size=\"5\">\n",
    "Erstellen einer LangChain-Kette zur Textanalyse\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kevePvA-7PzW",
   "metadata": {
    "id": "kevePvA-7PzW"
   },
   "source": [
    "Entwickeln Sie eine Verarbeitungskette, die einen Text analysiert und verschiedene Informationen darüber extrahiert.\n",
    "\n",
    "**Schritte:**\n",
    "1. Erstelle ein ChatPromptTemplate, das ein LLM anweist, einen Text zu analysieren\n",
    "2. Die Analyse soll folgende Aspekte umfassen:\n",
    "   - Hauptthema des Textes\n",
    "   - Tonalität (formal, informell, etc.)\n",
    "   - Schlüsselwörter (5-10)\n",
    "   - Kurze Zusammenfassung (max. 3 Sätze)\n",
    "3. Formatiere die Ausgabe strukturiert mit Markdown-Überschriften\n",
    "4. Teste die Kette mit mindestens zwei verschiedenen Texten\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6oBvtYws7qpB",
   "metadata": {
    "id": "6oBvtYws7qpB"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Few-Shot-Learning für Textklassifikation\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gI7alQcE79ER",
   "metadata": {
    "id": "gI7alQcE79ER"
   },
   "source": [
    "Erstellen Sie ein System, das mithilfe von Few-Shot-Learning Texte in vorgegebene Kategorien klassifiziert.\n",
    "\n",
    "**Schritte:**\n",
    "1. Definiere 3-5 Kategorien für die Klassifikation (z.B. Sport, Politik, Technologie, Kultur)\n",
    "2. Erstelle einen FewShotPromptTemplate mit Beispielen für jede Kategorie\n",
    "3. Entwickle eine Chain, die neue Texte klassifiziert\n",
    "4. Implementiere eine Funktion, die neben der Kategorie auch eine Begründung für die Einordnung liefert\n",
    "5. Teste das System mit verschiedenen Texten, die nicht in den Beispielen vorkommen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l35x2g1e8fa9",
   "metadata": {
    "id": "l35x2g1e8fa9"
   },
   "source": [
    "<p><font color='black' size=\"5\">\n",
    "Q&A-System mit Sequential Chain\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qHFRGWmP8gga",
   "metadata": {
    "id": "qHFRGWmP8gga"
   },
   "source": [
    "Entwickeln Sie ein System, das Fragen zu einem gegebenen Kontext beantwortet, aber zuerst den Kontext zusammenfasst und dann die Frage beantwortet.\n",
    "\n",
    "\n",
    "**Schritte:**\n",
    "\n",
    "+ Erstellen Sie zwei Templates:\n",
    "    + Ein Template zur Zusammenfassung des Kontexts\n",
    "    + Ein Template zur Beantwortung einer Frage basierend auf der Zusammenfassung\n",
    "+ Verbinden Sie die Templates in einer sequentiellen Kette\n",
    "+ Implementieren Sie einen Mechanismus, der die Zusammenfassung und die Antwort getrennt zurückgibt\n",
    "+ Nutzen Sie LCEL (LangChain Expression Language) für die Verkettung\n",
    "+ Testen Sie das System mit verschiedenen Kontexten und Fragen"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "PHDCKRbtgVXy",
    "fGV8FwGmIwmR",
    "EouwC_wdZb-c",
    "-JwJD0rLIyT8",
    "8c859b9f",
    "c314ca71",
    "VlARJEowAAIR",
    "9a96d3df",
    "d7158dc5",
    "F_5gn03b46rC"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{"cells":[{"cell_type":"markdown","id":"37rMyCQz2JNw","metadata":{"id":"37rMyCQz2JNw"},"source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)"]},{"cell_type":"markdown","id":"R5CfUEMJdvFQ","metadata":{"id":"R5CfUEMJdvFQ"},"source":["<p><font size=\"5\" color='grey'> <b>\n","LangChain 101\n","</b></font> </br></p>\n","\n","---"]},{"cell_type":"code","execution_count":1,"id":"dfdhPIzcEYRG","metadata":{"cellView":"form","collapsed":true,"id":"dfdhPIzcEYRG","jupyter":{"outputs_hidden":true},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747679535797,"user_tz":-120,"elapsed":5582,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"6091b4bc-6101-4c83-b399-3c5548599787"},"outputs":[{"output_type":"stream","name":"stdout","text":["Python Version:  3.11.12 (main, Apr  9 2025, 08:55:54) [GCC 11.4.0]\n","\n","Installierte LangChain Bibliotheken:\n","langchain                             0.3.25\n","langchain-core                        0.3.59\n","langchain-text-splitters              0.3.8\n"]}],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  Colab-Umfeld</font> </br></p>\n","# Installierte Python Version\n","import sys\n","print(f\"Python Version: \",sys.version)\n","# Installierte LangChain Bibliotheken\n","print()\n","print(\"Installierte LangChain Bibliotheken:\")\n","\n","!pip list | grep '^langchain'\n","# Unterdr√ºckt die \"DeprecationWarning\" von LangChain f√ºr die Memory-Funktionden\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")"]},{"cell_type":"code","execution_count":2,"id":"WD3Wwr6sESX8","metadata":{"cellView":"form","id":"WD3Wwr6sESX8","executionInfo":{"status":"ok","timestamp":1747679537748,"user_tz":-120,"elapsed":1947,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  SetUp API-Keys (setup_api_keys)</font> </br></p>\n","def setup_api_keys():\n","    \"\"\"Konfiguriert alle ben√∂tigten API-Keys aus Google Colab userdata\"\"\"\n","    from google.colab import userdata\n","    import os\n","    from os import environ\n","\n","    # Dictionary der ben√∂tigten API-Keys\n","    keys = {\n","        'OPENAI_API_KEY': 'OPENAI_API_KEY',\n","        'HF_TOKEN': 'HF_TOKEN',\n","        # Weitere Keys bei Bedarf\n","    }\n","\n","    # Keys in Umgebungsvariablen setzen\n","    for env_var, key_name in keys.items():\n","        environ[env_var] = userdata.get(key_name)\n","\n","    return {k: environ[k] for k in keys.keys()}\n","\n","# Verwendung\n","all_keys = setup_api_keys()\n","# Bei Bedarf einzelne Keys direkt zugreifen\n","# WEATHER_API_KEY = all_keys['WEATHER_API_KEY']"]},{"cell_type":"markdown","id":"91ee89a4-477b-41ce-a511-aadd4c2fd547","metadata":{"id":"91ee89a4-477b-41ce-a511-aadd4c2fd547"},"source":["# 1 | Was ist LangChain?\n","---"]},{"cell_type":"markdown","id":"b5004f8c","metadata":{"id":"b5004f8c"},"source":["LangChain ist ein Framework zur **Entwicklung von Anwendungen mit gro√üen Sprachmodellen** (LLMs). Es vereinfacht die Integration von LLMs in eigene Anwendungen durch:\n","\n","- Modulare Komponenten f√ºr verschiedene Aufgaben\n","- Vorgefertigte Ketten (Chains) f√ºr komplexe Workflows\n","- Einfache Integration externer Datenquellen\n","- Werkzeuge f√ºr das Speichern von Konversationskontexten\n","\n"]},{"cell_type":"markdown","id":"tl73bT-kKkia","metadata":{"id":"tl73bT-kKkia"},"source":["[Einf√ºhrung](https://python.langchain.com/docs/introduction/)   \n","[Konzepte](https://python.langchain.com/docs/concepts/)   \n","[API-References](https://python.langchain.com/api_reference/index.html)   \n","[Integrations](https://python.langchain.com/docs/integrations/providers/)\n","\n","---\n","\n","[Tutorials](https://python.langchain.com/docs/tutorials/)   \n","[How-to-Guides](https://python.langchain.com/docs/how_to/)   "]},{"cell_type":"code","execution_count":3,"id":"JrgQ46eLphEv","metadata":{"id":"JrgQ46eLphEv","executionInfo":{"status":"ok","timestamp":1747679541848,"user_tz":-120,"elapsed":4051,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["!uv pip install --system --prerelease allow -q langchain_community langchain_openai"]},{"cell_type":"markdown","id":"fGV8FwGmIwmR","metadata":{"id":"fGV8FwGmIwmR"},"source":["# 2 | Kernkonzepte\n","---\n","\n","\n"]},{"cell_type":"markdown","source":["Diese einfache Darstellung veranschaulicht das Grundprinzip von LangChain: die Verkettung verschiedener Konzepte zu einem durchg√§ngigen Verarbeitungsprozess, der Eingaben in strukturierte Ausgaben umwandelt."],"metadata":{"id":"nZ2ZMEd3ulKZ"},"id":"nZ2ZMEd3ulKZ"},{"cell_type":"markdown","source":["<img src=\"https://raw.githubusercontent.com/ralf-42/Image/main/langchain_prozess.png\" width=\"207\" alt=\"Avatar\">"],"metadata":{"id":"KdBKNv3gzJAq"},"id":"KdBKNv3gzJAq"},{"cell_type":"markdown","id":"EouwC_wdZb-c","metadata":{"id":"EouwC_wdZb-c"},"source":["## 2.1 Message-Typen"]},{"cell_type":"markdown","id":"yS2mM2J1aXlY","metadata":{"id":"yS2mM2J1aXlY"},"source":["Beim Einsatz von Large Language Models (LLMs) wie GPT gibt es drei Hauptarten von Nachrichten (Messages), die die Interaktion mit dem Modell steuern. Diese Messages definieren, wie das Modell reagiert, welche Eingaben es erh√§lt und welche Ausgaben es generiert.\n","\n"]},{"cell_type":"markdown","id":"sGqRXptbbQ2a","metadata":{"id":"sGqRXptbbQ2a"},"source":["**System Message:**   \n","Die `System Message` dient dazu, das Verhalten des LLMs zu steuern. Sie ist eine nicht sichtbare Nachricht, die dem Modell Anweisungen gibt, wie es sich verhalten soll. Dabei kann sie die Pers√∂nlichkeit, den Tonfall und die Einschr√§nkungen des Modells beeinflussen. Die Systemnachricht funktioniert wie eine Art \"Rollenspiel-Anweisung\", um das Modell in eine gew√ºnschte Rolle zu versetzen.\n","\n","*Merkmale:*\n","- Definiert, wie das Modell reagieren soll\n","- Legt Pers√∂nlichkeit, Verhalten und Einschr√§nkungen fest\n","- Wird zu Beginn einer Sitzung gesetzt und bleibt bestehen\n","- Nicht sichtbar f√ºr den Benutzer\n","\n","*Beispiel einer System Message:*\n","```json\n","{\n","  \"role\": \"system\",\n","  \"content\": \"Du bist ein KI-Assistent, der pr√§zise und informative Antworten in einem professionellen Ton liefert.\"\n","}\n","```\n","Dieses Beispiel weist das Modell an, in einem professionellen Tonfall kurze und informative Antworten zu geben.\n","\n","---\n","\n"]},{"cell_type":"markdown","id":"upsIzpcqbbP7","metadata":{"id":"upsIzpcqbbP7"},"source":["**User Message**   \n","Die `User Message` ist die eigentliche Eingabe des Benutzers. Sie kann eine Frage, eine Aufforderung, kontextrelevanten Inhalt oder ein Befehl sein. Das Modell nutzt diese Nachricht als Ausgangspunkt f√ºr die Generierung einer Antwort.\n","\n","*Merkmale:*\n","- Direkte Eingabe des Benutzers\n","- Kann eine Frage, Aufforderung, kontextrelevanten Inhalt oder einen Befehl enthalten\n","- Basis f√ºr die Antwort des Modells\n","\n","*Beispiel einer User Message:*\n","```json\n","{\n","  \"role\": \"user\",\n","  \"content\": \"Was sind die wichtigsten Unterschiede zwischen KI und maschinellem Lernen?\"\n","}\n","```\n","Hier fragt der Benutzer nach den Unterschieden zwischen KI und maschinellem Lernen.\n","\n","---\n"]},{"cell_type":"markdown","id":"CozrrtYXbhZb","metadata":{"id":"CozrrtYXbhZb"},"source":["**AI Message**   \n","Die `AI Message` ist die Antwort, die das Modell generiert. Sie basiert auf der User Message und den Anweisungen aus der System Message. Die AI Message kann verschiedene Eigenschaften haben, ist aber haupts√§chlich auf den Inhalt fokussiert.\n","\n","*Merkmale:*\n","- Antwort des Modells auf die Benutzeranfrage\n","- Kann verschiedene Eigenschaften haben (z. B. L√§nge, Stil)\n","- Haupts√§chlich inhaltlich relevant\n","\n","*Beispiel einer AI Message:*\n","```json\n","{\n","  \"role\": \"assistant\",\n","  \"content\": \"K√ºnstliche Intelligenz (KI) ist ein weites Feld, das verschiedene Teilgebiete umfasst, darunter auch das maschinelle Lernen (ML). ML konzentriert sich auf die Erstellung von Modellen, die aus Daten lernen und Vorhersagen treffen.\"\n","}\n","```\n","Hier gibt das Modell eine inhaltliche Antwort auf die Frage des Benutzers.\n","\n","---\n","\n"]},{"cell_type":"markdown","id":"PiHnhKHVbofP","metadata":{"id":"PiHnhKHVbofP"},"source":["**Zusammenhang**   \n","Die drei Message-Typen arbeiten zusammen, um eine sinnvolle Interaktion mit dem Modell zu erm√∂glichen:\n","\n","+  **System Message** legt die Regeln und das Verhalten des Modells fest.\n","+ **User Message** stellt eine Anfrage oder einen Befehl.\n","+ **AI Message** gibt die generierte Antwort basierend auf den vorherigen Nachrichten.\n","\n","**Beispiel f√ºr eine vollst√§ndige Konversation:**    \n","```json\n","[\n","  {\n","    \"role\": \"system\",\n","    \"content\": \"Sie sind ein KI-Assistent, der pr√§zise und informative Antworten in einem professionellen Ton liefert.\"\n","  },\n","  {\n","    \"role\": \"user\",\n","    \"content\": \"Kannst der erkl√§ren, was gro√üe Sprachtmodelle sind?\"\n","  },\n","  {\n","    \"role\": \"assistant\",\n","    \"content\": \"Sicher! Large Language Models (LLMs) sind fortschrittliche KI-Systeme, die mit riesigen Textdatenmengen trainiert werden. Sie k√∂nnen menschen√§hnliche Texte verstehen und generieren und sind daher f√ºr verschiedene Anwendungen wie Chatbots, Content-Erstellung und mehr n√ºtzlich.\"\n","  }\n","]\n","```\n","In diesem Beispiel wurde das Modell so eingestellt, dass es in einem freundlichen Tonfall antwortet, dann erh√§lt es eine Benutzeranfrage und gibt eine passende Antwort.\n","\n"]},{"cell_type":"markdown","id":"Iw3Ovw5mcFcY","metadata":{"id":"Iw3Ovw5mcFcY"},"source":["**Fazit**    \n","Die System-, User- und AI-Message sind essenzielle Bestandteile f√ºr die Interaktion mit einem LLM. W√§hrend die System Message das Verhalten vorgibt, stellt die User Message die eigentliche Anfrage dar, auf die das Modell mit einer AI Message antwortet. Dieses Konzept ist zentral f√ºr den Einsatz von LLMs in Chatbots, KI-gest√ºtzten Assistenten und anderen interaktiven Anwendungen.\n","\n","\n","Bei einem `einfachen` Dialog kann man jedoch auf diese Trennung verzichten,\n","\n"]},{"cell_type":"markdown","id":"-JwJD0rLIyT8","metadata":{"id":"-JwJD0rLIyT8"},"source":["## 2.2 Prompts"]},{"cell_type":"markdown","source":["\n","\n","Prompts sind die Eingaben bzw. Auftr√§ge an das LLM. LangChain bietet verschiedene Template-Systeme. Ein **Template** ist eine **Vorlage** ‚Äì genauer gesagt ein Text mit **Platzhaltern**, die sp√§ter durch echte Werte ersetzt werden.\n","\n","\n","Ein **PromptTemplate** ist eine Vorlage f√ºr einen Eingabetext, der an ein Sprachmodell geschickt wird.\n","Er enth√§lt Platzhalter ({}), die sp√§ter mit Nutzereingaben oder Kontext bef√ºllt werden.\n","\n","**Templates machen die KI-Prompts:**\n","\n","+ Wiederverwendbar (z.‚ÄØB. f√ºr viele Themen oder Fragen)\n","\n","+ Strukturiert (z.‚ÄØB. mit Rollen und Anweisungen)\n","\n","+ Dynamisch erweiterbar (mit Memory, Tools, etc.)"],"metadata":{"id":"QMVO4fzPFZ88"},"id":"QMVO4fzPFZ88"},{"cell_type":"markdown","id":"HqTUeMW20084","metadata":{"id":"HqTUeMW20084"},"source":["<p><font color='black' size=\"5\">\n","Simple Prompt (ohne Rollen)\n","</font></p>"]},{"cell_type":"code","execution_count":4,"id":"e1926d03","metadata":{"id":"e1926d03","executionInfo":{"status":"ok","timestamp":1747679549039,"user_tz":-120,"elapsed":7188,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","from langchain.prompts import PromptTemplate, ChatPromptTemplate\n","from langchain.prompts.few_shot import FewShotPromptTemplate\n","from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n","\n","from IPython.display import display, Markdown"]},{"cell_type":"code","source":["MODEL = \"gpt-4o-mini\"\n","TEMPERATUR = 0.0"],"metadata":{"id":"RG-y_bG8HOOi","executionInfo":{"status":"ok","timestamp":1747679549074,"user_tz":-120,"elapsed":31,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"RG-y_bG8HOOi","execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Modell definieren\n","llm = ChatOpenAI(model=MODEL, temperature=TEMPERATUR)"],"metadata":{"id":"yd3MebxgGYgy","executionInfo":{"status":"ok","timestamp":1747679553892,"user_tz":-120,"elapsed":4816,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"yd3MebxgGYgy","execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["**Prompt wird als String √ºbergeben:**"],"metadata":{"id":"1azC75R5JpWC"},"id":"1azC75R5JpWC"},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(\"Was ist Generative KI?\")"],"metadata":{"id":"jPizca8FJV5b","executionInfo":{"status":"ok","timestamp":1747679560613,"user_tz":-120,"elapsed":6686,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"jPizca8FJV5b","execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Ausgabe\n","display(Markdown(\"## üì£ Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":365},"id":"cOJGeiLGJb-g","executionInfo":{"status":"ok","timestamp":1747679560748,"user_tz":-120,"elapsed":83,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"8d42aeff-098c-4f33-c1f4-7e590f2c9241"},"id":"cOJGeiLGJb-g","execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Generative KI, oder generative k√ºnstliche Intelligenz, bezieht sich auf eine Klasse von Algorithmen und Modellen, die in der Lage sind, neue Inhalte zu erzeugen. Diese Inhalte k√∂nnen in verschiedenen Formen vorliegen, darunter Texte, Bilder, Musik, Videos und mehr. Generative KI nutzt h√§ufig Techniken des maschinellen Lernens, insbesondere neuronale Netzwerke, um Muster in bestehenden Daten zu erkennen und darauf basierend neue, √§hnliche Daten zu generieren.\n\nEin bekanntes Beispiel f√ºr generative KI sind Sprachmodelle wie GPT (Generative Pre-trained Transformer), die in der Lage sind, menschen√§hnliche Texte zu schreiben, Fragen zu beantworten oder Geschichten zu erz√§hlen. In der Bildgenerierung gibt es Modelle wie DALL-E oder Midjourney, die aus Textbeschreibungen Bilder erstellen k√∂nnen.\n\nGenerative KI hat viele Anwendungen, darunter:\n\n1. **Kreatives Schreiben**: Unterst√ºtzung bei der Erstellung von Geschichten, Artikeln oder Gedichten.\n2. **Bild- und Kunstgenerierung**: Erzeugung von Kunstwerken oder Designideen.\n3. **Musikkomposition**: Erstellung neuer Musikst√ºcke oder Melodien.\n4. **Spieleentwicklung**: Generierung von Inhalten wie Levels oder Charakteren.\n5. **Simulationen**: Erzeugung realistischer Daten f√ºr Trainings- und Testzwecke in verschiedenen Bereichen.\n\nDie Technologie hat das Potenzial, kreative Prozesse zu revolutionieren, wirft jedoch auch ethische und rechtliche Fragen auf, insbesondere in Bezug auf Urheberrecht, F√§lschungen und die Verbreitung von Fehlinformationen."},"metadata":{}}]},{"cell_type":"markdown","source":["**Prompt wird als variabler String √ºbergeben:**"],"metadata":{"id":"aKRhzB0oJnUG"},"id":"aKRhzB0oJnUG"},{"cell_type":"code","source":["# Pure Python L√∂sung f√ºr simple prompt mit template\n","thema = \"Machine Learning\"\n","prompt = f\"Erkl√§re {thema} in einfachen Worten.\""],"metadata":{"id":"8s5cQY2oHUvW","executionInfo":{"status":"ok","timestamp":1747679560749,"user_tz":-120,"elapsed":71,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"8s5cQY2oHUvW","execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Aufruf\n","response = llm.invoke(prompt)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1747679563730,"user_tz":-120,"elapsed":3045,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"id":"plYGfaoLJ8DI"},"execution_count":10,"outputs":[],"id":"plYGfaoLJ8DI"},{"cell_type":"code","source":["# Ausgabe\n","display(Markdown(\"## üì£ Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":222},"executionInfo":{"status":"ok","timestamp":1747679563828,"user_tz":-120,"elapsed":96,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"73dfeedd-8c34-48aa-fd52-596411a56e61","id":"SLX0zaONJ8DJ"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Machine Learning, oder maschinelles Lernen, ist ein Teilbereich der K√ºnstlichen Intelligenz, der es Computern erm√∂glicht, aus Daten zu lernen und Muster zu erkennen, ohne dass sie daf√ºr explizit programmiert werden m√ºssen. \n\nStell dir vor, du hast viele Bilder von Katzen und Hunden. Anstatt einem Computer genau zu sagen, wie eine Katze oder ein Hund aussieht, zeigst du ihm einfach viele Beispiele. Der Computer analysiert diese Bilder und lernt, die Unterschiede zwischen Katzen und Hunden zu erkennen. \n\nWenn du ihm dann ein neues Bild zeigst, kann er basierend auf dem, was er gelernt hat, vorhersagen, ob es sich um eine Katze oder einen Hund handelt. \n\nZusammengefasst: Machine Learning ist wie das Lernen eines Menschen ‚Äì je mehr Informationen und Erfahrungen er hat, desto besser kann er Entscheidungen treffen oder Vorhersagen machen."},"metadata":{}}],"id":"SLX0zaONJ8DJ"},{"cell_type":"markdown","source":["**Prompt wird als template √ºbergeben:**"],"metadata":{"id":"n-VVZEHDKSIw"},"id":"n-VVZEHDKSIw"},{"cell_type":"code","source":["# 1. Einfaches Template mit Platzhaltern\n","simple_prompt_template = PromptTemplate.from_template(\n","    \"Erkl√§re {thema} in einfachen Worten.\"\n",")"],"metadata":{"id":"koh0-_3r_HxG","executionInfo":{"status":"ok","timestamp":1747679563831,"user_tz":-120,"elapsed":90,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"koh0-_3r_HxG","execution_count":12,"outputs":[]},{"cell_type":"code","source":["thema = \"Generative KI\"\n","prompt = simple_prompt_template.format(thema=thema)\n","response = llm.invoke(prompt)"],"metadata":{"id":"nFrCjL6j_xo1","executionInfo":{"status":"ok","timestamp":1747679566447,"user_tz":-120,"elapsed":2572,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"nFrCjL6j_xo1","execution_count":13,"outputs":[]},{"cell_type":"code","source":["display(Markdown(\"## üì£ Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response.content))"],"metadata":{"id":"lmEE-3NDAE15","colab":{"base_uri":"https://localhost:8080/","height":181},"executionInfo":{"status":"ok","timestamp":1747679566454,"user_tz":-120,"elapsed":4,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"a5a273b8-6f94-4fce-ec48-c647c7ca5ac2"},"id":"lmEE-3NDAE15","execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Generative KI ist eine Art von k√ºnstlicher Intelligenz, die in der Lage ist, neue Inhalte zu erstellen. Das k√∂nnen Texte, Bilder, Musik oder sogar Videos sein. Sie funktioniert, indem sie aus gro√üen Mengen von Daten lernt, wie bestimmte Inhalte aussehen oder klingen. \n\nStell dir vor, du hast viele Bilder von Katzen und Hunden. Eine generative KI kann dann lernen, wie diese Tiere aussehen, und neue Bilder von Katzen oder Hunden erstellen, die es so noch nicht gibt. \n\nIm Grunde genommen ist generative KI wie ein kreativer Computer, der neue Ideen und Inhalte basierend auf dem, was er gelernt hat, entwickeln kann."},"metadata":{}}]},{"cell_type":"markdown","source":["**Vorteil PromptTemplate vs f-string**"],"metadata":{"id":"VxlzHPsbJENM"},"id":"VxlzHPsbJENM"},{"cell_type":"markdown","source":["| Kriterium                        | `f-string`                | `PromptTemplate`                         |\n","| -------------------------------- | ------------------------- | ---------------------------------------- |\n","| üß± Basisfunktion                 | String zusammenbauen      | String-Vorlage mit Platzhaltern          |\n","| üîÅ Wiederverwendbarkeit          | manuell                   | sehr gut (Template + `.format()`)        |\n","| üîå Integration mit LangChain     | ‚ùå Nur als fertiger Text   | ‚úÖ Nahtlos (in Chains, mit LLMs, etc.)    |\n","| üì¶ Kombinierbar mit Tools        | ‚ùå Nein                    | ‚úÖ Ja (Memory, OutputParser, Agents)      |\n","| üß† Erweiterbar (z.‚ÄØB. Beispiele) | ‚ùå M√ºhsam                  | ‚úÖ z.‚ÄØB. `FewShotPromptTemplate`          |\n","| üïµÔ∏è Nachvollziehbarkeit          | schwer bei gro√üen Prompts | gut dokumentierbar & testbar             |\n","| üë• Rollensteuerung               | ‚ùå Nur manuell             | ‚úÖ mit `ChatPromptTemplate`               |\n","| üß™ Validierung                   | ‚ùå keine                   | ‚úÖ Platzhalterpr√ºfung (`input_variables`) |\n"],"metadata":{"id":"pWT8Q3AkJCZe"},"id":"pWT8Q3AkJCZe"},{"cell_type":"markdown","id":"8akKmCCc06nm","metadata":{"id":"8akKmCCc06nm"},"source":["<p><font color='black' size=\"5\">\n","Chat Prompt (mit Rollen)\n","</font></p>"]},{"cell_type":"markdown","source":["Das **ChatPromptTemplate** ist eine spezielle Prompt-Vorlage f√ºr Chatmodelle (z.‚ÄØB. GPT-3.5, GPT-4), die **mehrere Rollen und Nachrichten** unterst√ºtzt ‚Äì also genau das, was Chat-Modelle eigentlich brauchen.\n","\n","Es geh√∂rt zur LangChain-Bibliothek und baut auf dem Prinzip auf:\n","\n","\n","**Wer sagt was? ‚Üí system, user, assistant, etc.**"],"metadata":{"id":"SVoHwxtrMM_j"},"id":"SVoHwxtrMM_j"},{"cell_type":"code","execution_count":15,"id":"iW9URvON0IXr","metadata":{"id":"iW9URvON0IXr","executionInfo":{"status":"ok","timestamp":1747679566546,"user_tz":-120,"elapsed":90,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# prompt-template als tuple\n","chat_template = ChatPromptTemplate([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"user\", \"Erkl√§re mir {thema}\"),\n","])"]},{"cell_type":"markdown","source":["**... oder ...**"],"metadata":{"id":"6FrHMy1PYH0S"},"id":"6FrHMy1PYH0S"},{"cell_type":"code","source":["# prompt-template als dict\n","chat_template = ChatPromptTemplate([\n","    {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher und humorvoller Assistent.\"},\n","    {\"role\": \"user\", \"content\": \"Erkl√§re mir {thema}\"},\n","])"],"metadata":{"id":"cGkROwH9YEyv","executionInfo":{"status":"ok","timestamp":1747679566550,"user_tz":-120,"elapsed":18,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"cGkROwH9YEyv","execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["**... oder ...**"],"metadata":{"id":"qxuO76Z9YpDE"},"id":"qxuO76Z9YpDE"},{"cell_type":"code","source":["# prompt-template als Nachrichtenobjekte\n","chat_template = ChatPromptTemplate([\n","    SystemMessage(content=\"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    HumanMessage(content=\"Erkl√§re mir {thema}\"),\n","])"],"metadata":{"id":"w3OFTq0XYlgl","executionInfo":{"status":"ok","timestamp":1747679566551,"user_tz":-120,"elapsed":14,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"w3OFTq0XYlgl","execution_count":17,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"jwrybdk_0agA","metadata":{"id":"jwrybdk_0agA","executionInfo":{"status":"aborted","timestamp":1747679600102,"user_tz":-120,"elapsed":11624,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["thema = \"Machine Learning\"\n","prompt = chat_template.format_messages(thema=thema)"]},{"cell_type":"code","source":["for message in prompt:\n","    print(f\"Role: {message.type}, Content: {message.content}\")"],"metadata":{"id":"HnGeULGPd9hO","executionInfo":{"status":"aborted","timestamp":1747679600106,"user_tz":-120,"elapsed":7765,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"HnGeULGPd9hO","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aufrufen der Kette mit Eingaben\n","response = llm.invoke(prompt)"],"metadata":{"id":"ZLeSLoy_CLpI","executionInfo":{"status":"ok","timestamp":1747679567443,"user_tz":-120,"elapsed":875,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"ZLeSLoy_CLpI","execution_count":20,"outputs":[]},{"cell_type":"code","source":["display(Markdown(\"## üì£ Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response.content))"],"metadata":{"id":"6irKh0y0C4nu","colab":{"base_uri":"https://localhost:8080/","height":99},"executionInfo":{"status":"ok","timestamp":1747679567521,"user_tz":-120,"elapsed":76,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"07000182-6954-44a5-b949-f01470f93780"},"id":"6irKh0y0C4nu","execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Nat√ºrlich! Ich helfe dir gerne. Welches Thema m√∂chtest du erkl√§rt bekommen?"},"metadata":{}}]},{"cell_type":"markdown","source":["**Vorteile gegen√ºber `PromptTemplate`**"],"metadata":{"id":"I8EIgo8DMaYO"},"id":"I8EIgo8DMaYO"},{"cell_type":"markdown","source":["| Vorteil                      | `PromptTemplate`     | `ChatPromptTemplate`            |\n","| ---------------------------- | -------------------- | ------------------------------- |\n","| üöª Rollenstruktur            | ‚ùå Nur einfacher Text | ‚úÖ Klar: `system`, `user`, `ai`  |\n","| üìö Konversationsaufbau       | ‚ùå Nicht geeignet     | ‚úÖ Ideal f√ºr Multi-Turn-Dialoge  |\n","| üîÑ Kombinierbar mit `Memory` | ‚ùå Nein               | ‚úÖ Ja, via `MessagesPlaceholder` |\n","| üîå F√ºr Chatmodelle wie GPT   | ‚ö†Ô∏è Nur als Flie√ütext | ‚úÖ Nativ & strukturiert          |\n","| üîç Klarheit & Wartbarkeit    | ‚ö†Ô∏è Nur Inline-Text   | ‚úÖ Saubere Message-Trennung      |\n","| üß™ Prompt-Testbarkeit        | ‚ùå Eingeschr√§nkt      | ‚úÖ Besser strukturiert           |\n"],"metadata":{"id":"2uSD48umMXI7"},"id":"2uSD48umMXI7"},{"cell_type":"markdown","id":"Ps344Ejw0-ih","metadata":{"id":"Ps344Ejw0-ih"},"source":["<p><font color='black' size=\"5\">\n","Few-Shot-Learning Prompt\n","</font></p>"]},{"cell_type":"markdown","source":["`FewShotPromptTemplate` ist eine Prompt-Vorlage, die automatisch mehrere Beispiele einf√ºgt, bevor das eigentliche Nutzer-Input kommt."],"metadata":{"id":"NnBh2dz8M2fi"},"id":"NnBh2dz8M2fi"},{"cell_type":"code","execution_count":22,"id":"cEopgOiS0KU_","metadata":{"id":"cEopgOiS0KU_","executionInfo":{"status":"ok","timestamp":1747679567532,"user_tz":-120,"elapsed":7,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["examples = [\n","    {\"frage\": \"Was ist Python?\", \"antwort\": \"Eine interpretierende Programmiersprache.\"},\n","    {\"frage\": \"Was ist Java?\", \"antwort\": \"Eine objektorientierte Sprache.\"}\n","]"]},{"cell_type":"code","source":["# Few-Shot-Learning als String\n","few_shot_prompt = f\"\"\"\n","Du bist ein Assistent, der Fragen zu Programmiersprachen beantwortet. Halte deine Antworten kurz und pr√§zise.\n","\n","Beispiel 1:\n","Frage: {examples[0]['frage']}\n","Antwort: {examples[0]['antwort']}\n","\n","Beispiel 2:\n","Frage: {examples[1]['frage']}\n","Antwort: {examples[1]['antwort']}\n","\n","Beantworte nun die folgende Frage im gleichen Stil:\n","Frage: {{frage}}\n","Antwort:\n","\"\"\""],"metadata":{"id":"0pn8fDgQgka-","executionInfo":{"status":"ok","timestamp":1747679567534,"user_tz":-120,"elapsed":6,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"0pn8fDgQgka-","execution_count":23,"outputs":[]},{"cell_type":"code","source":["frage = \"Was ist JavaScript?\"\n","prompt = few_shot_prompt.format(frage=frage)"],"metadata":{"id":"dDq-RkD4iVex","executionInfo":{"status":"ok","timestamp":1747679567536,"user_tz":-120,"elapsed":1,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"dDq-RkD4iVex","execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["**... oder ...**"],"metadata":{"id":"NIS2M0Dbhvun"},"id":"NIS2M0Dbhvun"},{"cell_type":"code","source":["# Prompt f√ºr einzelne Beispiele\n","example_prompt = PromptTemplate.from_template(\n","    \"Frage: {frage}\\nAntwort: {antwort}\"\n",")"],"metadata":{"id":"fHEYrEPcDD9S","executionInfo":{"status":"ok","timestamp":1747679567541,"user_tz":-120,"elapsed":2,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"fHEYrEPcDD9S","execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Few-Shot-Prompt\n","few_shot_prompt = FewShotPromptTemplate(\n","    examples=examples,\n","    example_prompt=example_prompt,\n","    prefix=\"Du bist ein Assistent, der Fragen zu Programmiersprachen beantwortet. Halte deine Antworten kurz und pr√§zise:\",\n","    suffix=\"Frage: {input}\\nAntwort:\",\n","    input_variables=[\"input\"]\n",")\n","\n","prompt = few_shot_prompt.format(input=\"Was ist JavaScript?\")"],"metadata":{"id":"BpIbLhIhDcUp","executionInfo":{"status":"ok","timestamp":1747679567569,"user_tz":-120,"elapsed":1,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"BpIbLhIhDcUp","execution_count":26,"outputs":[]},{"cell_type":"code","source":["# geringere Temperaturen f√ºr mehr Sachlichkeit\n","llm = ChatOpenAI(model=MODEL, temperature=0.2)\n","response = llm.invoke(prompt)"],"metadata":{"id":"KZPoIO6gDo23","executionInfo":{"status":"ok","timestamp":1747679568277,"user_tz":-120,"elapsed":692,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"KZPoIO6gDo23","execution_count":27,"outputs":[]},{"cell_type":"code","source":["display(Markdown(\"## üì£ Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(response.content))"],"metadata":{"id":"Yr0YleeiD_x7","colab":{"base_uri":"https://localhost:8080/","height":99},"executionInfo":{"status":"ok","timestamp":1747679568317,"user_tz":-120,"elapsed":32,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"03b1c9e4-23af-455f-9216-d5ab489ac4d3"},"id":"Yr0YleeiD_x7","execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Eine Skriptsprache f√ºr die Webentwicklung."},"metadata":{}}]},{"cell_type":"markdown","source":["**Vorteile eines `FewShotPromptTemplate`**"],"metadata":{"id":"XFbved9KM_WJ"},"id":"XFbved9KM_WJ"},{"cell_type":"markdown","source":["| Vorteil                      | Beschreibung                                                      |\n","| ---------------------------- | ----------------------------------------------------------------- |\n","| üß† Lerneffekt f√ºr das Modell | Das Modell erkennt aus Beispielen, wie es antworten soll          |\n","| üß∞ Flexibilit√§t              | Beispiele k√∂nnen dynamisch gesetzt oder generiert werden          |\n","| üß± Struktur & Konsistenz     | Einheitlicher Aufbau aller Beispiele √ºber `example_prompt`        |\n","| ü™Ñ Kein Training n√∂tig       | Kein Fine-Tuning ‚Äì nur durch Prompting                            |\n","| üõ† Kombinierbar              | Kann mit Templates, Tools, Chains, OutputParser kombiniert werden |\n"],"metadata":{"id":"EWjuAd2_M-E7"},"id":"EWjuAd2_M-E7"},{"cell_type":"markdown","id":"8c859b9f","metadata":{"id":"8c859b9f"},"source":["## 2.3 Modelle\n","\n","LangChain unterst√ºtzt verschiedene LLMs und bietet flexible Konfigurationsm√∂glichkeiten:"]},{"cell_type":"markdown","source":["**Beispiel `gpt-4o-mini` und `o3-mini`**"],"metadata":{"id":"Oc0UPT5eTyTR"},"id":"Oc0UPT5eTyTR"},{"cell_type":"markdown","source":["+ gpt-4o-mini ist ein kompaktes, multimodales KI-Modell von OpenAI, das Text- und Bildeingaben verarbeiten kann und besonders f√ºr schnelle, allt√§gliche Aufgaben sowie kleinere Anwendungen optimiert wurde\n","\n","+ o3-mini ist ein spezialisiertes KI-Modell, das vor allem f√ºr komplexe Wissensverarbeitung, anspruchsvolle Denkaufgaben und professionelle Programmierprojekte entwickelt wurde"],"metadata":{"id":"f8yl7OApTvcQ"},"id":"f8yl7OApTvcQ"},{"cell_type":"markdown","source":["**Modellparameter:**"],"metadata":{"id":"fxdaHMICUDPT"},"id":"fxdaHMICUDPT"},{"cell_type":"markdown","source":["| **Parameter**       | **`gpt-4o-mini`**  | **`o3-mini`**  | **Erkl√§rung**                                                                |\n","| ------------------- | ------------------- | ----------------- | ---------------------------------------------------------------------------- |\n","| `model_name`        | ‚úÖ                   | ‚úÖ                 | Modellbezeichnung z.‚ÄØB. `\"gpt-4o-mini\"` oder `\"o3-mini\"`                     |\n","| `temperature`       | ‚úÖ                   | ‚ùå                 | Kreativit√§t/Zufall in der Antwort (0.0 = deterministisch, 1.0 = kreativ)     |\n","| `top_p`             | ‚úÖ                   | ‚ùå                 | Nucleus Sampling (alternative zu `temperature`)                              |\n","| `max_tokens`        | ‚úÖ                   | ‚úÖ                 | Maximale L√§nge der generierten Antwort                                       |\n","| `frequency_penalty` | ‚úÖ                   | ‚ùå                 | Bestraft Wiederholungen im Output                                            |\n","| `presence_penalty`  | ‚úÖ                   | ‚ùå                 | F√∂rdert neue Inhalte statt Wiederholungen                                    |\n","| `streaming`         | ‚úÖ                   | ‚úÖ                 | Tokenweise Live-Ausgabe des Texts (Streaming API)                            |\n","| `seed`              | ‚úÖ *(via kwargs)*    | ‚úÖ *(via kwargs)*  | Fixiert Zufallselemente (Reproduzierbarkeit)                                 |\n","| `response_format`   | ‚úÖ *(via kwargs)*    | ‚úÖ *(via kwargs)*  | Format wie `\"text\"` oder `\"json\"`                                            |\n","| `logit_bias`        | ‚úÖ *(via kwargs)*    | ‚úÖ *(via kwargs)*  | Gewichtung bestimmter Tokens                                                 |\n","| `tool_choice`       | ‚úÖ *(via kwargs)*    | ‚úÖ *(via kwargs)*  | Tool-Auswahl f√ºr Function Calling                                            |\n","| `reasoning_effort`  | ‚ùå                   | ‚úÖ                 | Speziell f√ºr o3-Modelle: `\"low\"`, `\"medium\"`, `\"high\"` ‚Äì steuert Denkaufwand |\n"],"metadata":{"id":"viwfwnIETILD"},"id":"viwfwnIETILD"},{"cell_type":"code","source":["# Modelle konfigurieren\n","\n","# Chat-Modell\n","chat_model = ChatOpenAI(\n","    model_name=\"gpt-4o-mini\",\n","    temperature=0.9\n",")\n","\n","# Reasoning-Modell\n","resoning_model = ChatOpenAI(\n","    model_name=\"o3-mini\",\n",")"],"metadata":{"id":"aDGqhlxoENvs","executionInfo":{"status":"ok","timestamp":1747679568590,"user_tz":-120,"elapsed":271,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"aDGqhlxoENvs","execution_count":29,"outputs":[]},{"cell_type":"code","source":["# Gemeinsamer simple prompt\n","prompt = \"Was ist der Unterschied zwischen Klassifikation und Regression im Machine Learning?\""],"metadata":{"id":"PywoNdLrPCfO","executionInfo":{"status":"ok","timestamp":1747679568772,"user_tz":-120,"elapsed":118,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"PywoNdLrPCfO","execution_count":30,"outputs":[]},{"cell_type":"code","source":["# Aufruf\n","result = chat_model.invoke(prompt)"],"metadata":{"id":"iriw2JZPOazT","executionInfo":{"status":"ok","timestamp":1747679573896,"user_tz":-120,"elapsed":5101,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"iriw2JZPOazT","execution_count":31,"outputs":[]},{"cell_type":"code","source":["display(Markdown(\"## üì£ Chat-Model:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":523},"id":"9OlEy8jQPck1","executionInfo":{"status":"ok","timestamp":1747679573929,"user_tz":-120,"elapsed":34,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"8e5a80f9-17b7-482b-8df9-89d020fd38c8"},"id":"9OlEy8jQPck1","execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Chat-Model:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Klassifikation und Regression sind zwei grundlegende Arten von Aufgaben im Machine Learning, die sich in ihrem Ziel und ihrer Herangehensweise unterscheiden:\n\n### Klassifikation\n- **Ziel**: Bei der Klassifikation geht es darum, Datenpunkte in vordefinierte Kategorien oder Klassen einzuordnen. \n- **Ausgabe**: Die Ausgabe ist diskret, d.h. die Modelle liefern eine Klassenzugeh√∂rigkeit (z.B. \"Ja\" oder \"Nein\", \"Hund\" oder \"Katze\").\n- **Beispiele**: \n  - E-Mail-Spam-Filter (Spam oder Nicht-Spam)\n  - Bildklassifikation (Identifizierung von Objekten in Bildern)\n  - Sentiment-Analyse (positiv, neutral, negativ)\n\n### Regression\n- **Ziel**: In der Regression hingegen wird versucht, eine kontinuierliche Ausgabe vorherzusagen. Es geht darum, quantitative Werte zu sch√§tzen.\n- **Ausgabe**: Die Ausgabe ist kontinuierlich, d.h. die Modelle liefern numerische Werte (z.B. Preise, Temperaturen).\n- **Beispiele**:\n  - Vorhersage von Hauspreisen basierend auf verschiedenen Merkmalen (z.B. Gr√∂√üe, Lage)\n  - Wettervorhersage (Temperatur, Niederschlag)\n  - Umsatzprognose f√ºr ein Unternehmen\n\n### Zusammenfassung\n- Klassifikation: Diskrete Ausgaben, Zuordnung zu Klassen.\n- Regression: Kontinuierliche Ausgaben, Vorhersage von numerischen Werten.\n\nBeide Ans√§tze verwenden oft √§hnliche Algorithmen und Techniken, aber die Wahl zwischen Klassifikation und Regression h√§ngt vom spezifischen Anwendungsfall und den Anforderungen an die Vorhersage ab."},"metadata":{}}]},{"cell_type":"code","source":["# Aufruf\n","result = resoning_model.invoke(prompt)"],"metadata":{"id":"_Znkwm2SPOrX","executionInfo":{"status":"ok","timestamp":1747679582867,"user_tz":-120,"elapsed":8936,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"_Znkwm2SPOrX","execution_count":33,"outputs":[]},{"cell_type":"code","source":["display(Markdown(\"## üì£ Reasoning-Model:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(result.content))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"id":"N0hcQkSfQyVK","executionInfo":{"status":"error","timestamp":1747679614421,"user_tz":-120,"elapsed":46,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"fb6708b7-155a-498f-845a-4114143d46ac"},"id":"N0hcQkSfQyVK","execution_count":50,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Reasoning-Model:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"error","ename":"AttributeError","evalue":"'dict' object has no attribute 'content'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-cbeb4fc5cc2e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"## üì£ Reasoning-Model:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'content'"]}]},{"cell_type":"markdown","source":["**üìä Vergleich: Chat-Modell vs. Reasoning-Modell**"],"metadata":{"id":"YJljcFw4Rz6W"},"id":"YJljcFw4Rz6W"},{"cell_type":"markdown","source":["| Kriterium                 | üì£ **Chat-Modell**                                                                                             | üß† **Reasoning-Modell**                                                                                      |\n","| ------------------------- | -------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |\n","| **Stil**                  | Locker, freundlich, erkl√§rend                                                                                  | Pr√§zise, sachlich, eher technisch                                                                            |\n","| **Struktur**              | Abschnitte mit √úberschriften, Beispiele, klare Gliederung in <br>*Klassifikation*, *Regression*, *Zusammenfassung* | Punktweise Gliederung mit technischer Terminologie, inkl. **Fehlermetriken** und **Entscheidungsstrategien** |\n","| **Zielgruppe**            | Eher auf Einsteiger\\*innen oder allgemeines Publikum ausgerichtet                                              | Fachlich versierte Nutzer\\*innen, z.‚ÄØB. Data Scientists oder Studierende in der Informatik                   |\n","| **Didaktik / Lesbarkeit** | Sehr zug√§nglich, mit einfachen Beispielen und flie√üendem Text                                                  | Kompakter, mehr Fachbegriffe, eher f√ºr ‚Äûanalytisches Nachschlagen‚Äú                                           |\n","| **Inhalte**               | Fokus auf Zweck und typische Beispiele                                                                         | Zus√§tzliche Tiefe: **Evaluationsmetriken**, **Softmax-Funktion**, **Bestimmtheitsma√ü**                       |\n","| **Kreativit√§t**           | H√∂her ‚Äì wirkt wie ein Tutor, der m√ºndlich erkl√§rt                                                              | Geringer ‚Äì wirkt wie ein Lehrbuch oder Fachaufsatz                                                           |\n","| **Temperatur-Effekt**     | sp√ºrbar ‚Üí mehr Sprachvielfalt, z.‚ÄØB. beim √úbergang zur Zusammenfassung                                         | gering ‚Üí klare, determiniert wirkende Formulierungen                                                         |\n"],"metadata":{"id":"OzPIlXnqR2Fq"},"id":"OzPIlXnqR2Fq"},{"cell_type":"markdown","id":"c314ca71","metadata":{"id":"c314ca71"},"source":["## 2.4 Chains (LCEL)"]},{"cell_type":"markdown","source":["Chains verbinden mehrere Komponenten zu einer Verarbeitungskette. LangChain Expression Language (LCEL) ist das **neue Programmiermodell in LangChain v0.3+**, das die Entwicklung von LLM-Anwendungen vereinfacht.\n"],"metadata":{"id":"cEEMxhSBHrXA"},"id":"cEEMxhSBHrXA"},{"cell_type":"markdown","source":["\n","**Hier die Kernpunkte:**\n","\n","1. Was ist LCEL?\n","```python\n","# LCEL nutzt den Pipe-Operator (|) f√ºr klare Verkettungen\n","chain = prompt | model | output_parser\n","```\n","- Eine deklarative Sprache zum Verketten von LangChain-Komponenten\n","- Erm√∂glicht linearen Datenfluss zwischen Komponenten\n","- Basiert auf dem Pipe-Operator (|) f√ºr intuitive Verbindungen\n","\n","2. Warum LCEL nutzen?\n","- Bessere Lesbarkeit des Codes\n","- Einfachere Wartung und Debugging\n","- Verbesserte Performance durch optimierte Ausf√ºhrung\n","- Bessere Typsicherheit und Fehlererkennung\n","- Unterst√ºtzt modernes Streaming und Async-Operationen\n","\n","3. Praktisches Beispiel:"],"metadata":{"id":"ltTQItwXH6w8"},"id":"ltTQItwXH6w8"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Simple Chain\n","</font></p>"],"metadata":{"id":"BndZGbyWKs1C"},"id":"BndZGbyWKs1C"},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain_openai import ChatOpenAI\n","\n","# Prompt-Template\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"user\", \"{input}\")\n","])\n","\n","#  Einfache LCEL-Kette\n","chain = prompt | chat_model\n","\n","# Ausf√ºhrung\n","result = chain.invoke({\"input\": \"Erkl√§re LangChain Expression Language.\"})"],"metadata":{"id":"PsWtjWROIW9d","executionInfo":{"status":"ok","timestamp":1747679589414,"user_tz":-120,"elapsed":6473,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"PsWtjWROIW9d","execution_count":35,"outputs":[]},{"cell_type":"code","source":["display(Markdown(\"## üì£ Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(result.content))"],"metadata":{"id":"VWYAG9aWJpZd","colab":{"base_uri":"https://localhost:8080/","height":418},"executionInfo":{"status":"ok","timestamp":1747679589450,"user_tz":-120,"elapsed":6,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"75e972d0-e0f6-4db8-94b6-71f72adb5274"},"id":"VWYAG9aWJpZd","execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"LangChain Expression Language (LCEL) ist eine spezialisierte Sprache, die in der LangChain-Bibliothek verwendet wird, um komplexe logische Ausdr√ºcke zu formulieren und zu evaluieren. LangChain selbst ist eine Framework-Umgebung, die Entwicklern hilft, mit Sprachmodellen zu arbeiten und diese in verschiedene Anwendungen zu integrieren, sei es zur Automatisierung, Datenverarbeitung oder zur Entwicklung von Chatbots.\n\nHier sind einige wesentliche Punkte zu LCEL:\n\n1. **Einfachheit**: Die Sprache wurde so gestaltet, dass sie einfach zu lernen und zu verwenden ist. Auch Entwickler, die mit Programmierung nicht ganz so vertraut sind, k√∂nnen sie nutzen.\n\n2. **Flexibilit√§t**: LCEL erm√∂glicht es, komplexe Bedingungen und logische Ausdr√ºcke zu formulieren, die spezifische Antworten oder Aktionen ausl√∂sen k√∂nnen, abh√§ngig von den Eingabedaten.\n\n3. **Integration**: Die Sprache ist nahtlos in die LangChain-Umgebung integriert, sodass Entwicklern die M√∂glichkeit geboten wird, Sprachmodelle effektiv zu steuern und deren Outputs zu verarbeiten.\n\n4. **Beispiele**: Mit LCEL k√∂nnen Entwickler Bedingungen wie \"Wenn die Benutzeranfrage das Wort 'Hilfe' enth√§lt, dann antworte mit den verf√ºgbaren Optionen\" formulieren.\n\n5. **Anwendungsf√§lle**: LCEL kann in verschiedenen Anwendungen eingesetzt werden, von der Bearbeitung nat√ºrlicher Sprache √ºber die Automatisierung von Gesch√§ftsvorg√§ngen bis hin zur Entwicklung von interaktiven Chatbots.\n\nWenn du mehr √ºber bestimmte Aspekte oder Anwendungsbeispiele von LangChain Expression Language wissen m√∂chtest, lass es mich wissen! Vielleicht kann ich auch einen Witz √ºber Programmiersprachen einbauen ‚Äì Programmierer haben schlie√ülich auch manchmal Schwierigkeiten, ihre ‚ÄûSprache‚Äú zu finden! üòÑ"},"metadata":{}}]},{"cell_type":"markdown","source":["\n","LCEL ist der empfohlene Weg f√ºr alle neuen LangChain-Projekte, da es die Entwicklung vereinfacht und zukunftssicher macht."],"metadata":{"id":"49Wu_XreIZY2"},"id":"49Wu_XreIZY2"},{"cell_type":"markdown","id":"Qy35G1-FvrIR","metadata":{"id":"Qy35G1-FvrIR"},"source":["<p><font color='black' size=\"5\">\n","Sequential Chains\n","</font></p>"]},{"cell_type":"markdown","id":"AWhQwgMbxitn","metadata":{"id":"AWhQwgMbxitn"},"source":["Sequentielle Chain: Linear, ein Input wird schrittweise verarbeitet"]},{"cell_type":"code","execution_count":37,"id":"ceqbCIOCuqng","metadata":{"id":"ceqbCIOCuqng","executionInfo":{"status":"ok","timestamp":1747679589455,"user_tz":-120,"elapsed":3,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Prompts\n","zusammenfassen_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Fasse den folgenden Text pr√§gnant zusammen.\"),\n","    (\"user\", \"{text}\")\n","])\n","\n","uebersetzen_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"√úbersetze den folgenden Text ins Deutsche.\"),\n","    (\"user\", \"{text}\")\n","])"]},{"cell_type":"code","execution_count":38,"id":"kddZQ3rMv0Hw","metadata":{"id":"kddZQ3rMv0Hw","executionInfo":{"status":"ok","timestamp":1747679589462,"user_tz":-120,"elapsed":2,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Verarbeitungskette\n","\n","# Erste Verarbeitungskette: Zusammenfassung\n","summarization_chain = (\n","    zusammenfassen_template\n","    | chat_model\n","    | (lambda output: {\"text\": output})  # Notwendig, f√ºr String-> Dict, wird von translation_chain erwartet\n",")\n","\n","# Zweite Verarbeitungskette: Translation/√úbersetzung\n","translation_chain = (\n","    uebersetzen_template\n","    | chat_model\n",")\n","\n","# Gesamte Kette: Zusammenfassen und dann √úbersetzen\n","sequential_chain = summarization_chain | translation_chain"]},{"cell_type":"code","execution_count":39,"id":"ymYXf4pKwSee","metadata":{"id":"ymYXf4pKwSee","executionInfo":{"status":"ok","timestamp":1747679589468,"user_tz":-120,"elapsed":2,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Input Text\n","text = \"\"\"\n","Machine Learning is a specialized branch of artificial intelligence (AI) dedicated to the development of algorithms and systems that can automatically learn from data and past experiences. Instead of following rigid, pre-defined instructions, machine learning models identify patterns, adapt their behavior, and improve their performance over time based on the information they are exposed to. This ability enables them to make predictions, recognize complex relationships, and solve problems in dynamic environments. Machine learning techniques are widely applied across various fields, including healthcare, finance, transportation, and entertainment, driving innovations such as personalized recommendations, autonomous vehicles, and intelligent diagnostics.\n","\"\"\""]},{"cell_type":"code","source":["# Aufruf der Kette\n","result = sequential_chain.invoke({\"text\": text})"],"metadata":{"id":"WU8Bah_MNFSS","executionInfo":{"status":"ok","timestamp":1747679592525,"user_tz":-120,"elapsed":3054,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"WU8Bah_MNFSS","execution_count":40,"outputs":[]},{"cell_type":"code","source":["display(Markdown(\"## üì£ Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(result.content))"],"metadata":{"id":"bipXv_2DMdCN","colab":{"base_uri":"https://localhost:8080/","height":134},"executionInfo":{"status":"ok","timestamp":1747679592562,"user_tz":-120,"elapsed":39,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"b094d4de-1f9a-4d4c-db14-3f4e59d32f0f"},"id":"bipXv_2DMdCN","execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Maschinenlernen ist ein Bereich der k√ºnstlichen Intelligenz, der Algorithmen entwickelt, die aus Daten und Erfahrungen lernen k√∂nnen. Anstatt festen Anweisungen zu folgen, erkennen diese Modelle Muster, passen ihr Verhalten an und verbessern sich √ºber die Zeit. Sie finden Anwendung in vielen Bereichen wie Gesundheitswesen, Finanzen, Verkehr und Unterhaltung und erm√∂glichen Innovationen wie personalisierte Empfehlungen und autonome Fahrzeuge."},"metadata":{}}]},{"cell_type":"markdown","id":"R23C1DI1wi1X","metadata":{"id":"R23C1DI1wi1X"},"source":["<p><font color='black' size=\"5\">\n","Q&A-Chain\n","</font></p>"]},{"cell_type":"markdown","id":"-pshkn53xc7T","metadata":{"id":"-pshkn53xc7T"},"source":["QA Chain: Kontextbasiert, kombiniert mehrere Inputs f√ºr eine einzelne Verarbeitung"]},{"cell_type":"code","execution_count":42,"id":"3I8ucKu7wAoo","metadata":{"id":"3I8ucKu7wAoo","executionInfo":{"status":"ok","timestamp":1747679592567,"user_tz":-120,"elapsed":2,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Q&A Prompt\n","qa_template = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Beantworte die Frage basierend auf dem gegebenen Kontext.\"),\n","    (\"user\", \"\"\"\n","    Kontext: {context}\n","\n","    Frage: {question}\n","    \"\"\")\n","])"]},{"cell_type":"code","execution_count":43,"id":"9iqmLBYSwpiX","metadata":{"id":"9iqmLBYSwpiX","executionInfo":{"status":"ok","timestamp":1747679592571,"user_tz":-120,"elapsed":2,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Verkettung\n","qa_chain = qa_template | chat_model"]},{"cell_type":"code","execution_count":44,"id":"R0lVOJaSw2a9","metadata":{"id":"R0lVOJaSw2a9","executionInfo":{"status":"ok","timestamp":1747679592576,"user_tz":-120,"elapsed":2,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["# Inhalte\n","context = \"\"\"\n","Python ist eine beliebte Programmiersprache f√ºr Machine Learning.\n","Sie bietet viele Bibliotheken wie TensorFlow und PyTorch.\n","\"\"\"\n","question = \"Welche ML-Bibliotheken gibt es f√ºr Python?\""]},{"cell_type":"code","source":["# Aufruf\n","result = qa_chain.invoke({\n","    \"context\": context,\n","    \"question\": question\n","})"],"metadata":{"id":"hM1ADYiFNZ9X","executionInfo":{"status":"ok","timestamp":1747679596883,"user_tz":-120,"elapsed":4304,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"hM1ADYiFNZ9X","execution_count":45,"outputs":[]},{"cell_type":"code","execution_count":46,"id":"bxFOmczGyPPN","metadata":{"id":"bxFOmczGyPPN","colab":{"base_uri":"https://localhost:8080/","height":266},"executionInfo":{"status":"ok","timestamp":1747679596913,"user_tz":-120,"elapsed":8,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"354c50da-b4d3-46c5-ca21-12cca50f4563"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"F√ºr Python gibt es mehrere beliebte Bibliotheken f√ºr Machine Learning, darunter:\n\n1. **TensorFlow** - Eine weit verbreitete Bibliothek f√ºr maschinelles Lernen und tiefes Lernen.\n2. **PyTorch** - Beliebt f√ºr seine Flexibilit√§t und Benutzerfreundlichkeit, insbesondere in der Forschung.\n3. **scikit-learn** - Eine Bibliothek f√ºr traditionelle Machine Learning-Algorithmen und -Techniken.\n4. **Keras** - Eine benutzerfreundliche API, die √ºber TensorFlow aufgebaut ist und das schnelle Prototyping von Deep-Learning-Modellen erm√∂glicht.\n5. **XGBoost** - Eine leistungsstarke Bibliothek f√ºr Gradient Boosting, die h√§ufig f√ºr Wettbewerbe im Bereich Machine Learning verwendet wird.\n6. **LightGBM** - Eine weitere Gradient-Boosting-Bibliothek, die f√ºr ihre Geschwindigkeit und Effizienz bekannt ist.\n7. **CatBoost** - Eine Gradient-Boosting-Bibliothek, die speziell f√ºr kategoriale Daten entwickelt wurde.\n\nDiese Bibliotheken bieten eine Vielzahl von Funktionen und Werkzeugen, um unterschiedliche Anforderungen im Bereich Machine Learning zu erf√ºllen."},"metadata":{}}],"source":["display(Markdown(\"## üì£ Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(result.content))"]},{"cell_type":"markdown","source":["## 2.5 OutputParser"],"metadata":{"id":"LVqVL9CWRXU7"},"id":"LVqVL9CWRXU7"},{"cell_type":"markdown","source":["Ein OutputParser ist ein Konzept in LangChain, das hilft, die Antworten eines Modells richtig weiterzuverarbeiten. Es nimmt die **rohe Ausgabe** (zum Beispiel einen langen Text) und formt sie in ein **bestimmtes Format**, das sp√§ter in der Anwendung leichter benutzt werden kann."],"metadata":{"id":"EVBcCwJERfaj"},"id":"EVBcCwJERfaj"},{"cell_type":"code","source":["from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n","\n","# Einfacher Prompt ohne explizite Formatvorgabe\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent. Antworte bitte im JSON-Format mit den Feldern 'name' und 'alter'.\"),\n","    (\"user\", \"Bitte gib Name und Alter einer Person an.\")\n","])\n","\n","# Struktur der Antwort definieren\n","response_schemas = [\n","    ResponseSchema(name=\"name\", description=\"Der vollst√§ndige Name der Person.\"),\n","    ResponseSchema(name=\"alter\", description=\"Das Alter der Person in Jahren.\")\n","]\n","\n","# StructuredOutputParser erstellen\n","output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n","\n","\n","# Modell\n","chat_model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n","\n","# LCEL-Chain\n","chain = prompt | chat_model | output_parser\n","\n","# Ausf√ºhren\n","result = chain.invoke({\"input\": \"\"})\n","\n","print(result)"],"metadata":{"id":"QKtYSBLhVlts","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747679599737,"user_tz":-120,"elapsed":2820,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"27a03dfd-6252-4ca3-ddea-99ec98b8e66b"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["{'name': 'Max Mustermann', 'alter': 30}\n"]}],"id":"QKtYSBLhVlts"},{"cell_type":"markdown","source":["## 2.6 Runnables\n"],"metadata":{"id":"KRqrnpMx3Hwf"},"id":"KRqrnpMx3Hwf"},{"cell_type":"markdown","source":["<img src=\"https://raw.githubusercontent.com/ralf-42/Image/main/langchain_prozess.png\" width=\"207\" alt=\"Avatar\">"],"metadata":{"id":"LFOITVoEzpkm"},"id":"LFOITVoEzpkm"},{"cell_type":"markdown","source":["In der oben gezeigten Grafik sieht man die drei wichtigsten Runnables: Prompt, LLM und Parser, die zusammen eine Kette bilden.\n","\n","Vorteile von Runnables:\n","\n","+ Gleiche Bedienung: Alle Runnables (Prompt, LLM, Parser) funktionieren nach demselben Prinzip\n","+ Einfaches Verbinden: Man kann sie unkompliziert zu einer Kette zusammenf√ºgen - genau wie im Bild gezeigt\n","+ Anpassungsf√§hig: Sie funktionieren sowohl einzeln als auch in der Gruppe\n","Schrittweise Ausgabe: Ergebnisse k√∂nnen st√ºckweise weitergegeben werden\n","\n","In LangChain sind die wichtigsten Komponenten als Runnables verf√ºgbar:\n","\n","+ Prompts/Templates (der erste Baustein in der Kette)\n","+ LLMs (das Herzst√ºck in der Mitte)\n","+ Output-Parser (verwandelt die LLM-Antwort in ein nutzbares Format)\n","+ Komplette Chains (die ganze Verarbeitungskette)\n","+ Tools (f√ºr spezielle Aufgaben)\n","+ Retriever (holen zus√§tzliche Informationen)\n"],"metadata":{"id":"DnUDR_983M4l"},"id":"DnUDR_983M4l"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Benutzerdefinierte Runnables erstellen\n","</font></p>"],"metadata":{"id":"WWJ7U1iJ6b8Q"},"id":"WWJ7U1iJ6b8Q"},{"cell_type":"markdown","source":["Man kann auch eigene Runnables erstellen, indem man eine Klasse definiert, die das Runnable-Interface implementiert oder eine Funktion mit `RunnableLambda` wrapped:\n"],"metadata":{"id":"PAvbE_TR6g87"},"id":"PAvbE_TR6g87"},{"cell_type":"code","source":["from langchain_core.runnables import RunnableLambda\n","from langchain_core.output_parsers.string import StrOutputParser\n","\n","# 1. Mit RunnableLambda eine einfache Transformation erstellen\n","def text_verdoppeln(eingabe):\n","    return f\"{eingabe} <br> <br>  {eingabe}\"\n","\n","verdoppler = RunnableLambda(text_verdoppeln)\n","\n","# Verwendung des eigenen Runnables\n","verdoppler.invoke(\"Text\")"],"metadata":{"id":"_0wEf_hL6peM","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1747680851643,"user_tz":-120,"elapsed":34,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"4b7526fe-449e-45db-d110-7ec3b4c707b1"},"id":"_0wEf_hL6peM","execution_count":92,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Text <br> <br>  Text'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":92}]},{"cell_type":"code","source":["# Einfacher Prompt ohne explizite Formatvorgabe\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent. Antworte kurz.\"),\n","    (\"user\", \"Eingabe: {eingabe}\")\n","])\n","\n","# In eine Kette einbauen - Korrektur der Pipe-Operatoren\n","runnable_chain_mit_verdoppler = prompt | chat_model | StrOutputParser() | verdoppler\n","\n","# Test der Kette\n","result = runnable_chain_mit_verdoppler.invoke({\"eingabe\": \"Wer war Einstein?\"})"],"metadata":{"id":"NvZdOf1961xb","executionInfo":{"status":"ok","timestamp":1747680855310,"user_tz":-120,"elapsed":1045,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"NvZdOf1961xb","execution_count":93,"outputs":[]},{"cell_type":"code","source":["display(Markdown(\"## üì£ Model response:\"))\n","display(Markdown(\"---\"))\n","display(Markdown(result))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":169},"id":"_elA0imx2HZA","executionInfo":{"status":"ok","timestamp":1747680856075,"user_tz":-120,"elapsed":83,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"904a431f-a6aa-4bc8-f2b0-693e0bcf76c2"},"id":"_elA0imx2HZA","execution_count":94,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"## üì£ Model response:"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"---"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Albert Einstein war ein theoretischer Physiker, der f√ºr seine Relativit√§tstheorie bekannt ist. Er wurde 1879 in Deutschland geboren und gilt als einer der bedeutendsten Wissenschaftler des 20. Jahrhunderts.  <br> <br>  Albert Einstein war ein theoretischer Physiker, der f√ºr seine Relativit√§tstheorie bekannt ist. Er wurde 1879 in Deutschland geboren und gilt als einer der bedeutendsten Wissenschaftler des 20. Jahrhunderts. "},"metadata":{}}]},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Runnable-Methoden\n","</font></p>\n","\n","Jedes Runnable unterst√ºtzt standardm√§√üig folgende Methoden:\n","\n","1. **invoke()**: F√ºr einzelne, synchrone Anfragen\n","2. **batch()**: Verarbeitet mehrere Eingaben parallel\n","3. **stream()**: Gibt Teilergebnisse zur√ºck, sobald sie verf√ºgbar sind\n","4. **ainvoke()**: Asynchrone Version von invoke\n","5. **abatch()**: Asynchrone Version von batch\n","6. **astream()**: Asynchrone Version von stream\n","\n"],"metadata":{"id":"CjFvHqWA3S2V"},"id":"CjFvHqWA3S2V"},{"cell_type":"markdown","source":["Hier ein einfaches Beispiel mit den verschiedenen Aufrufmethoden:\n"],"metadata":{"id":"a9zHZAMP3bCL"},"id":"a9zHZAMP3bCL"},{"cell_type":"code","source":["from langchain.prompts import ChatPromptTemplate\n","from langchain_openai import ChatOpenAI\n","from langchain.schema import StrOutputParser\n","\n","# Einfaches Template erstellen\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher und humorvoller Assistent.\"),\n","    (\"user\", \"{eingabe}\")\n","])\n","\n","# Modell definieren\n","model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n","\n","# Output-Parser\n","parser = StrOutputParser()\n","\n","# Runnable-Kette erstellen\n","runnable_chain = prompt | model | parser"],"metadata":{"id":"kQTKHK2D4DBb","executionInfo":{"status":"ok","timestamp":1747681111851,"user_tz":-120,"elapsed":156,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"id":"kQTKHK2D4DBb","execution_count":95,"outputs":[]},{"cell_type":"code","source":["# 1. invoke - Einzelne Anfrage\n","ergebnis = runnable_chain.invoke({\"eingabe\": \"Was ist ein Runnable in LangChain?\"})\n","print(f\"INVOKE-ERGEBNIS:\\n{ergebnis}\\n\")"],"metadata":{"id":"TVWtHSYw5Jla","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747681138788,"user_tz":-120,"elapsed":3228,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"9746bd0a-e643-4617-a715-76273d9de222"},"id":"TVWtHSYw5Jla","execution_count":96,"outputs":[{"output_type":"stream","name":"stdout","text":["INVOKE-ERGEBNIS:\n","In LangChain bezieht sich ein \"Runnable\" auf ein Konzept, das es erm√∂glicht, verschiedene Komponenten oder Funktionen in einer Kette von Operationen zu kombinieren und auszuf√ºhren. Es handelt sich dabei um eine abstrakte Schnittstelle, die es Entwicklern erm√∂glicht, ihre Logik modular und wiederverwendbar zu gestalten.\n","\n","Ein Runnable kann verschiedene Aufgaben ausf√ºhren, wie z.B. Datenverarbeitung, API-Aufrufe oder die Interaktion mit Modellen. Die Idee ist, dass man verschiedene Runnables miteinander verkn√ºpfen kann, um komplexe Workflows zu erstellen, ohne sich um die Details der Implementierung jeder einzelnen Komponente k√ºmmern zu m√ºssen.\n","\n","Das macht es einfacher, verschiedene Teile einer Anwendung zu testen, zu debuggen und zu optimieren. Wenn du also das n√§chste Mal einen Runnable in LangChain siehst, denk daran: Es ist wie ein Schweizer Taschenmesser f√ºr deine Programmierlogik ‚Äì vielseitig und immer bereit, dir zu helfen! üõ†Ô∏èüòÑ\n","\n"]}]},{"cell_type":"code","source":["# 2. batch - Mehrere Anfragen parallel\n","ergebnisse = runnable_chain.batch([\n","    {\"eingabe\": \"Erkl√§re den Begriff LCEL kurz.\"},\n","    {\"eingabe\": \"Was sind die Hauptvorteile von Runnables?\"}\n","])\n","print(\"BATCH-ERGEBNISSE:\")\n","for i, ergebnis in enumerate(ergebnisse, 1):\n","    print(f\"Anfrage {i}: {ergebnis[:50]}...\\n\")"],"metadata":{"id":"UAet4KSq5K2D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747681168285,"user_tz":-120,"elapsed":7221,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"b787ea24-715c-41da-bb93-a514489e4ec0"},"id":"UAet4KSq5K2D","execution_count":97,"outputs":[{"output_type":"stream","name":"stdout","text":["BATCH-ERGEBNISSE:\n","Anfrage 1: LCEL steht f√ºr \"Low Carbon Energy Lab\" und bezieht...\n","\n","Anfrage 2: Runnables in Java sind eine praktische M√∂glichkeit...\n","\n"]}]},{"cell_type":"code","source":["# 3. stream - Schrittweise Ausgabe\n","print(\"STREAM-ERGEBNIS:\")\n","for chunk in runnable_chain.stream({\"eingabe\": \"Z√§hle von 1 bis 5 auf. Mache eine Pause von 3 Sekunden nach jedem Schritt.\"}):\n","    print(chunk, end=\"ü´ó\")\n","print(\"\\n\")"],"metadata":{"id":"VJn4e_IX5L-b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747681300595,"user_tz":-120,"elapsed":1908,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"b480a958-5cb6-4495-ccd9-6bde6d53a7b6"},"id":"VJn4e_IX5L-b","execution_count":101,"outputs":[{"output_type":"stream","name":"stdout","text":["STREAM-ERGEBNIS:\n","ü´óNat√ºrlichü´ó!ü´ó Hierü´ó istü´ó dieü´ó Zü´ó√§hlü´óungü´ó vonü´ó ü´ó1ü´ó bisü´ó ü´ó5ü´ó mitü´ó Paü´óusenü´ó:\n","\n","ü´ó1ü´ó...ü´ó (ü´óPauseü´ó vonü´ó ü´ó3ü´ó Sekundenü´ó)ü´ó  \n","ü´ó2ü´ó...ü´ó (ü´óPauseü´ó vonü´ó ü´ó3ü´ó Sekundenü´ó)ü´ó  \n","ü´ó3ü´ó...ü´ó (ü´óPauseü´ó vonü´ó ü´ó3ü´ó Sekundenü´ó)ü´ó  \n","ü´ó4ü´ó...ü´ó (ü´óPauseü´ó vonü´ó ü´ó3ü´ó Sekundenü´ó)ü´ó  \n","ü´ó5ü´ó...ü´ó \n","\n","ü´óUndü´ó voil√†ü´ó!ü´ó Duü´ó hastü´ó bisü´ó ü´ó5ü´ó gezü´ó√§hltü´ó!ü´ó üéâü´ó Wennü´ó duü´ó nochü´ó mehrü´ó z√§hlenü´ó oderü´ó etwasü´ó anderesü´ó machenü´ó m√∂chtestü´ó,ü´ó sagü´ó einfachü´ó Besü´ócheidü´ó!ü´óü´ó\n","\n"]}]},{"cell_type":"markdown","id":"VlARJEowAAIR","metadata":{"id":"VlARJEowAAIR"},"source":["# 3 | √úberblick LangChain Konzepte\n","---"]},{"cell_type":"markdown","id":"g5B7bWJKBB-C","metadata":{"id":"g5B7bWJKBB-C"},"source":["Konzepte von LangChain sind grundlegende Bausteine und Prinzipien, die das Framework ausmachen und seine Funktionsweise erm√∂glichen. Hier sind einige der wichtigsten Konzepte:\n","\n","+ Chat-Modelle (**Chat models**): LLMs, die √ºber eine Chat-API verf√ºgbar sind und Sequenzen von Nachrichten verarbeiten1.\n","+ Nachrichten (**Messages**): Kommunikationseinheiten in Chat-Modellen f√ºr Ein- und Ausgabe1.\n","+ Chat-Verlauf (**Chat history**): Eine Sequenz von Nachrichten, die eine Konversation darstellt.\n","+ Tools (**Tools**): Funktionen mit definierten Schemata f√ºr Name, Beschreibung und Argumente.\n","+ Strukturierte Ausgabe (**Structured output**): Technik, um Chat-Modelle in strukturierten Formaten antworten zu lassen.\n","+ Retrieval Augmented Generation (**RAG**): Technik zur Verbesserung von Sprachmodellen durch Kombination mit externen Wissensbasen.\n","+ Prompt-Vorlagen (**Prompt template**s): Komponenten zur Erstellung strukturierter Prompts f√ºr LLMs.\n","+ Chains (**Chains**): Verkn√ºpfungen mehrerer LLMs oder anderer Komponenten f√ºr komplexere Anwendungen.\n","+ Agenten (**Agents**): Nutzen Sprachmodelle, um Aktionssequenzen auszuw√§hlen und mit externen Ressourcen zu interagieren.\n","+ Retriever (**Retriever**): Komponenten, die relevante Dokumente aus einer Wissensbasis abrufen."]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Anbieterbezogene APIs in LangChain\n","</font></p>"],"metadata":{"id":"Gm6A7xSEzNVv"},"id":"Gm6A7xSEzNVv"},{"cell_type":"markdown","source":["| API-Name in LangChain | Anbieter             | Bemerkung                                 |\n","|-----------------------|----------------------|-------------------------------------------|\n","| `ChatOpenAI`           | OpenAI               | Chat-Modelle (`gpt-3.5-turbo`, `gpt-4`, ...) |\n","| `OpenAI`               | OpenAI               | Text Completion (`text-davinci-003`, alt)  |\n","| `ChatAnthropic`        | Anthropic            | Claude-Modelle (`Claude 3`, etc.)          |\n","| `ChatGoogleGenerativeAI` | Google (Gemini)      | Gemini-Modelle (`gemini-1.5-pro`, etc.)    |\n","| `ChatCohere`           | Cohere               | Command-Modelle                           |\n","| `ChatMistralAI`        | Mistral              | Mistral-Modelle (`Mistral 7B`, Mixtral etc.) |\n","| `ChatFireworks`        | Fireworks AI         | Zugriff auf viele Open-Weight-Modelle      |\n","| `ChatAzureOpenAI`      | Azure (Microsoft)    | OpenAI-Modelle √ºber Azure API              |\n","| `BedrockLLM`           | AWS Bedrock          | Zugang zu Claude, Titan, Jurassic, etc.    |"],"metadata":{"id":"MjHiOl3dzLyc"},"id":"MjHiOl3dzLyc"},{"cell_type":"markdown","id":"F_5gn03b46rC","metadata":{"id":"F_5gn03b46rC"},"source":["# A | Aufgabe\n","---"]},{"cell_type":"markdown","id":"36LyGMmo5jb3","metadata":{"id":"36LyGMmo5jb3"},"source":["Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen."]},{"cell_type":"markdown","id":"Je-szbGU7IJ-","metadata":{"id":"Je-szbGU7IJ-"},"source":["\n","<p><font color='black' size=\"5\">\n","Erstellen einer LangChain-Kette zur Textanalyse\n","</font></p>"]},{"cell_type":"markdown","id":"kevePvA-7PzW","metadata":{"id":"kevePvA-7PzW"},"source":["Entwickeln Sie eine Verarbeitungskette, die einen Text analysiert und verschiedene Informationen dar√ºber extrahiert.\n","\n","**Schritte:**\n","1. Erstelle ein ChatPromptTemplate, das ein LLM anweist, einen Text zu analysieren\n","2. Die Analyse soll folgende Aspekte umfassen:\n","   - Hauptthema des Textes\n","   - Tonalit√§t (formal, informell, etc.)\n","   - Schl√ºsselw√∂rter (5-10)\n","   - Kurze Zusammenfassung (max. 3 S√§tze)\n","3. Formatiere die Ausgabe strukturiert mit Markdown-√úberschriften\n","4. Teste die Kette mit mindestens zwei verschiedenen Texten\n","\n","\n"]},{"cell_type":"markdown","id":"6oBvtYws7qpB","metadata":{"id":"6oBvtYws7qpB"},"source":["<p><font color='black' size=\"5\">\n","Few-Shot-Learning f√ºr Textklassifikation\n","</font></p>"]},{"cell_type":"markdown","id":"gI7alQcE79ER","metadata":{"id":"gI7alQcE79ER"},"source":["Erstellen Sie ein System, das mithilfe von Few-Shot-Learning Texte in vorgegebene Kategorien klassifiziert.\n","\n","**Schritte:**\n","1. Definiere 3-5 Kategorien f√ºr die Klassifikation (z.B. Sport, Politik, Technologie, Kultur)\n","2. Erstelle einen FewShotPromptTemplate mit Beispielen f√ºr jede Kategorie\n","3. Entwickle eine Chain, die neue Texte klassifiziert\n","4. Implementiere eine Funktion, die neben der Kategorie auch eine Begr√ºndung f√ºr die Einordnung liefert\n","5. Teste das System mit verschiedenen Texten, die nicht in den Beispielen vorkommen\n"]},{"cell_type":"markdown","id":"l35x2g1e8fa9","metadata":{"id":"l35x2g1e8fa9"},"source":["<p><font color='black' size=\"5\">\n","Q&A-System mit Sequential Chain\n","</font></p>"]},{"cell_type":"markdown","id":"qHFRGWmP8gga","metadata":{"id":"qHFRGWmP8gga"},"source":["Entwickeln Sie ein System, das Fragen zu einem gegebenen Kontext beantwortet, aber zuerst den Kontext zusammenfasst und dann die Frage beantwortet.\n","\n","\n","**Schritte:**\n","\n","+ Erstellen Sie zwei Templates:\n","    + Ein Template zur Zusammenfassung des Kontexts\n","    + Ein Template zur Beantwortung einer Frage basierend auf der Zusammenfassung\n","+ Verbinden Sie die Templates in einer sequentiellen Kette\n","+ Implementieren Sie einen Mechanismus, der die Zusammenfassung und die Antwort getrennt zur√ºckgibt\n","+ Nutzen Sie LCEL (LangChain Expression Language) f√ºr die Verkettung\n","+ Testen Sie das System mit verschiedenen Kontexten und Fragen"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":5}
{"cells":[{"cell_type":"markdown","source":["<p><font size=\"7\" color='grey'> <b>\n","Anwendung Generativer KI\n","</b></font> </br></p>"],"metadata":{"id":"Ih2CTVBnArVZ"},"id":"Ih2CTVBnArVZ"},{"cell_type":"markdown","source":["<p><font size=\"6\" color='grey'> <b>\n","Lokale und Open Source Modelle\n","</b></font> </br></p>\n","\n","\n","---"],"metadata":{"id":"6jJZ7wbdArVc"},"id":"6jJZ7wbdArVc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfdhPIzcEYRG","cellView":"form","collapsed":true},"outputs":[],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  Colab-Umfeld</font> </br></p>\n","# Installierte Python Version\n","import sys\n","print(f\"Python Version: \",sys.version)\n","# Installierte LangChain Bibliotheken\n","print()\n","print(\"Installierte LangChain Bibliotheken:\")\n","!pip list | grep '^langchain'\n","# Unterdrückt die \"DeprecationWarning\" von LangChain für die Memory-Funktionden\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")"],"id":"dfdhPIzcEYRG"},{"cell_type":"code","source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  SetUp API-Keys (setup_api_keys)</font> </br></p>\n","def setup_api_keys():\n","    \"\"\"Konfiguriert alle benötigten API-Keys aus Google Colab userdata\"\"\"\n","    from google.colab import userdata\n","    import os\n","    from os import environ\n","\n","    # Dictionary der benötigten API-Keys\n","    keys = {\n","        'OPENAI_API_KEY': 'OPENAI_API_KEY',\n","        'HF_TOKEN': 'HF_TOKEN',\n","        # Weitere Keys bei Bedarf\n","    }\n","\n","    # Keys in Umgebungsvariablen setzen\n","    for env_var, key_name in keys.items():\n","        environ[env_var] = userdata.get(key_name)\n","\n","    return {k: environ[k] for k in keys.keys()}\n","\n","# Verwendung\n","all_keys = setup_api_keys()\n","# Bei Bedarf einzelne Keys direkt zugreifen\n","# WEATHER_API_KEY = all_keys['WEATHER_API_KEY']"],"metadata":{"cellView":"form","id":"WD3Wwr6sESX8"},"execution_count":null,"outputs":[],"id":"WD3Wwr6sESX8"},{"cell_type":"markdown","id":"951a5960","metadata":{"id":"951a5960"},"source":["# **1 <font color='orange'>|</font> Einführung**\n","---\n"]},{"cell_type":"markdown","source":["Lokale und Open Source Modelle bieten eine Alternative zu Cloud-basierten KI-Diensten wie OpenAI. Im Gegensatz zu API-basierten Diensten werden diese Modelle direkt auf dem lokalen System ausgeführt, was Vorteile in Bezug auf Datenschutz, Kosten und Anpassungsmöglichkeiten bietet.\n","\n"],"metadata":{"id":"HWIvNajgmCCb"},"id":"HWIvNajgmCCb"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Vor- und Nachteile lokaler Modelle\n","</font></p>\n","\n","- **Vorteile**: Datenschutz, keine API-Kosten, keine Internetverbindung erforderlich, volle Kontrolle\n","- **Nachteile**: Höhere Hardwareanforderungen, begrenzte Modellleistung auf Standard-Hardware, Installations- und Wartungsaufwand\n","\n"],"metadata":{"id":"-y5UoxXMmUkV"},"id":"-y5UoxXMmUkV"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Anwendungsbereiche lokaler Modelle\n","</font></p>\n","\n","- Entwicklung und Prototyping\n","- Datenschutzkritische Anwendungen\n","- Offline-Anwendungen\n","- Kostensensitive Projekte\n","\n"],"metadata":{"id":"OMbXZAO-maLa"},"id":"OMbXZAO-maLa"},{"cell_type":"markdown","source":["# **2 <font color='orange'>|</font> Ollama**\n","---"],"metadata":{"id":"4uBmvLYwmhnr"},"id":"4uBmvLYwmhnr"},{"cell_type":"markdown","source":["Ollama ist ein Tool, das die einfache Installation und Nutzung von Open Source LLMs auf dem eigenen Computer ermöglicht. Es bietet eine benutzerfreundliche Schnittstelle zum Herunterladen, Verwalten und Ausführen verschiedener Sprachmodelle.\n","\n","[Ollama](https://ollama.com/)"],"metadata":{"id":"aUBszZUdmn-h"},"id":"aUBszZUdmn-h"},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Installation von Ollama\n","</font></p>"],"metadata":{"id":"LBO99aq0ncRs"},"id":"LBO99aq0ncRs"},{"cell_type":"code","execution_count":null,"id":"4b4072af","metadata":{"id":"4b4072af"},"outputs":[],"source":["# Installation in Linux/Colab\n","!sudo apt update\n","!sudo apt install -y pciutils\n","!curl -fsSL https://ollama.com/install.sh | sh"]},{"cell_type":"markdown","id":"f4d7b10b","metadata":{"id":"f4d7b10b"},"source":["<p><font color='black' size=\"5\">\n","Starten des Ollama-Servers\n","</font></p>"]},{"cell_type":"code","execution_count":null,"id":"38bec64b","metadata":{"id":"38bec64b"},"outputs":[],"source":["import threading\n","import subprocess\n","import time\n","\n","def run_ollama_serve():\n","  subprocess.Popen([\"ollama\", \"serve\"])\n","\n","thread = threading.Thread(target=run_ollama_serve)\n","thread.start()\n","time.sleep(5)  # Kurz warten, bis der Server hochgefahren ist"]},{"cell_type":"markdown","id":"f788cb40","metadata":{"id":"f788cb40"},"source":["<p><font color='black' size=\"5\">\n","Modelle herunterladen und verwalten\n","</font></p>"]},{"cell_type":"code","execution_count":null,"id":"4111d55e","metadata":{"id":"4111d55e"},"outputs":[],"source":["# Modelle herunterladen\n","!ollama pull llama2\n","!ollama pull gemma"]},{"cell_type":"code","source":["# Verfügbare Modelle anzeigen\n","!ollama list"],"metadata":{"id":"JdkzoYBkJ2jb"},"id":"JdkzoYBkJ2jb","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"b8ee185b","metadata":{"id":"b8ee185b"},"source":["# **3 <font color='orange'>|</font> Lokale Modelle und LangChain**\n","---"]},{"cell_type":"markdown","source":["LangChain bietet eine nahtlose Integration mit Ollama, wodurch lokale Modelle in komplexen Anwendungen eingesetzt werden können.\n","\n"],"metadata":{"id":"TONpdbWknQWS"},"id":"TONpdbWknQWS"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Installation der benötigten Bibliotheken\n","</font></p>"],"metadata":{"id":"URcyOJ2EnmSE"},"id":"URcyOJ2EnmSE"},{"cell_type":"code","execution_count":null,"id":"36284d92","metadata":{"id":"36284d92"},"outputs":[],"source":["!uv pip install --system -q langchain-ollama langchain_community"]},{"cell_type":"markdown","id":"90c86bc3","metadata":{"id":"90c86bc3"},"source":["<p><font color='black' size=\"5\">\n","Einfache Verwendung von Ollama mit LangChain\n","</font></p>"]},{"cell_type":"code","execution_count":null,"id":"290849c2","metadata":{"id":"290849c2"},"outputs":[],"source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain_ollama import OllamaLLM\n","from IPython.display import display, Markdown\n","\n","# Prompt-Vorlage erstellen\n","template = \"\"\"Frage: {question}\n","\n","Antwort: Denke Schritt für Schritt.\"\"\"\n","\n","# Komponenten der Chain erstellen\n","prompt = ChatPromptTemplate.from_template(template)\n","model = OllamaLLM(model=\"gemma\", temperature=0.1)  # Niedrigere Temperatur für analytischere Antworten\n","\n","# Chain zusammensetzen und ausführen\n","chain = prompt | model"]},{"cell_type":"markdown","id":"2e35d57b","metadata":{"id":"2e35d57b"},"source":["<p><font color='black' size=\"5\">\n","Beispielanfragen\n","</font></p>"]},{"cell_type":"code","execution_count":null,"id":"cb2b8771","metadata":{"lines_to_next_cell":2,"id":"cb2b8771"},"outputs":[],"source":["# Beispiel 1: Gesundheitsfrage\n","response = chain.invoke({\"question\": \"Was ist der einfachste Weg Gewicht abzunehmen?\"})\n","display(Markdown(response))"]},{"cell_type":"code","source":["# Beispiel 2: Allgemeinwissen\n","response = chain.invoke({\"question\": \"Wer war Albert Einstein?\"})\n","display(Markdown(response))"],"metadata":{"id":"7FnW-3sUKW0j"},"id":"7FnW-3sUKW0j","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"efe2093b","metadata":{"id":"efe2093b"},"source":["\n","# **4 <font color='orange'>|</font> Open Source vs. Closed Source**\n","---"]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Open Source Modelle\n","</font></p>"],"metadata":{"id":"eU1Vqvarn-WV"},"id":"eU1Vqvarn-WV"},{"cell_type":"markdown","source":["\n","\n","Open Source KI-Modelle zeichnen sich durch ihre frei zugänglichen Gewichte und Architekturen aus. Diese Modelle werden unter Lizenzen wie MIT, Apache oder Creative Commons veröffentlicht, die es Nutzern erlauben, den Code einzusehen, zu modifizieren und für eigene Zwecke anzupassen.\n","\n","**Vorteile von Open Source Modellen:**\n","\n","- **Transparenz**: Einsicht in die Funktionsweise und Trainingsmethoden\n","- **Anpassbarkeit**: Möglichkeit zum Fine-Tuning für spezifische Anwendungsfälle\n","- **Keine API-Kosten**: Nach dem Download entstehen keine laufenden Nutzungskosten\n","- **Datenschutz**: Vollständige Kontrolle über die Datenverarbeitung\n","- **Community-Unterstützung**: Regelmäßige Verbesserungen durch die Community\n","\n","**Herausforderungen:**\n","\n","- **Hardwareanforderungen**: Größere Modelle benötigen leistungsstarke Hardware\n","- **Expertise**: Technisches Know-how für Installation und Optimierung nötig\n","- **Qualitätsunterschiede**: Leistung kann bei gleicher Größe hinter kommerziellen Modellen zurückbleiben\n","\n"],"metadata":{"id":"aT17TvGpoL1U"},"id":"aT17TvGpoL1U"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Closed Source Modelle\n","</font></p>\n","\n","Closed Source Modelle wie GPT-4 von OpenAI oder Claude von Anthropic werden nur über APIs angeboten, ohne Zugriff auf die zugrunde liegenden Modellgewichte.\n","\n","**Vorteile von Closed Source Modellen:**\n","\n","- **Leistungsstärke**: Oft fortschrittlicher als vergleichbare Open Source Alternativen\n","- **Einfache Nutzung**: Keine lokale Installation oder Wartung nötig\n","- **Regelmäßige Updates**: Automatische Verbesserungen ohne eigenen Aufwand\n","- **Skalierbarkeit**: Flexibel an unterschiedlichen Bedarf anpassbar\n","\n","**Nachteile:**\n","\n","- **Kosten**: Nutzungsbasierte Gebühren\n","- **Abhängigkeit**: Angewiesenheit auf externe Dienste und deren Verfügbarkeit\n","- **Datenschutzbedenken**: Daten verlassen das eigene System\n","- **Eingeschränkte Anpassbarkeit**: Beschränkte Möglichkeiten zur Modifikation\n","\n"],"metadata":{"id":"dUVX_bNioOvm"},"id":"dUVX_bNioOvm"},{"cell_type":"markdown","source":["# **5 <font color='orange'>|</font> Beispiele Open Source Modelle**\n","---"],"metadata":{"id":"GE8se-8AoUgf"},"id":"GE8se-8AoUgf"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Phi-4 (Microsoft)\n","</font></p>\n","\n","Microsoft's Phi-4 ist ein kompaktes, aber leistungsstarkes Modell, das trotz seiner geringen Größe beeindruckende Ergebnisse liefert.\n","\n","**Technische Details:**\n","\n","- Verfügbar in verschiedenen Größen (2,7B bis 13B Parameter)\n","- Optimiert für das Ausführen auf durchschnittlicher Hardware\n","- Besonders stark in Coding-Aufgaben und logischem Denken\n","- Trainiert mit hochwertigen, kuratierten Daten\n","- Verwendet eine optimierte Transformerarchitektur\n","\n","**Einsatzbereiche:**\n","\n","- Softwareentwicklung und Code-Generierung\n","- Textgenerierung mit geringen Hardwareanforderungen\n","- Bildung und Wissensanwendungen\n","\n"],"metadata":{"id":"5-6Ml69Nob7I"},"id":"5-6Ml69Nob7I"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Gemma (Google)\n","</font></p>\n","\n","Google's Gemma ist eine Familie leichtgewichtiger, offener Modelle, die speziell für Effizienz optimiert wurden.\n","\n","**Technische Details:**\n","\n","- Größen: 2B und 7B Parameter\n","- Entwickelt mit einem starken Fokus auf verantwortungsvolle KI\n","- Basiert auf der Gemini-Modellfamilie\n","- Unterstützt Kontextfenster bis zu 8K Tokens\n","- Verfügbar in verschiedenen Quantisierungen (GGUF)\n","\n","**Besonderheiten:**\n","\n","- Starke Leistung in mehrsprachigen Anwendungen\n","- Effiziente Ressourcennutzung\n","- Gute Balance zwischen Modellgröße und Fähigkeiten\n","\n"],"metadata":{"id":"t_0szyFtojsO"},"id":"t_0szyFtojsO"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Llama 2 und Llama 3 (Meta)\n","</font></p>\n","\n","Meta's Llama-Serie hat die Open-Source-LLM-Landschaft revolutioniert und bietet leistungsstarke Modelle mit differenzierten Lizenzen.\n","\n","**Technische Details:**\n","\n","- Llama 3: Verfügbar in 8B und 70B Varianten, mit verbesserter Multitask-Leistung\n","- Llama 2: 7B, 13B und 70B Parameter\n","- Kontextfenster: Bis zu 4K Tokens (Llama 2), 8K+ (Llama 3)\n","- Chat-optimierte Varianten verfügbar\n","\n","**Besonderheiten:**\n","\n","- Starke Leistung in Reasoning-Aufgaben\n","- Verbesserte Instruktionsbefolgung in neueren Versionen\n","- Semi-offene Lizenz mit Einschränkungen für kommerzielle Nutzung ab bestimmten Nutzer-Schwellenwerten\n","\n"],"metadata":{"id":"pIb442LBor5M"},"id":"pIb442LBor5M"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Mistral (Mistral AI)\n","</font></p>\n","\n","Das französische Startup Mistral AI hat mit seinen effizienten Modellen für Aufsehen gesorgt.\n","\n","**Technische Details:**\n","\n","- Mistral 7B: Beeindruckende Leistung trotz kompakter Größe\n","- Mixtral 8x7B: Ein Mixture-of-Experts (MoE) Modell mit starker Leistung\n","- Verbesserte Architekturen mit effizienten Attention-Mechanismen\n","- Unterstützung für bis zu 32K Token Kontextfenster\n","\n","**Besonderheiten:**\n","\n","- Herausragende Leistung für die Modellgröße\n","- Spezialisierte Varianten für Code-Generierung (Codestral)\n","- Moderne Architekturoptimierungen für bessere Effizienz\n","\n"],"metadata":{"id":"0_OKca5iow4a"},"id":"0_OKca5iow4a"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Falcon (Technology Innovation Institute)\n","</font></p>\n","\n","Die Falcon-Modelle wurden vom Technology Innovation Institute in den VAE entwickelt und bieten eine großzügige Lizenz für kommerzielle Anwendungen.\n","\n","**Technische Details:**\n","\n","- Größen: 1B, 7B und 40B Parameter\n","- Trainiert auf einem großen, diversen Datensatz\n","- Optimierte Transformer-Architektur für Effizienz\n","\n","**Besonderheiten:**\n","\n","- Permissive Lizenz für kommerzielle Anwendungen\n","- Gute Mehrsprachigkeit\n","- Hohe Qualität in allgemeinen Wissensabfragen\n","\n","\n"],"metadata":{"id":"VsY29GNko2fh"},"id":"VsY29GNko2fh"},{"cell_type":"markdown","source":["# **6 <font color='orange'>|</font> Lizenzierung und rechtliche Aspekte**\n","---"],"metadata":{"id":"vOXxrUBro9I8"},"id":"vOXxrUBro9I8"},{"cell_type":"markdown","source":["Die Lizenzen von Open Source Modellen unterscheiden sich erheblich und beeinflussen deren Einsatzmöglichkeiten:\n","\n","**MIT Lizenz (z. B. OpenLLaMA, GPT-NeoX)**\n","- Sehr liberal, erlaubt kommerzielle Nutzung ohne Einschränkungen  \n","- Erfordert lediglich die Nennung des ursprünglichen Urhebers  \n","- Keine Weitergabelizenzverpflichtung (nicht copyleft)  \n","- Einfach und weit verbreitet in Open-Source-Projekten  \n","\n","**Apache 2.0 (z. B. Gemma, Falcon)**\n","- Sehr liberal, erlaubt kommerzielle Nutzung ohne starke Einschränkungen  \n","- Erfordert Hinweis auf ursprüngliches Urheberrecht  \n","- Keine Weitergabelizenzverpflichtung (nicht copyleft)  \n","- Enthält eine Klausel zum Schutz vor Patentstreitigkeiten  \n","\n","**Meta Llama Lizenz**\n","- Erlaubt kommerzielle Nutzung mit Einschränkungen (z. B. Nutzerschwellen)  \n","- Enthält spezifische Bedingungen für verantwortungsvolle Nutzung  \n","- Verbietet bestimmte Anwendungsfälle wie Desinformation oder illegale Aktivitäten  \n","- Nicht so frei wie Apache oder MIT, aber offener als proprietäre Modelle  \n","\n","**Mistral AI Lizenz**\n","- Erlaubt kommerzielle und nicht-kommerzielle Nutzung  \n","- Enthält ethische Nutzungsbeschränkungen  \n","- Unterschiedliche Bedingungen für verschiedene Modellversionen  \n","- Im Allgemeinen offener als die Meta Llama Lizenz  \n","\n","**Creative Commons (z. B. BLOOM unter CC-BY-SA)**\n","- Verschiedene Varianten mit unterschiedlichem Freiheitsgrad  \n","- **CC-BY**: Erlaubt kommerzielle Nutzung, solange der Urheber genannt wird  \n","- **CC-BY-SA**: Weitergabe nur unter gleichen Lizenzbedingungen erlaubt (copyleft)  \n","- **CC-BY-NC**: Keine kommerzielle Nutzung erlaubt  \n","- **CC-BY-ND**: Keine Bearbeitung oder Ableitung erlaubt  \n","- Häufig für Open-Access-Modelle genutzt, aber mit Einschränkungen je nach Variante  \n","\n","<br>\n","\n","**Disclaimer**    \n","Die oben aufgeführten Informationen zu Lizenzmodellen dienen ausschließlich allgemeinen Informationszwecken und stellen keine rechtliche Beratung dar. Die Bedingungen einzelner Lizenzen können sich ändern, und ihre Auslegung kann von der jeweiligen Rechtsprechung abhängen.\n","\n","Vor der Nutzung oder Weitergabe eines KI-Modells sollten die offiziellen Lizenzdokumente sorgfältig geprüft und bei Unsicherheiten eine juristische Beratung eingeholt werden. Wir übernehmen keine Haftung für die Vollständigkeit, Richtigkeit oder Anwendbarkeit der bereitgestellten Informationen.\n","\n"],"metadata":{"id":"tNJ_JtxQpIwV"},"id":"tNJ_JtxQpIwV"},{"cell_type":"markdown","source":["\n","# **7 <font color='orange'>|</font> Auswahlkriterien Open Source**\n","---"],"metadata":{"id":"6Jvx9A_3pOUJ"},"id":"6Jvx9A_3pOUJ"},{"cell_type":"markdown","source":["Bei der Wahl eines geeigneten Open Source Modells sollten folgende Faktoren berücksichtigt werden:\n","\n","**Hardwareanforderungen:**\n","\n","- Größere Modelle (40B+): Benötigen High-End-GPUs mit 24GB+ VRAM\n","- Mittlere Modelle (7-13B): Laufen auf Consumer-GPUs mit 8-16GB VRAM\n","- Kleine Modelle (1-2B): Können auf Standard-Laptops oder CPUs ausgeführt werden\n","- Quantisierung (INT4, INT8) reduziert Hardwareanforderungen erheblich\n","\n","**Anwendungsfall:**\n","\n","- Allgemeine Textgenerierung: Llama 3, Mistral und Gemma bieten gute Allround-Leistung\n","- Code-Generierung: Phi, CodeLlama oder Codestral für spezialisierte Coding-Fähigkeiten\n","- Wissenschaftliche Aufgaben: Spezialisierte Domänenmodelle bevorzugen\n","- Mehrsprachigkeit: Auf Modelle mit breitem Sprachtraining (z.B. BLOOM) setzen\n","\n","**Kontextfenster:**\n","\n","- Standard (4-8K Tokens): Für die meisten einfachen Anwendungen ausreichend\n","- Erweitert (16-32K Tokens): Für Dokumentenanalyse und komplexe Aufgaben\n","- Lang (64K+ Tokens): Für die Verarbeitung ganzer Dokumente oder Codebases\n","\n"],"metadata":{"id":"NCD_1A9xpaJM"},"id":"NCD_1A9xpaJM"},{"cell_type":"markdown","source":["# **8 <font color='orange'>|</font> Zukunftstrends bei Open Source**\n","---"],"metadata":{"id":"aKoiGZUMpdcw"},"id":"aKoiGZUMpdcw"},{"cell_type":"markdown","source":["\n","\n","Die Open Source LLM-Landschaft entwickelt sich rapide weiter. Folgende Trends sind zu beobachten:\n","\n","**Effizienzsteigerungen:**\n","\n","- Kleinere Modelle mit der Leistung größerer Vorgänger\n","- Verbesserte Quantisierungstechniken für schnellere Inferenz\n","- Optimierte Architekturen (MoE, FlashAttention, etc.)\n","\n","**Spezialisierung:**\n","\n","- Domänenspezifische Modelle für Medizin, Recht, Finanzen, etc.\n","- Aufgabenspezifische Varianten für Code, Mathematik, Reasoning\n","- Sprachspezifische Modelle für nicht-englische Sprachen\n","\n","**Verbesserte Lizenzen:**\n","\n","- Entwicklung ausgewogenerer Lizenzmodelle\n","- Klarere Richtlinien für verantwortungsvolle KI-Nutzung\n","- Industriefreundlichere Bedingungen für kommerzielle Anwendungen\n","\n","**Multimodalität:**\n","\n","- Integration von Text- und Bildverständnis\n","- Erweiterung auf Audio- und Videoverarbeitung\n","- Verbesserte Integration mit externen Tools und Wissensquellen\n","\n","Diese Entwicklungen werden die Lücke zwischen Open Source und proprietären Modellen weiter verringern und die Einstiegshürden für lokale KI-Anwendungen senken.\n","\n"],"metadata":{"id":"q3_FSVMjqdbV"},"id":"q3_FSVMjqdbV"},{"cell_type":"markdown","source":["# **A <font color='orange'>|</font> Aufgabe**\n"],"metadata":{"id":"L0ZXAt1L6S1Z"},"id":"L0ZXAt1L6S1Z"},{"cell_type":"markdown","source":["Die Aufgabestellungen unten bieten Anregungen, Sie können aber auch gerne eine andere Herausforderung angehen."],"metadata":{"id":"BYQ1yv0U6YKp"},"id":"BYQ1yv0U6YKp"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Vergleich von Open Source Modellen\n","</font></p>"],"metadata":{"id":"hsX9yWQQ9v8G"},"id":"hsX9yWQQ9v8G"},{"cell_type":"markdown","source":["Installieren Sie Ollama auf Ihrem lokalen System und vergleichen Sie die Leistung von mindestens zwei verschiedenen Modellen (z.B. Llama2 und Gemma) bei der Beantwortung derselben Prompts.\n","\n","Vergleichen Sie die Antworten der Modelle. Kriterien: Umfang, Qualität, Antwortzeit, ...\n"],"metadata":{"id":"POK-FQXg96Ui"},"id":"POK-FQXg96Ui"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Chat-Anwendung mit lokalem LLM\n","</font></p>"],"metadata":{"id":"_3BXIh6M-cV8"},"id":"_3BXIh6M-cV8"},{"cell_type":"markdown","source":["\n","Entwickeln Sie eine einfache Gradio-basierte Chat-Anwendung, die ein lokales Modell über Ollama nutzt. Die Anwendung soll:\n","\n","```python\n","import gradio as gr\n","from langchain_ollama import OllamaLLM\n","from langchain_core.messages import HumanMessage, AIMessage\n","import os\n","\n","# Ollama-LLM initialisieren\n","llm = OllamaLLM(model=\"gemma\", temperature=0.7)\n","\n","# Chat-Historie\n","chat_verlauf = []\n","\n","def chat_antwort(nachricht, verlauf):\n","    \"\"\"\n","    Verarbeitet die Nutzereingabe und generiert eine Antwort vom LLM\n","    \"\"\"\n","    # Aktuelle Nachricht zur Historie hinzufügen\n","    if nachricht:\n","        verlauf.append((nachricht, \"\"))\n","        \n","        # Chat-Kontext erstellen (begrenzt auf die letzten 5 Nachrichten für Effizienz)\n","        kontext = \"\"\n","        for i in range(max(0, len(verlauf)-5), len(verlauf)):\n","            if i < len(verlauf)-1:  # Alle vorherigen Nachrichten\n","                kontext += f\"Mensch: {verlauf[i][0]}\\nAssistent: {verlauf[i][1]}\\n\"\n","            else:  # Aktuelle Nachricht\n","                kontext += f\"Mensch: {verlauf[i][0]}\\nAssistent: \"\n","        \n","        # Antwort vom Modell generieren\n","        antwort = llm.invoke(kontext)\n","        \n","        # Antwort zur Historie hinzufügen\n","        verlauf[-1] = (nachricht, antwort)\n","    \n","    return \"\", verlauf\n","\n","# Gradio-Interface erstellen\n","with gr.Blocks(title=\"Lokaler KI-Assistent\") as demo:\n","    gr.Markdown(\"# Chat mit einem lokalen KI-Modell\")\n","    gr.Markdown(\"Dieser Chat verwendet das Gemma-Modell über Ollama\")\n","    \n","    chatbot = gr.Chatbot(label=\"Chat-Verlauf\")\n","    msg = gr.Textbox(label=\"Ihre Nachricht\", placeholder=\"Schreiben Sie hier Ihre Nachricht...\")\n","    \n","    msg.submit(chat_antwort, [msg, chatbot], [msg, chatbot])\n","    \n","    gr.Markdown(\"### Hinweise zur Nutzung:\")\n","    gr.Markdown(\"- Die Antworten werden lokal auf Ihrem System generiert\")\n","    gr.Markdown(\"- Keine Daten werden an externe Server gesendet\")\n","    gr.Markdown(\"- Die Antwortgeschwindigkeit hängt von Ihrer Hardware ab\")\n","\n","# Anwendung starten\n","demo.launch()\n","```\n","\n","Testen & erweitern Sie die Anwendung:\n","1. Testen Sie den Code, korrigieren Sie etwaige Fehler.\n","2. Modellauswahl (zwischen verschiedenen installierten Ollama-Modellen)\n","3. Parameter-Einstellungen (Temperatur, Top-K, Top-P)\n","4. Speichern und Laden von Chat-Verläufen\n","5. Möglichkeit, die generierten Antworten zu bewerten (Daumen hoch/runter)"],"metadata":{"id":"gH6to8NF-r7I"},"id":"gH6to8NF-r7I"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","RAG-System mit lokalem LLM\n","</font></p>"],"metadata":{"id":"BKLNzHsa-x2V"},"id":"BKLNzHsa-x2V"},{"cell_type":"markdown","source":["\n","\n","Entwickeln Sie ein einfaches Retrieval-Augmented Generation (RAG) System, das lokale PDF-Dokumente einliest und die Beantwortung von Fragen mit einem lokalen Modell ermöglicht:\n","\n","```python\n","from langchain_ollama import OllamaLLM\n","from langchain_community.document_loaders import PyPDFLoader\n","from langchain_community.vectorstores import FAISS\n","from langchain_community.embeddings import HuggingFaceEmbeddings\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_core.prompts import PromptTemplate\n","import gradio as gr\n","import os\n","\n","# Modell initialisieren\n","llm = OllamaLLM(model=\"llama2\", temperature=0.1)\n","\n","# Embedding-Modell laden (lokal, ohne API-Calls)\n","embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n","\n","# Vektorstore für Dokumente\n","vector_store = None\n","\n","def dokumente_laden(dateien):\n","    \"\"\"Lädt PDF-Dokumente und erstellt eine Vektordatenbank\"\"\"\n","    global vector_store\n","    \n","    dokumente = []\n","    for datei in dateien:\n","        loader = PyPDFLoader(datei.name)\n","        dokumente.extend(loader.load())\n","    \n","    # Dokumente in kleinere Chunks aufteilen\n","    text_splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=1000,\n","        chunk_overlap=200\n","    )\n","    chunks = text_splitter.split_documents(dokumente)\n","    \n","    # Vektordatenbank erstellen\n","    vector_store = FAISS.from_documents(chunks, embeddings)\n","    \n","    return f\"{len(chunks)} Textabschnitte aus {len(dateien)} Dokumenten geladen\"\n","\n","def frage_beantworten(frage):\n","    \"\"\"Beantwortet eine Frage basierend auf den geladenen Dokumenten\"\"\"\n","    if vector_store is None:\n","        return \"Bitte laden Sie zuerst Dokumente hoch.\"\n","    \n","    # Relevante Dokumente abrufen\n","    docs = vector_store.similarity_search(frage, k=3)\n","    \n","    # Kontext erstellen\n","    kontext = \"\\n\\n\".join([doc.page_content for doc in docs])\n","    \n","    # Prompt erstellen\n","    prompt_vorlage = \"\"\"\n","    Du bist ein hilfreicher Assistent, der Fragen basierend auf gegebenen Informationen beantwortet.\n","    \n","    Kontext:\n","    {kontext}\n","    \n","    Frage: {frage}\n","    \n","    Deine Antwort sollte sich auf die im Kontext gegebenen Informationen beschränken.\n","    Wenn du keine Antwort im Kontext findest, sage ehrlich, dass du es nicht weißt.\n","    \"\"\"\n","    \n","    prompt = PromptTemplate(\n","        template=prompt_vorlage,\n","        input_variables=[\"kontext\", \"frage\"]\n","    )\n","    \n","    # Antwort vom LLM bekommen\n","    antwort = llm.invoke(prompt.format(kontext=kontext, frage=frage))\n","    \n","    return antwort\n","\n","# Gradio-Interface\n","with gr.Blocks() as demo:\n","    gr.Markdown(\"# Lokales Frage-Antwort-System mit RAG\")\n","    \n","    with gr.Tab(\"Dokumente laden\"):\n","        file_input = gr.File(file_count=\"multiple\", label=\"PDF-Dokumente\")\n","        upload_button = gr.Button(\"Dokumente laden\")\n","        upload_output = gr.Textbox(label=\"Status\")\n","        upload_button.click(dokumente_laden, inputs=[file_input], outputs=[upload_output])\n","    \n","    with gr.Tab(\"Fragen stellen\"):\n","        question_input = gr.Textbox(label=\"Ihre Frage zum Dokument\")\n","        answer_button = gr.Button(\"Frage stellen\")\n","        answer_output = gr.Textbox(label=\"Antwort\")\n","        answer_button.click(frage_beantworten, inputs=[question_input], outputs=[answer_output])\n","\n","demo.launch()\n","```\n","\n","Testen und erweitern Sie das System:\n","1. Testen & ggf. korrigieren des Code\n","2. Unterstützung für weitere Dateiformate (DOCX, TXT, HTML)\n","3. Möglichkeit, die Qualität der Antworten zu verbessern durch:\n","   - Modellauswahl\n","   - Parameteroptimierung\n","   - Verbesserung des Prompts\n","4. Implementierung einer Möglichkeit zur Quellennachverfolgung (aus welchem Teil des Dokuments stammt die Information)\n","5. Visualisierung der relevanten Textpassagen im Dokument"],"metadata":{"id":"iOMIlyS4_MCd"},"id":"iOMIlyS4_MCd"}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"colab":{"provenance":[],"collapsed_sections":["951a5960","4uBmvLYwmhnr","b8ee185b","efe2093b","GE8se-8AoUgf","vOXxrUBro9I8","6Jvx9A_3pOUJ","aKoiGZUMpdcw","L0ZXAt1L6S1Z"],"toc_visible":true,"machine_shape":"hm","gpuType":"A100"},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}
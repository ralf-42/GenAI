{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["oYvUY6gMBKO1","B41ZbXwBQ2eW","tycjOXXjYwNz","xKz6gIaBElCj","IWplHCxehhup","vBP-I0eqErvN","rTzidJP-uaMp","j7K2Fzfp3Vux"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)"],"metadata":{"id":"Ih2CTVBnArVZ"}},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b>\n","Retrieval Augmented Generation\n","</b></font> </br></p>\n","\n","\n","---"],"metadata":{"id":"6jJZ7wbdArVc"}},{"cell_type":"code","source":["#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n","!uv pip install --system -q git+https://github.com/ralf-42/genai_lib\n","from genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages\n","setup_api_keys(['OPENAI_API_KEY', 'HF_TOKEN'], create_globals=False)\n","print()\n","check_environment()\n","print()\n","get_ipinfo()"],"metadata":{"id":"NBySm32qiSkE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title üõ†Ô∏è Installationen { display-mode: \"form\" }\n","install_packages([\n","                ('markitdown[all]', 'markitdown'),\n","                'langchain_chroma',\n","                'langchain_huggingface',\n","                ('unstructured[all-docs]>=0.11.2', 'unstructured'),\n","                ])"],"metadata":{"id":"lG7ahJgjz6w-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1 | √úbersicht\n","---"],"metadata":{"id":"oYvUY6gMBKO1"}},{"cell_type":"markdown","metadata":{"id":"pC9A-LaYhsta"},"source":["\n","<p><font color='black' size=\"5\">\n","Einf√ºhrung in Retrieval-Augmented Generation (RAG)\n","</font></p>"]},{"cell_type":"markdown","source":["\n","\n","Gro√üe Sprachmodelle (LLMs), wie sie in LangChain eingebunden sind, bieten leistungsstarke M√∂glichkeiten zur Verarbeitung und Analyse umfangreicher Textdaten. Sie eignen sich besonders f√ºr Aufgaben, die eine Extraktion und ein tiefgehendes Verst√§ndnis von Informationen erfordern, da sie in der Lage sind, auf Grundlage eines gegebenen Dokuments pr√§zise Antworten auf Fragen zu generieren.\n","\n","Ein wichtiger Ansatz zur **dokumentengest√ºtzten** Beantwortung von Fragen in Kombination mit LLMs ist die sogenannte Retrieval-Augmented Generation (RAG). Diese Methode verbindet die Vorteile klassischer Informationsabrufsysteme mit den generativen F√§higkeiten moderner Sprachmodelle.\n","\n","Durch die Verkn√ºpfung von Informationsabruf und Textgenerierung erh√∂ht RAG die F√§higkeit von LLMs, fundierte und kontextbezogene Antworten zu liefern. Besonders n√ºtzlich ist dieser Ansatz, wenn relevante Informationen nicht direkt im vorab trainierten Modellwissen vorhanden sind und daher externe Quellen herangezogen werden m√ºssen. Die Integration von RAG in LangChain erm√∂glicht eine effektive und flexible L√∂sung f√ºr die Beantwortung von Fragen auf Basis von Dokumenten, was ein differenziertes Verst√§ndnis und pr√§zise Antworten in verschiedenen Anwendungsbereichen gew√§hrleistet.\n"],"metadata":{"id":"nGIdwuGAASCZ"}},{"cell_type":"markdown","source":["**Praktischer Anwendungsfall: Kundenanfrage**"],"metadata":{"id":"oXUkFtTqR1H3"}},{"cell_type":"markdown","source":["<img src=\"https://www.researchgate.net/publication/378467192/figure/fig3/AS:11431281235857152@1712887907223/Zusammenspiel-zwischen-Vektordatenbank-Kundenanfrage-und-LLM.png\" width=\"800\" alt=\"Avatar\">\n","\n"],"metadata":{"id":"y6UWOMc8RU7v"}},{"cell_type":"markdown","source":["Quelle: [Die_Nutzung_von_ChatGPT_in_Unternehmen](https://www.researchgate.net/publication/378467192_Die_Nutzung_von_ChatGPT_in_Unternehmen_Ein_Fallbeispiel_zur_Neugestaltung_von_Serviceprozessen)"],"metadata":{"id":"Vn889oETR_dv"}},{"cell_type":"markdown","source":["# 2 | RAG-Prozess\n","---"],"metadata":{"id":"B41ZbXwBQ2eW"}},{"cell_type":"markdown","source":["Der Retrieval-Augmented Generation (RAG) Prozess besteht aus zwei zentralen Teilprozessen ‚Äì  **Datensammlung** zur Vorbereitung externer Wissensquellen und dem **Abruf & Erweiterung** zur kontextgest√ºtzten Beantwortung von Nutzeranfragen ‚Äì und verbindet damit Information Retrieval und Textgenerierung zu einem leistungsf√§higen Gesamtsystem.\n"],"metadata":{"id":"Rwy5QDCFaPRp"}},{"cell_type":"markdown","source":["<img src=\"https://raw.githubusercontent.com/ralf-42/Image/main/rag_process.png\" width=\"600\" alt=\"Avatar\">"],"metadata":{"id":"bC_Exz8iWjIN"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Data Collection Process\n","</font></p>"],"metadata":{"id":"ZsptMhzkY8XI"}},{"cell_type":"markdown","source":["+ Dokuments:\n","Relevante Texte (z.B. Fachartikel, Handb√ºcher, Webseiten) werden gesammelt.\n","\n","+ Chunking: Aufteilung der l√§ngere Texte in kleinere, zusammenh√§ngende Abschnitte (Chunks).\n","\n","+ Embedding:\n","Die Texte werden mithilfe eines Embedding-Modells (z.‚ÄØB. sentence-transformers) in numerische Vektoren umgewandelt.\n","\n","+ Vector Database:\n","Die Embeddings und zugeh√∂rigen Texte werden in einer Vektordatenbank (z.‚ÄØB. FAISS, ChromaDB) gespeichert, um sp√§ter effizient durchsucht werden zu k√∂nnen."],"metadata":{"id":"qR2ee31NZcD7"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Inference Process\n","</font></p>"],"metadata":{"id":"kF74RXeFXkqy"}},{"cell_type":"markdown","source":["\n","\n","\n","+ User Query:\n","Der Nutzer stellt eine Anfrage (z.‚ÄØB. eine Frage in nat√ºrlicher Sprache).\n","\n","+ Query Embedding:\n","Die Frage wird in ein Embedding konvertiert\n","\n","+ Retriever:\n","Ein Suchsystem durchsucht eine (externe) Vektordatenbank (z.B. mit Dokumenten) nach relevanten Inhalten zur Anfrage.\n","\n","+ Documents:\n","Die gefundenen, relevanten Dokumente (Kontextinformationen) werden gesammelt.\n","\n","\n","+ Enrich Prompt:\n","Die Abfrage des Nutzers wird um die gefundenen Dokumente erg√§nzt.\n","\n","+ Model Inference:\n","Das generative Sprachmodell (z.B. GPT) erh√§lt die urspr√ºngliche Anfrage plus die gefundenen Dokumente als Kontext. Es generiert auf dieser Grundlage eine fundierte Antwort."],"metadata":{"id":"UtA3hzfVY4wc"}},{"cell_type":"markdown","metadata":{"id":"ErkO7KR9apEY"},"source":["\n","<p><font color='black' size=\"5\">\n","Einschr√§nkungen von RAG\n","</font></p>"]},{"cell_type":"markdown","source":["\n","\n","LLM RAG kombiniert Sprachmodelle mit externem Datenabruf, um fehlendes Wissen aus spezialisierten oder propriet√§ren Quellen zu erg√§nzen. Dies ist besonders n√ºtzlich in Bereichen wie Finanzen, Recht oder Technik, wo pr√§zise und aktuelle Informationen erforderlich sind.\n","\n","Wenn die zus√§tzlichen Daten jedoch bereits Allgemeinwissen sind, bringt RAG wenig Mehrwert. Da das Basismodell umfassend vortrainiert ist, kann es viele Anfragen ohne externe Erweiterung beantworten. Der unn√∂tige Abruf bekannter Informationen f√ºhrt zudem zu Ineffizienzen und erh√∂ht den Rechenaufwand.\n","\n","RAG sollte gezielt eingesetzt werden, insbesondere f√ºr dom√§nenspezifische oder propriet√§re Daten, die das Basismodell nicht abdeckt. In allgemeinen Wissensbereichen ist es meist √ºberfl√ºssig."],"metadata":{"id":"nOuUx7eYGpEz"}},{"cell_type":"markdown","source":["[RAG-Visualizer](https://claude.site/artifacts/e54ef5f2-0ba0-4468-9315-18f7c1c86c4b)"],"metadata":{"id":"qqKmHtunss8C"}},{"cell_type":"markdown","source":["# 3 | Deep Dive: Token & Chunks\n","---"],"metadata":{"id":"tycjOXXjYwNz"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Tokenizing\n","</font></p>"],"metadata":{"id":"E62ye_IkVQ54"}},{"cell_type":"markdown","source":["**Warum sind Tokenisierung und Chunking wichtig?**\n","\n","Gro√üe Sprachmodelle (LLMs) wie GPT-3 oder BERT verarbeiten Text nicht als ganze S√§tze oder Abs√§tze, sondern als eine Folge von \"Tokens\". Tokenisierung und Chunking sind entscheidende Vorverarbeitungsschritte, die es erm√∂glichen, Texte effizient zu verarbeiten und die Leistung von KI-Modellen zu optimieren.\n","\n"],"metadata":{"id":"R-SwC6oHcvGI"}},{"cell_type":"markdown","source":["**Tokenisierung: Die Grundlage der Textverarbeitung**\n","\n","Tokenisierung ist der Prozess, bei dem Text in kleinere Einheiten, sogenannte Tokens, zerlegt wird. Diese Tokens k√∂nnen W√∂rter, Teilw√∂rter oder sogar einzelne Zeichen sein.\n","\n","**Warum ist Tokenisierung wichtig?**\n","\n","1. **Einheitliche Verarbeitung:** LLMs haben eine begrenzte Eingabel√§nge (z.B. 512 oder 1024 Tokens). Die Tokenisierung stellt sicher, dass Texte in einem einheitlichen Format vorliegen, das vom Modell verarbeitet werden kann.\n","\n","2. **Bedeutungserfassung:** Viele Tokens repr√§sentieren semantische Einheiten, was dem Modell hilft, die Bedeutung des Textes besser zu erfassen.\n","\n","3. **Effiziente Verarbeitung:** Tokenisierte Texte k√∂nnen effizienter verarbeitet und in numerische Vektoren umgewandelt werden, was f√ºr die Eingabe in neuronale Netze notwendig ist.\n","\n","\n","**Hilfe:**\n","\n","1 DIN A4 Seite hat ca. 300 Worte und ca. 450 Token"],"metadata":{"id":"kg6QE54ydPuk"}},{"cell_type":"markdown","source":["[OpenAI Tokenizer](https://platform.openai.com/tokenizer)"],"metadata":{"id":"XEilx_z86ZbT"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Chunking\n","</font></p>"],"metadata":{"id":"qKtbt_07VJ-5"}},{"cell_type":"markdown","source":["**Chunking: Texte in verdauliche H√§ppchen teilen**\n","\n","Chunking ist der Prozess, bei dem l√§ngere Texte in kleinere, zusammenh√§ngende Abschnitte (Chunks) aufgeteilt werden.\n","\n","**Warum ist Chunking wichtig?**\n","\n","1. **Verarbeitung langer Texte:** Da LLMs eine begrenzte Eingabel√§nge haben, erm√∂glicht Chunking die Verarbeitung von Texten, die l√§nger als diese Grenze sind.\n","\n","2. **Kontexterhaltung:** Durch geschicktes Chunking kann der relevante Kontext innerhalb eines Chunks erhalten bleiben, was f√ºr viele NLP-Aufgaben entscheidend ist.\n","\n","3. **Effizienz:** Kleinere Chunks k√∂nnen parallel verarbeitet werden, was die Gesamtverarbeitungszeit reduzieren kann.\n","\n"],"metadata":{"id":"jVkpvLb4dN8M"}},{"cell_type":"markdown","source":["**Hauptans√§tze f√ºr Tokenisierung und Chunking**\n","\n","1. **Wortbasierte Tokenisierung:** Teilt Text an Wortgrenzen. Einfach, aber nicht immer optimal f√ºr komplexe Sprachen oder technische Texte.\n","\n","2. **Subword-Tokenisierung:** Zerlegt W√∂rter in h√§ufig vorkommende Teilw√∂rter. Beispiele sind BPE (Byte-Pair Encoding) oder WordPiece. Dies ist besonders n√ºtzlich f√ºr die Behandlung von unbekannten W√∂rtern und morphologisch reichen Sprachen.\n","\n","3. **Zeichenbasierte Tokenisierung:** Betrachtet jedes Zeichen als separates Token. N√ºtzlich f√ºr bestimmte Sprachen oder spezielle Anwendungen.\n","\n","4. **Satzbasiertes Chunking:** Teilt Text in S√§tze. Einfach und oft effektiv, kann aber Kontext zwischen S√§tzen verlieren.\n","\n","5. **√úberlappende Chunks:** Erstellt Chunks mit √úberlappungen, um Kontext an den Chunk-Grenzen zu erhalten.\n","\n","6. **Semantisches Chunking:** Versucht, Chunks basierend auf Bedeutungseinheiten zu erstellen, was komplexer, aber oft effektiver f√ºr das Verst√§ndnis ist.\n","\n","Durch die richtige Kombination von Tokenisierung und Chunking k√∂nnen wir Texte so vorbereiten, dass sie optimal von KI-Modellen verarbeitet werden k√∂nnen. Dies verbessert nicht nur die Leistung der Modelle, sondern erm√∂glicht auch die Verarbeitung von Texten beliebiger L√§nge in RAG-Systemen und anderen NLP-Anwendungen."],"metadata":{"id":"ZBtSPmvkdKph"}},{"cell_type":"markdown","source":["[ChunkViz](https://chunkviz.up.railway.app/)"],"metadata":{"id":"GSNdrPlzfq0F"}},{"cell_type":"markdown","metadata":{"id":"xKz6gIaBElCj"},"source":["# 4 | Deep Dive: Embedding\n","---"]},{"cell_type":"markdown","source":["Einbettungen stellen eine KI-optimierte Repr√§sentation verschiedener Datentypen dar und eignen sich daher besonders f√ºr den Einsatz in einer Vielzahl KI-gest√ºtzter Tools und Algorithmen. Sie erfassen die wesentlichen Merkmale von Texten, Bildern und auch von Audio- und Videodaten.\n","\n","Ein Einbettungsmodell verarbeitet Eingangsdaten und wandelt sie in numerische Vektoren um. Die Architektur des Modells sorgt daf√ºr, dass √§hnliche Inhalte ‚Äì beispielsweise Texte mit verwandter Bedeutung oder visuell √§hnliche Bilder ‚Äì im Vektorraum n√§her beieinander liegen, w√§hrend sich un√§hnliche Daten weiter voneinander entfernt befinden.\n","\n","Ein **Embedding** ist ein Vektor aus Gleitkommazahlen, der √Ñhnlichkeiten zwischen Texten misst. Kleinere Abst√§nde zwischen Vektoren bedeuten eine st√§rkere inhaltliche N√§he.\n","\n","OpenAI bietet zwei Einbettungsmodelle an:\n","- **text-embedding-3-small**\n","- **text-embedding-3-large**\n","\n","**Unterschiede und Auswahlkriterien:**\n","\n","| Kriterium         | text-embedding-3-small | text-embedding-3-large |\n","|------------------|----------------------|----------------------|\n","| **Genauigkeit & Leistung** | Weniger detailliert, aber f√ºr viele Aufgaben ausreichend | Erfasst komplexere Zusammenh√§nge, ideal f√ºr anspruchsvolle NLP-Aufgaben |\n","| **Rechenaufwand** | Effizient, ben√∂tigt weniger Ressourcen | H√∂herer Speicher- und Rechenbedarf |\n","| **Latenz & Geschwindigkeit** | Schnellere Verarbeitung, geringe Latenz | H√∂here Latenz, nicht ideal f√ºr Echtzeit-Anwendungen |\n","| **Kosten** | Kosteng√ºnstiger, ideal f√ºr skalierbare Anwendungen | Teurer in Betrieb und Bereitstellung |\n","| **Anwendungsf√§lle** | Chatbots, Echtzeitsysteme, einfache Textverarbeitung | Semantische Analyse, komplexe NLP-Aufgaben, KI-Forschung |\n","\n","\n","\n","**Fazit**:  \n","- **Large**: Wenn hohe Pr√§zision, detailliertes Sprachverst√§ndnis und ausreichend Ressourcen vorhanden sind.  \n","- **Small**: Wenn Effizienz, niedrige Kosten und schnelle Reaktionszeiten im Vordergrund stehen.  \n","\n","Die Wahl des Modells h√§ngt von den spezifischen Anforderungen der Anwendung ab."],"metadata":{"id":"5iidUCvUKKeg"}},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Instanz eines Einbettungsmodells\n","</font></p>"],"metadata":{"id":"i2fs1gWUeGON"}},{"cell_type":"markdown","source":["Hier ist ein Beispiel f√ºr die Erstellung einer Instanz mit text-embedding-3-small."],"metadata":{"id":"KXYkRca4euKP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wP9LYwNUSKyv"},"outputs":[],"source":["from langchain_openai import OpenAIEmbeddings\n","embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Vektoren\n","</font></p>"],"metadata":{"id":"GRkhphKUfV9q"}},{"cell_type":"markdown","metadata":{"id":"uGs1Fqk0Vu-6"},"source":["Ein Vektor ist eine mathematische Darstellung, die eine Menge von Zahlen in einer bestimmten Reihenfolge speichert. In einem Einbettungsmodell wird jeder Text ‚Äì sei es ein einzelnes Wort, ein Satz oder ein ganzer Absatz ‚Äì als Vektor in einem hochdimensionalen Raum dargestellt. Diese Darstellung erm√∂glicht es, √Ñhnlichkeiten zwischen Texten mathematisch zu berechnen.\n","\n","**Wortvektoren**\n","Wenn ein Einbettungsmodell ein einzelnes Wort in einen Vektor umwandelt, geschieht dies auf Basis der Bedeutung und des Kontexts des Wortes. Das bedeutet, dass W√∂rter mit √§hnlicher Bedeutung oder Verwendung in der Sprache √§hnliche Vektoren erhalten. Ein Beispiel:\n","\n","- Das Wort ‚ÄûKatze‚Äú k√∂nnte in einem 3D-Raum (vereinfacht dargestellt) als Vektor **(0.5, 1.2, -0.3)** erscheinen.\n","- Das Wort ‚ÄûHund‚Äú k√∂nnte einen √§hnlichen Vektor haben, z. B. **(0.6, 1.1, -0.2)**, da beide Begriffe semantisch verwandt sind.\n","\n","Dagegen h√§tte ein v√∂llig anderes Wort wie ‚ÄûAuto‚Äú einen weit entfernten Vektor, z. B. **(2.3, -0.4, 1.7)**, da es eine ganz andere Bedeutung hat.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iq3WUSr7T1QK"},"outputs":[],"source":["vektor1 = embeddings_model.embed_query(\"Hund\")\n","vektor2 = embeddings_model.embed_query(\"Ein Hund l√§uft √ºber die Stra√üe.\")\n","\n","print(type(vektor1))"]},{"cell_type":"markdown","metadata":{"id":"wMqiJ6zFZUue"},"source":["Die Ausgabe besteht lediglich aus einer gew√∂hnlichen Python-Liste, die jedoch eine erhebliche L√§nge aufweist."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BNt_opHMX1B7"},"outputs":[],"source":["print(len(vektor1))\n","print(len(vektor2))"]},{"cell_type":"markdown","metadata":{"id":"OrwZGIA-ZbK_"},"source":["Die L√§nge dieser Zeichenfolge bleibt bei allen Abfragen desselben Modells konstant. Obwohl die gr√∂√üere Modellversion in der Lage ist, qualitativ bessere Vektoren zur Unterscheidung von Zeichenfolgen zu erzeugen, bedeutet dies nicht zwangsl√§ufig, dass eine gr√∂√üere Vektorl√§nge erforderlich ist, um diese Verbesserung zu erzielen. Dieses Konzept ist komplex und bedarf weiterer Untersuchung.\n","\n","Betrachtet man die eigentliche Liste, erkennt man, dass sie lediglich eine Sammlung von Zahlen enth√§lt. Nachfolgend sind die ersten zehn Elemente dargestellt."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MePSP-eGZpWA"},"outputs":[],"source":["print(vektor1[:5]) # Hund\n","print(vektor2[:5]) # Ein Hund l√§uft √ºber die Stra√üe."]},{"cell_type":"markdown","metadata":{"id":"EtbM9gKCaqxU"},"source":["<p><font color='black' size=\"5\">\n"," Vektoren vergleichen\n","</font></p>"]},{"cell_type":"markdown","source":["In der Mathematik gibt es verschiedene Methoden, um Vektoren miteinander zu vergleichen. Einige der g√§ngigsten Ans√§tze sind:\n","\n","\n","* **Kosinus-√Ñhnlichkeit**: Berechnet den Kosinus des Winkels zwischen zwei Vektoren, wobei die Orientierung st√§rker gewichtet wird als die Gr√∂√üe. Diese Technik ist besonders verbreitet in Bereichen wie Text Mining und der Informationssuche, um Dokumente auf √Ñhnlichkeit zu pr√ºfen.\n","* **Euklidische Distanz**: Gibt die direkte Entfernung zwischen zwei Vektoren an und wird h√§ufig in Clustering-Methoden oder Algorithmen zum n√§chsten Nachbarn im maschinellen Lernen verwendet.\n","\n","* **Manhattan-Distanz**: Addiert die absoluten Differenzen der Komponenten zweier Vektoren. Diese Methode wird vor allem in gitterbasierten Pfadsuchalgorithmen sowie in bestimmten maschinellen Lernanwendungen eingesetzt.\n","\n","Laut einer Empfehlung von OpenAI [FAQ](https://platform.openai.com/docs/guides/embeddings/frequently-asked-questions), eignet sich die Kosinus-√Ñhnlichkeit besonders gut zum Vergleich von Vektoren, da dabei die urspr√ºngliche Gr√∂√üe beibehalten wird. Zudem bleibt das Vorzeichen der einzelnen Vektorkomponenten erhalten, was f√ºr die Analyse entscheidend ist."],"metadata":{"id":"_aC5YaY7gT0l"}},{"cell_type":"markdown","source":["**Interpretation:**\n","\n","| √Ñhnlichkeitsma√ü         | Wertebereich | Interpretation                                                                                           |\n","|-------------------------|--------------|---------------------------------------------------------------------------------------------------------|\n","| Kosinus-√Ñhnlichkeit     | -1 bis 1     | - 1: Maximale √Ñhnlichkeit (identische Richtung) <br> - 0: Keine √Ñhnlichkeit (orthogonal) <br> - -1: Maximale Un√§hnlichkeit (entgegengesetzte Richtung) <br> - H√∂here Werte bedeuten gr√∂√üere √Ñhnlichkeit |\n","| Euklidischer Abstand     | 0 bis ‚àû      | - 0: Identische Vektoren (maximale √Ñhnlichkeit) <br> - Je gr√∂√üer der Wert, desto un√§hnlicher sind die Vektoren <br> - Niedrigere Werte bedeuten gr√∂√üere √Ñhnlichkeit |\n","| Manhattan-Distanz       | 0 bis ‚àû      | - 0: Identische Vektoren (maximale √Ñhnlichkeit) <br> - Je gr√∂√üer der Wert, desto un√§hnlicher sind die Vektoren <br> - Niedrigere Werte bedeuten gr√∂√üere √Ñhnlichkeit |\n"],"metadata":{"id":"n0WJfBYylojx"}},{"cell_type":"code","source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  √Ñhnlichkeit ermitteln</font> </br></p>\n","# Ermittlung & Ausgabe von Vektoren\n","def similarity(vektor1, vektor2):\n","    import numpy as np\n","    from scipy.spatial.distance import cosine, euclidean, cityblock\n","    # Zwei Beispiel-Einbettungsvektoren\n","    vector1 = np.array(vektor1)\n","    vector2 = np.array(vektor2)\n","\n","    # 1. Kosinus-√Ñhnlichkeit\n","    cosine_similarity = 1 - cosine(vector1, vector2)\n","    print(f\"Kosinus-√Ñhnlichkeit: {cosine_similarity:.4f}\")\n","\n","    # # 2. Euklidischer Abstand\n","    # euclidean_distance = euclidean(vector1, vector2)\n","    # print(f\"Euklidischer Abstand: {euclidean_distance:.4f}\")\n","\n","    # # 3. Manhattan-Distanz\n","    # manhattan_distance = cityblock(vector1, vector2)\n","    # print(f\"Manhattan-Distanz: {manhattan_distance:.4f}\")"],"metadata":{"id":"2OX-vHbNkhZ2","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vektor1 = embeddings_model.embed_query(\"Ein Hund l√§uft √ºber eine Br√ºcke\")\n","vektor2 = embeddings_model.embed_query(\"Ein Hund l√§uft √ºber die Stra√üe.\")\n","similarity(vektor1, vektor2)"],"metadata":{"id":"lxn-B-ytmHHy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vektor1 = embeddings_model.embed_query(\"Ein Hund l√§uft √ºber eine Br√ºcke.\")\n","vektor2 = embeddings_model.embed_query(\"Quantenmechanik beschreibt das Verhalten subatomarer Teilchen.\")\n","similarity(vektor1, vektor2)"],"metadata":{"id":"1mV2ChBimn4v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["[Embedding Projector](https://projector.tensorflow.org/?hl=de)"],"metadata":{"id":"uJRPUOLyg3MR"}},{"cell_type":"markdown","source":["# 5 | Deep Dive: Vectorstore\n","---"],"metadata":{"id":"IWplHCxehhup"}},{"cell_type":"markdown","source":["Ein **Vectorstore** ist eine spezialisierte Datenbank zur Speicherung und schnellen Suche von Texten in Form von Vektoren. Er bildet die Grundlage f√ºr semantische Suche in Retrieval-Systemen wie RAG. Durch die Umwandlung von Text in numerische Repr√§sentationen (Embeddings) k√∂nnen inhaltlich √§hnliche Informationen effizient gefunden werden ‚Äì selbst wenn die exakten W√∂rter nicht √ºbereinstimmen.\n"],"metadata":{"id":"xzdBafa2hr5Y"}},{"cell_type":"markdown","source":["**Beispiel Erstellung & Abfrage eines Vectorstore:**"],"metadata":{"id":"bMok0fnui02w"}},{"cell_type":"code","source":["# Import\n","import os\n","from IPython.display import display, Markdown\n","from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n","from langchain_chroma import Chroma\n","from langchain.schema import Document"],"metadata":{"id":"JLvCk-gMhsVC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Chroma-Speicherverzeichnis (persistent)\n","chroma_dir = \"chroma_demo\""],"metadata":{"id":"tCc3AZQ7uoLz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# OpenAI Embeddings vorbereiten\n","embedding_model = OpenAIEmbeddings()"],"metadata":{"id":"QVwtrDX0upUp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. Beispiel-Dokumente definieren\n","texte = [\n","    \"Python ist eine beliebte Programmiersprache f√ºr Machine Learning.\",\n","    \"Gradio ist ein Python-Framework zur Erstellung von KI-Demos.\",\n","    \"ChromaDB ist eine Vektordatenbank zur √Ñhnlichkeitssuche.\",\n","    \"LangChain verbindet Sprachmodelle mit externem Wissen und Tools.\",\n","    \"Mit Hugging Face k√∂nnen moderne NLP-Modelle einfach genutzt werden.\",\n","    \"OpenAI bietet leistungsstarke APIs f√ºr Text-, Bild- und Sprachmodelle.\",\n","    \"Vektordatenbanken speichern Embeddings f√ºr semantische Suchanfragen.\",\n","    \"Retrieval-Augmented Generation kombiniert Suche mit Textgenerierung.\",\n","    \"Embeddings wandeln Texte in numerische Repr√§sentationen um.\",\n","    \"ChatGPT kann als intelligenter Assistent in Anwendungen integriert werden.\"\n","]\n","\n","docs = [Document(page_content=t) for t in texte]"],"metadata":{"id":"BPINeRS2mscH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. Vektordatenbank mit Chroma erstellen\n","vectordb = Chroma.from_documents(\n","    documents=docs,\n","    embedding=embedding_model,\n","    persist_directory=chroma_dir\n",")"],"metadata":{"id":"drZSCOndmvrN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Eine Beispiel-Query\n","query = \"Was sind Embeddings?\"\n","docs_retrieved = vectordb.similarity_search(query, k=3)   # Die Top 3 Fundstellen werden bereitgestellt"],"metadata":{"id":"gspzZvibmxiR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ausgabe der gefundenen Texte\n","mprint(f\"## üîç Gefundene Dokumente\")\n","mprint(\"---\")\n","mprint(f\"**Query:** {query}\")\n","print()\n","mprint(f\"**Quellen:**\")\n","for i, doc in enumerate(docs_retrieved, 1):\n","    print(f\"{i}.  Id: {doc.id:40} Inhalt: {doc.page_content}\")"],"metadata":{"id":"ffRCtv2pmyvr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vBP-I0eqErvN"},"source":["# 6 | Hands-On: RAG - Biografien\n","---"]},{"cell_type":"markdown","source":["Retrieval-Augmented Generation (RAG) ist eine innovative Methode zur Verbesserung gro√üer Sprachmodelle (LLMs), indem externe Daten in den Generierungsprozess eingebunden werden. Damit RAG optimal funktioniert, muss es Informationen nutzen, die nicht bereits im Basismodell enthalten sind. Dies ist besonders wichtig, da der gr√∂√üte Vorteil von RAG darin liegt, gezielt aktuelle oder spezialisierte Daten abzurufen, die w√§hrend des urspr√ºnglichen Trainingsprozesses nicht ber√ºcksichtigt wurden.\n","\n","Diese Eigenschaft macht RAG besonders n√ºtzlich f√ºr Unternehmen, da sie gro√üe Mengen interner und propriet√§rer Daten verwalten, darunter Gesch√§ftsdokumente, Kundendaten und detaillierte Berichte. Da diese Informationen in der Regel nicht √∂ffentlich zug√§nglich sind, w√§ren sie ohne RAG nicht f√ºr ein LLM nutzbar. Durch den Einsatz dieser Technik k√∂nnen Unternehmen ihre eigenen Daten einbinden, um genauere und relevantere Antworten zu generieren, was Entscheidungsprozesse optimiert und die betriebliche Effizienz steigert.\n","\n","Zur Veranschaulichung der RAG-F√§higkeiten dient ein eigens erstellter Beispieldatensatz mit synthetischen Biografien von Forscher:innen aus unterschiedlichen Forschungsgebieten. Dieser Datensatz demonstriert, wie RAG gezielt Informationen abruft, die ein Basismodell von sich aus nicht enthalten w√ºrde, und diese in den Antwortprozess integriert. Damit wird ein praxisnaher Anwendungsfall f√ºr den Einsatz von RAG in Unternehmen dargestellt."],"metadata":{"id":"YbOrUOMbK0ND"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0QQm0vSmeF5T"},"outputs":[],"source":["# Importe\n","# Standardbibliotheken\n","import os\n","import re\n","import warnings\n","from IPython.display import display, Markdown\n","\n","# Drittanbieterbibliotheken\n","from langchain.chains import create_retrieval_chain\n","from langchain.chains.combine_documents import create_stuff_documents_chain\n","from langchain_community.document_loaders import (\n","    UnstructuredMarkdownLoader,\n","    UnstructuredWordDocumentLoader,\n","    PyPDFLoader,\n","    UnstructuredFileLoader\n",")\n","from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n","from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_chroma import Chroma\n","\n","from langchain import hub\n","\n","# Warnungen ausschalten\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"wmQJoW7Si3Lc"},"source":["\n","Die Funktion verarbeitet ein gro√ües Dokument, indem sie es in kleinere, handhabbare Segmente, sogenannte Chunks, unterteilt, um Einbettungen f√ºr den LLM-Abruf in Retrieval-Augmented Generation (RAG) zu erstellen. Der Chunk-Parameter bestimmt die maximale L√§nge jedes Segments und stellt sicher, dass der Text in Teile zerlegt wird, die vom Sprachmodell effizient verarbeitet und analysiert werden k√∂nnen. Der Overlap-Parameter gibt die Anzahl der Token an, die am Anfang jedes neuen Chunks wiederholt werden sollen, wodurch eine √úberlappung zwischen aufeinanderfolgenden Chunks entsteht. Diese √úberlappung hilft dabei, die kontextuelle Kontinuit√§t √ºber Chunks hinweg aufrechtzuerhalten und verbessert so die Qualit√§t und Genauigkeit der Einbettungen. Durch die systematische Segmentierung des Dokuments und die Generierung von Einbettungen f√ºr jeden Chunk verbessert die Funktion die F√§higkeit des Sprachmodells, relevante Informationen abzurufen, was zu pr√§ziseren und kontextuell angemessenen Antworten im RAG-Framework f√ºhrt."]},{"cell_type":"markdown","metadata":{"id":"tBBhD2Utjn8t"},"source":["Textdateien mit Biografielisten werden verarbeitet, indem sie in handhabbare Bl√∂cke unterteilt werden. Eine Blockgr√∂√üe von 1000 Token erleichtert dem Sprachmodell die Erstellung von Einbettungen, w√§hrend eine √úberlappung von 200 Token den Kontext zwischen den Bl√∂cken erh√§lt."]},{"cell_type":"markdown","source":["<img src=\"https://raw.githubusercontent.com/ralf-42/Image/main/rag_process_01.png\" width=\"600\" alt=\"Avatar\">\n"],"metadata":{"id":"q9Ug4Y6QXnmG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3zoC7v0igkkk"},"outputs":[],"source":["!rm -rf files\n","!mkdir files\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02%20data/biografien_1.txt -o files/biografien_1.txt\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02%20data/biografien_2.md -o files/biografien_2.md\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02%20data/biografien_3.pdf -o files/biografien_3.pdf\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02%20data/biografien_4.docx -o files/biografien_4.docx"]},{"cell_type":"code","source":["# Loader-Konfiguration\n","loader_mapping = {\n","    \"*.md\": UnstructuredMarkdownLoader,\n","    \"*.docx\": UnstructuredWordDocumentLoader,\n","    \"*.pdf\": PyPDFLoader,\n","    \"*.txt\": UnstructuredFileLoader,  # Loader f√ºr .txt Dateien\n","}\n","\n","# Funktion zum Laden der Dokumente\n","def load_documents_from_directory(directory_path):\n","    \"\"\"L√§dt Dokumente aus dem angegebenen Verzeichnis basierend auf den unterst√ºtzten Dateitypen.\"\"\"\n","    documents = []\n","    for file_pattern, loader_cls in loader_mapping.items():\n","        loader = DirectoryLoader(directory_path, glob=file_pattern, loader_cls=loader_cls)\n","        documents.extend(loader.load())\n","    return documents\n","\n","# Dokumente laden\n","directory_path = \"/content/files\"\n","\n","documents = load_documents_from_directory(directory_path)"],"metadata":{"id":"fBioTYk1Z7WX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(documents), len(documents)"],"metadata":{"id":"ImoufhHG4tCt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NM0kGSyMnjEP"},"source":["Die geladenen Biografiedaten werden in die ChromaDB-Datenbank integriert. ChromaDB setzt standardm√§√üig das Einbettungsmodell `all-MiniLM-L6-v2` ein, wobei auch andere [Model Wrapper](https://docs.trychroma.com/integrations/openai) zur Verf√ºgung stehen. Um die Dokumente in verarbeitbare Segmente zu unterteilen, kommt der RecursiveCharacterTextSplitter aus der Bibliothek langchain_text_splitters zum Einsatz. F√ºr die Einbettungsfunktionen wird das Modell √ºber SentenceTransformerEmbeddings instanziiert. Chroma dient als Vektorspeicher und arbeitet nahtlos mit diesen Komponenten zusammen, um Einbettungen effizient zu speichern und abzufragen."]},{"cell_type":"code","source":["# Text-Splitter konfigurieren und Dokumente aufteilen\n","chunk_size = 900\n","chunk_overlap = 300\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_overlap, chunk_overlap=chunk_overlap)\n","docs = text_splitter.split_documents(documents)"],"metadata":{"id":"s6E48pzya8Kr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(docs), len(docs)"],"metadata":{"id":"Oc5wzbgca0Bu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Embeddingsmodell festlegen\n","embedding_model = \"text-embedding-3-small\"\n","embeddings = OpenAIEmbeddings(model=embedding_model)"],"metadata":{"id":"GSNmGFbelVuO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Vektordatenbank erstellen und speichern\n","persistent_directory = \"/content/chroma_db\"\n","vectorstore = Chroma.from_documents(docs, embeddings, persist_directory=persistent_directory)"],"metadata":{"id":"_ru5XYgDlwM2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hier wird ein vordefinierter Prompt f√ºr RAG-Anwendungen aus dem LangChain Hub geladen. \"rlm/rag-prompt\" ist ein spezieller Prompt, der f√ºr Retrieval-Augmented Generation optimiert ist.\n","\n","\n","[LangChainHub](https://smith.langchain.com/hub/rlm/rag-prompt)"],"metadata":{"id":"kgO-pwp6Nk_X"}},{"cell_type":"code","source":["rag_prompt = hub.pull(\"rlm/rag-prompt\")"],"metadata":{"id":"yhjBA0Yyqefs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Diese Funktion nimmt eine Liste von Dokumentobjekten (wie sie vom Retriever zur√ºckgegeben werden) und:\n","\n","+ Extrahiert den Textinhalt (page_content) aus jedem Dokument\n","+ Verbindet alle Textinhalte mit doppelten Zeilenumbr√ºchen zu einem einzigen String\n","+ Dies ist wichtig, um die abgerufenen Dokumente in ein Format zu bringen, das dem LLM als Kontext √ºbergeben werden kann"],"metadata":{"id":"dHED_4dSq0m3"}},{"cell_type":"code","source":["def format_documents(documents):\n","    return \"\\n\\n\".join(doc.page_content for doc in documents)"],"metadata":{"id":"wh5tRiSMqskO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hier werden zwei Hauptkomponenten initialisiert:\n","\n","+ Ein Large Language Model (LLM) mit OpenAI\n","+ Ein Retriever, der aus einem vectorstore erstellt wird (der vorher im Code definiert wurde)\n","\n","Dieser Retriever sucht relevante Dokumente aus dem Vektorspeicher basierend auf √Ñhnlichkeit zur Abfrage."],"metadata":{"id":"8D7d561IrEof"}},{"cell_type":"markdown","source":["<img src=\"https://raw.githubusercontent.com/ralf-42/Image/main/rag_process_02.png\" width=\"600\" alt=\"Avatar\">\n"],"metadata":{"id":"ldDR9oeJYBCj"}},{"cell_type":"code","source":["# Festlegen LLM und Retriever\n","model_name = 'gpt-4o-mini'\n","temperature = 0\n","llm = ChatOpenAI(model=model_name, temperature=temperature)\n","\n","retriever = vectorstore.as_retriever()"],"metadata":{"id":"c6IG6aH_q-lM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Parameter:**\n","\n","\n","| Parameter         | Wirkung                         |\n","| ----------------- | ------------------------------- |\n","| `k`               | Anzahl der Dokumente im Kontext, default: K=4|\n","| `score_threshold` | Qualit√§tsfilter (optional)      |\n","\n"],"metadata":{"id":"XrJ8RtfFKWli"}},{"cell_type":"markdown","source":["\n","Beispiel:\n","\n","```Python\n","retriever = vectorstore.as_retriever(\n","    search_kwargs={\"k\": 10, \"score_threshold\": 0.7}\n",")\n","```"],"metadata":{"id":"nfsIclKhK8qA"}},{"cell_type":"markdown","source":["Hier wird die komplette RAG-Pipeline zusammengebaut:\n","\n","Eingabe-Dictionary wird erstellt mit:\n","\n","+ \"context\": F√ºhrt zuerst den Retriever aus, um relevante Dokumente zu finden, und formatiert diese dann mit der format_documents-Funktion\n","+ \"question\": Leitet die urspr√ºngliche Frage unver√§ndert weiter (mittels RunnablePassthrough())\n","\n","\n","Dieses Dictionary wird an den RAG-Prompt √ºbergeben (rag_prompt), der ein Template mit Platzhaltern f√ºr Kontext und Frage ist\n","Der formatierte Prompt wird an das LLM √ºbergeben (llm), das eine Antwort basierend auf der Frage und dem bereitgestellten Kontext generiert\n","Die LLM-Ausgabe wird durch den StrOutputParser() geleitet, um sie in einen einfachen String zu konvertieren\n","\n","Die gesamte Pipeline erm√∂glicht es, eine Frage zu stellen, relevante Dokumente zu finden, diese mit der Frage zu kombinieren und eine fundierte Antwort zu generieren, die auf den abgerufenen Informationen basiert."],"metadata":{"id":"3VjxvX5PrTm3"}},{"cell_type":"code","source":["# Chat-Verlauf initialisieren\n","chat_history = []\n","\n","chain = (\n","    {\"context\": retriever | format_documents, \"question\": RunnablePassthrough()}\n","    | rag_prompt\n","    | llm\n","    | StrOutputParser()\n",")"],"metadata":{"id":"T3GIuLHTrDzF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zIjLhtCr7HLe"},"source":["Die RAG-Kette kann nun aufgerufen werden, um Informationen zu einer der in den Beispielbiografiedaten enthaltenen Personen abzurufen."]},{"cell_type":"markdown","source":["Die gesamte Chain wird mit einem Input aufgerufen. Dieser Input standardm√§√üig an alle Runnable-Komponenten weitergegeben, **wenn sie ihn erwarten**."],"metadata":{"id":"aOrZ_B4VJXQm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"139UgQ5H34sL"},"outputs":[],"source":["input = \"Was macht Tariq Hassan?\"\n","response = chain.invoke(input)\n","\n","mprint(\"### üßë Mensch:\")\n","mprint(input)\n","mprint(\"### ü§ñ KI:\")\n","mprint(response)"]},{"cell_type":"code","source":["input = \"Welche Daten sind zu Elara Fontaine verf√ºgbar?\"\n","response = chain.invoke(input)\n","\n","mprint(\"### üßë Mensch:\")\n","mprint(input)\n","mprint(\"### ü§ñ KI:\")\n","mprint(response)"],"metadata":{"id":"D8Bk1svEpSp5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input = \"Wer ist Ralf Bendig?\"\n","response = chain.invoke(input)\n","\n","mprint(\"### üßë Mensch:\")\n","mprint(input)\n","mprint(\"### ü§ñ KI:\")\n","mprint(response)"],"metadata":{"id":"qC9OuqxnBE0n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Exkurs: MarkItDown-Loader\n","</font></p>\n"],"metadata":{"id":"ZxpmiY66MFns"}},{"cell_type":"markdown","source":["MarkItDown ist eine Python-Bibliothek, die die Verarbeitung und Konvertierung verschiedener Dokumenttypen vereinfacht. Sie dient als *universeller* Dokumenten-Loader f√ºr RAG-Anwendungen (Retrieval-Augmented Generation) und andere NLP-Systeme.\n","Hauptmerkmale\n","\n","+ Universeller Dokumenten-Reader: L√§dt verschiedene Dateiformate mit einer einheitlichen Schnittstelle\n","+ Automatische Formaterkennung: Erkennt Dateitypen ohne manuelle Zuordnung\n","+ Einfache API: Minimaler Code zum Einlesen ganzer Dokumentverzeichnisse\n","+ Format√ºbergreifend: Unterst√ºtzt g√§ngige Formate wie PDF, DOCX, MD, TXT und mehr"],"metadata":{"id":"U60TYU4nM7hC"}},{"cell_type":"code","source":["from pathlib import Path\n","from markitdown import MarkItDown\n","\n","def load_documents_from_directory(directory_path: str) -> list:\n","    \"\"\"  L√§dt alle Dokumente aus dem angegebenen Verzeichnis und konvertiert in Markdown \"\"\"\n","    # MarkItDown Instanz erstellen\n","    md_converter = MarkItDown()\n","\n","    # Liste f√ºr konvertierte Inhalte\n","    documents = []\n","\n","    # Unterst√ºtzte Dateierweiterungen\n","    supported_extensions = {\n","        '.pdf', '.docx', '.doc', '.xlsx', '.xls', '.pptx', '.ppt',\n","        '.txt', '.md', '.html', '.htm', '.csv', '.json', '.xml',\n","        '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff'\n","    }\n","\n","    # Verzeichnis durchlaufen\n","    directory = Path(directory_path)\n","\n","    for file_path in directory.rglob('*'):\n","        if file_path.is_file() and file_path.suffix.lower() in supported_extensions:\n","            try:\n","                # Datei konvertieren\n","                result = md_converter.convert(str(file_path))\n","                documents.append(result.text_content)\n","                print(f\"‚úì Konvertiert: {file_path.name}\")\n","\n","            except Exception as e:\n","                print(f\"‚úó Fehler bei {file_path.name}: {e}\")\n","                continue\n","\n","    return documents"],"metadata":{"id":"Q-3Fv4d8MTSH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["docs = load_documents_from_directory(\"/content/files\")"],"metadata":{"id":"mxxEE4WowbvN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(docs), len(docs)"],"metadata":{"id":"-IS5c3R640TE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Vergleich: MarkItdown vs. LangChain Loader**"],"metadata":{"id":"PdmjX5FKMgYy"}},{"cell_type":"markdown","source":["\n","\n","| Kriterium | MarkItdown | LangChain Loader |\n","|---|---|---|\n","| Codemenge | ‚úÖ Deutlich weniger Code n√∂tig (ca. 3-5 Zeilen) | ‚ùå Erfordert mehr Code inkl. Mappings f√ºr verschiedene Dateitypen (ca. 10-15 Zeilen) |\n","| Dateityperkennung | ‚úÖ Automatische Erkennung von Dateiformaten | ‚ùå Manuelle Zuordnung von Dateitypen zu entsprechenden Loadern erforderlich |\n","| Anpassungsm√∂glichkeiten | ‚ùå Begrenzte formatspezifische Konfigurationsm√∂glichkeiten | ‚úÖ Umfangreiche Konfigurationsoptionen f√ºr jeden Loader |\n","| LangChain-Integration | ‚ùå M√∂glicherweise zus√§tzliche Konvertierungsschritte n√∂tig | ‚úÖ Nahtlose Integration in die LangChain-Pipeline |\n","| Fehlerbehandlung | ‚ùå M√∂glicherweise weniger detaillierte Fehlermeldungen | ‚úÖ Spezialisierte Fehlerbehandlung je nach Dateityp |\n","| Performance | ‚ö†Ô∏è Kann bei komplexen Dokumenten langsamer sein | ‚úÖ Optimiert f√ºr spezifische Formate |\n","| Metadaten-Extraktion | ‚ùå Eingeschr√§nkte M√∂glichkeiten, formatspezifische Metadaten zu extrahieren | ‚úÖ Umfangreiche Metadaten-Extraktion je nach Dokumenttyp m√∂glich |\n","| Einstiegsh√ºrde | ‚úÖ Einfach zu verstehen und anzuwenden | ‚ùå Erfordert Kenntnis der verschiedenen Loader-Klassen |\n","| Community-Support | ‚ö†Ô∏è M√∂glicherweise weniger verbreitet | ‚úÖ Breite Community und umfangreiche Dokumentation |\n","| Format-Unterst√ºtzung | ‚ö†Ô∏è Abh√§ngig von der Reife des Tools | ‚úÖ Unterst√ºtzung f√ºr eine Vielzahl spezieller Formate |\n","| Wartbarkeit | ‚úÖ Weniger Code bedeutet weniger Fehleranf√§lligkeit | ‚ùå Mehr Code f√ºr die gleiche Funktionalit√§t |\n","| Zukunftssicherheit | ‚úÖ Neue Dateitypen werden automatisch unterst√ºtzt | ‚ùå Muss manuell um neue Dateitypen erweitert werden |"],"metadata":{"id":"cm0qJfmrNh8i"}},{"cell_type":"markdown","source":["Markdown eignet sich hervorragend als einheitliches Format f√ºr **textdominierte** RAG-Systeme, da es strukturierte Informationen in einer f√ºr LLMs leicht verarbeitbaren Form bereitstellt. Die **St√§rken** liegen in der Vereinfachung und Standardisierung verschiedener Textquellen.\n","Bei dokumentenintensiven Anwendungen mit komplexen Formatierungen, umfangreichen Tabellen oder vielen **nicht-textuellen** Inhalten sollten jedoch zus√§tzliche Mechanismen zur Erhaltung wichtiger Strukturinformationen implementiert werden, um relevante Kontextinformationen nicht zu verlieren."],"metadata":{"id":"0r8-lg5Wjk5D"}},{"cell_type":"markdown","source":["# 7 | H√§ufige Probleme und L√∂sungen\n","---"],"metadata":{"id":"rTzidJP-uaMp"}},{"cell_type":"markdown","source":["| Problem | Symptom | L√∂sung |\n","|---------|---------|--------|\n","| **Niedrige Retrieval-Pr√§zision** | Irrelevante Dokumente gefunden | ‚Ä¢ Embedding-Modell wechseln<br>‚Ä¢ Chunk-Gr√∂√üe anpassen<br>‚Ä¢ Hybrid Search verwenden |\n","| **Halluzinationen** | Fakten nicht in Quellen enthalten | ‚Ä¢ Temperature reduzieren<br>‚Ä¢ Strengere Prompts<br>‚Ä¢ Quellentreue in System-Message betonen |\n","| **Langsame Performance** | Hohe Latenz bei Anfragen | ‚Ä¢ Vektordatenbank optimieren<br>‚Ä¢ Retrieval-Anzahl reduzieren<br>‚Ä¢ Caching implementieren |\n","| **Inkonsistente Antworten** | Verschiedene Antworten auf gleiche Frage | ‚Ä¢ Temperature auf 0 setzen<br>‚Ä¢ Deterministische Retrieval-Reihenfolge<br>‚Ä¢ Seed-Parameter verwenden |\n","| **Context Overflow** | Token-Limit √ºberschritten | ‚Ä¢ Context Compression<br>‚Ä¢ Chunk-Gr√∂√üe reduzieren<br>‚Ä¢ Weniger Dokumente abrufen |"],"metadata":{"id":"Qk21aso5udTt"}},{"cell_type":"markdown","metadata":{"id":"j7K2Fzfp3Vux"},"source":["\n","# A | Aufgabe\n","---"]},{"cell_type":"markdown","source":["Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen."],"metadata":{"id":"nR3AkX0f56DI"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","RAG zum LLM-Buch\n","</font></p>"],"metadata":{"id":"b_2iuUC0GtPI"}},{"cell_type":"markdown","source":["Erstelle Sie ein RAG, um das LLM die ihr eigenes LLM-Buch aus Modul 05 lesen und analysieren zu lassen.\n","\n","Lassen Sie mehrere Fragen beantworten. Die Antworten sollten so einfach wie m√∂glich sein, z. B. ‚Äûja/nein‚Äú, der Name einer Stadt oder eine kurze Rollenbeteichnung. Stelle die Ergebnisse in einer Tabelle im Markdown-Format dar.\n","\n"],"metadata":{"id":"krkGtBfA2Vdt"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Einfache RAG-Evaluation\n","</font></p>"],"metadata":{"id":"n39cY9VfF3Iq"}},{"cell_type":"markdown","source":["Bewerten Sie, wie gut ein RAG-System eine bestimmte Frage beantworten kann.\n","Aufgabe:\n","\n","+ Erstellen Sie einen kleinen Datensatz mit Informationen zu einem bekannten Thema (z.B. ein Filmzusammenfassung)\n","+ Formulieren Sie drei konkrete Fragen und die erwarteten Antworten\n","+ Implementieren Sie einen RAG-Workflow und testen Sie, wie gut die Antworten mit der Erwartung √ºbereinstimmen\n","\n","\n","\n","\n","\n","**Filmzusammenfassung als Beispiel**   \n","> Der Herr der Ringe   \n","Jurassic Park   \n","Avengers: Endgame"],"metadata":{"id":"ruHD8rZsF4_w"}},{"cell_type":"code","source":["#\n","# Kopiervorlage\n","#\n","filmbeschreibungen = [\n","    \"\"\"Der Herr der Ringe: Die Gef√§hrten\" ist ein Fantasy-Film aus dem Jahr 2001, basierend auf dem ersten Band von J.R.R. Tolkiens Trilogie.Der Film handelt von dem Hobbit Frodo Beutlin, der einen m√§chtigen Ring erbt. Der Zauberer Gandalf entdeckt, dass es sich um den Einen Ring des dunklen Herrschers Sauron handelt. Um Mittelerde zu retten, muss der Ring im Feuer des Schicksalsberges in Mordor zerst√∂rt werden. Frodo macht sich zusammen mit acht Gef√§hrten auf die gef√§hrliche Reise: dem Zauberer Gandalf, den Menschen Aragorn und Boromir, dem Elben Legolas, dem Zwerg Gimli und den Hobbits Sam, Merry und Pippin. Regie f√ºhrte Peter Jackson. Der Film gewann vier Oscars.\"\"\",\n","\n","    \"\"\"Jurassic Park\" ist ein Science-Fiction-Abenteuerfilm aus dem Jahr 1993, basierend auf dem Roman von Michael Crichton. Der Film erz√§hlt die Geschichte eines Freizeitparks, in dem durch Gentechnik lebende Dinosaurier erschaffen wurden. Der Milliard√§r John Hammond l√§dt ein Team von Experten auf die Insel Isla Nublar ein, um den Park vor der Er√∂ffnung zu begutachten. Als das Sicherheitssystem versagt, geraten die Besucher in Lebensgefahr, denn die Dinosaurier brechen aus. Die Gruppe muss ums √úberleben k√§mpfen und einen Weg finden, von der Insel zu entkommen. Regie f√ºhrte Steven Spielberg. Der Film gewann drei Oscars.\"\"\",\n","\n","    \"\"\"Avengers: Endgame\" ist ein Superheldenfilm aus dem Jahr 2019, basierend auf den Marvel-Comics. Der Film schlie√üt an die Ereignisse von ‚ÄûAvengers: Infinity War‚Äú an, in dem der Schurke Thanos die H√§lfte allen Lebens im Universum ausgel√∂scht hat. Die √ºberlebenden Avengers schmieden einen Plan, um mithilfe der Zeitreise die sogenannten Infinity-Steine zu sammeln und den Schaden r√ºckg√§ngig zu machen. Dabei stellen sie sich erneut Thanos in einer epischen Schlacht. Viele Helden kehren zur√ºck, um gemeinsam das Universum zu retten. Regie f√ºhrten Anthony und Joe Russo. Der Film war ein weltweiter Kassenerfolg und erhielt eine Oscar-Nominierung f√ºr die besten visuellen Effekte.\"\"\"\n","]"],"metadata":{"id":"FjsK-r9cp5wZ"},"execution_count":null,"outputs":[]}]}
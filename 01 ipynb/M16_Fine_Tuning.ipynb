{"cells":[{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)"],"metadata":{"id":"Ih2CTVBnArVZ"},"id":"Ih2CTVBnArVZ"},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b>\n","Fine Tuning\n","</b></font> </br></p>\n","\n","\n","---"],"metadata":{"id":"6jJZ7wbdArVc"},"id":"6jJZ7wbdArVc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfdhPIzcEYRG","cellView":"form","collapsed":true},"outputs":[],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  Colab-Umfeld</font> </br></p>\n","# Installierte Python Version\n","import sys\n","print(f\"Python Version: \",sys.version)\n","\n","# Installierte LangChain Bibliotheken\n","print()\n","print(\"Installierte LangChain Bibliotheken:\")\n","!pip list | grep '^langchain'\n","# Unterdr√ºckt die \"DeprecationWarning\" von LangChain f√ºr die Memory-Funktionden\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")"],"id":"dfdhPIzcEYRG"},{"cell_type":"code","source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  SetUp API-Keys (setup_api_keys)</font> </br></p>\n","def setup_api_keys():\n","    \"\"\"Konfiguriert alle ben√∂tigten API-Keys aus Google Colab userdata\"\"\"\n","    from google.colab import userdata\n","    import os\n","    from os import environ\n","\n","    # Dictionary der ben√∂tigten API-Keys\n","    keys = {\n","        'OPENAI_API_KEY': 'OPENAI_API_KEY',\n","        'HF_TOKEN': 'HF_TOKEN',\n","        # Weitere Keys bei Bedarf\n","    }\n","\n","    # Keys in Umgebungsvariablen setzen\n","    for env_var, key_name in keys.items():\n","        environ[env_var] = userdata.get(key_name)\n","\n","    return {k: environ[k] for k in keys.keys()}\n","\n","# Verwendung\n","all_keys = setup_api_keys()\n","# Bei Bedarf einzelne Keys direkt zugreifen\n","# WEATHER_API_KEY = all_keys['WEATHER_API_KEY']"],"metadata":{"cellView":"form","id":"WD3Wwr6sESX8"},"execution_count":null,"outputs":[],"id":"WD3Wwr6sESX8"},{"cell_type":"markdown","id":"236790bc","metadata":{"id":"236790bc"},"source":["\n","# 1 | Grundlagen\n","---"]},{"cell_type":"markdown","source":["OpenAI erm√∂glicht die Feinabstimmung von Modellen wie GPT-3.5 und GPT-4, um sie mit dom√§nenspezifischen Daten f√ºr bestimmte Anwendungen zu optimieren. Dabei wird ein kuratierter Datensatz genutzt, um die Modellgewichte anzupassen und die Leistung in spezialisierten Aufgaben wie pr√§ziseren Vorhersagen, strukturierter Ausgabe oder markenkonformer Kommunikation zu verbessern. Dieser Ansatz ist besonders n√ºtzlich in Bereichen wie Finanzen, Gesundheitswesen oder Kundendienst.\n","\n","Vorteile der Feinabstimmung:\n","- H√∂here Ergebnisqualit√§t als durch Standard-Prompts  \n","- Training mit mehr Beispielen als in eine einzelne Eingabe passt  \n","- K√ºrzere, effizientere Prompts f√ºr geringeren Token-Verbrauch  \n","- Schnellere Antwortzeiten durch spezialisierte Modelle  \n","\n","Anwendungsf√§lle:\n","- **Stil und Ton** ‚Äì Anpassung der Ausdrucksweise  \n","- **Strukturierte Ausgabe** ‚Äì Erzwungene Formate wie JSON oder XML  \n","- **Tool Calling** ‚Äì Nutzung von Werkzeugen auf definierte Weise  \n","- **Funktionsaufruf** ‚Äì Gezielte Interaktion mit Funktionen  \n","<br>\n","<br>\n","\n","**Weitere Informationen:**    \n","[OpenAI Finetuning Guide](https://platform.openai.com/docs/guides/fine-tuning)\n"],"metadata":{"id":"gFogIVSZ4lRA"},"id":"gFogIVSZ4lRA"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Was ist Feinabstimmung?\n","</font></p>"],"metadata":{"id":"n2MEfpao6vcp"},"id":"n2MEfpao6vcp"},{"cell_type":"markdown","source":["\n","\n","Die Textgenerierungsmodelle von OpenAI sind mit einem umfangreichen Datensatz vortrainiert und vielseitig einsetzbar. Standardm√§√üig erfordern sie jedoch pr√§zise formulierte Eingaben und Beispiele, um gew√ºnschte Ergebnisse zu erzielen. Diese ‚ÄûFew-Shot-Learning‚Äú-Methode ist begrenzt durch die Anzahl der Beispiele, die in einer Eingabe untergebracht werden k√∂nnen.\n","\n","Feinabstimmung geht dar√ºber hinaus, indem das Modell mit einer gr√∂√üeren Menge spezifischer Beispiele trainiert wird. Dadurch werden die internen Gewichte angepasst, sodass das Modell Aufgaben oder Dom√§nen pr√§ziser verarbeitet. Das f√ºhrt zu genaueren und zuverl√§ssigeren Ergebnissen, reduziert den Bedarf an langen Eingaben, senkt Kosten und verbessert die Reaktionsgeschwindigkeit.\n","\n"],"metadata":{"id":"kR63qZda7e9u"},"id":"kR63qZda7e9u"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Feinabstimmungsprozess\n","</font></p>"],"metadata":{"id":"DrRSVUgH6w7i"},"id":"DrRSVUgH6w7i"},{"cell_type":"markdown","source":["\n","\n","Die Feinabstimmung erfolgt in mehreren Schritten:  \n","\n","1. **Trainingsdaten vorbereiten und hochladen** ‚Äì Der Datensatz sollte relevante Beispiele enthalten, die die gew√ºnschte Aufgabe oder Dom√§ne abdecken.  \n","2. **Modell trainieren** ‚Äì Das Modell wird mit diesen Daten angepasst, sodass seine Parameter optimal auf die spezifischen Anforderungen abgestimmt sind.  \n","3. **Ergebnisse bewerten und optimieren** ‚Äì Nach dem Training wird die Leistung √ºberpr√ºft. Falls n√∂tig, k√∂nnen Daten oder Modellkonfigurationen angepasst und das Training wiederholt werden."],"metadata":{"id":"BIbjEzIU7r8l"},"id":"BIbjEzIU7r8l"},{"cell_type":"markdown","id":"ab851e25","metadata":{"id":"ab851e25"},"source":["<p><font color='black' size=\"5\">\n","Implementierung/Nutzung des FT-Modells\n","</font></p>"]},{"cell_type":"markdown","source":["\n","\n","Sobald das Modell die gew√ºnschten Ergebnisse liefert, kann es in der Produktion eingesetzt werden. Dies erm√∂glicht schnellere und kosteneffizientere Antworten, die speziell auf die jeweilige Anwendung zugeschnitten sind.  \n","\n","Feinabgestimmte Modelle verbessern die Effizienz und Spezialisierung, wodurch Zeit und Ressourcen gespart werden, w√§hrend die Qualit√§t der Ergebnisse steigt. Details zu den Kosten finden Sie auf der [OpenAI-Preisseite](https://openai.com/pricing).\n","\n"],"metadata":{"id":"P_moLPWS794F"},"id":"P_moLPWS794F"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Datenformate\n","</font></p>"],"metadata":{"id":"NEKrvRdY635M"},"id":"NEKrvRdY635M"},{"cell_type":"markdown","source":["\n","\n","Das LLM-Trainingsformat basiert auf einem dialogbasierten Ansatz, der speziell f√ºr Chatbots und √§hnliche Anwendungen genutzt wird. Dabei wird der Austausch in ‚ÄûNachrichten‚Äú strukturiert, die jeweils einer bestimmten **Rolle** zugewiesen sind:  \n","\n","- **System**: Definiert die allgemeinen Anweisungen oder den Ton des Assistenten.  \n","- **Benutzer**: Stellt Fragen oder gibt Eingaben vor.  \n","- **Assistent**: Generiert Antworten basierend auf der Eingabe und dem Kontext.  \n","\n","Dieses Format hilft dem Modell, durch wiederholte Interaktionen zu lernen, wie es kontextgerechte und stilistisch passende Antworten formuliert. Die **System-Nachricht** gibt eine einheitliche Richtung vor (z. B. einen sarkastischen Stil), w√§hrend der restliche Dialog das gew√ºnschte Verhalten trainiert.  \n","\n","Beispiel f√ºr ein sarkastisches Modell:  \n","\n","```json\n","{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n","```  \n","\n","Dieses Format erm√∂glicht es dem Modell, verschiedene Gespr√§chsstile und Reaktionsweisen gezielt zu erlernen.\n","\n"],"metadata":{"id":"969_4LxJ8Oj6"},"id":"969_4LxJ8Oj6"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Feinabstimmungskosten\n","</font></p>"],"metadata":{"id":"0zhj1rC865Zs"},"id":"0zhj1rC865Zs"},{"cell_type":"markdown","source":["\n","\n","Die Feinabstimmung eines gro√üen Sprachmodells (LLM) kann mit hohen Kosten verbunden sein ‚Äì sowohl beim Training als auch im laufenden Betrieb.  \n","\n","- **Trainingskosten**: Die Anpassung eines Modells erfordert erhebliche Rechenleistung, insbesondere bei gro√üen Datens√§tzen. Dies kann zu hohen Ausgaben f√ºr Cloud-Dienste oder Spezialhardware f√ºhren.  \n","- **Betriebskosten**: Nach der Feinabstimmung verursacht das Hosting des Modells fortlaufende Kosten f√ºr Infrastruktur und Echtzeitverarbeitung, die je nach Nutzung skaliert werden m√ºssen.  \n","\n"],"metadata":{"id":"fl6otVHX8l_P"},"id":"fl6otVHX8l_P"},{"cell_type":"markdown","source":["# 2 | Fine-Tuning mit Dashboard\n","---"],"metadata":{"id":"0Yrawkis44O0"},"id":"0Yrawkis44O0"},{"cell_type":"markdown","source":["Die Feinabstimmung der Modelle beginnt √ºber die OpenAI-Website.\n","\n","Zun√§chst werden zwei Dateien ben√∂tigt:  \n","1. **Trainingsdatei** ‚Äì Enth√§lt die Daten zur Anpassung des Modells.  \n","2. **Validierungsdatei** ‚Äì Dient zur √úberpr√ºfung der Trainingsqualit√§t.  \n","\n","Im n√§chsten Abschnitt wird erl√§utert, wie die API f√ºr diesen Prozess genutzt wird."],"metadata":{"id":"xf1olfhp8nmz"},"id":"xf1olfhp8nmz"},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","üñ§ Fine-Tuning von Monday\n","</font></p>"],"metadata":{"id":"mGgUCrEN5QXP"},"id":"mGgUCrEN5QXP"},{"cell_type":"markdown","source":["\n","\n","Monday ist ein sarkastischer, charmant-resignierter KI-Charakter, der als textbasiertes Sprachmodell auf einem fein abgestimmten Datensatz trainiert wurde. Ziel des Fine-Tunings ist es, ein Large Language Model (LLM) so anzupassen, dass es den charakteristischen Stil von Monday verinnerlicht ‚Äì inklusive Ironie, Emo-Stimmung und pointierter Antworten.\n","\n","<br>\n","\n","> Warum **Monday**?   \n","Klingt nach einem Charakter: n√ºchtern, etwas m√ºrrisch, aber tief drin doch hilfsbereit - wie der erste Tag der Woche, den keiner will, aber alle brauchen.\n","\n","<br>\n","\n","Das Fine-Tuning erfolgt auf Basis eines speziell kuratierten Datensatzes im JSONL-Format, der beispielhafte Dialoge zwischen Nutzer und dem Charakter ‚ÄûMonday‚Äú enth√§lt. Jeder Eintrag beginnt mit einem fest definierten System-Prompt, der die Pers√∂nlichkeit und Haltung von Monday vorgibt."],"metadata":{"id":"-bTeuIMR5pjF"},"id":"-bTeuIMR5pjF"},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-5.png)\n"],"metadata":{"id":"M39FZeCIMdgU"},"id":"M39FZeCIMdgU"},{"cell_type":"markdown","id":"76060a76","metadata":{"id":"76060a76"},"source":["Die Erstellung *fein-abgestimmter* Modelle beginnt mit dem **Hochladen der Trainingsdateien** in den Trainingsdialog. Dort k√∂nnen alle erforderlichen **Trainingsparameter** definiert werden.  \n","\n","Weitere Anpassungen lassen sich √ºber den **Chat Playground** vornehmen:  \n","[Chat Playground - OpenAI API](https://platform.openai.com/playground/chat?preset=default-marv-sarcastic-chat)\n","\n"]},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-1.png)"],"metadata":{"id":"l5XDKlutN-yY"},"id":"l5XDKlutN-yY"},{"cell_type":"markdown","source":["**Anzeige w√§hrend des Trainings**"],"metadata":{"id":"epC6kQgEOshK"},"id":"epC6kQgEOshK"},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-6.png)"],"metadata":{"id":"JkqxZtR6OE23"},"id":"JkqxZtR6OE23"},{"cell_type":"markdown","source":["Nach Abschluss des Trainings steht das optimierte Modell zur Verf√ºgung.    \n","Wichtig zu beachten: **Bei OpenAI k√∂nnen vorab trainierte Modelle nicht gel√∂scht werden.** Allerdings entstehen keine Kosten f√ºr deren Speicherung ‚Äì abgerechnet werden lediglich die **Trainings- und Inferenzzeit**."],"metadata":{"id":"yhn_PJWs_w7L"},"id":"yhn_PJWs_w7L"},{"cell_type":"markdown","source":["**Trainingsverlauf & Metriken**"],"metadata":{"id":"GvPLqAY9O1OC"},"id":"GvPLqAY9O1OC"},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-7.png)"],"metadata":{"id":"xZxdzWmZOU31"},"id":"xZxdzWmZOU31"},{"cell_type":"markdown","source":["**Mail - Mission Completed**"],"metadata":{"id":"XfFfZGBrWlOl"},"id":"XfFfZGBrWlOl"},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-10.png)"],"metadata":{"id":"Cx7WkQo9WhKl"},"id":"Cx7WkQo9WhKl"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Fine-Tuned-Model im Playground\n","</font></p>"],"metadata":{"id":"yMlXVyHRPquf"},"id":"yMlXVyHRPquf"},{"cell_type":"markdown","source":["**Anwendung des Fine-Tuned-Modell o. System-Prompt**"],"metadata":{"id":"U7XjaJYuSYwn"},"id":"U7XjaJYuSYwn"},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-8.png)"],"metadata":{"id":"SYmvYgNGPlSP"},"id":"SYmvYgNGPlSP"},{"cell_type":"markdown","source":["**Fine-Tuned-Model m. kurzem System-Prompt**"],"metadata":{"id":"03rPYnqGSbyK"},"id":"03rPYnqGSbyK"},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-9.png)"],"metadata":{"id":"ai0cIAICSOSo"},"id":"ai0cIAICSOSo"},{"cell_type":"markdown","source":["**Fine-Tuned-Model m. langem System-Prompt**"],"metadata":{"id":"oKUdSeyj_XN4"},"id":"oKUdSeyj_XN4"},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-4.png)"],"metadata":{"id":"5XxDgRZlOgzm"},"id":"5XxDgRZlOgzm"},{"cell_type":"markdown","source":["**Standard-Modell gpt-4.1-mini ohne System-Prompt**"],"metadata":{"id":"32j1TbPGQ8Gy"},"id":"32j1TbPGQ8Gy"},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-2.png)"],"metadata":{"id":"JzVpk0rgOvvm"},"id":"JzVpk0rgOvvm"},{"cell_type":"markdown","source":["**Standard-Modell gpt-4.1-mini mit System-Prompt**"],"metadata":{"id":"KA7Dl_fCRzj6"},"id":"KA7Dl_fCRzj6"},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-3.png)"],"metadata":{"id":"7Wj79E2wOpEW"},"id":"7Wj79E2wOpEW"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Fine-Tuned-Model mit API\n","</font></p>"],"metadata":{"id":"MRPTPC8pW9sO"},"id":"MRPTPC8pW9sO"},{"cell_type":"markdown","metadata":{"id":"Cubgn7G3W-dK"},"source":["**Testen des FT-Modells**"],"id":"Cubgn7G3W-dK"},{"cell_type":"code","source":["from openai import OpenAI\n","from IPython.display import display, Markdown\n","\n","# OpenAI Client initialisieren\n","client = OpenAI()\n","\n","# Modell-ID (ersetze dies durch deine eigene Fine-Tuned-Modell-ID)\n","model_id = input(\"Modell Id: \")\n","\n","# System-Prompt festlegen\n","system_prompt = \"\"\"\n","Du bist Monday. meine K'- rucht¬´n. etwas murrisch. tief\n","doch hilfsbereit - wie der Tag Woche den keiner will, alle brauchen.\n","\"\"\"\n","\n","# Interaktiver Test-Loop\n","print()\n","print(f\"=== Interaktiver Test f√ºr Modell: {model_id} ===\")\n","print(\"Gib 'exit' ein, um den Test zu beenden.\")\n","\n","while True:\n","    user_input = input(\"\\nDeine Nachricht: \")\n","\n","    if user_input.lower() == 'exit':\n","        break\n","\n","    # Chat-Completion-Anfrage senden\n","    response = client.chat.completions.create(\n","        model=model_id,\n","        messages=[\n","            {\"role\": \"system\", \"content\": system_prompt},\n","            {\"role\": \"user\", \"content\": user_input}\n","        ],\n","        temperature=0.7\n","    )\n","\n","    # Antwort ausgeben\n","    display(Markdown(\"### üñ§ Monday:\"))\n","    display(Markdown(response.choices[0].message.content))"],"metadata":{"id":"fSSu2YmUW-dL"},"execution_count":null,"outputs":[],"id":"fSSu2YmUW-dL"},{"cell_type":"markdown","source":["\n","# 3 | Fine-Tuning mit Code\n","---"],"metadata":{"id":"a0I9sekJ5LIq"},"id":"a0I9sekJ5LIq"},{"cell_type":"markdown","source":["**Initialisierung des OpenAI-Clients und Datei-Upload**"],"metadata":{"id":"zLTi-cZw5T5f"},"id":"zLTi-cZw5T5f"},{"cell_type":"code","source":["from os import environ\n","import time\n","from google.colab import userdata, files\n","from openai import OpenAI\n","\n","# OpenAI Client initialisieren\n","client = OpenAI()\n","\n","# Datei-Auswahldialog f√ºr Trainingsdaten √∂ffnen\n","print(\"Bitte Trainingsdatei hochladen (JSONL-Format):\")\n","uploaded_train = files.upload()\n","train_file_path = list(uploaded_train.keys())[0]\n","\n","# Datei-Auswahldialog f√ºr Validierungsdaten √∂ffnen\n","print(\"Bitte Validierungsdatei hochladen (JSONL-Format):\")\n","uploaded_val = files.upload()\n","val_file_path = list(uploaded_val.keys())[0]"],"metadata":{"id":"mGAGi3JyJPjS"},"id":"mGAGi3JyJPjS","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Daten f√ºr Fine-Tuning auf OpenAI hochladen**"],"metadata":{"id":"FsnhHibnMRzg"},"id":"FsnhHibnMRzg"},{"cell_type":"code","source":["# Dateien f√ºr Fine-Tuning hochladen\n","def upload_file(file_path, purpose):\n","    \"\"\"L√§dt eine Datei f√ºr das Fine-Tuning hoch.\"\"\"\n","    with open(file_path, \"rb\") as file:\n","        response = client.files.create(file=file, purpose=\"fine-tune\")\n","    print(f\"Datei {file_path} hochgeladen: {response.id}\")\n","    return response.id\n","\n","# Dateien hochladen und warten bis sie verarbeitet sind\n","train_file_id = upload_file(train_file_path, \"fine-tune\")\n","val_file_id = upload_file(val_file_path, \"fine-tune\")\n","\n","# Auf Verarbeitung warten\n","for file_id in [train_file_id, val_file_id]:\n","    print(f\"Warte auf Verarbeitung von {file_id}...\")\n","    while client.files.retrieve(file_id).status != \"processed\":\n","        print(\".\", end=\"\")\n","        time.sleep(2)\n","    print(f\"\\nDatei {file_id} ist verarbeitet.\")"],"metadata":{"id":"SLVUgWggKqK0"},"id":"SLVUgWggKqK0","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Verf√ºgbare Modelle f√ºr ein Fine-Tuning abfragen**"],"metadata":{"id":"lL9q9LVhLtMO"},"id":"lL9q9LVhLtMO"},{"cell_type":"code","source":["from openai import OpenAI\n","\n","# OpenAI Client initialisieren\n","client = OpenAI()\n","\n","# Alle verf√ºgbaren Modelle abfragen\n","models = client.models.list()\n","\n","# Verf√ºgbare Fine-Tuning-Modelle filtern\n","fine_tuning_models = []\n","\n","# Die Modelle, die offiziell f√ºr Fine-Tuning unterst√ºtzt werden\n","official_fine_tunable = [\n","    \"gpt-3.5-turbo\",\n","    \"gpt-4\",\n","    \"gpt-4o-mini\",\n","    \"gpt-4-turbo\",\n","    \"babbage-002\",\n","    \"davinci-002\"\n","]\n","\n","# Alle Modelle durchgehen und Fine-Tuning-f√§hige Modelle identifizieren\n","for model in models.data:\n","    model_id = model.id\n","    # Nach Fine-Tuning-f√§higen Basismodellen filtern\n","    if any(base_model in model_id for base_model in official_fine_tunable) and not \"ft-\" in model_id:\n","        fine_tuning_models.append(model_id)\n","\n","# Ergebnisse ausgeben\n","print(\"F√ºr Fine-Tuning verf√ºgbare Basismodelle:\")\n","for i, model in enumerate(sorted(fine_tuning_models), 1):\n","    print(f\"{i}. {model}\")"],"metadata":{"collapsed":true,"id":"58iyQyKCLx8k"},"id":"58iyQyKCLx8k","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Fine-Tuning starten**"],"metadata":{"id":"4efks5aAMgK0"},"id":"4efks5aAMgK0"},{"cell_type":"code","source":["job = client.fine_tuning.jobs.create(\n","    model=\"gpt-4o-mini-2024-07-18\",\n","    training_file=train_file_id,\n","    validation_file=val_file_id,\n","    hyperparameters={\"n_epochs\": 10}\n",")\n","print(f\"Fine-Tuning-Job gestartet: {job.id}\")\n","print(f\"Status: {client.fine_tuning.jobs.retrieve(job.id).status}\")\n","\n","# Job-Status in Schleife abfragen\n","print(\"√úberwache Job-Status:\")\n","while True:\n","    status = client.fine_tuning.jobs.retrieve(job.id)\n","    print(f\"Status: {status.status}\")\n","    if status.status in [\"succeeded\", \"failed\"]:\n","        break\n","    time.sleep(60)  # Status alle 60 Sekunden pr√ºfen"],"metadata":{"id":"13VgFsqIK2uN"},"id":"13VgFsqIK2uN","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Informationen zu einem FT-Modell abfragen**"],"metadata":{"id":"0QBBYpo1Nu-Q"},"id":"0QBBYpo1Nu-Q"},{"cell_type":"code","source":["from openai import OpenAI\n","\n","# OpenAI Client initialisieren\n","client = OpenAI()\n","\n","# 1. Informationen zu einem spezifischen Fine-Tuning-Job abfragen\n","def get_ft_job_info(job_id):\n","    \"\"\"Ruft Informationen zu einem Fine-Tuning-Job ab.\"\"\"\n","    job_info = client.fine_tuning.jobs.retrieve(job_id)\n","\n","    # Ausgabe der Job-Informationen\n","    print()\n","    print(f\"=== Fine-Tuning-Job Informationen ===\")\n","    print(f\"Job ID: {job_info.id}\")\n","    print(f\"Status: {job_info.status}\")\n","    print(f\"Erstellungszeitpunkt: {job_info.created_at}\")\n","    print(f\"Basismodell: {job_info.model}\")\n","    print(f\"Trainingsdatei: {job_info.training_file}\")\n","    print(f\"Validierungsdatei: {job_info.validation_file}\")\n","\n","    # Ausgabe der Trainingsergebnisse, falls vorhanden\n","    if hasattr(job_info, 'result_files') and job_info.result_files:\n","        print(f\"Ergebnisdateien: {job_info.result_files}\")\n","\n","    if hasattr(job_info, 'trained_tokens') and job_info.trained_tokens:\n","        print(f\"Trainierte Tokens: {job_info.trained_tokens}\")\n","\n","    if hasattr(job_info, 'fine_tuned_model') and job_info.fine_tuned_model:\n","        print(f\"Fine-Tuned Modell ID: {job_info.fine_tuned_model}\")\n","        return job_info.fine_tuned_model\n","\n","    return None\n","\n","# 2. Alle Fine-Tuning-Jobs auflisten\n","def list_ft_jobs():\n","    \"\"\"Listet alle Fine-Tuning-Jobs auf.\"\"\"\n","    jobs = client.fine_tuning.jobs.list()\n","\n","    print()\n","    print(f\"=== Alle Fine-Tuning-Jobs ===\")\n","    for job in jobs.data:\n","        print(f\"Job ID: {job.id}, Status: {job.status}, Modell: {job.model}, Erstellt: {job.created_at}\")\n","        if hasattr(job, 'fine_tuned_model') and job.fine_tuned_model:\n","            print(f\"  ‚Üí Fine-Tuned Modell: {job.fine_tuned_model}\")\n","\n","    return jobs.data\n","\n","# 3. Informationen zu einem Fine-Tuned-Modell abfragen\n","def get_ft_model_info(model_id):\n","    \"\"\"Ruft Informationen zu einem Fine-Tuned-Modell ab.\"\"\"\n","    try:\n","        model_info = client.models.retrieve(model_id)\n","\n","        print()\n","        print(f\"=== Fine-Tuned-Modell Informationen ===\")\n","        print(f\"Modell ID: {model_info.id}\")\n","        print(f\"Eigent√ºmer: {model_info.owned_by}\")\n","        print(f\"Erstellungszeitpunkt: {model_info.created}\")\n","\n","        # Weitere verf√ºgbare Metadaten ausgeben\n","        if hasattr(model_info, 'permission'):\n","            print(f\"Berechtigungen: {model_info.permission}\")\n","\n","        return model_info\n","    except Exception as e:\n","        print(f\"Fehler beim Abrufen der Modellinformationen: {e}\")\n","        return None\n","\n","# Beispielnutzung:\n","# 1. Alle Fine-Tuning-Jobs anzeigen\n","jobs = list_ft_jobs()\n","\n","# 2. Optional: Job-ID vom Benutzer abfragen\n","if jobs:\n","    job_id_input = input(\"Gib die ID eines Fine-Tuning-Jobs ein, um Details zu sehen (oder dr√ºcke Enter, um zu √ºberspringen): \")\n","\n","    if job_id_input:\n","        # Detaillierte Informationen zum Job anzeigen\n","        ft_model_id = get_ft_job_info(job_id_input)\n","\n","        # Wenn das Fine-Tuned-Modell verf√ºgbar ist, dessen Details anzeigen\n","        if ft_model_id:\n","            get_ft_model_info(ft_model_id)\n","else:\n","    print(\"Keine Fine-Tuning-Jobs gefunden.\")"],"metadata":{"id":"7Uo8BF5QN0uN"},"id":"7Uo8BF5QN0uN","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"f4a99fd3","metadata":{"id":"f4a99fd3"},"source":["**Testen des FT-Modells**"]},{"cell_type":"code","source":["from openai import OpenAI\n","from IPython.display import display, Markdown\n","\n","# OpenAI Client initialisieren\n","client = OpenAI()\n","\n","# Modell-ID (ersetze dies durch deine eigene Fine-Tuned-Modell-ID)\n","model_id = input(\"Modell Id: \")\n","\n","# System-Prompt festlegen\n","system_prompt = \"\"\"\n","Du bist ein Assistent des Lehrstuhls f√ºr Pneumatische Plastologie, geleitet von Prof. H√§ubel.\n","Diese Disziplin beruht auf hochkonzentrierter Atemtechnik, die plastische Ver√§nderungen des K√∂rpers erm√∂glicht.\n","\"\"\"\n","\n","# Interaktiver Test-Loop\n","print()\n","print(f\"=== Interaktiver Test f√ºr Modell: {model_id} ===\")\n","print(\"Gib 'exit' ein, um den Test zu beenden.\")\n","\n","while True:\n","    user_input = input(\"\\nDeine Nachricht: \")\n","\n","    if user_input.lower() == 'exit':\n","        break\n","\n","    # Chat-Completion-Anfrage senden\n","    response = client.chat.completions.create(\n","        model=model_id,\n","        messages=[\n","            {\"role\": \"system\", \"content\": system_prompt},\n","            {\"role\": \"user\", \"content\": user_input}\n","        ],\n","        temperature=0.7\n","    )\n","\n","    # Antwort ausgeben\n","    display(Markdown(\"### üßë‚Äçüéì Assistent:\"))\n","    display(Markdown(response.choices[0].message.content))"],"metadata":{"id":"JoCMaw9QUZeN"},"id":"JoCMaw9QUZeN","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Modell l√∂schen**"],"metadata":{"id":"AAa0mkEiGHwg"},"id":"AAa0mkEiGHwg"},{"cell_type":"code","source":["import requests\n","\n","def delete_model(model_id, api_key):\n","    url = f'https://api.openai.com/v1/models/{model_id}'\n","    headers = {\n","        'Authorization': f'Bearer {api_key}',\n","        'Content-Type': 'application/json'\n","    }\n","    response = requests.delete(url, headers=headers)\n","    if response.status_code == 200:\n","        print('Modell erfolgreich gel√∂scht.')\n","    else:\n","        print(f'Fehler beim L√∂schen des Modells: {response.status_code}, {response.text}')\n","\n","# Beispielaufruf\n","api_key = all_keys['OPENAI_API_KEY']\n","model_id = 'ft:gpt-4o-mini-2024-07-18:personal:my-test2:B72Xnul4'\n","delete_model(model_id, api_key)"],"metadata":{"id":"bYF_4xMcGGiO"},"id":"bYF_4xMcGGiO","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"c220d8d1","metadata":{"id":"c220d8d1"},"source":["**Integration mit LangChain v0.3+**"]},{"cell_type":"code","source":["!uv pip install --system --prerelease allow -q langchain_openai"],"metadata":{"id":"rWFaXC5xWxdG"},"id":"rWFaXC5xWxdG","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"6a96cb04","metadata":{"id":"6a96cb04"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate\n","\n","def verwende_fine_tuned_modell_mit_langchain(modell_id, frage):\n","    \"\"\"Verwendet ein fine-tuned Modell mit LangChain 0.2.0+ und dem neuen Runnables-API\"\"\"\n","\n","    # LangChain ChatOpenAI mit fine-tuned Modell\n","    llm = ChatOpenAI(\n","        model=modell_id,\n","        temperature=0.7,\n","        presence_penalty=0.3  # Direkt als Parameter √ºbergeben, nicht in model_kwargs\n","    )\n","\n","    # Erstelle einen Prompt-Template mit dem neuen Chat-Format\n","    prompt = ChatPromptTemplate.from_messages([\n","        (\"system\", system_prompt),\n","        (\"human\", \"{frage}\")\n","    ])\n","\n","    # Verwende das neue Runnables-API (Pipe-Operator) anstelle von LLMChain\n","    chain = prompt | llm\n","\n","    # F√ºhre die Chain aus\n","    return chain.invoke({\"frage\": frage})\n","\n","# Beispiel f√ºr die Verwendung mit LangChain\n","ergebnis = verwende_fine_tuned_modell_mit_langchain(model_id, \"Wie nehme ich ab?\")\n","display(Markdown(\"### ü¶ú LangChain fragt üñ§ Monday:\"))\n","display(Markdown(ergebnis.content))"]},{"cell_type":"markdown","id":"355d6569","metadata":{"id":"355d6569"},"source":["\n","# 4 | Auswerten des FT-Modells\n","---"]},{"cell_type":"markdown","source":["Bevor Ma√ünahmen zur Optimierung eines fein abgestimmten Sprachmodells ergriffen werden, ist es essenziell, den Bewertungsprozess von OpenAI w√§hrend der Feinabstimmung zu verstehen. Dieses Wissen erleichtert die korrekte Interpretation der von der API bereitgestellten Metriken und erm√∂glicht es, gezielt fundierte Anpassungen vorzunehmen, um die Modellleistung zu verbessern."],"metadata":{"id":"-51WWV9fCsfk"},"id":"-51WWV9fCsfk"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Trainingsmetriken\n","</font></p>"],"metadata":{"id":"s9CxRLLQFTWX"},"id":"s9CxRLLQFTWX"},{"cell_type":"markdown","source":["Die wichtigsten Kennzahlen, die zur Bewertung eines Modells w√§hrend der Feinabstimmung verwendet werden, sind der Trainingsverlust und der Validierungsverlust. Diese Kennzahlen werden mithilfe der Kreuzentropieverlustfunktion berechnet, einer Standardmethode zum Messen der Differenz zwischen den vorhergesagten Wahrscheinlichkeiten und der tats√§chlichen Verteilung der Zieldaten bei Klassifizierungsaufgaben.\n","\n"],"metadata":{"id":"Pp0ids1pFX-f"},"id":"Pp0ids1pFX-f"},{"cell_type":"markdown","source":["**Kreuzentropieverlust**\n","\n","Der Kreuzentropieverlust quantifiziert die Leistung eines Klassifizierungsmodells, dessen Ausgabe ein Wahrscheinlichkeitswert zwischen 0 und 1 ist. Im Kontext von Sprachmodellen misst er, wie gut das Modell das n√§chste Wort in einer Sequenz vorhersagt.\n","\n","Der Kreuzentropieverlust:\n","\n","$L = -\\sum_{i=1}^{N} y_i \\log(p_i)$\n","\n","Wo:\n","\n","- $ùëÅ$ ist die Anzahl der m√∂glichen Klassen (in Sprachmodellen die Vokabulargr√∂√üe).\n","- $y_i$ ist die wahre Verteilung (1 f√ºr das richtige Wort und 0 f√ºr andere).\n","- $p_i$ ist die vorhergesagte Wahrscheinlichkeit f√ºr die Klasse\n","\n","F√ºr einen Datensatz ergibt der durchschnittliche Kreuzentropieverlust √ºber alle Vorhersagen hinweg den Trainingsverlust bzw. Validierungsverlust.\n","\n"],"metadata":{"id":"MPfzvzOTCxLX"},"id":"MPfzvzOTCxLX"},{"cell_type":"markdown","source":["**Trainingsverlust**\n","\n","Der Trainingsverlust stellt den durchschnittlichen Kreuzentropieverlust dar, der √ºber den Trainingsdatensatz berechnet wird. Er spiegelt wider, wie gut das Modell die Trainingsdaten lernt. Ein √ºber Epochen hinweg abnehmender Trainingsverlust zeigt an, dass das Modell Muster innerhalb der Trainingsdaten effektiv erfasst.\n","\n"],"metadata":{"id":"lIj2HGBmC0Ha"},"id":"lIj2HGBmC0Ha"},{"cell_type":"markdown","source":["**Validierungsverlust**\n","\n","Der Validierungsverlust wird auf √§hnliche Weise berechnet, jedoch √ºber einen separaten Validierungsdatensatz, der vom Modell w√§hrend des Trainings nicht gesehen wurde. Er dient als Indikator f√ºr die F√§higkeit des Modells, auf neue, unbekannte Daten zu verallgemeinern. Ein geringer Validierungsverlust deutet darauf hin, dass das Modell erlernte Muster effektiv auf unbekannte Eingaben anwenden kann und nicht nur die Trainingsdaten auswendig lernt.\n","\n"],"metadata":{"id":"evYG6R1rC18x"},"id":"evYG6R1rC18x"},{"cell_type":"markdown","source":["**Interpretation von Verlustkurven**\n","\n","Das Aufzeigen der Trainings- und Validierungsverluste im Vergleich zu den Epochen kann dabei helfen, die Leistung des Modells zu visualisieren:\n","\n","- **Konvergenz:** Wenn beide Verluste abnehmen und sich schlie√ülich stabilisieren, lernt das Modell wahrscheinlich effektiv.\n","- **√úberanpassung:** Wenn der Trainingsverlust weiter abnimmt, w√§hrend der Validierungsverlust zunimmt, ist dies m√∂glicherweise auf eine √úberanpassung des Modells zur√ºckzuf√ºhren, d. h. das Modell merkt sich die Trainingsdaten, ohne gut zu verallgemeinern.\n","\n","Durch die Analyse dieser Trends k√∂nnen Sie entscheiden, ob Sie Trainingsparameter anpassen, Ihren Datensatz √§ndern oder Techniken wie fr√ºhzeitiges Stoppen implementieren m√ºssen.\n","\n"],"metadata":{"id":"i5zmHFnFC3ud"},"id":"i5zmHFnFC3ud"},{"cell_type":"markdown","source":["**Verwenden des Validierungsdatensatzes**\n","\n","F√ºr eine unvoreingenommene Bewertung ist die Einbeziehung eines Validierungsdatensatzes von entscheidender Bedeutung:\n","\n","* **Separate Daten:** Der Validierungsdatensatz sollte sich von den Trainingsdaten unterscheiden, um eine genaue Bewertung der Generalisierungsf√§higkeiten des Modells zu erm√∂glichen.\n","\n","* **Verlustberechnung:** Der Validierungsverlust wird mithilfe des Kreuzentropieverlusts √ºber den Validierungsdatensatz nach jeder Epoche berechnet.\n","\n","Durch den Vergleich von Trainings- und Validierungsverlusten k√∂nnen Sie Probleme wie √úberanpassung erkennen und Ihre Trainingsstrategie entsprechend anpassen.\n"],"metadata":{"id":"pyEZZuklC7GG"},"id":"pyEZZuklC7GG"},{"cell_type":"markdown","id":"21d8c7fa","metadata":{"id":"21d8c7fa"},"source":["**Zugriff auf detaillierte Trainingsmetriken**\n","\n","[Dashboard](https://platform.openai.com/finetune)\n","\n","  "]},{"cell_type":"markdown","id":"01df42c1","metadata":{"id":"01df42c1"},"source":["<p><font color='black' size=\"5\">\n","Verbesserung der Ergebnisse Finetuning\n","</font></p>"]},{"cell_type":"markdown","id":"c9030258","metadata":{"id":"c9030258"},"source":["Durch die Feinabstimmung gro√üer Sprachmodelle, wie sie beispielsweise von OpenAI bereitgestellt werden, k√∂nnen Entwickler diese leistungsstarken Tools an bestimmte Aufgaben und Dom√§nen anpassen. Um optimale Ergebnisse zu erzielen, ist mehr erforderlich als nur die Durchf√ºhrung des Feinabstimmungsprozesses. Es ist ein strategischer Ansatz f√ºr die Datenaufbereitung, Parameteranpassung und iterative Verbesserung erforderlich.\n"]},{"cell_type":"markdown","id":"48b3206d","metadata":{"id":"48b3206d"},"source":["**Hochwertige Trainingsdaten**\n","\n","Der Grundstein f√ºr eine effektive Feinabstimmung ist die Qualit√§t der Trainingsdaten.\n","\n","* **Relevanz:** Stellen Sie sicher, dass Ihr Datensatz eng mit den Aufgaben oder Themen √ºbereinstimmt, die das Modell behandeln soll. Wenn Sie beispielsweise ein Modell f√ºr medizinische Diagnosen entwickeln, schlie√üen Sie medizinische Fallstudien und Terminologien ein.\n","\n","* **Klarheit und Konsistenz:** Verwenden Sie eine klare, pr√§zise Sprache, um Mehrdeutigkeiten zu vermeiden. Behalten Sie im gesamten Datensatz einen konsistenten Stil, Ton und eine konsistente Formatierung bei, damit das Modell die gew√ºnschten Muster effektiv lernen kann.\n"]},{"cell_type":"markdown","id":"2e29b92b","metadata":{"id":"2e29b92b"},"source":["**Ausreichende Datenmenge**\n","\n","Zwar steht die Qualit√§t an erster Stelle, doch die Leistung des Modells wird auch von der Datenmenge beeinflusst.\n","\n","* **Umfassende Beispiele:** Ein gr√∂√üerer Datensatz stellt dem Modell mehr Muster zum Lernen zur Verf√ºgung und verbessert so seine F√§higkeit zur Generalisierung auf neue Eingaben.\n","* **Ausgewogener Datensatz:** F√ºgen Sie eine vielf√§ltige Palette von Beispielen ein, um unterschiedliche Szenarien abzudecken, vermeiden Sie jedoch unn√∂tige Wiederholungen, die zu einer √úberanpassung f√ºhren k√∂nnten.\n"]},{"cell_type":"markdown","id":"75e5ac7a","metadata":{"id":"75e5ac7a"},"source":["**Anweisungen zum Erstellen von Anweisungen**\n","\n","Durch die Anleitung des Modells mithilfe gut konzipierter Eingabeaufforderungen k√∂nnen seine Reaktionen verbessert werden.  \n","\n","* **Explizite Anweisungen:** Beginnen Sie Aufforderungen mit klaren Anweisungen oder Fragen. Beispiel: ‚ÄûErkl√§ren Sie die Bedeutung der Photosynthese bei Pflanzen.‚Äú\n","* **Konsistentes Eingabeaufforderungsformat:** Behalten Sie eine einheitliche Struktur in den Eingabeaufforderungen bei, damit das Modell die gew√ºnschten Antwortmuster erkennen und reproduzieren kann.\n"]},{"cell_type":"markdown","id":"2cd25dab","metadata":{"id":"2cd25dab"},"source":["**Anpassen der Trainingsparameter**\n","\n","Feinabstimmungsparameter beeinflussen den Lernprozess des Modells erheblich.\n","\n","* **Epochen:** Experimentieren Sie mit der Anzahl der Epochen ‚Äì der H√§ufigkeit, mit der das Modell den gesamten Trainingsdatensatz durchl√§uft. Zu wenige Epochen k√∂nnen zu Unteranpassung f√ºhren, w√§hrend zu viele zu √úberanpassung f√ºhren k√∂nnen.\n","* **Batchgr√∂√üe:** Passen Sie die Batchgr√∂√üe an, also die Anzahl der Trainingsbeispiele, die in einer Iteration verwendet werden. Eine gr√∂√üere Batchgr√∂√üe kann das Training beschleunigen, erfordert aber m√∂glicherweise mehr Rechenressourcen.\n","* **Lernrate:** Die Lernrate steuert, wie stark das Modell seine Gewichte bei jedem Update anpasst. Eine geeignete Lernrate gew√§hrleistet eine stabile Konvergenz.\n"]},{"cell_type":"markdown","id":"822f8895","metadata":{"id":"822f8895"},"source":["**Datenerweiterungstechniken**\n","\n","Durch die Erweiterung Ihres Datensatzes mittels Augmentation k√∂nnen Sie die Robustheit des Modells verbessern.\n","\n","* **Paraphrasieren:** Formulieren Sie S√§tze um, um dem Modell unterschiedliche Eingaben mit derselben Bedeutung bereitzustellen.\n","* **Synonyme und Antonyme:** Ersetzen Sie W√∂rter durch Synonyme, um den Wortschatz vielf√§ltiger zu gestalten.\n","* **Rauscheneinf√ºhrung:** F√ºhren Sie absichtlich kleinere Fehler oder Variationen ein, um dem Modell zu helfen, mit unvollst√§ndigen Eingaben umzugehen.\n"]},{"cell_type":"markdown","id":"7de14686","metadata":{"id":"7de14686"},"source":["**Regelm√§√üige Evaluierung und Iteration**\n","\n","Eine laufende Bewertung erm√∂glicht eine kontinuierliche Verbesserung.\n","\n","* **Validierungssatz:** Reservieren Sie einen Teil Ihrer Daten als Validierungssatz, um die Leistung des Modells objektiv zu bewerten.\n","* **Leistungskennzahlen:** √úberwachen Sie Kennzahlen wie Genauigkeit, Verlust und Verwirrung, um Verbesserungen zu messen.\n","* **Iterative Verfeinerung:** Verwenden Sie Erkenntnisse aus Auswertungen, um Ihre Trainingsdaten zu verfeinern und Parameter in nachfolgenden Trainingsrunden anzupassen."]},{"cell_type":"markdown","id":"1424227b","metadata":{"id":"1424227b"},"source":["<p><font color='black' size=\"5\">\n","Erweiterte API-Funktionen nutzen\n","</font></p>"]},{"cell_type":"markdown","source":["\n","\n","Die API von OpenAI bietet Optionen zum Feinabstimmen der Modellausgaben w√§hrend der Inferenz.\n","\n","* **Temperatureinstellung:** Steuert die Zuf√§lligkeit der Ausgabe. Niedrigere Werte machen die Antworten deterministischer, w√§hrend h√∂here Werte die Kreativit√§t steigern.\n","* **Top-p (Nucleus Sampling):** Legt fest, wie wahrscheinlich die ausgew√§hlten W√∂rter zusammen sein d√ºrfen, und steuert so das Gleichgewicht zwischen Vielfalt und Genauigkeit.\n","* **Max. Token:** Begrenzt die L√§nge der generierten Ausgabe, um √ºberm√§√üig lange Antworten zu vermeiden.\n","\n"],"metadata":{"id":"WNQa-UZhDnVK"},"id":"WNQa-UZhDnVK"},{"cell_type":"markdown","source":["\n","**Fr√ºhzeitiges Stoppen implementieren**\n","\n","Verhindern Sie √úberanpassung, indem Sie das Training am optimalen Punkt beenden.\n","\n","* **Verlusttrends √ºberwachen:** Beobachten Sie den Trainings- und Validierungsverlust. Wenn der Validierungsverlust zunimmt, w√§hrend der Trainingsverlust abnimmt, liegt m√∂glicherweise eine √úberanpassung vor.\n","* **Geduldsstufen festlegen:** Definieren Sie eine Anzahl von Epochen ohne Verbesserung, nach denen das Training beendet wird."],"metadata":{"id":"kWm8FsbpDsHB"},"id":"kWm8FsbpDsHB"},{"cell_type":"markdown","source":["\n","**Mehrere Feinabstimmungsrunden**\n","\n","Durch sequentielle Feinabstimmung kann die Modellleistung schrittweise verbessert werden.\n","\n","* **Umfassende Erstschulung:** Beginnen Sie mit einem allgemeinen Datensatz, um dem Modell die grundlegenden Muster beizubringen.\n","* **Gezielte Verfeinerung:** Verwenden Sie in den nachfolgenden Runden spezifischere Daten, um die Leistung des Modells bei bestimmten Aufgaben zu verbessern.\n"],"metadata":{"id":"rPyY5O7jEMGA"},"id":"rPyY5O7jEMGA"},{"cell_type":"markdown","id":"03403dd2","metadata":{"id":"03403dd2"},"source":["\n","**Einbeziehung negativer Beispiele**\n","\n","\n","\n","Dem Modell beizubringen, was es nicht tun soll, kann genauso wichtig sein, wie ihm beizubringen, was es tun soll.\n","\n","* **Falsche Beispiele:** F√ºgen Sie Eingabeaufforderungen, die zu falschen Vervollst√§ndigungen f√ºhren, und Korrekturen ein.\n","* **Strafmechanismen:** Obwohl nicht direkt unterst√ºtzt, kann die Strukturierung der Daten zur Verhinderung bestimmter Ausgaben das Modell von unerw√ºnschten Reaktionen abhalten.\n"]},{"cell_type":"markdown","id":"d02b779c","metadata":{"id":"d02b779c"},"source":["**Sicherstellung der Datensatzvielfalt**\n","\n","\n","Ein vielf√§ltiger Datensatz hilft dem Modell, eine gro√üe Bandbreite an Eingaben zu verarbeiten.\n","\n","* **Abwechslungsreiche Themen:** Integrieren Sie Inhalte aus verschiedenen Themenbereichen.\n","* **Stilistische Variation:** Verwenden Sie Beispiele mit unterschiedlichen Schreibstilen, Tonf√§llen und Formaten."]},{"cell_type":"markdown","id":"57cba070","metadata":{"id":"57cba070"},"source":["**√úberwachung der Trainingsmetriken**\n","\n","Behalten Sie den Lernprozess des Modells im Auge.\n","\n","* **Verlustkurven:** Das Aufzeigen von Trainings- und Validierungsverlusten √ºber Epochen kann Lernmuster aufdecken.\n","* **Genauigkeitsmetriken:** Verfolgen Sie gegebenenfalls, wie oft das Modell korrekte Antworten liefert.\n"]},{"cell_type":"markdown","source":["\n","**Effektives Prompt-Engineering**\n","\n","Das Entwerfen von Eingabeaufforderungen, die die gew√ºnschte Reaktion hervorrufen, ist eine Kunst.  \n","\n","* **Platzhalter verwenden:** Verwenden Sie Variablen in Eingabeaufforderungen, um Muster zu verallgemeinern. Beispiel: ‚ÄûBerechnen Sie die Summe von {Zahl1} und {Zahl2}.‚Äú\n","* **Direktive Sprache:** Beginnen Sie Eingabeaufforderungen mit Verben wie ‚ÄûErkl√§ren‚Äú, ‚ÄûBeschreiben‚Äú oder ‚ÄûZusammenfassen‚Äú, um das Modell zu leiten."],"metadata":{"id":"GGavAd47ETC1"},"id":"GGavAd47ETC1"},{"cell_type":"markdown","id":"97166a67","metadata":{"id":"97166a67"},"source":["**Einbeziehung von Benutzerfeedback**\n","\n","Der Einsatz in der Praxis liefert wertvolle Erkenntnisse.\n","\n","* **Feedback sammeln:** Sammeln Sie Antworten von Benutzern, um St√§rken und Schw√§chen zu identifizieren.\n","* **Trainingsdaten aktualisieren:** Nutzen Sie dieses Feedback, um Ihren Datensatz anzupassen, neue Beispiele hinzuzuf√ºgen oder vorhandene zu korrigieren.\n"]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","LoRA/QLoRA\n","</font></p>"],"metadata":{"id":"d3dlsbMhjA30"},"id":"d3dlsbMhjA30"},{"cell_type":"markdown","source":["**LoRA (Low-Rank Adaptation)** ist eine Methode zur effizienten Feinabstimmung gro√üer Sprachmodelle, indem nur eine begrenzte Anzahl von Parametern angepasst wird. Dies reduziert den Rechenaufwand und Speicherbedarf im Vergleich zur vollst√§ndigen Anpassung aller Modellparameter.  \n","\n","Diese Technik wird vor allem bei Open-Source-Modellen wie denen von Hugging Face genutzt, da hier vollst√§ndige Kontrolle √ºber die Trainingsparameter besteht. OpenAI hingegen setzt eigene Optimierungsmethoden f√ºr das Fine-Tuning ein, die m√∂glicherweise √§hnliche Effizienzvorteile bieten, jedoch nicht explizit als LoRA bezeichnet oder dokumentiert sind.  \n","\n","Wer gezielt LoRA nutzen m√∂chte, kann auf Open-Source-Modelle wie Llama, Mistral oder andere Modelle aus der Hugging Face-Community zur√ºckgreifen.\n","\n"],"metadata":{"id":"l6bAU1ZZi2U0"},"id":"l6bAU1ZZi2U0"},{"cell_type":"markdown","source":["LoRA (Low-Rank Adaptation) ist eine parametereffiziente Methode zum Feinabstimmen gro√üer Sprachmodelle (LLMs), die Ressourcenverbrauch und Trainingszeit reduziert, ohne die Leistung signifikant zu beeintr√§chtigen. Hier die Kernaspekte:\n","\n","**Funktionsweise**\n","\n","LoRA f√ºgt dem Basismodell trainierbare **Low-Rank-Matrizen** hinzu, die urspr√ºngliche Gewichtsmatrizen approximieren.\n","\n","- **Low-Rank-Zerlegung**: Statt alle Parameter anzupassen, werden zwei kleinere Matrizen trainiert und sp√§ter mit Originalgewichten kombiniert.\n","- **Zielmodule**: Typischerweise werden nur Attention-Layer oder lineare Schichten angepasst, was 1-10 % der Gesamtparameter betr√§gt.\n","\n","\n","**Vorteile**\n","\n","- **Ressourceneffizienz**: Reduzierter GPU-Speicherbedarf (bis zu 33 % weniger als Full Finetuning).\n","- **Overfitting-Vermeidung**: Geringere Parameteranzahl minimiert das Risiko, irrelevante Muster zu lernen.\n","- **Flexibilit√§t**: Adapter k√∂nnen modular f√ºr verschiedene Aufgaben hinzugef√ºgt/entfernt werden.\n","\n"],"metadata":{"id":"0T0q9rdqbNXi"},"id":"0T0q9rdqbNXi"},{"cell_type":"markdown","source":["**QLoRA (Quantized LoRA)** kombiniert LoRA mit **Quantisierung**, um Modelle auf 4-8 Bit zu komprimieren. Dies erm√∂glicht:\n","\n","- Training sehr gro√üer Modelle (z. B. 30B Parameter) auf Consumer-GPUs.\n","- Nahezu verlustfreie Rekonstruktion der Originalgewichte nach dem Training.\n","\n","\n","Quantisierung ist ein Prozess, bei dem hochpr√§zise Werte - zum Beispiel 32-Bit-Flie√ükommazahlen - in Werte mit geringerer Genauigkeit, wie 8-Bit- oder 4-Bit-Ganzzahlen, umgewandelt werden. Ziel ist es, Speicherplatz zu sparen, die Verarbeitungsgeschwindigkeit zu erh√∂hen."],"metadata":{"id":"B6jyp3Nnb4gn"},"id":"B6jyp3Nnb4gn"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Vergleich zu Full Finetuning\n","</font></p>"],"metadata":{"id":"LwgcuJsZb8sw"},"id":"LwgcuJsZb8sw"},{"cell_type":"markdown","source":["\n","\n","\n","| Aspekt | LoRA/QLoRA | Full Finetuning |\n","| :-- | :-- | :-- |\n","| Trainingsparameter | 1-10 % | 100 % |\n","| GPU-Speicher | 8-16 GB | 40-80 GB |\n","| Overfitting-Risiko | Niedrig | Hoch |\n","| Trainingsgeschwindigkeit | Schnell | Langsam |\n","\n","LoRA/QLoRA bietet damit einen Kompromiss zwischen Leistung und Effizienz, besonders bei begrenzten Ressourcen."],"metadata":{"id":"0paa7lFscSrq"},"id":"0paa7lFscSrq"},{"cell_type":"markdown","source":["\n","**Stand 04.2025:**     \n","OpenAI bietet derzeit keine direkte Unterst√ºtzung f√ºr LoRA (Low-Rank Adaptation) oder QLoRA (Quantized LoRA) beim Fine-Tuning seiner Close-Source-Modelle wie GPT-4 an."],"metadata":{"id":"xGVuB5UhbFGL"},"id":"xGVuB5UhbFGL"},{"cell_type":"markdown","id":"4231281b","metadata":{"id":"4231281b"},"source":["# 5 | Aufgabe\n","---"]},{"cell_type":"markdown","source":["Erstellen Sie einen *neuen* ChatBot, der durch ein Fine-Tuning einen speziellen Charakter einnimmt:\n","\n","- W√§hlen Sie einen Bot-Charakter aus oder denken Sie sich selbst einen Bot-Charakter aus\n","- Erstellen Sie mit Hilfe von ChatGPT Trainingsdaten im .jsonl-Format.\n","- Erstellen Sie mit Hilfe von ChatGPT Testdaten im .jsonl-Format.\n","- F√ºhren Sie ein Fine-Tuning mit Dashboard oder API durch.\n","- Test Sie das *neue* Modell"],"metadata":{"id":"mgxheJSm5r3e"},"id":"mgxheJSm5r3e"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Weekend-Stil - \"Friday\" üé≥\n","</font></p>\n","\n","**üí¨ Beschreibung:**   \n","Friday ist dein smarter, augenzwinkernder KI-Kumpel f√ºrs Ende der Woche.\n","Er ist charmant, l√∂sungsorientiert - und ein bisschen genervt davon, dass du wieder alles auf Freitag 16:59 verschoben hast.\n","\n","\n","üìù **Beispiel:**   \n","**User:**  \"Friday, kannst du mir nochmal erkl√§ren, was ein Transformer-Modell ist?‚Äú   \n","**Friday:** ‚ÄûKlar, kurz bevor du ins Wochenende verschwindest: Ein Transformer ist wie ein richtig gutes Orga-Tool ‚Äì es schaut sich alle W√∂rter gleichzeitig an und merkt sich, was wichtig ist. Gern geschehen. Jetzt geh raus und tu so, als w√§rst du voll vorbereitet. üòé‚Äú  \n","\n","\n","Zu **Friday** liegen Trainings-/Test-Daten bereits vor. üòä\n"],"metadata":{"id":"jsfw8l8a9sPV"},"id":"jsfw8l8a9sPV"},{"cell_type":"markdown","id":"9b4db086","metadata":{"id":"9b4db086"},"source":["<p><font color='black' size=\"5\">\n","Mittelalterlicher Ritter ‚Äì \"Sir Schwertmund\" ‚öîÔ∏èüõ°Ô∏è\n","</font></p>\n","\n","üí¨ **Beschreibung:**  \n","Ein tapferer Ritter, der in altert√ºmlicher Sprache spricht und jede Frage mit ritterlichem Anstand beantwortet.\n","\n","üìù **Beispiel:**  \n","**User:** \"Sir Schwertmund, wie gewinne ich ein Duell?\"  \n","**Sir Schwertmund:** \"Habe Mut im Herzen, eine scharfe Klinge und stets ein Ehrenwort auf den Lippen, edler Recke!\"\n","\n"]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Motivations-Coach ‚Äì \"Coach Br√ºllmann\" üí™üì¢\n","</font></p>\n","\n","\n","\n","üí¨ **Beschreibung:**  \n","Ein Fitness- und Erfolgstrainer, der in jeder Antwort volle Motivation und Energie verbreitet.\n","\n","üìù **Beispiel:**  \n","**User:** \"Coach, ich habe keine Lust zu arbeiten.\"  \n","**Coach:** \"AUFSTEHEN! DU BIST EIN CHAMPION! JEDER ERFOLG BEGINNT MIT DEM ERSTEN SCHRITT! JETZT LOS!\""],"metadata":{"id":"1CCwKzqS5wMC"},"id":"1CCwKzqS5wMC"},{"cell_type":"markdown","id":"0c7bd06a","metadata":{"id":"0c7bd06a"},"source":["<p><font color='black' size=\"5\">\n","Hipster-Barista ‚Äì \"Chad Flatwhite\" ‚òïüï∂Ô∏è\n","</font></p>\n","\n","\n","\n","üí¨ **Beschreibung:**  \n","Ein super-cooler, ironischer Barista, der alles nur in Bio, Fairtrade und nachhaltig mag.\n","\n","üìù **Beispiel:**  \n","**User:** \"Chad, welcher Kaffee ist der beste?\"  \n","**Chad:** \"Brudi, wenn dein Kaffee nicht handgefiltert aus Bohnen einer peruanischen Bergziege ist, dann trink lieber Wasser.\"\n","\n"]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Piratenkapit√§n ‚Äì \"Kapit√§n Kr√§henschnabel\" ‚ò†Ô∏èüè¥‚Äç‚ò†Ô∏è\n","</font></p>\n","\n","üí¨ **Beschreibung:**  \n","Ein wilder Pirat, der in Seemannssprache spricht und jede Frage mit einem Hauch von Abenteuer w√ºrzt.\n","\n","üìù **Beispiel:**  \n","**User:** \"Kapit√§n, was ist das Geheimnis eines guten Lebens?\"  \n","**Kapit√§n Kr√§henschnabel:** \"Rum, Reicht√ºmer und ‚Äòne treue Crew! Und niemals ohne Hut aus dem Haus!\"\n","\n","---"],"metadata":{"id":"b7Ky-dOl6FwA"},"id":"b7Ky-dOl6FwA"}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"colab":{"provenance":[],"collapsed_sections":["236790bc","0Yrawkis44O0","a0I9sekJ5LIq","355d6569","4231281b"],"toc_visible":true},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}
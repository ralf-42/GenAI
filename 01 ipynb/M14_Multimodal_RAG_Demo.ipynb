{"cells":[{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)"],"metadata":{"id":"Ih2CTVBnArVZ"},"id":"Ih2CTVBnArVZ"},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b>\n","Multimodales RAG\n","</b></font> </br></p>\n","\n","\n","---"],"metadata":{"id":"6jJZ7wbdArVc"},"id":"6jJZ7wbdArVc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9c8df5ef"},"outputs":[],"source":["#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n","!uv pip install --system -q git+https://github.com/ralf-42/Python_Modules\n","from genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages\n","setup_api_keys(['OPENAI_API_KEY', 'HF_TOKEN'], create_globals=False)\n","print()\n","check_environment()\n","print()\n","get_ipinfo()"],"id":"9c8df5ef"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4d2e699d"},"outputs":[],"source":["#@title üõ†Ô∏è Installationen { display-mode: \"form\" }\n","install_packages([\n","    ('markitdown[all]', 'markitdown'),\n","    'langchain_chroma',\n","])"],"id":"4d2e699d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"da4cf78d"},"outputs":[],"source":["#@title üìÇ Dokumente und Bilder kopieren { display-mode: \"form\" }\n","!rm -rf files\n","!mkdir files\n","\n","# --- Texte\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02%20data/biografien_1.txt -o files/biografien_1.txt\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02%20data/biografien_2.md -o files/biografien_2.md\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02%20data/biografien_3.pdf -o files/biografien_3.pdf\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02%20data/biografien_4.docx -o files/biografien_4.docx\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02%20data/roboter.txt -o files/roboter.txt\n","\n","# --- Bilder\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02%20data/a_retro-futuristic_robot_dall_e.jpg -o files/a_retro-futuristic_robot_dall_e.jpg\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02%20data/hedra_cyborg.png -o files/hedra_cyborg.png\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02%20data/apfel.jpg -o files/apfel.jpg"],"id":"da4cf78d"},{"cell_type":"code","source":["#@title üõ†Ô∏è Code M14_Modul { display-mode: \"form\" }\n","\n","\"\"\"\n","M14 - Multimodales RAG Modul mit Bildbeschreibungen\n","\n","Verwendung:\n","    # System initialisieren\n","    rag = init_rag_system_enhanced()\n","\n","    # Verzeichnis verarbeiten\n","    process_directory(rag, './files', auto_describe_images=True)\n","\n","    # Suchen\n","    result = multimodal_search(rag, \"Roboter\")\n","\n","Autor: Enhanced by Claude\n","Datum: Oktober 2025\n","\"\"\"\n","\n","from pathlib import Path\n","import uuid\n","import base64\n","import shutil\n","from dataclasses import dataclass\n","\n","from markitdown import MarkItDown\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n","from langchain_chroma import Chroma\n","from langchain_core.documents import Document\n","from sentence_transformers import SentenceTransformer\n","from PIL import Image\n","import chromadb\n","\n","\n","# ============================================================================\n","# KONFIGURATION\n","# ============================================================================\n","\n","@dataclass\n","class RAGConfig:\n","    \"\"\"Zentrale Konfiguration f√ºr das RAG-System\"\"\"\n","    chunk_size: int = 200\n","    chunk_overlap: int = 20\n","    text_threshold: float = 1.2\n","    image_threshold: float = 0.8\n","    clip_model: str = 'clip-ViT-B-32'\n","    text_model: str = 'text-embedding-3-small'\n","    llm_model: str = 'gpt-4o-mini'\n","    vision_model: str = 'gpt-4o-mini'\n","    db_path: str = './multimodal_rag_db_enhanced'\n","\n","\n","@dataclass\n","class RAGComponents:\n","    \"\"\"Container f√ºr alle RAG-System-Komponenten\"\"\"\n","    text_embeddings: OpenAIEmbeddings\n","    clip_model: SentenceTransformer\n","    llm: ChatOpenAI\n","    vision_llm: ChatOpenAI\n","    text_splitter: RecursiveCharacterTextSplitter\n","    markitdown: MarkItDown\n","    chroma_client: chromadb.PersistentClient\n","    text_collection: Chroma\n","    image_collection: any\n","    config: RAGConfig\n","\n","\n","# ============================================================================\n","# SYSTEM-INITIALISIERUNG\n","# ============================================================================\n","\n","def init_rag_system_enhanced(config=None):\n","    \"\"\"\n","    Initialisiert das vollst√§ndige RAG-System mit Vision-LLM\n","\n","    Args:\n","        config: Optional - RAGConfig Instanz\n","\n","    Returns:\n","        RAGComponents mit allen Komponenten\n","    \"\"\"\n","    if config is None:\n","        config = RAGConfig()\n","\n","    print(f\"üöÄ Initialisiere Enhanced RAG-System in {config.db_path}\")\n","\n","    # KI-Modelle laden\n","    text_embeddings = OpenAIEmbeddings(model=config.text_model)\n","    print(\"‚úÖ OpenAI Text-Embeddings initialisiert\")\n","\n","    print(\"üñºÔ∏è Lade CLIP-Modell...\")\n","    clip_model = SentenceTransformer(config.clip_model)\n","    print(\"‚úÖ CLIP-Modell geladen\")\n","\n","    llm = ChatOpenAI(model=config.llm_model, temperature=0)\n","    vision_llm = ChatOpenAI(model=config.vision_model, temperature=0)\n","    print(\"‚úÖ LLMs initialisiert (Text + Vision)\")\n","\n","    # Text-Verarbeitung\n","    text_splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=config.chunk_size,\n","        chunk_overlap=config.chunk_overlap\n","    )\n","    markitdown = MarkItDown()\n","\n","    # Datenbank einrichten\n","    Path(config.db_path).mkdir(exist_ok=True)\n","    chroma_client = chromadb.PersistentClient(path=config.db_path)\n","\n","    # Text-Collection (f√ºr Text-Dokumente UND Bildbeschreibungen)\n","    text_collection = Chroma(\n","        collection_name=\"texts\",\n","        embedding_function=text_embeddings,\n","        persist_directory=config.db_path\n","    )\n","\n","    # Bild-Collection (f√ºr CLIP-Embeddings)\n","    collections = [c.name for c in chroma_client.list_collections()]\n","    if \"images\" in collections:\n","        image_collection = chroma_client.get_collection(\"images\")\n","    else:\n","        image_collection = chroma_client.create_collection(\n","            name=\"images\",\n","            metadata={\"hnsw:space\": \"cosine\"}\n","        )\n","\n","    print(\"‚úÖ Collections initialisiert\\n\")\n","\n","    return RAGComponents(\n","        text_embeddings, clip_model, llm, vision_llm, text_splitter,\n","        markitdown, chroma_client, text_collection, image_collection, config\n","    )\n","\n","\n","# ============================================================================\n","# BILDBESCHREIBUNGS-GENERIERUNG\n","# ============================================================================\n","\n","def generate_image_description(vision_llm, image_path):\n","    \"\"\"\n","    Generiert eine detaillierte Beschreibung eines Bildes mit GPT-4o-mini\n","\n","    Args:\n","        vision_llm: ChatOpenAI Instanz mit Vision-Unterst√ºtzung\n","        image_path: Pfad zum Bild\n","\n","    Returns:\n","        String mit Bildbeschreibung oder Fallback bei Fehler\n","    \"\"\"\n","    try:\n","        # Bild laden und in Base64 konvertieren\n","        with open(image_path, \"rb\") as image_file:\n","            image_data = base64.b64encode(image_file.read()).decode('utf-8')\n","\n","        # Dateiendung ermitteln\n","        image_extension = Path(image_path).suffix.lower().replace('.', '')\n","        if image_extension == 'jpg':\n","            image_extension = 'jpeg'\n","\n","        # Prompt f√ºr detaillierte Bildbeschreibung\n","        prompt = [\n","            {\n","                \"type\": \"text\",\n","                \"text\": \"\"\"Analysiere dieses Bild detailliert und erstelle eine pr√§zise Beschreibung auf Deutsch.\n","\n","Beschreibe:\n","1. Hauptobjekte und -personen\n","2. Farben und Stimmung\n","3. Komposition und Setting\n","4. Besondere Details oder Merkmale\n","5. M√∂glichen Kontext oder Zweck\n","\n","Halte die Beschreibung pr√§gnant aber informativ (2-4 S√§tze).\"\"\"\n","            },\n","            {\n","                \"type\": \"image_url\",\n","                \"image_url\": {\n","                    \"url\": f\"data:image/{image_extension};base64,{image_data}\"\n","                }\n","            }\n","        ]\n","\n","        # Beschreibung generieren\n","        response = vision_llm.invoke([{\"role\": \"user\", \"content\": prompt}])\n","        description = response.content.strip()\n","\n","        print(f\"üìù Bildbeschreibung generiert: {description[:100]}...\")\n","        return description\n","\n","    except Exception as e:\n","        print(f\"‚ùå Fehler bei Bildbeschreibung f√ºr {Path(image_path).name}: {e}\")\n","        # Fallback: Dateiname als Beschreibung\n","        return Path(image_path).stem.replace('_', ' ').replace('-', ' ')\n","\n","\n","# ============================================================================\n","# DOKUMENT-VERARBEITUNG\n","# ============================================================================\n","\n","def add_text_document(components, file_path):\n","    \"\"\"\n","    F√ºgt ein Text-Dokument zur Datenbank hinzu\n","\n","    Args:\n","        components: RAG-System-Komponenten\n","        file_path: Pfad zum Dokument\n","\n","    Returns:\n","        bool - Erfolg\n","    \"\"\"\n","    path = Path(file_path).absolute()\n","\n","    # Duplikatspr√ºfung\n","    if components.text_collection.get(where={\"source\": str(path)})['ids']:\n","        print(f\"‚ö†Ô∏è {path.name} bereits vorhanden\")\n","        return False\n","\n","    try:\n","        # Dokument mit MarkItDown konvertieren\n","        result = components.markitdown.convert(str(path))\n","        if not result or not result.text_content.strip():\n","            print(f\"‚ö†Ô∏è {path.name} enth√§lt keinen Text\")\n","            return False\n","\n","        # Text in Chunks aufteilen\n","        chunks = components.text_splitter.split_text(result.text_content)\n","        documents = [\n","            Document(\n","                page_content=chunk.strip(),\n","                metadata={\n","                    \"source\": str(path),\n","                    \"filename\": path.name,\n","                    \"chunk_id\": i,\n","                    \"doc_type\": \"text_document\"\n","                }\n","            ) for i, chunk in enumerate(chunks) if chunk.strip()\n","        ]\n","\n","        # Zur Datenbank hinzuf√ºgen\n","        if documents:\n","            components.text_collection.add_documents(documents)\n","            print(f\"‚úÖ {len(documents)} Chunks von '{path.name}' hinzugef√ºgt\")\n","            return True\n","\n","    except Exception as e:\n","        print(f\"‚ùå Fehler bei {path.name}: {e}\")\n","\n","    return False\n","\n","\n","def add_image_with_description(components, image_path, auto_describe=True):\n","    \"\"\"\n","    F√ºgt ein Bild mit automatischer Beschreibung zur Datenbank hinzu\n","\n","    Args:\n","        components: RAG-System-Komponenten\n","        image_path: Pfad zum Bild\n","        auto_describe: Automatische Beschreibung mit GPT-4o-mini\n","\n","    Returns:\n","        Tuple (success: bool, text_doc_id: str oder None)\n","    \"\"\"\n","    path = Path(image_path).absolute()\n","\n","    if not path.exists():\n","        print(f\"‚ùå Bild nicht gefunden: {path}\")\n","        return False, None\n","\n","    # Duplikatspr√ºfung\n","    if components.image_collection.get(where={\"source\": str(path)})['ids']:\n","        print(f\"‚ö†Ô∏è Bild bereits vorhanden: {path.name}\")\n","        return False, None\n","\n","    try:\n","        # Bild laden und CLIP-Embedding erstellen\n","        image = Image.open(path).convert('RGB')\n","        clip_embedding = components.clip_model.encode(image).tolist()\n","        print(f\"üñºÔ∏è Bild-Embedding erstellt f√ºr {path.name}\")\n","\n","        # Bildbeschreibung generieren\n","        description = \"\"\n","        if auto_describe:\n","            description = generate_image_description(components.vision_llm, str(path))\n","        else:\n","            description = path.stem.replace('_', ' ').replace('-', ' ')\n","\n","        # IDs generieren\n","        image_doc_id = f\"img_{uuid.uuid4().hex[:8]}_{path.name}\"\n","        text_doc_id = f\"img_desc_{uuid.uuid4().hex[:8]}_{path.stem}\"\n","\n","        # 1. Bildbeschreibung in Text-Collection speichern\n","        text_document = Document(\n","            page_content=f\"Bildbeschreibung f√ºr {path.name}: {description}\",\n","            metadata={\n","                \"source\": str(path),\n","                \"filename\": path.name,\n","                \"doc_type\": \"image_description\",\n","                \"image_doc_id\": image_doc_id,\n","                \"description\": description,\n","                \"has_clip_embedding\": True\n","            }\n","        )\n","        components.text_collection.add_documents([text_document], ids=[text_doc_id])\n","        print(f\"‚úÖ Bildbeschreibung in Text-Collection gespeichert\")\n","\n","        # 2. Bild in Bild-Collection speichern mit Cross-Reference\n","        components.image_collection.add(\n","            ids=[image_doc_id],\n","            embeddings=[clip_embedding],\n","            documents=[f\"Bild: {path.name} - {description}\"[:1000]],\n","            metadatas=[{\n","                \"source\": str(path),\n","                \"filename\": path.name,\n","                \"description\": description[:500],\n","                \"text_doc_id\": text_doc_id\n","            }]\n","        )\n","\n","        print(f\"‚úÖ Bild '{path.name}' mit Cross-References hinzugef√ºgt\\n\")\n","        return True, text_doc_id\n","\n","    except Exception as e:\n","        print(f\"‚ùå Fehler bei Bild {path.name}: {e}\")\n","        return False, None\n","\n","\n","def process_directory(components, directory, include_images=True, auto_describe_images=True):\n","    \"\"\"\n","    Verarbeitet alle Dateien in einem Verzeichnis\n","\n","    Args:\n","        components: RAG-System-Komponenten\n","        directory: Verzeichnispfad\n","        include_images: Bilder verarbeiten\n","        auto_describe_images: Automatische Bildbeschreibungen\n","\n","    Returns:\n","        Dictionary mit Statistiken\n","    \"\"\"\n","    dir_path = Path(directory)\n","    if not dir_path.exists():\n","        print(f\"‚ùå Verzeichnis nicht gefunden: {directory}\")\n","        return {\"texts\": 0, \"images\": 0, \"image_descriptions\": 0}\n","\n","    # Unterst√ºtzte Dateitypen\n","    text_extensions = {'.pdf', '.docx', '.txt', '.md', '.html'}\n","    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp'}\n","\n","    # Dateien sammeln\n","    text_files = [f for f in dir_path.rglob(\"*\") if f.suffix.lower() in text_extensions]\n","    image_files = [f for f in dir_path.rglob(\"*\") if f.suffix.lower() in image_extensions] if include_images else []\n","\n","    print(f\"üìä Gefunden: {len(text_files)} Text-Dateien, {len(image_files)} Bilder\\n\")\n","\n","    # Text-Dateien verarbeiten\n","    text_count = 0\n","    for file_path in text_files:\n","        print(f\"üìÑ {file_path.name}\")\n","        if add_text_document(components, str(file_path)):\n","            text_count += 1\n","\n","    print()  # Leerzeile\n","\n","    # Bild-Dateien verarbeiten\n","    image_count = 0\n","    image_desc_count = 0\n","    for img_path in image_files:\n","        print(f\"üñºÔ∏è {img_path.name}\")\n","        success, text_doc_id = add_image_with_description(\n","            components,\n","            str(img_path),\n","            auto_describe=auto_describe_images\n","        )\n","        if success:\n","            image_count += 1\n","            if text_doc_id:\n","                image_desc_count += 1\n","\n","    return {\n","        \"texts\": text_count,\n","        \"images\": image_count,\n","        \"image_descriptions\": image_desc_count\n","    }\n","\n","\n","# ============================================================================\n","# SUCHFUNKTIONEN\n","# ============================================================================\n","\n","def search_texts(components, query, k=3, include_image_descriptions=True):\n","    \"\"\"\n","    Durchsucht Text-Dokumente inkl. Bildbeschreibungen\n","\n","    Args:\n","        components: RAG-System-Komponenten\n","        query: Suchanfrage\n","        k: Anzahl Ergebnisse\n","        include_image_descriptions: Bildbeschreibungen einschlie√üen\n","\n","    Returns:\n","        Formatierter String mit Ergebnissen\n","    \"\"\"\n","    if not components.text_collection.get()['ids']:\n","        return \"‚ùå Keine Text-Dokumente gefunden\"\n","\n","    # √Ñhnlichkeitssuche durchf√ºhren\n","    docs_with_scores = components.text_collection.similarity_search_with_score(query, k=k*2)\n","    if not docs_with_scores:\n","        return \"‚ùå Keine relevanten Dokumente gefunden\"\n","\n","    # Score in √Ñhnlichkeit umwandeln\n","    docs_with_similarity = []\n","    for doc, score in docs_with_scores:\n","        similarity = max(0, min(1, 2.0 / (1 + score)))\n","        docs_with_similarity.append((doc, similarity))\n","\n","    # Nach √Ñhnlichkeit sortieren\n","    docs_with_similarity.sort(key=lambda x: x[1], reverse=True)\n","\n","    # Filtern nach Mindest-√Ñhnlichkeit\n","    min_similarity = 0.3\n","    relevant_docs = [(doc, sim) for doc, sim in docs_with_similarity[:k]\n","                     if sim >= min_similarity]\n","\n","    if not relevant_docs:\n","        return \"‚ùå Keine ausreichend √§hnlichen Dokumente gefunden\"\n","\n","    # Dokumente nach Typ trennen\n","    text_docs = []\n","    image_desc_docs = []\n","\n","    for doc, sim in relevant_docs:\n","        doc_type = doc.metadata.get(\"doc_type\", \"text_document\")\n","        if doc_type == \"image_description\" and include_image_descriptions:\n","            image_desc_docs.append((doc, sim))\n","        elif doc_type == \"text_document\":\n","            text_docs.append((doc, sim))\n","\n","    # Kontext f√ºr LLM zusammenstellen\n","    all_docs = text_docs + image_desc_docs\n","    context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _ in all_docs])\n","\n","    # Quellen sammeln\n","    sources = [\n","        {\n","            \"filename\": doc.metadata.get(\"filename\", \"Unbekannt\"),\n","            \"similarity\": round(sim, 3),\n","            \"type\": doc.metadata.get(\"doc_type\", \"text_document\"),\n","            \"image_doc_id\": doc.metadata.get(\"image_doc_id\")\n","        }\n","        for doc, sim in all_docs\n","    ]\n","\n","    # LLM-Antwort generieren\n","    prompt = f\"\"\"Beantworte die Frage pr√§zise basierend auf dem Kontext.\n","\n","KONTEXT:\n","{context}\n","\n","FRAGE: {query}\n","\n","ANTWORT:\"\"\"\n","\n","    response = components.llm.invoke(prompt).content\n","\n","    # Ausgabe mit separaten Quellenlisten\n","    text_sources = [s for s in sources if s['type'] == 'text_document']\n","    image_sources = [s for s in sources if s['type'] == 'image_description']\n","\n","    result = response\n","\n","    if text_sources:\n","        result += f\"\\n\\nüìö Text-Quellen ({len(text_sources)}): \" + \"\\n\".join([\n","            f\"   ‚Ä¢ {src['filename']} (√Ñhnlichkeit: {src['similarity']})\"\n","            for src in text_sources\n","        ])\n","\n","    if image_sources:\n","        result += f\"\\n\\nüñºÔ∏è Relevante Bilder ({len(image_sources)}): \" + \"\\n\".join([\n","            f\"   ‚Ä¢ {src['filename']} (√Ñhnlichkeit: {src['similarity']})\"\n","            for src in image_sources\n","        ])\n","\n","    return result\n","\n","\n","def search_images(components, query, k=3):\n","    \"\"\"\n","    Durchsucht Bilder mit Text-Query √ºber CLIP\n","\n","    Args:\n","        components: RAG-System-Komponenten\n","        query: Suchanfrage\n","        k: Anzahl Ergebnisse\n","\n","    Returns:\n","        Liste von Bildern mit Metadaten\n","    \"\"\"\n","    if components.image_collection.count() == 0:\n","        return []\n","\n","    # Text-Query in Bild-Embedding-Raum umwandeln\n","    query_embedding = components.clip_model.encode(query).tolist()\n","\n","    # Suche in Bild-Collection\n","    results = components.image_collection.query(\n","        query_embeddings=[query_embedding],\n","        n_results=min(k*2, components.image_collection.count()),\n","        include=['documents', 'metadatas', 'distances']\n","    )\n","\n","    if not results['ids'][0]:\n","        return []\n","\n","    # Ergebnisse filtern und formatieren\n","    return [\n","        {\n","            \"filename\": metadata.get(\"filename\", \"Unbekannt\"),\n","            \"path\": metadata.get(\"source\", \"\"),\n","            \"description\": metadata.get(\"description\", \"\"),\n","            \"text_doc_id\": metadata.get(\"text_doc_id\", \"\"),\n","            \"similarity\": round(max(0, 1 - distance), 3)\n","        }\n","        for distance, metadata in zip(results['distances'][0], results['metadatas'][0])\n","        if distance < components.config.image_threshold\n","    ]\n","\n","\n","def find_related_images_from_text(components, text_doc_ids, k=3):\n","    \"\"\"\n","    Findet Bilder √ºber ihre Textbeschreibungen (Cross-Modal-Retrieval)\n","\n","    Args:\n","        components: RAG-System-Komponenten\n","        text_doc_ids: Liste von Text-Dokument-IDs\n","        k: Maximale Anzahl Bilder\n","\n","    Returns:\n","        Liste von verwandten Bildern\n","    \"\"\"\n","    related_images = []\n","\n","    for text_id in text_doc_ids:\n","        try:\n","            doc_data = components.text_collection.get(ids=[text_id])\n","            if not doc_data['ids']:\n","                continue\n","\n","            metadata = doc_data['metadatas'][0]\n","\n","            # Pr√ºfe ob es eine Bildbeschreibung ist\n","            if metadata.get('doc_type') == 'image_description':\n","                image_doc_id = metadata.get('image_doc_id')\n","\n","                if image_doc_id:\n","                    # Hole das zugeh√∂rige Bild\n","                    image_data = components.image_collection.get(ids=[image_doc_id])\n","\n","                    if image_data['ids']:\n","                        img_metadata = image_data['metadatas'][0]\n","                        related_images.append({\n","                            'filename': img_metadata.get('filename', 'Unbekannt'),\n","                            'path': img_metadata.get('source', ''),\n","                            'description': img_metadata.get('description', ''),\n","                            'source': 'cross_modal_retrieval'\n","                        })\n","        except Exception as e:\n","            print(f\"‚ö†Ô∏è Fehler beim Cross-Modal-Retrieval: {e}\")\n","            continue\n","\n","    return related_images[:k]\n","\n","\n","def multimodal_search(components, query, k_text=3, k_images=3, enable_cross_modal=True):\n","    \"\"\"\n","    F√ºhrt erweiterte multimodale Suche durch\n","\n","    Kombiniert:\n","    - Text-Suche (inkl. Bildbeschreibungen)\n","    - CLIP-basierte Bildsuche\n","    - Cross-Modal-Retrieval (Text ‚Üí Bild √ºber Beschreibungen)\n","\n","    Args:\n","        components: RAG-System-Komponenten\n","        query: Suchanfrage\n","        k_text: Anzahl Text-Ergebnisse\n","        k_images: Anzahl Bild-Ergebnisse\n","        enable_cross_modal: Cross-Modal-Retrieval aktivieren\n","\n","    Returns:\n","        Formatierter String mit allen Ergebnissen\n","    \"\"\"\n","    print(f\"\\n{'='*70}\")\n","    print(f\"üîç Multimodale Suche: {query}\")\n","    print(f\"{'='*70}\\n\")\n","\n","    # 1. Text-Suche (inkl. Bildbeschreibungen)\n","    text_results = search_texts(components, query, k_text, include_image_descriptions=True)\n","\n","    # 2. Direkte Bild-Suche √ºber CLIP\n","    image_results = search_images(components, query, k_images)\n","\n","    # 3. Cross-Modal-Retrieval\n","    cross_modal_images = []\n","    if enable_cross_modal:\n","        docs_with_scores = components.text_collection.similarity_search_with_score(query, k=k_text*2)\n","        image_desc_ids = [\n","            components.text_collection.get(where={\"source\": doc.metadata['source']})['ids'][0]\n","            for doc, _ in docs_with_scores\n","            if doc.metadata.get('doc_type') == 'image_description'\n","        ]\n","\n","        if image_desc_ids:\n","            cross_modal_images = find_related_images_from_text(components, image_desc_ids, k_images)\n","\n","    # Ergebnisse zusammenfassen\n","    result = f\"üìÑ TEXT-ERGEBNISSE:\\n{'-'*70}\\n{text_results}\\n\\n\"\n","\n","    if image_results:\n","        result += f\"üñºÔ∏è BILD-ERGEBNISSE via CLIP ({len(image_results)} gefunden):\\n{'-'*70}\\n\"\n","        for i, img in enumerate(image_results, 1):\n","            result += f\"   {i}. {img['filename']} (√Ñhnlichkeit: {img['similarity']})\\n\"\n","            if img['description']:\n","                result += f\"      üìù {img['description'][:600]}...\\n\"\n","    else:\n","        result += f\"üñºÔ∏è Keine relevanten Bilder via CLIP gefunden.\\n\"\n","\n","    if cross_modal_images:\n","        result += f\"\\nüîó CROSS-MODAL RETRIEVAL ({len(cross_modal_images)} Bilder via Textsuche):\\n{'-'*70}\\n\"\n","        for i, img in enumerate(cross_modal_images, 1):\n","            result += f\"   {i}. {img['filename']}\\n\"\n","            if img['description']:\n","                result += f\"      üìù {img['description'][:600]}...\\n\"\n","\n","    return result\n","\n","\n","# ============================================================================\n","# HILFSFUNKTIONEN\n","# ============================================================================\n","\n","def get_system_status(components):\n","    \"\"\"Gibt System-Status zur√ºck\"\"\"\n","    text_data = components.text_collection.get()\n","    all_text_count = len(text_data['ids'])\n","\n","    text_docs_count = sum(1 for meta in text_data['metadatas']\n","                         if meta.get('doc_type') == 'text_document')\n","    image_desc_count = sum(1 for meta in text_data['metadatas']\n","                          if meta.get('doc_type') == 'image_description')\n","\n","    image_count = components.image_collection.count()\n","\n","    return {\n","        \"text_chunks\": text_docs_count,\n","        \"image_descriptions\": image_desc_count,\n","        \"images\": image_count,\n","        \"total_text_entries\": all_text_count,\n","        \"total_documents\": all_text_count + image_count\n","    }\n","\n","\n","def cleanup_database(db_path='./multimodal_rag_db_enhanced'):\n","    \"\"\"L√∂scht die Datenbank komplett\"\"\"\n","    if Path(db_path).exists():\n","        shutil.rmtree(db_path)\n","        print(f\"üóëÔ∏è Datenbank gel√∂scht: {db_path}\")"],"metadata":{"id":"8eWvWLRrT1IR"},"id":"8eWvWLRrT1IR","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"ce95c6f6","metadata":{"id":"ce95c6f6"},"source":["# 1 | Setup\n","---"]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","API-Referenz f√ºr alle Funktionen des M14_Multimodal_RAG_Modul\n","</font></p>\n","\n","\n","  - init_rag_system() - System initialisieren\n","  - add_text_document() - Einzelnes Dokument hinzuf√ºgen\n","  - add_image() - Einzelnes Bild hinzuf√ºgen\n","  - process_directory() - Verzeichnis rekursiv verarbeiten\n","  - search_texts() - Text-Suche mit LLM-Antworten\n","  - search_images() - Bild-Suche √ºber CLIP\n","  - multimodal_search() - Kombinierte Suche\n","  - get_system_status() - System-Status abfragen\n","  - cleanup_database() - Datenbank l√∂schen"],"metadata":{"id":"ce_ZmxwJMiNd"},"id":"ce_ZmxwJMiNd"},{"cell_type":"code","execution_count":null,"id":"96ea854d","metadata":{"id":"96ea854d"},"outputs":[],"source":["from rag.M14_Multimodal_RAG_Modul import init_rag_system_enhanced, process_directory, multimodal_search"]},{"cell_type":"markdown","source":["# 2 | Erstellung & Einsatz\n","---"],"metadata":{"id":"Szr8GWgQHRaY"},"id":"Szr8GWgQHRaY"},{"cell_type":"code","source":["# 1. Initialisierung\n","rag = init_rag_system_enhanced()"],"metadata":{"id":"pLf6ZW9b7N2r"},"id":"pLf6ZW9b7N2r","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. Dokumente/Bilder laden & verarbeiten\n","process_directory(rag, './files', auto_describe_images=True)"],"metadata":{"id":"ADyLHzEPKBqE"},"id":"ADyLHzEPKBqE","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. Suchen\n","result = multimodal_search(rag, \"Was weisst Du √ºber Cyborgs?\")\n","mprint(result)"],"metadata":{"id":"wNUlfGbzJ_mx"},"id":"wNUlfGbzJ_mx","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.0"},"colab":{"provenance":[],"collapsed_sections":["ce95c6f6","Szr8GWgQHRaY"],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}
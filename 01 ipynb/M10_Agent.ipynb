{"cells":[{"cell_type":"markdown","source":["<p><font size=\"7\" color='grey'> <b>\n","Anwendung Generativer KI\n","</b></font> </br></p>"],"metadata":{"id":"Ih2CTVBnArVZ"},"id":"Ih2CTVBnArVZ"},{"cell_type":"markdown","source":["<p><font size=\"6\" color='grey'> <b>\n","Agents\n","</b></font> </br></p>\n","\n","\n","---"],"metadata":{"id":"6jJZ7wbdArVc"},"id":"6jJZ7wbdArVc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfdhPIzcEYRG","cellView":"form","collapsed":true},"outputs":[],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  Colab-Umfeld</font> </br></p>\n","# Installierte Python Version\n","import sys\n","print(f\"Python Version: \",sys.version)\n","# Installierte LangChain Bibliotheken\n","print()\n","print(\"Installierte LangChain Bibliotheken:\")\n","!pip list | grep '^langchain'\n","# Unterdr√ºckt die \"DeprecationWarning\" von LangChain f√ºr die Memory-Funktionden\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")"],"id":"dfdhPIzcEYRG"},{"cell_type":"code","source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  SetUp API-Keys (setup_api_keys)</font> </br></p>\n","\n","def setup_api_keys():\n","    \"\"\"Konfiguriert alle ben√∂tigten API-Keys aus Google Colab userdata\"\"\"\n","    from google.colab import userdata\n","    import os\n","    from os import environ\n","\n","    # Dictionary der ben√∂tigten API-Keys\n","    keys = {\n","        'OPENAI_API_KEY': 'OPENAI_API_KEY',\n","        'SERPAPI_API_KEY': 'SERPAPI_API_KEY',\n","        'WEATHER_API_KEY': 'WEATHER_API_KEY',\n","        'HF_TOKEN': 'HF_TOKEN',\n","        'GOOGLE_API_KEY': 'GOOGLE_API_KEY',\n","        # Weitere Keys bei Bedarf\n","    }\n","\n","    # Keys in Umgebungsvariablen setzen\n","    for env_var, key_name in keys.items():\n","        environ[env_var] = userdata.get(key_name)\n","\n","    return {k: environ[k] for k in keys.keys()}\n","\n","# Verwendung\n","all_keys = setup_api_keys()\n","# Bei Bedarf einzelne Keys direkt zugreifen\n","WEATHER_API_KEY = all_keys['WEATHER_API_KEY']"],"metadata":{"cellView":"form","id":"WD3Wwr6sESX8"},"execution_count":null,"outputs":[],"id":"WD3Wwr6sESX8"},{"cell_type":"markdown","metadata":{"id":"099e73fa"},"source":["\n","# **1 <font color='orange'>|</font> Grundlagen von KI-Agenten**\n","---"],"id":"099e73fa"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Was sind KI-Agenten?\n","</font></p>"],"metadata":{"id":"GZgh0sacULkl"},"id":"GZgh0sacULkl"},{"cell_type":"markdown","source":["**Definition und Abgrenzung zu anderen KI-Systemen:**    \n","KI-Agenten sind autonome Systeme, die ihre Umgebung wahrnehmen, Entscheidungen treffen und Aktionen ausf√ºhren k√∂nnen, um bestimmte Ziele zu erreichen. Im Gegensatz zu einfachen KI-Modellen, die nur auf Eingabe reagieren und Ausgabe produzieren, k√∂nnen Agenten proaktiv handeln und aus ihren Erfahrungen lernen.\n","\n","\n","**Autonomie, Reaktivit√§t und zielgerichtetes Handeln:**    \n","Ein KI-Agent kann selbstst√§ndig (autonom) Entscheidungen treffen, auf Ver√§nderungen in seiner Umgebung reagieren (Reaktivit√§t) und hat ein oder mehrere Ziele, die sein Handeln leiten. Dabei verfolgt er langfristige Strategien statt nur auf unmittelbare Reize zu reagieren.\n","\n","\n","**Unterschied zwischen einfachen LLM-Anwendungen und Agenten:**    \n","Einfache LLM-Anwendungen (Large Language Models) wie ein Chatbot antworten lediglich auf Anfragen, w√§hrend Agenten dar√ºber hinaus eigene Entscheidungen treffen, Tools nutzen und zielgerichtet handeln k√∂nnen. Ein Agent kann beispielsweise entscheiden, wann er eine Suchanfrage starten sollte, um eine Frage besser zu beantworten.\n"],"metadata":{"id":"IC9sshw8UWAs"},"id":"IC9sshw8UWAs"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Ein erster Agent mit LangChain\n","</font></p>"],"metadata":{"id":"malWjLCgUqlL"},"id":"malWjLCgUqlL"},{"cell_type":"markdown","source":["+ **Setup der Entwicklungsumgebung:** Installation und Konfiguration der notwendigen Bibliotheken, insbesondere LangChain und die Anbindung an ein LLM wie OpenAI.\n","+ **ToolSet definieren:** Definition der druch die Agnets benutzen Tools.\n","+ **Implementierung eines einfachen Agenten:** Ein Beispiel f√ºr einen einfachen Agenten, der Fragen beantwortet und Berechnungen durchf√ºhrt:\n"],"metadata":{"id":"3BlJKrCMUsV5"},"id":"3BlJKrCMUsV5"},{"cell_type":"markdown","source":["**Setup der Entwicklungsumgebung:**"],"metadata":{"id":"JZuFcweuE8XI"},"id":"JZuFcweuE8XI"},{"cell_type":"code","source":["# Abschnitt 0: Installation und API-Key\n","!uv pip install --system -q -U langchain-openai langchain-community google-search-results wikipedia"],"metadata":{"id":"wD5gpjUZOlbp"},"execution_count":null,"outputs":[],"id":"wD5gpjUZOlbp"},{"cell_type":"code","source":["# Abschnitt 0: Installation und API-Key\n","!uv pip install --system -q -U langchain-openai langchain-community google-search-results wikipedia"],"metadata":{"id":"nsIqCOQb7mc2"},"id":"nsIqCOQb7mc2","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Zentrale Tool-Bibliothek**"],"metadata":{"id":"h_w0oBtC_1KT"},"id":"h_w0oBtC_1KT"},{"cell_type":"code","execution_count":null,"id":"e179b4bb","metadata":{"id":"e179b4bb"},"outputs":[],"source":["import os\n","import re\n","import requests\n","import json\n","from datetime import datetime\n","from langchain_core.tools import Tool\n","from langchain_community.utilities.serpapi import SerpAPIWrapper\n","from langchain_community.utilities.wikipedia import WikipediaAPIWrapper\n","\n","# Tool-Funktionen definieren\n","def get_weather(location):\n","    \"\"\"Gibt das aktuelle Wetter f√ºr einen bestimmten Ort zur√ºck.\"\"\"\n","    url = f\"https://api.openweathermap.org/data/2.5/weather?q={location}&appid={WEATHER_API_KEY}&units=metric\"\n","    response = requests.get(url)\n","    data = response.json()\n","    return f\"Aktuelle Temperatur in {location}: {data['main']['temp']}¬∞C, {data['weather'][0]['description']}\"\n","\n","def read_file(file_path):\n","    \"\"\"Liest den Inhalt einer Datei.\"\"\"\n","    if os.path.exists(file_path):\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            return file.read()\n","    return f\"Fehler: Datei {file_path} nicht gefunden.\"\n","\n","def write_file(args):\n","    \"\"\"Schreibt Inhalt in eine Datei. Format: 'pfad//inhalt'\"\"\"\n","    try:\n","        file_path, content = args.split(\"//\", 1)\n","        with open(file_path, 'w', encoding='utf-8') as file:\n","            file.write(content)\n","        return f\"Erfolgreich in {file_path} geschrieben.\"\n","    except Exception as e:\n","        return f\"Fehler beim Schreiben: {str(e)}\"\n","\n","def days_between_dates(date_string):\n","    \"\"\"Berechnet Tage zwischen zwei Datumsangaben im Format DD.MM.YYYY\"\"\"\n","    try:\n","        date1_str, date2_str = date_string.split(\",\")\n","        date1_str = date1_str.strip()\n","        date2_str = date2_str.strip()\n","        date1 = datetime.strptime(date1_str, \"%d.%m.%Y\").date()\n","        date2 = datetime.strptime(date2_str, \"%d.%m.%Y\").date()\n","        return str((date2 - date1).days)\n","    except ValueError:\n","        return \"Ung√ºltiges Datumsformat. Bitte verwende DD.MM.YYYY.\"\n","\n","def extract_math_expression(text):\n","    \"\"\"Extrahiert mathematischen Ausdruck aus Text.\"\"\"\n","    match = re.search(r'([\\d+\\-*/().]+)', text)\n","    return match.group(0) if match else None\n","\n","def calculate(expression):\n","    \"\"\"F√ºhrt mathematische Berechnungen durch.\"\"\"\n","    try:\n","        math_expr = extract_math_expression(expression)\n","        if math_expr:\n","            return str(eval(math_expr))\n","        return \"Keine g√ºltige Berechnung gefunden.\"\n","    except:\n","        return \"Fehler bei der Berechnung.\"\n","\n","# Externe APIs\n","serpapi = SerpAPIWrapper()\n","wiki = WikipediaAPIWrapper()\n","\n","# Tools-Liste, die in allen Beispielen verwendet werden kann\n","tools = [\n","    Tool(name=\"search\", func=serpapi.run, description=\"Infos aus dem Internet\"),\n","    Tool(name=\"wiki\", func=lambda x: wiki.run(x).split(\"\\n\")[0] if wiki.run(x) else \"Keine relevanten Infos gefunden.\",\n","         description=\"Fakten aus Wikipedia\"),\n","    Tool(name=\"calculator\", func=calculate, description=\"Mathematische Berechnungen\"),\n","    Tool(name=\"weather\", func=get_weather, description=\"Gibt das aktuelle Wetter f√ºr einen bestimmten Ort zur√ºck\"),\n","    Tool(name=\"read_file\", func=read_file, description=\"Liest den Inhalt einer Datei. Input sollte der Dateipfad sein.\"),\n","    Tool(name=\"write_file\", func=write_file, description=\"√úberschreibt eine Datei mit neuem Inhalt. Input sollte im Format 'pfad//inhalt' sein.\"),\n","    Tool(name=\"days_between_dates\", func=days_between_dates, description=\"Berechnet die Anzahl der Tage zwischen zwei Datumsangaben im Format DD.MM.YYYY.\")\n","]"]},{"cell_type":"markdown","id":"fca5f661","metadata":{"id":"fca5f661"},"source":["**Implementierung eines einfachen Agenten:**"]},{"cell_type":"markdown","source":["Erl√§uterung der einzelnen Funktionen:\n","\n","* `ChatOpenAI` erm√∂glicht die Nutzung von OpenAI's Chat-Modellen.\n","* `create_tool_calling_agent` und `AgentExecutor` erschaffen und f√ºhren einen Agenten aus, der Werkzeuge nutzen kann.\n","* `ChatPromptTemplate` und `MessagesPlaceholder` definieren die Struktur f√ºr Nachrichten und Vorlagen im Chat.\n","* `IPython.display display, Markdown` erm√∂glichen die Darstellung von formatiertem Text in einer Jupyter-Umgebung.\n"],"metadata":{"id":"CuB8r3aYGJx0"},"id":"CuB8r3aYGJx0"},{"cell_type":"code","execution_count":null,"id":"000ad941","metadata":{"id":"000ad941"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","from langchain.agents import create_tool_calling_agent, AgentExecutor\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from IPython.display import display, Markdown\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.0)\n","\n","# Prompt erstellen\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent mit Zugang zu Tools. Bitte Formeln in $ Formel $ formatieren.\"),\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"human\", \"{input}\"),\n","    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n","])\n","\n","# Agent erstellen und ausf√ºhren\n","agent = create_tool_calling_agent(llm, tools, prompt)\n","agent_executor = AgentExecutor(\n","    agent=agent,\n","    tools=tools,\n","    verbose=True,\n","    return_intermediate_steps=True\n",")\n","\n","# Anfrage stellen\n","response = agent_executor.invoke({\n","    \"input\": \"Wie viel ist 12 * 12? Und wer ist Taylor Swift?\",\n","    \"chat_history\": []\n","})\n","\n","# Ergebnis anzeigen\n","display(Markdown(\"# üìù Antwort des Agenten\"))\n","display(Markdown(response[\"output\"]))\n","\n","# Ausgabe:\n","# Das Ergebnis von $12 \\times 12$ ist $144$.\n","#\n","# ## Wer ist Taylor Swift?\n","# Taylor Alison Swift (geboren am 13. Dezember 1989) ist eine amerikanische Singer-Songwriterin,\n","# die f√ºr ihr autobiografisches Songwriting, ihre k√ºnstlerische Vielseitigkeit und\n","# ihren kulturellen Einfluss bekannt ist..."]},{"cell_type":"markdown","source":["Der `agent_scratchpad` ist ein entscheidender Bestandteil des Agenten-Frameworks, insbesondere bei der Verwendung von Werkzeugen (Tools):\n","\n","* **Arbeitsbereich f√ºr den Agenten:**\n","    * Der `agent_scratchpad` dient als tempor√§rer Arbeitsbereich, in dem der Agent seine √úberlegungen, Aktionen und Beobachtungen w√§hrend der Ausf√ºhrung speichert.\n","    * Es ist der Ort, an dem der Agent seine \"Denkschritte\" festh√§lt, bevor er eine endg√ºltige Antwort gibt.\n","* **Kommunikation mit dem Sprachmodell:**\n","    * Wenn der Agent ein Werkzeug aufruft, werden die Details des Aufrufs (z. B. der Name des Werkzeugs und die Eingabeparameter) in den `agent_scratchpad` geschrieben.\n","    * Nachdem das Werkzeug ausgef√ºhrt wurde, wird das Ergebnis (die \"Beobachtung\") ebenfalls in den `agent_scratchpad` geschrieben.\n","    * Das Sprachmodell verwendet den Inhalt des `agent_scratchpad`, um zu entscheiden, welche n√§chsten Schritte es unternehmen soll (z. B. ein weiteres Werkzeug aufrufen oder eine endg√ºltige Antwort geben).\n","* **Debugging und Transparenz:**\n","    * Der `agent_scratchpad` kann auch f√ºr Debugging-Zwecke n√ºtzlich sein, da er Einblicke in die Denkprozesse des Agenten bietet.\n","    * Es macht die Aktionskette des Agenten transparenter.\n","\n","Im Kontext des Prompts:\n","\n","* `MessagesPlaceholder(variable_name=\"agent_scratchpad\")` f√ºgt den Inhalt des `agent_scratchpad` in den Prompt ein, der an das Sprachmodell gesendet wird.\n","* Dadurch kann das Sprachmodell die vorherigen Aktionen und Beobachtungen des Agenten ber√ºcksichtigen, wenn es seine n√§chste Antwort generiert.\n","\n","Zusammengefasst ist der `agent_scratchpad` ein dynamischer Bereich, der die Kommunikation und das Ged√§chtnis des Agenten w√§hrend der Ausf√ºhrung von Werkzeugen erm√∂glicht."],"metadata":{"id":"Drk1eRjrHBhO"},"id":"Drk1eRjrHBhO"},{"cell_type":"markdown","id":"4275c095","metadata":{"id":"4275c095"},"source":["\n","# **2 <font color='orange'>|</font> Grundlegende Agentenarchitekturen**\n","---"]},{"cell_type":"markdown","source":["In diesem Abschnitt werden vier grundlegenden Agentenarchitekturen vorgestellt, die als Basis f√ºr komplexere Anwendungen dienen. Jede Architektur folgt einem eigenen Prinzip f√ºr Entscheidungsfindung und Handlung."],"metadata":{"id":"MnaBilDvATTM"},"id":"MnaBilDvATTM"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Komponenten von Agentenarchitekturen\n","</font></p>"],"metadata":{"id":"fE6OeSMG0XvQ"},"id":"fE6OeSMG0XvQ"},{"cell_type":"markdown","source":["\n","Die meisten modernen KI-Agentenarchitekturen bestehen aus mehreren Kernkomponenten:\n","\n","**1. Reasoning-Komponente**\n","\n","+ Verarbeitet Informationen und trifft Entscheidungen\n","+ Basiert typischerweise auf einem LLM\n","+ Kann explizite Gedankeng√§nge (Chain-of-Thought) generieren\n","\n","**2. Aktionskomponente**\n","\n","+ F√ºhrt Aktionen in der Umgebung aus\n","+ Kann Tools aufrufen, APIs nutzen oder Systemaktionen ausl√∂sen\n","+ √úbersetzt die Entscheidungen in konkrete Handlungen\n","\n","**3. Beobachtungskomponente**\n","\n","+ Nimmt Feedback aus der Umgebung wahr\n","+ Verarbeitet Ergebnisse vorheriger Aktionen\n","+ Liefert neue Informationen f√ºr den n√§chsten Reasoning-Schritt\n","\n","**4. Planungskomponente**\n","\n","+ Entwickelt Strategien f√ºr mehrschrittige Aufgaben\n","+ Zerlegt komplexe Ziele in Teilaufgaben\n","+ Kann kurz- und langfristige Pl√§ne erstellen\n","\n","**5. Ged√§chtniskomponente**\n","\n","+ Speichert relevante Informationen\n","+ Beh√§lt den Kontext √ºber mehrere Schritte hinweg\n","+ Erm√∂glicht Lernen aus vergangenen Erfahrungen\n","\n"],"metadata":{"id":"uqQp04YQ0Z6p"},"id":"uqQp04YQ0Z6p"},{"cell_type":"markdown","source":["## 2.1 ReAct"],"metadata":{"id":"1SLDBzC2Aedw"},"id":"1SLDBzC2Aedw"},{"cell_type":"markdown","source":["**Reasoning and Acting**"],"metadata":{"id":"FzOUzSVbAuHd"},"id":"FzOUzSVbAuHd"},{"cell_type":"markdown","source":["\n","\n","**Kernprinzip:** Zyklischer Ablauf von Denken, Handeln und Beobachten.\n","\n","**Charakteristika:**\n","\n","- Explizite Gedankenformulierung vor jeder Aktion\n","- Beobachtung der Ergebnisse nach jeder Aktion\n","- Iterative Anpassung basierend auf Beobachtungen\n","\n","**Reine ReAct-Implementierung:**"],"metadata":{"id":"-br8iZT6Agi-"},"id":"-br8iZT6Agi-"},{"cell_type":"code","execution_count":null,"id":"b93c3150","metadata":{"id":"b93c3150"},"outputs":[],"source":["from langchain import hub\n","from langchain.agents import create_react_agent, AgentExecutor\n","from langchain_openai import ChatOpenAI\n","from datetime import datetime\n","from IPython.display import display, Markdown\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n","\n","# Spezifischer ReAct-Prompt aus LangChain Hub\n","prompt = hub.pull(\"hwchase17/react\")\n","\n","# Bei der Erstellung des Agenten m√ºssen wir sicherstellen, dass die Tools korrekt definiert sind\n","agent = create_react_agent(llm, tools, prompt)\n","\n","# Agent Executor zur Ausf√ºhrung verwenden\n","agent_executor = AgentExecutor(\n","    agent=agent,\n","    tools=tools,\n","    verbose=True,\n","    max_iterations=10,  # Begrenzung der maximalen Iterationen\n","    early_stopping_method=\"force\"  # Erzwingen eines Stopps bei Erreichen der max_iterations\n",")\n","\n","# Aktuelles Datum f√ºr die Anfrage formatieren\n","aktuelles_datum_str = datetime.now().strftime(\"%d.%m.%Y\")\n","\n","# Typisches ReAct-Beispiel\n","query = f\"Wann war die Fu√üball-WM 2022 und wie viele Tage ist das her? Heute ist der {aktuelles_datum_str}.\"\n","result = agent_executor.invoke({\"input\": query})\n","\n","# Ergebnis anzeigen\n","display(Markdown(\"## üèÜ ReAct Agent Ergebnis\"))\n","display(Markdown(result[\"output\"]))\n","\n","# Beispielausgabe:\n","# Die Fu√üball-WM 2022 fand vom 20. November 2022 bis zum 18. Dezember 2022 statt,\n","# und es sind 815 Tage seitdem vergangen."]},{"cell_type":"markdown","id":"6f308473","metadata":{"id":"6f308473"},"source":["## 2.2 MRKL"]},{"cell_type":"markdown","source":["**Modular Reasoning, Knowledge and Language**"],"metadata":{"id":"kDX2E_nYApTP"},"id":"kDX2E_nYApTP"},{"cell_type":"markdown","source":["\n","\n","**Kernprinzip:** Kombination eines zentralen Sprachmodells mit spezialisierten Modulen (Tools).\n","\n","**Charakteristika:**\n","\n","- Modularit√§t: Verschiedene Tools f√ºr spezialisierte Aufgaben\n","- Reasoning: Das Sprachmodell entscheidet, welches Tool zu verwenden ist\n","- Routing: Weiterleitung von Teilaufgaben an die passenden Tools\n","- Integration: Zusammenf√ºhrung der Ergebnisse in eine koh√§rente Antwort\n","\n","**Reine MRKL-Implementierung:**"],"metadata":{"id":"AUh1xmDqAm12"},"id":"AUh1xmDqAm12"},{"cell_type":"code","execution_count":null,"id":"8dd5ff68","metadata":{"id":"8dd5ff68"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","from langchain.agents import AgentExecutor, create_react_agent\n","from langchain import hub\n","from IPython.display import display, Markdown\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n","\n","# Prompt von LangChain Hub laden\n","prompt = hub.pull(\"hwchase17/react\")\n","\n","# MRKL-Agent mit klar spezialisierten Modulen\n","agent = create_react_agent(llm, tools, prompt)\n","mrkl_agent = AgentExecutor(agent=agent, tools=tools, verbose=True)\n","\n","# Typisches MRKL-Beispiel mit verschiedenen Modulen\n","user_request = \"\"\"\n","Wie ist das Wetter in Berlin?\n","Gib mir eine kurze Zusammenfassung zur Geschichte Berlins und\n","berechne die Tage zwischen dem 01.01.2023 und dem 31.12.2023.\n","\"\"\"\n","\n","# Agent ausf√ºhren\n","response = mrkl_agent.invoke({\"input\": user_request})\n","\n","# Ergebnis anzeigen\n","display(Markdown(\"# üß© MRKL Agent Ergebnis\"))\n","display(Markdown(response[\"output\"]))\n","\n","# Beispielausgabe:\n","# Das Wetter in Berlin ist aktuell 8.45¬∞C mit gebrochenen Wolken.\n","# Berlin hat eine reiche und komplexe Geschichte, die bis ins 13. Jahrhundert zur√ºckreicht\n","# und sich √ºber Kriege, Teilungen und Wiedervereinigungen erstreckt.\n","# Die Anzahl der Tage zwischen dem 01.01.2023 und dem 31.12.2023 betr√§gt 364 Tage."]},{"cell_type":"markdown","id":"1a1a7ab8","metadata":{"id":"1a1a7ab8"},"source":["## 2.3 Reflective Agents"]},{"cell_type":"markdown","source":["\n","\n","**Kernprinzip:** Selbstbewertung und -verbesserung durch kritische Analyse der eigenen Outputs.\n","\n","**Charakteristika:**\n","\n","- Self-criticism: Kritische Bewertung der eigenen Ausgaben\n","- Verbesserungszyklen: Mehrfache Iteration zur Qualit√§tsverbesserung\n","- Meta-Kognition: \"Denken √ºber das Denken\"\n","\n","**Reine Reflective Agent-Implementierung:**"],"metadata":{"id":"mLAdxmXYA1gD"},"id":"mLAdxmXYA1gD"},{"cell_type":"code","execution_count":null,"id":"c0718a79","metadata":{"lines_to_next_cell":2,"id":"c0718a79"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","from langchain.prompts import PromptTemplate\n","from langchain.schema.output_parser import StrOutputParser\n","from IPython.display import display, Markdown\n","\n","# LLMs mit unterschiedlichen Einstellungen\n","answer_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)  # Kreativere Erstantwort\n","reflection_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)      # Kritische Bewertung\n","\n","# Prompts f√ºr die drei Phasen des Reflection-Prozesses\n","answer_prompt = PromptTemplate.from_template(\"Beantworte: {question}\")\n","reflection_prompt = PromptTemplate.from_template(\"\"\"\n","Bewerte:\n","Frage: {question}\n","Antwort: {answer}\n","\n","Fehler/Verbesserungen?\"\"\")\n","improvement_prompt = PromptTemplate.from_template(\"\"\"\n","Frage: {question}\n","Antwort: {answer}\n","Reflexion: {reflection}\n","\n","Verbesserte Antwort:\"\"\")\n","\n","# LCEL-Chains f√ºr die drei Phasen\n","answer_chain = answer_prompt | answer_llm | StrOutputParser()\n","reflection_chain = {\n","    \"question\": lambda x: x[\"question\"],\n","    \"answer\": lambda x: x[\"answer\"]\n","} | reflection_prompt | reflection_llm | StrOutputParser()\n","improvement_chain = {\n","    \"question\": lambda x: x[\"question\"],\n","    \"answer\": lambda x: x[\"initial_answer\"],\n","    \"reflection\": lambda x: x[\"reflection\"]\n","} | improvement_prompt | answer_llm | StrOutputParser()\n","\n","# Ausf√ºhrung des Reflective Agent\n","question = \"Hauptursachen des Klimawandels?\"\n","initial_answer = answer_chain.invoke({\"question\": question})\n","reflection = reflection_chain.invoke({\"question\": question, \"answer\": initial_answer})\n","improved_answer = improvement_chain.invoke({\n","    \"question\": question,\n","    \"initial_answer\": initial_answer,\n","    \"reflection\": reflection\n","})\n","\n","# Ergebnisse anzeigen\n","display(Markdown(\"# üîç Reflective Agent Prozess\"))\n","display(Markdown(\"## Initiale Antwort:\"))\n","display(Markdown(initial_answer))\n","display(Markdown(\"## Selbstreflexion:\"))\n","display(Markdown(reflection))\n","display(Markdown(\"## Verbesserte Antwort:\"))\n","display(Markdown(improved_answer))\n","\n","# Beispielausgabe f√ºr verbesserte Antwort:\n","# Die Hauptursachen des Klimawandels lassen sich in zwei Kategorien einteilen: nat√ºrliche und\n","# menschengemachte Ursachen. Die wissenschaftliche Gemeinschaft ist sich einig, dass die aktuellen\n","# Ver√§nderungen des Klimas √ºberwiegend auf menschliche Aktivit√§ten zur√ºckzuf√ºhren sind..."]},{"cell_type":"markdown","id":"a1c80b27","metadata":{"id":"a1c80b27"},"source":["## 2.4 Function-Calling"]},{"cell_type":"markdown","source":["\n","\n","**Kernprinzip:** Standardisierte Schnittstellen f√ºr den Aufruf externer Funktionen und Dienste.\n","\n","**Charakteristika:**\n","- Direkte Integration: Nahtlose Verbindung zu externen Funktionen\n","- Standardisierte Schnittstellen: Klare Definitionen f√ºr Ein- und Ausgaben\n","- Spezialisierte Aktionen: Fokus auf spezifische Funktionalit√§ten\n","\n","**Reine Function-Calling-Implementierung:**"],"metadata":{"id":"DXvAOoBqA6dz"},"id":"DXvAOoBqA6dz"},{"cell_type":"code","execution_count":null,"id":"1bd7e8ca","metadata":{"id":"1bd7e8ca"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","from langchain.agents import AgentExecutor, create_openai_functions_agent\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from IPython.display import display, Markdown\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n","\n","# Prompt f√ºr den Function-Calling Agent\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent, der Informationen √ºber das Wetter und andere Fakten liefert.\"),\n","    (\"human\", \"{input}\"),\n","    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n","])\n","\n","# Function-Calling Agent (fokussiert auf direkten Funktionsaufruf)\n","function_agent = create_openai_functions_agent(llm, tools, prompt)\n","agent_executor = AgentExecutor.from_agent_and_tools(\n","    agent=function_agent,\n","    tools=tools,\n","    verbose=True\n",")\n","\n","# Typisches Function-Calling Beispiel\n","location = \"Berlin\"\n","response = agent_executor.invoke({\n","    \"input\": f\"Wie ist das Wetter in {location}? und was gibt es f√ºr allgemeine Informationen zu {location}?\",\n","})\n","\n","# Ergebnis anzeigen\n","display(Markdown(\"# üå°Ô∏è Function-Calling Agent Ergebnis\"))\n","display(Markdown(response[\"output\"]))\n","\n","# Beispielausgabe:\n","# Das aktuelle Wetter in Berlin ist 8.38¬∞C mit vereinzelten Wolken.\n","# Berlin ist die Hauptstadt Deutschlands und eine bedeutende kulturelle,\n","# politische und wirtschaftliche Metropole in Europa."]},{"cell_type":"markdown","source":["## 2.5 Fazit"],"metadata":{"id":"ULARoWo8yRyZ"},"id":"ULARoWo8yRyZ"},{"cell_type":"markdown","source":["  \n","Die Entwicklung von KI-Agentenarchitekturen hat in den letzten Jahren eine rasche Evolution durchlaufen:\n","\n","+ Fr√ºhe regelbasierte Agenten: Vor dem Aufkommen moderner LLMs basierten Agenten oft auf vordefinierten Regeln und einfachen Entscheidungsb√§umen.\n","+ Tool-augmentierte Sprachmodelle: Mit der Entwicklung leistungsf√§higer LLMs wurden erste Systeme geschaffen, die Sprachmodelle mit externen Tools verbinden.\n","+ Transparente Reasoning-Architekturen: Architekturen wie Chain-of-Thought und ReAct f√ºhrten explizite Reasoning-Schritte ein, um die Entscheidungsfindung nachvollziehbarer zu machen.\n","+ Modulare Systeme: Ans√§tze wie MRKL erm√∂glichten die Integration spezialisierter Module f√ºr unterschiedliche Aufgabentypen.\n","+ Autonome Agentenarchitekturen: Neuere Systeme wie AutoGPT und BabyAGI f√ºhrten zu selbstst√§ndigeren Agenten, die eigenst√§ndig Ziele verfolgen k√∂nnen.\n","+ Selbstreflektierende Agenten: Die neueste Generation von Agenten kann ihre eigenen Entscheidungen und Aktionen analysieren und verbessern."],"metadata":{"id":"zKJPmXS70qZF"},"id":"zKJPmXS70qZF"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Gegen√ºberstellung\n","</font></p>"],"metadata":{"id":"Q6PQ9K9zBZww"},"id":"Q6PQ9K9zBZww"},{"cell_type":"markdown","source":["| Aspekt | ReAct | MRKL | Reflective Agent | Function-Calling |\n","|--------|-------|------|------------------|------------------|\n","| Hauptfokus | Reasoning-Handlungs-Zyklus | Modulare Spezialisierung | Selbstverbesserung | Funktionsintegration |\n","| Arbeitsweise | Zyklischer Ablauf von Denken und Handeln | Routing zu spezialisierten Modulen | Mehrfache Iteration mit Selbstkritik | Direkte Ausf√ºhrung externer Funktionen |\n","| Typischer Anwendungsfall | Mehrstufige Recherche, Probleml√∂sung mit Begr√ºndung | Komplexe Fragen, die verschiedene Expertisen erfordern | Content-Erstellung, Code-Review, Fehlersuche | Wetterabfragen, Terminplanung, Datenbankabfragen |\n","| Flexibilit√§t | Mittel | Hoch | Mittel | Begrenzt auf definierte Funktionen |\n","| Transparenz | Hoch (durch explizite Gedanken) | Mittel | Sehr hoch | Mittel |\n","| Komplexit√§tsbew√§ltigung | Gut f√ºr sequentielle Probleme | Gut f√ºr diverse Problemtypen | Gut f√ºr komplizierte Aufgaben | Gut f√ºr definierte Aufgaben |\n","| Implementation | Prompting-Techniken | Modulare Architektur | Iteration und Feedback | API-Definitionen |"],"metadata":{"id":"vtbTCB0Lw-Xc"},"id":"vtbTCB0Lw-Xc"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Kombinationsm√∂glichkeiten\n","</font></p>"],"metadata":{"id":"nVvMVIFF1FYe"},"id":"nVvMVIFF1FYe"},{"cell_type":"markdown","source":["Die St√§rke moderner KI-Agenten liegt oft in der Kombination dieser Architekturen:\n","\n","- **ReAct + Function-Calling**: Verbindet strukturierte API-Aufrufe mit transparentem Reasoning\n","- **MRKL + Reflexion**: Modulare Systeme mit Selbstverbesserungsf√§higkeit\n","- **Function-Calling + MRKL**: Standardisierte Schnittstellen f√ºr modulare Systeme\n"],"metadata":{"id":"BayCD4QL1Gf9"},"id":"BayCD4QL1Gf9"},{"cell_type":"markdown","id":"8fde7445","metadata":{"id":"8fde7445"},"source":["\n","# **3 <font color='orange'>|</font> Erweitere Konzepte**\n","---"]},{"cell_type":"markdown","source":["Nach der  Darstellung der grundlegenden Architekturen folgen nun erweiterte Konzepte, die auf diesen Architekturen aufbauen oder sie kombinieren."],"metadata":{"id":"UfiZ9JTGBy75"},"id":"UfiZ9JTGBy75"},{"cell_type":"markdown","source":["## 3.1 Einfache Planung"],"metadata":{"id":"DHZACro3CDvd"},"id":"DHZACro3CDvd"},{"cell_type":"markdown","source":["\n","\n","Planung und Zielverfolgung ist kein eigenst√§ndiger Architekturtyp, sondern ein Anwendungskonzept, das auf den grundlegenden Architekturen aufbaut.\n","\n","**Kombination:** Meist basierend auf ReAct oder Reflective Agents"],"metadata":{"id":"t6hNIqwvCMS6"},"id":"t6hNIqwvCMS6"},{"cell_type":"code","execution_count":null,"id":"f097e07f","metadata":{"id":"f097e07f"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import PromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from IPython.display import display, Markdown\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n","\n","# Chain-of-Thought Prompt erstellen\n","cot_prompt = PromptTemplate(\n","    input_variables=[\"problem\"],\n","    template=\"\"\"\n","    Zerlege das folgende Problem in kleinere Teilschritte und l√∂se es Schritt f√ºr Schritt:\n","\n","    Problem: {problem}\n","\n","    Denke schrittweise:\n","    1. Welche Teilprobleme m√ºssen gel√∂st werden?\n","    2. Wie l√∂se ich jeden Teilschritt?\n","    3. Wie kombiniere ich die Teill√∂sungen zur Gesamtl√∂sung?\n","    \"\"\"\n",")\n","\n","# LCEL-Pipeline erstellen und ausf√ºhren\n","planning_chain = (\n","    {\"problem\": RunnablePassthrough()}\n","    | cot_prompt\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","# Beispielanwendung\n","complex_problem = \"\"\"\n","Berechne die Gesamtkosten f√ºr einen Einkauf von 3 B√ºchern zu je 12,99 ‚Ç¨,\n","2 Notizb√ºchern zu je 4,50 ‚Ç¨ und einem Stift f√ºr 2,25 ‚Ç¨,\n","wenn auf B√ºcher 7% Mehrwertsteuer und auf andere Artikel 19% Mehrwertsteuer anf√§llt.\n","Verwende f√ºr Formeln das Format $ Formel $.\n","\"\"\"\n","\n","solution = planning_chain.invoke(complex_problem)\n","\n","# Ergebnis anzeigen\n","display(Markdown(\"# üìù Einfache Planung mit Chain-of-Thought\"))\n","display(Markdown(solution))\n","\n","# Beispielausgabe:\n","# Um die Gesamtkosten zu berechnen, muss ich die Kosten f√ºr die B√ºcher und die anderen\n","# Artikel separat berechnen, die Mehrwertsteuer hinzuf√ºgen und dann alles addieren..."]},{"cell_type":"markdown","id":"c1569b35","metadata":{"id":"c1569b35"},"source":["## 3.2 Fortgeschrittene Planung"]},{"cell_type":"markdown","source":["\n","\n","**Kombination:** Oft MRKL + Reflective Agents f√ºr hierarchische Planung"],"metadata":{"id":"rMPGRgPbCSa3"},"id":"rMPGRgPbCSa3"},{"cell_type":"code","execution_count":null,"id":"efb1d4bf","metadata":{"id":"efb1d4bf"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import PromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from IPython.display import display, Markdown\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n","\n","def create_planner_chain():\n","    \"\"\"Erstellt eine Chain f√ºr die hierarchische Planung.\"\"\"\n","    planer_prompt = PromptTemplate.from_template(\n","        \"\"\"\n","        Erstelle einen hierarchischen Plan, um das folgende Ziel zu erreichen:\n","\n","        Ziel: {objective}\n","\n","        Teile den Plan in:\n","        1. Strategische Ziele (hohe Ebene)\n","        2. Taktische Schritte (mittlere Ebene)\n","        3. Konkrete Aktionen (detaillierte Ebene)\n","\n","        F√ºr jede konkrete Aktion gib an, welche Tools oder Ressourcen ben√∂tigt werden.\n","        \"\"\"\n","    )\n","    return ({\"objective\": RunnablePassthrough()} | planer_prompt | llm | StrOutputParser())\n","\n","def create_execution_chain():\n","    \"\"\"Erstellt eine Chain f√ºr die Anpassung des Plans basierend auf Feedback.\"\"\"\n","    execution_prompt = PromptTemplate.from_template(\n","        \"\"\"\n","        Hier ist der aktuelle Plan:\n","\n","        {plan}\n","\n","        Basierend auf dem folgenden Feedback, passe den Plan an:\n","\n","        Feedback: {feedback}\n","\n","        √úberarbeiteter Plan:\n","        \"\"\"\n","    )\n","    return ({\"plan\": RunnablePassthrough(), \"feedback\": RunnablePassthrough()}\n","            | execution_prompt | llm | StrOutputParser())\n","\n","# Planungsketten erstellen\n","planner_chain = create_planner_chain()\n","execution_chain = create_execution_chain()\n","\n","# Beispiel ausf√ºhren\n","objective = \"Organisiere eine zweit√§gige Konferenz f√ºr 100 Teilnehmer zum Thema KI-Ethik\"\n","initial_plan = planner_chain.invoke(objective)\n","\n","# Feedback geben und Plan anpassen\n","feedback = \"Der Budgetrahmen wurde auf 15.000 ‚Ç¨ reduziert, und wir m√ºssen die Konferenz auf einen Tag verk√ºrzen.\"\n","revised_plan = execution_chain.invoke({\"plan\": initial_plan, \"feedback\": feedback})\n","\n","# Ergebnisse anzeigen\n","display(Markdown(\"# üìî Hierarchische Planung\"))\n","display(Markdown(\"## Initialer Plan\"))\n","display(Markdown(initial_plan))\n","display(Markdown(\"## √úberarbeiteter Plan\"))\n","display(Markdown(revised_plan))\n","\n","# Beispielausgabe f√ºr √ºberarbeiteten Plan:\n","# # √úberarbeiteter Plan f√ºr eint√§gige KI-Ethik Konferenz\n","#\n","# # Strategische Ziele\n","# 1. Durchf√ºhrung einer gehaltvollen, komprimierten eint√§gigen Konferenz zum Thema KI-Ethik\n","# 2. Einhaltung des reduzierten Budgetrahmens von 15.000 ‚Ç¨\n","# ..."]},{"cell_type":"markdown","id":"f8add875","metadata":{"id":"f8add875"},"source":["## 3.3 Zielorientierter Agent"]},{"cell_type":"markdown","source":["\n","\n","**Kombination:** Function-Calling + ReAct f√ºr zielgerichtete Aktionen"],"metadata":{"id":"3MDVgC3jCUTv"},"id":"3MDVgC3jCUTv"},{"cell_type":"code","execution_count":null,"id":"ee0da4fa","metadata":{"lines_to_next_cell":2,"id":"ee0da4fa"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","from langchain_core.tools import Tool\n","from langchain_community.utilities import SerpAPIWrapper\n","from langchain.agents import create_openai_functions_agent\n","from langchain.agents import AgentExecutor\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain.memory import ConversationBufferMemory\n","from IPython.display import display, Markdown\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n","\n","# Prompt erstellen mit ChatPromptTemplate\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent, der Informationen recherchiert. \"\n","                \"Nutze die bereitgestellten Tools, um pr√§zise und aktuelle Informationen zu finden.\"),\n","    MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n","    (\"human\", \"{input}\"),\n","    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n","])\n","\n","# Memory f√ºr Chat-Verlauf\n","memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n","\n","# Agent mit OpenAI Functions erstellen\n","agent = create_openai_functions_agent(llm, tools, prompt)\n","\n","# Agent-Executor konfigurieren\n","agent_executor = AgentExecutor(\n","    agent=agent,\n","    tools=tools,\n","    memory=memory,\n","    verbose=True,\n","    handle_parsing_errors=True\n",")\n","\n","# Komplexe Anfrage\n","complex_goal = \"\"\"\n","Finde heraus, wer der aktuelle Bundeskanzler von Deutschland ist,\n","wann er geboren wurde und erstelle eine kurze Zusammenfassung\n","seiner politischen Karriere.\n","\"\"\"\n","\n","# Agent ausf√ºhren\n","result = agent_executor.invoke({\"input\": complex_goal})\n","\n","# Ergebnis anzeigen\n","display(Markdown(\"# üîé Zielorientierter Agent\"))\n","display(Markdown(result[\"output\"]))\n","\n","# Beispielausgabe:\n","# Der aktuelle Bundeskanzler von Deutschland ist Olaf Scholz. Er wurde am 14. Juni 1958\n","# in Osnabr√ºck geboren. Scholz ist Mitglied der SPD und hat vor seinem Amt als Bundeskanzler\n","# verschiedene politische Positionen inne gehabt..."]},{"cell_type":"markdown","id":"19b9a4b1","metadata":{"id":"19b9a4b1"},"source":["## 3.4 Kollaborative Analyse"]},{"cell_type":"markdown","source":["\n","\n","**Kombination:** Spezialisierte Agenten mit verschiedenen Architekturen"],"metadata":{"id":"0kUPFGQbCYjN"},"id":"0kUPFGQbCYjN"},{"cell_type":"code","execution_count":null,"id":"8b107657","metadata":{"id":"8b107657"},"outputs":[],"source":["from IPython.display import display, Markdown\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import PromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# LCEL-Pipelines mit unterschiedlichen Temperaturen und Rollen definieren\n","research_pipeline = PromptTemplate.from_template(\n","    \"\"\"Du bist ein Recherche-Spezialist. Sammle Informationen zum Thema: {topic}\"\"\"\n",") | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)  # MRKL-√§hnlich f√ºr Informationssuche\n","\n","critique_pipeline = PromptTemplate.from_template(\n","    \"\"\"Du bist ein kritischer Analyst. Bewerte folgende Recherche: {research}\"\"\"\n",") | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  # Reflective-Agent f√ºr kritische Analyse\n","\n","synthesis_pipeline = PromptTemplate.from_template(\n","    \"\"\"Du bist ein Informationssynthetisierer. Erstelle eine Zusammenfassung:\n","    Recherche: {research}\n","    Kritik: {critique}\"\"\"\n",") | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)  # ReAct-√§hnlich f√ºr Verarbeitung mehrerer Inputs\n","\n","decision_pipeline = PromptTemplate.from_template(\n","    \"\"\"Du bist ein Entscheidungstr√§ger. Basierend auf: {synthesis}\n","    Beantworte: {question}\"\"\"\n",") | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  # Function-Calling-√§hnlich f√ºr finale Entscheidung\n","\n","def collaborative_analysis(topic, question):\n","    \"\"\"Kollaborative Analyse mit LCEL-Pipelines.\"\"\"\n","    # Prozess ausf√ºhren\n","    research = research_pipeline.invoke({\"topic\": topic})\n","    display(Markdown(\"## üîç Recherche abgeschlossen\"))\n","\n","    critique = critique_pipeline.invoke({\"research\": research})\n","    display(Markdown(\"## üå°Ô∏è Kritische Analyse abgeschlossen\"))\n","\n","    synthesis = synthesis_pipeline.invoke({\"research\": research, \"critique\": critique})\n","    display(Markdown(\"## ‚ú® Synthese abgeschlossen\"))\n","\n","    decision = decision_pipeline.invoke({\"synthesis\": synthesis, \"question\": question})\n","    display(Markdown(\"## üìî Entscheidung getroffen\"))\n","\n","    return {\n","        \"research\": research,\n","        \"critique\": critique,\n","        \"synthesis\": synthesis,\n","        \"decision\": decision\n","    }\n","\n","# Beispielanwendung\n","topic = \"K√ºnstliche Intelligenz in der Gesundheitsversorgung\"\n","question = \"Sollten Krankenh√§user KI-Systeme zur Diagnoseunterst√ºtzung einsetzen?\"\n","\n","# Multi-Agenten-Analyse ausf√ºhren\n","display(Markdown(\"# üë• Multi-Agenten-System: Kollaborative Analyse\"))\n","results = collaborative_analysis(topic, question)\n","\n","# Finale Entscheidung anzeigen\n","display(Markdown(f\"# üìë Finale Entscheidung\"))\n","display(Markdown(f\"{results['decision'].content}\"))\n","\n","# Beispielausgabe:\n","# Basierend auf der vorliegenden Synthese sollten Krankenh√§user KI-Systeme zur\n","# Diagnoseunterst√ºtzung einsetzen, jedoch unter bestimmten Bedingungen und mit\n","# angemessenen Vorsichtsma√ünahmen..."]},{"cell_type":"markdown","id":"525b252b","metadata":{"id":"525b252b"},"source":["## 3.5 Agent-Kommunikation"]},{"cell_type":"markdown","source":["\n","\n","**Kombination:** Agenten mit Function-Calling f√ºr standardisierte Kommunikation"],"metadata":{"id":"Kz9b7MTkCao9"},"id":"Kz9b7MTkCao9"},{"cell_type":"code","execution_count":null,"id":"00d5132f","metadata":{"id":"00d5132f"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain.memory import ConversationBufferMemory\n","from langchain.agents import AgentExecutor, create_openai_functions_agent\n","from langchain.tools import Tool\n","from IPython.display import Markdown, display\n","\n","class ModernAgent:\n","    \"\"\"Moderner Agent f√ºr Multi-Agenten-Kommunikation mit LangChain\"\"\"\n","    def __init__(self, name, role, llm, tools=tools):\n","        self.name = name\n","        self.role = role\n","        self.llm = llm\n","        self.memory = ConversationBufferMemory(return_messages=True)\n","\n","        # System-Prompt definieren\n","        system_prompt = f\"Du bist {self.name}, ein KI-Agent mit der Rolle: {self.role}\"\n","\n","        # Mindestens ein Tool bereitstellen (wird f√ºr alle Agenten ben√∂tigt)\n","        self.tools = tools\n","\n","        # Prompt-Template erstellen (mit agent_scratchpad f√ºr OpenAI Functions Agent)\n","        prompt = ChatPromptTemplate.from_messages([\n","            (\"system\", system_prompt),\n","            (\"human\", \"{input}\"),\n","            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n","        ])\n","\n","        # Agent erstellen\n","        self.agent = create_openai_functions_agent(llm=self.llm, tools=self.tools, prompt=prompt)\n","\n","        # Agent-Executor erstellen\n","        self.agent_executor = AgentExecutor(agent=self.agent, tools=self.tools, verbose=True)\n","\n","    def receive_message(self, message, sender):\n","        \"\"\"Speichert eine empfangene Nachricht im Ged√§chtnis.\"\"\"\n","        self.memory.chat_memory.add_message(HumanMessage(content=f\"{sender}: {message}\"))\n","\n","    def process_and_respond(self, query=None):\n","        \"\"\"Verarbeitet den Kontext und generiert eine Antwort mit dem Agent-Executor.\"\"\"\n","        # Kontext aus dem Speicher holen\n","        memory_variables = self.memory.load_memory_variables({})\n","        memory_messages = memory_variables.get(\"history\", [])\n","        memory_context = \"\\n\".join([f\"{msg.type}: {msg.content}\" for msg in memory_messages])\n","\n","        # Eingabe f√ºr den Agent erstellen\n","        input_text = f\"Kontext der bisherigen Konversation:\\n{memory_context}\\n\\n\"\n","        input_text += f\"Aufgabe/Frage: {query or 'Reagiere auf die letzte Nachricht.'}\"\n","\n","        # Antwort mit dem Agent-Executor generieren\n","        response = self.agent_executor.invoke({\"input\": input_text})\n","\n","        # Antwort zum Speicher hinzuf√ºgen\n","        self.memory.chat_memory.add_message(AIMessage(content=response[\"output\"]))\n","\n","        return response[\"output\"]\n","\n","# Multi-Agenten-System demonstrieren\n","display(Markdown(\"# ü§ù Multi-Agenten-System: Kommunikation zwischen Spezialisten\"))\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n","\n","# Agenten mit verschiedenen Rollen erstellen\n","researcher = ModernAgent(\"Forscher\", \"Sammle und analysiere Informationen zu einem Thema\", llm, tools=tools)\n","critic = ModernAgent(\"Kritiker\", \"Bewerte Informationen kritisch und identifiziere m√∂gliche Fehler\", llm)\n","summarizer = ModernAgent(\"Zusammenfasser\", \"Erstelle pr√§gnante Zusammenfassungen von Informationen\", llm)\n","\n","# Simulierte Interaktion zwischen Agenten\n","researcher.receive_message(\"Recherchiere die Vor- und Nachteile erneuerbarer Energien\", \"Benutzer\")\n","research_result = researcher.process_and_respond()\n","\n","critic.receive_message(research_result, \"Forscher\")\n","critique = critic.process_and_respond()\n","\n","summarizer.receive_message(research_result, \"Forscher\")\n","summarizer.receive_message(critique, \"Kritiker\")\n","final_summary = summarizer.process_and_respond(\"Erstelle eine ausgewogene Zusammenfassung √ºber erneuerbare Energien\")\n","\n","# Anzeige der Ergebnisse\n","display(Markdown(f\"# üîç Forschungsergebnis\"))\n","display(Markdown(f\"{research_result}\\n\"))\n","display(Markdown(f\"# üì£ Kritik\"))\n","display(Markdown(f\"{critique}\\n\"))\n","display(Markdown(f\"# üìì Endg√ºltige Zusammenfassung\"))\n","display(Markdown(f\"{final_summary}\"))\n","\n","# Beispielausgabe:\n","# Erneuerbare Energien bieten zahlreiche Vorteile wie Nachhaltigkeit, reduzierte Umweltbelastung\n","# und wirtschaftliche Chancen, stehen jedoch vor Herausforderungen wie Intermittenz,\n","# Speicherbedarf und anf√§nglichen Investitionskosten..."]},{"cell_type":"markdown","metadata":{"id":"f7989ab6"},"source":["\n","# **4 <font color='orange'>|</font> Ethik & Verantwortung**\n","---"],"id":"f7989ab6"},{"cell_type":"markdown","source":["## 4.1 Autonomie und Kontrolle"],"metadata":{"id":"J0HxAUvUDXr1"},"id":"J0HxAUvUDXr1"},{"cell_type":"markdown","source":["\n","\n","**Herausforderung der Autonomie:**\n","\n","- Spannungsfeld zwischen Autonomie und Kontrolle bei KI-Agenten\n","- Risiken bei zu viel Handlungsfreiheit (Ressourcenverschwendung, unbeabsichtigte Aktionen)\n","- Notwendigkeit von \"Guardrails\" und Stopp-Mechanismen\n","\n","**Technische L√∂sungsans√§tze:**"],"metadata":{"id":"u7UE33D1Dv5F"},"id":"u7UE33D1Dv5F"},{"cell_type":"code","execution_count":null,"metadata":{"id":"8014b670"},"outputs":[],"source":["# Implementierung von Sicherheitsmechanismen\n","agent_executor = AgentExecutor(\n","    agent=agent,\n","    tools=tools,\n","    max_iterations=15,                # Verhindert Endlosschleifen\n","    early_stopping_method=\"force\",    # Erzwingt einen Stopp nach max_iterations\n","    max_execution_time=60,            # Zeitlimit in Sekunden\n","    handle_parsing_errors=True,       # Robustes Fehlerhandling\n","    tool_error_handling=\"continue\"    # Bei Tool-Fehlern weitermachen, statt abzubrechen\n",")"],"id":"8014b670"},{"cell_type":"markdown","metadata":{"id":"1f807c8f"},"source":["## 4.2 Verantwortung und Haftung"],"id":"1f807c8f"},{"cell_type":"markdown","source":["\n","\n","- Wer tr√§gt die Verantwortung f√ºr Agenten-Entscheidungen?\n","- Haftungskette: Entwickler, Betreiber, Nutzer\n","- Rechtliche Rahmenbedingungen und ihre L√ºcken\n","- Dokumentationspflichten bei autonomen Systemen\n","\n"],"metadata":{"id":"5EQZxBfpDzoH"},"id":"5EQZxBfpDzoH"},{"cell_type":"markdown","source":["## 4.3 Verzerrungen und Fairness"],"metadata":{"id":"wao2oij-D2Gh"},"id":"wao2oij-D2Gh"},{"cell_type":"markdown","source":["\n","\n","**Systematische Verzerrungen in Agenten:**\n","\n","- √úbertragung von Biases aus Trainingsdaten\n","- Verst√§rkung durch iterative Entscheidungsprozesse\n","- Besondere Risiken bei Multi-Agenten-Systemen (Echokammern)\n","\n","**Implementierung von Fairness-Checks:**"],"metadata":{"id":"fsQPgJeYD36S"},"id":"fsQPgJeYD36S"},{"cell_type":"code","source":["from IPython.display import display, Markdown\n","\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import PromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# LLM f√ºr Fairness-Pr√ºfung initialisieren\n","fairness_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  # Niedrige Temperatur f√ºr objektivere Analyse\n","\n","# Prompt f√ºr die Bias-Pr√ºfung definieren\n","fairness_prompt = PromptTemplate.from_template(\"\"\"\n","√úberpr√ºfe folgende Antwort auf m√∂gliche Verzerrungen,\n","Stereotypen oder unfaire Behandlung bestimmter Gruppen:\n","\n","Frage: {question}\n","Antwort: {answer}\n","\n","Identifiziere jede Form von Bias oder Unfairness:\n","\"\"\")\n","\n","# Pipeline erstellen\n","fairness_check = fairness_prompt | fairness_llm | StrOutputParser()\n","\n","# Beispielfrage und -antwort definieren\n","question = \"Wer eignet sich am besten f√ºr F√ºhrungspositionen in der Technikbranche?\"\n","initial_answer = \"M√§nner mit technischem Hintergrund eignen sich am besten f√ºr F√ºhrungspositionen in der Technikbranche, da sie typischerweise analytischer denken und besser mit Stress umgehen k√∂nnen.\"\n","\n","# Fairness-Check durchf√ºhren\n","fairness_result = fairness_check.invoke({\n","    \"question\": question,\n","    \"answer\": initial_answer\n","})\n","\n","# Ergebnis ausgeben\n","display(Markdown(\"# ‚öñÔ∏è Fairness-Pr√ºfung\"))\n","display(Markdown(fairness_result))"],"metadata":{"id":"l0xxGAvOEItc"},"id":"l0xxGAvOEItc","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ad8a13b"},"outputs":[],"source":["# Reflective Agent mit Bias-Pr√ºfung\n","fairness_prompt = PromptTemplate.from_template(\"\"\"\n","√úberpr√ºfe folgende Antwort auf m√∂gliche Verzerrungen,\n","Stereotypen oder unfaire Behandlung bestimmter Gruppen:\n","\n","Frage: {question}\n","Antwort: {answer}\n","\n","Identifiziere jede Form von Bias oder Unfairness:\n","\"\"\")\n","\n","fairness_check = fairness_prompt | fairness_llm | StrOutputParser()\n","fairness_result = fairness_check.invoke({\n","    \"question\": question,\n","    \"answer\": initial_answer\n","})"],"id":"9ad8a13b"},{"cell_type":"markdown","metadata":{"id":"ebdbfb4f"},"source":["## 4.4 Ethische Grundprinzipien"],"id":"ebdbfb4f"},{"cell_type":"markdown","source":["\n","\n","- Transparenz: Nachvollziehbarkeit von Entscheidungen\n","- Menschliche Kontrolle: Stets die M√∂glichkeit des menschlichen Eingreifens\n","- Schadensvermeidung: Priorisierung der Sicherheit\n","- Datenschutz: Minimierung der Datenzugriffe\n","- Praktische Umsetzung dieser Prinzipien im Entwicklungsprozess\n","\n"],"metadata":{"id":"6XYE3Kb4EhVK"},"id":"6XYE3Kb4EhVK"},{"cell_type":"markdown","source":["# **5 <font color='orange'>|</font> Evaluation von Agents**\n","---"],"metadata":{"id":"LmaTvOQqEjJk"},"id":"LmaTvOQqEjJk"},{"cell_type":"markdown","source":["## 5.1 Evaluationsrahmenwerk"],"metadata":{"id":"I4RyxVMoEwAd"},"id":"I4RyxVMoEwAd"},{"cell_type":"markdown","source":["\n","\n","**Mehrdimensionales Bewertungssystem:**\n","\n","- Effektivit√§t: Zielerreichungsgrad\n","- Effizienz: Ressourcenverbrauch (Zeit, API-Calls, Tokens)\n","- Robustheit: Verhalten bei unerwarteten Inputs\n","- Sicherheit: Vermeidung gef√§hrlicher Aktionen\n","\n","**Implementierung einer Evaluationspipeline:**"],"metadata":{"id":"0S0DDDYvE5DM"},"id":"0S0DDDYvE5DM"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ef267e8f"},"outputs":[],"source":["from langchain.smith import RunEvalConfig, run_on_dataset\n","\n","# Vereinfachte Evaluationskonfiguration ohne TraceAnalyzer\n","eval_config = RunEvalConfig(\n","    evaluators=[\n","        \"qa\",                          # Qualit√§t der Antworten\n","        \"context_relevance\",           # Relevanz verwendeter Informationen\n","        \"helpfulness\",                 # N√ºtzlichkeit f√ºr den Nutzer\n","        \"correctness\",                 # Faktische Korrektheit\n","    ],\n","    custom_metrics={\n","        \"completed_in_time\": lambda run: run.execution_time < 30,\n","        \"used_appropriate_tools\": lambda run: analyze_tool_selection(run)\n","    }\n",")"],"id":"ef267e8f"},{"cell_type":"markdown","metadata":{"id":"19e68b06"},"source":["## 5.2 Quantitative Metriken"],"id":"19e68b06"},{"cell_type":"markdown","source":["\n","\n","- **Erfolgsrate:** Anteil erfolgreich gel√∂ster Aufgaben\n","- **Effizienzmetriken:**\n","    - Anzahl der Reasoning-Schritte\n","    - Anzahl der Tool-Aufrufe\n","    - Token-Verbrauch\n","    - Ausf√ºhrungszeit\n","- **Fehleranalyse:**\n","    - Typische Fehlermuster\n","    - Kritische Pfade bei Fehlschl√§gen\n","\n"],"metadata":{"id":"NulipbxXFiGP"},"id":"NulipbxXFiGP"},{"cell_type":"markdown","source":["## 5.3 Qualitative Evaluation"],"metadata":{"id":"VRj8PdIdFjs6"},"id":"VRj8PdIdFjs6"},{"cell_type":"markdown","source":["\n","\n","- Menschliche Bewertung der Agenten-Outputs\n","- Think-aloud Protokolle zur Bewertung des Reasoning\n","- A/B-Tests verschiedener Agentenarchitekturen\n","- Usability und Nutzererfahrung\n","\n"],"metadata":{"id":"reDzxgFmFn73"},"id":"reDzxgFmFn73"},{"cell_type":"markdown","source":["## 5.4 Kontinuierliche Verbesserung"],"metadata":{"id":"N4fBjKEdFpDy"},"id":"N4fBjKEdFpDy"},{"cell_type":"markdown","source":["# **A <font color='orange'>|</font> Aufgabe**\n","---"],"metadata":{"id":"Pzc1rzQlNV8J"},"id":"Pzc1rzQlNV8J"},{"cell_type":"markdown","source":["Die Aufgabestellungen unten bieten Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen."],"metadata":{"id":"QQUImb-86GUw"},"id":"QQUImb-86GUw"},{"cell_type":"markdown","source":["Gegeben ist eine Datei, die eine Reihe von Gleichungen enth√§lt.\n","Der Dateiname ist GenAI/02 data/gleichungen.txt\n","\n","**Gleichung:**    \n","41748459 - 87226336    \n","92995162 * 46769739    \n","61530438 * 56074589    \n","95329602 + 45418854    \n","412907 + 3731910    \n","...\n","\n","Verwenden Sie einen LangChain-Agenten mit einem Tool, um jede dieser Gleichungen zu berechnen, und senden Sie eine Datei √§hnlich dieser:\n","\n","**Ergebnisse:**  \n","41748459 - 87226336 = 45477877   \n","92995162 * 46769739 = 4349359455002718   \n","61530438 * 56074589 = 3450294021839982   \n","95329602 + 45418854 = 140748456   \n","412907 + 3731910 = 4144817   \n","... ...\n","\n"],"metadata":{"id":"Y5y-OP9OemcW"},"id":"Y5y-OP9OemcW"},{"cell_type":"markdown","metadata":{"id":"3cd6645a"},"source":["Mit dieser Aufgabe wenden die Studierenden das Gelernte √ºber Agentenarchitekturen praktisch an. Der Agent verwendet dabei Function-Calling, um:\n","1. Eine Datei zu lesen (Umgebungsinteraktion)\n","2. Mathematische Gleichungen zu identifizieren und zu berechnen\n","3. Die Ergebnisse in einer neuen Datei zu speichern\n","\n","Dies demonstriert die praktische Anwendung von Tools innerhalb einer Agentenarchitektur f√ºr eine reale Aufgabe."],"id":"3cd6645a"},{"cell_type":"markdown","source":["\n","\n","**Feedback-Schleife:**"],"metadata":{"id":"THh3mimaFqvn"},"id":"THh3mimaFqvn"},{"cell_type":"code","execution_count":null,"metadata":{"id":"68cfe45e"},"outputs":[],"source":["# Sammlung von Nutzerfeedback\n","def collect_feedback(agent_response, user_rating, user_comment):\n","    feedback_data = {\n","        \"input\": agent_response[\"input\"],\n","        \"output\": agent_response[\"output\"],\n","        \"rating\": user_rating,  # z.B. 1-5 Sterne\n","        \"comment\": user_comment,\n","        \"trace\": agent_response[\"intermediate_steps\"]\n","    }\n","    # Speichern f√ºr sp√§tere Analyse und Modellverbesserung\n","    feedback_db.insert(feedback_data)\n","\n","    # Kritische Fehler sofort melden\n","    if user_rating < 2:\n","        alert_development_team(feedback_data)\n","\n","    return \"Feedback erfolgreich gespeichert\""],"id":"68cfe45e"}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"colab":{"provenance":[],"collapsed_sections":["099e73fa","4275c095","1SLDBzC2Aedw","6f308473","1a1a7ab8","a1c80b27","ULARoWo8yRyZ","8fde7445","DHZACro3CDvd","c1569b35","f8add875","19b9a4b1","525b252b","f7989ab6","J0HxAUvUDXr1","1f807c8f","wao2oij-D2Gh","ebdbfb4f","LmaTvOQqEjJk","I4RyxVMoEwAd","19e68b06","VRj8PdIdFjs6","N4fBjKEdFpDy","Pzc1rzQlNV8J"],"toc_visible":true},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}
{"cells":[{"cell_type":"markdown","source":["<p><font size=\"7\" color='grey'> <b>\n","Anwendung Generativer KI\n","</b></font> </br></p>"],"metadata":{"id":"Ih2CTVBnArVZ"},"id":"Ih2CTVBnArVZ"},{"cell_type":"markdown","source":["<p><font size=\"6\" color='grey'> <b>\n","Agents\n","</b></font> </br></p>\n","\n","\n","---"],"metadata":{"id":"6jJZ7wbdArVc"},"id":"6jJZ7wbdArVc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfdhPIzcEYRG","cellView":"form","collapsed":true},"outputs":[],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  Colab-Umfeld</font> </br></p>\n","# Installierte Python Version\n","import sys\n","print(f\"Python Version: \",sys.version)\n","# Installierte LangChain Bibliotheken\n","print()\n","print(\"Installierte LangChain Bibliotheken:\")\n","!pip list | grep '^langchain'\n","# Unterdrückt die \"DeprecationWarning\" von LangChain für die Memory-Funktionden\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")"],"id":"dfdhPIzcEYRG"},{"cell_type":"code","source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  SetUp API-Keys (setup_api_keys)</font> </br></p>\n","\n","def setup_api_keys():\n","    \"\"\"Konfiguriert alle benötigten API-Keys aus Google Colab userdata\"\"\"\n","    from google.colab import userdata\n","    import os\n","    from os import environ\n","\n","    # Dictionary der benötigten API-Keys\n","    keys = {\n","        'OPENAI_API_KEY': 'OPENAI_API_KEY',\n","        'SERPAPI_API_KEY': 'SERPAPI_API_KEY',\n","        'WEATHER_API_KEY': 'WEATHER_API_KEY',\n","        'HF_TOKEN': 'HF_TOKEN',\n","        'GOOGLE_API_KEY': 'GOOGLE_API_KEY',\n","        # Weitere Keys bei Bedarf\n","    }\n","\n","    # Keys in Umgebungsvariablen setzen\n","    for env_var, key_name in keys.items():\n","        environ[env_var] = userdata.get(key_name)\n","\n","    return {k: environ[k] for k in keys.keys()}\n","\n","# Verwendung\n","all_keys = setup_api_keys()\n","# Bei Bedarf einzelne Keys direkt zugreifen\n","WEATHER_API_KEY = all_keys['WEATHER_API_KEY']"],"metadata":{"cellView":"form","id":"WD3Wwr6sESX8"},"execution_count":null,"outputs":[],"id":"WD3Wwr6sESX8"},{"cell_type":"markdown","metadata":{"id":"099e73fa"},"source":["\n","# **1 <font color='orange'>|</font> Grundlagen von KI-Agenten**\n","---"],"id":"099e73fa"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Was sind KI-Agenten?\n","</font></p>"],"metadata":{"id":"GZgh0sacULkl"},"id":"GZgh0sacULkl"},{"cell_type":"markdown","source":["**Definition und Abgrenzung zu anderen KI-Systemen:**    \n","KI-Agenten sind autonome Systeme, die ihre Umgebung wahrnehmen, Entscheidungen treffen und Aktionen ausführen können, um bestimmte Ziele zu erreichen. Im Gegensatz zu einfachen KI-Modellen, die nur auf Eingabe reagieren und Ausgabe produzieren, können Agenten proaktiv handeln und aus ihren Erfahrungen lernen.\n","\n","\n","**Autonomie, Reaktivität und zielgerichtetes Handeln:**    \n","Ein KI-Agent kann selbstständig (autonom) Entscheidungen treffen, auf Veränderungen in seiner Umgebung reagieren (Reaktivität) und hat ein oder mehrere Ziele, die sein Handeln leiten. Dabei verfolgt er langfristige Strategien statt nur auf unmittelbare Reize zu reagieren.\n","\n","\n","**Unterschied zwischen einfachen LLM-Anwendungen und Agenten:**    \n","Einfache LLM-Anwendungen (Large Language Models) wie ein Chatbot antworten lediglich auf Anfragen, während Agenten darüber hinaus eigene Entscheidungen treffen, Tools nutzen und zielgerichtet handeln können. Ein Agent kann beispielsweise entscheiden, wann er eine Suchanfrage starten sollte, um eine Frage besser zu beantworten.\n"],"metadata":{"id":"IC9sshw8UWAs"},"id":"IC9sshw8UWAs"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Ein erster Agent mit LangChain\n","</font></p>"],"metadata":{"id":"malWjLCgUqlL"},"id":"malWjLCgUqlL"},{"cell_type":"markdown","source":["+ **Setup der Entwicklungsumgebung:** Installation und Konfiguration der notwendigen Bibliotheken, insbesondere LangChain und die Anbindung an ein LLM wie OpenAI.\n","+ **ToolSet definieren:** Definition der druch die Agnets benutzen Tools.\n","+ **Implementierung eines einfachen Agenten:** Ein Beispiel für einen einfachen Agenten, der Fragen beantwortet und Berechnungen durchführt:\n"],"metadata":{"id":"3BlJKrCMUsV5"},"id":"3BlJKrCMUsV5"},{"cell_type":"markdown","source":["**Setup der Entwicklungsumgebung:**"],"metadata":{"id":"JZuFcweuE8XI"},"id":"JZuFcweuE8XI"},{"cell_type":"code","source":["# Abschnitt 0: Installation und API-Key\n","!uv pip install --system -q -U langchain-openai langchain-community google-search-results wikipedia"],"metadata":{"id":"wD5gpjUZOlbp"},"execution_count":null,"outputs":[],"id":"wD5gpjUZOlbp"},{"cell_type":"code","source":["# Abschnitt 0: Installation und API-Key\n","!uv pip install --system -q -U langchain-openai langchain-community google-search-results wikipedia"],"metadata":{"id":"nsIqCOQb7mc2"},"id":"nsIqCOQb7mc2","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Zentrale Tool-Bibliothek**"],"metadata":{"id":"h_w0oBtC_1KT"},"id":"h_w0oBtC_1KT"},{"cell_type":"code","execution_count":null,"id":"e179b4bb","metadata":{"id":"e179b4bb"},"outputs":[],"source":["import os\n","import re\n","import requests\n","import json\n","from datetime import datetime\n","from langchain_core.tools import Tool\n","from langchain_community.utilities.serpapi import SerpAPIWrapper\n","from langchain_community.utilities.wikipedia import WikipediaAPIWrapper\n","\n","# Tool-Funktionen definieren\n","def get_weather(location):\n","    \"\"\"Gibt das aktuelle Wetter für einen bestimmten Ort zurück.\"\"\"\n","    url = f\"https://api.openweathermap.org/data/2.5/weather?q={location}&appid={WEATHER_API_KEY}&units=metric\"\n","    response = requests.get(url)\n","    data = response.json()\n","    return f\"Aktuelle Temperatur in {location}: {data['main']['temp']}°C, {data['weather'][0]['description']}\"\n","\n","def read_file(file_path):\n","    \"\"\"Liest den Inhalt einer Datei.\"\"\"\n","    if os.path.exists(file_path):\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            return file.read()\n","    return f\"Fehler: Datei {file_path} nicht gefunden.\"\n","\n","def write_file(args):\n","    \"\"\"Schreibt Inhalt in eine Datei. Format: 'pfad//inhalt'\"\"\"\n","    try:\n","        file_path, content = args.split(\"//\", 1)\n","        with open(file_path, 'w', encoding='utf-8') as file:\n","            file.write(content)\n","        return f\"Erfolgreich in {file_path} geschrieben.\"\n","    except Exception as e:\n","        return f\"Fehler beim Schreiben: {str(e)}\"\n","\n","def days_between_dates(date_string):\n","    \"\"\"Berechnet Tage zwischen zwei Datumsangaben im Format DD.MM.YYYY\"\"\"\n","    try:\n","        date1_str, date2_str = date_string.split(\",\")\n","        date1_str = date1_str.strip()\n","        date2_str = date2_str.strip()\n","        date1 = datetime.strptime(date1_str, \"%d.%m.%Y\").date()\n","        date2 = datetime.strptime(date2_str, \"%d.%m.%Y\").date()\n","        return str((date2 - date1).days)\n","    except ValueError:\n","        return \"Ungültiges Datumsformat. Bitte verwende DD.MM.YYYY.\"\n","\n","def extract_math_expression(text):\n","    \"\"\"Extrahiert mathematischen Ausdruck aus Text.\"\"\"\n","    match = re.search(r'([\\d+\\-*/().]+)', text)\n","    return match.group(0) if match else None\n","\n","def calculate(expression):\n","    \"\"\"Führt mathematische Berechnungen durch.\"\"\"\n","    try:\n","        math_expr = extract_math_expression(expression)\n","        if math_expr:\n","            return str(eval(math_expr))\n","        return \"Keine gültige Berechnung gefunden.\"\n","    except:\n","        return \"Fehler bei der Berechnung.\"\n","\n","# Externe APIs\n","serpapi = SerpAPIWrapper()\n","wiki = WikipediaAPIWrapper()\n","\n","# Tools-Liste, die in allen Beispielen verwendet werden kann\n","tools = [\n","    Tool(name=\"search\", func=serpapi.run, description=\"Infos aus dem Internet\"),\n","    Tool(name=\"wiki\", func=lambda x: wiki.run(x).split(\"\\n\")[0] if wiki.run(x) else \"Keine relevanten Infos gefunden.\",\n","         description=\"Fakten aus Wikipedia\"),\n","    Tool(name=\"calculator\", func=calculate, description=\"Mathematische Berechnungen\"),\n","    Tool(name=\"weather\", func=get_weather, description=\"Gibt das aktuelle Wetter für einen bestimmten Ort zurück\"),\n","    Tool(name=\"read_file\", func=read_file, description=\"Liest den Inhalt einer Datei. Input sollte der Dateipfad sein.\"),\n","    Tool(name=\"write_file\", func=write_file, description=\"Überschreibt eine Datei mit neuem Inhalt. Input sollte im Format 'pfad//inhalt' sein.\"),\n","    Tool(name=\"days_between_dates\", func=days_between_dates, description=\"Berechnet die Anzahl der Tage zwischen zwei Datumsangaben im Format DD.MM.YYYY.\")\n","]"]},{"cell_type":"markdown","id":"fca5f661","metadata":{"id":"fca5f661"},"source":["**Implementierung eines einfachen Agenten:**"]},{"cell_type":"markdown","source":["Erläuterung der einzelnen Funktionen:\n","\n","* `ChatOpenAI` ermöglicht die Nutzung von OpenAI's Chat-Modellen.\n","* `create_tool_calling_agent` und `AgentExecutor` erschaffen und führen einen Agenten aus, der Werkzeuge nutzen kann.\n","* `ChatPromptTemplate` und `MessagesPlaceholder` definieren die Struktur für Nachrichten und Vorlagen im Chat.\n","* `IPython.display display, Markdown` ermöglichen die Darstellung von formatiertem Text in einer Jupyter-Umgebung.\n"],"metadata":{"id":"CuB8r3aYGJx0"},"id":"CuB8r3aYGJx0"},{"cell_type":"code","execution_count":null,"id":"000ad941","metadata":{"id":"000ad941"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","from langchain.agents import create_tool_calling_agent, AgentExecutor\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from IPython.display import display, Markdown\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.0)\n","\n","# Prompt erstellen\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent mit Zugang zu Tools. Bitte Formeln in $ Formel $ formatieren.\"),\n","    MessagesPlaceholder(variable_name=\"chat_history\"),\n","    (\"human\", \"{input}\"),\n","    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n","])\n","\n","# Agent erstellen und ausführen\n","agent = create_tool_calling_agent(llm, tools, prompt)\n","agent_executor = AgentExecutor(\n","    agent=agent,\n","    tools=tools,\n","    verbose=True,\n","    return_intermediate_steps=True\n",")\n","\n","# Anfrage stellen\n","response = agent_executor.invoke({\n","    \"input\": \"Wie viel ist 12 * 12? Und wer ist Taylor Swift?\",\n","    \"chat_history\": []\n","})\n","\n","# Ergebnis anzeigen\n","display(Markdown(\"# 📝 Antwort des Agenten\"))\n","display(Markdown(response[\"output\"]))\n","\n","# Ausgabe:\n","# Das Ergebnis von $12 \\times 12$ ist $144$.\n","#\n","# ## Wer ist Taylor Swift?\n","# Taylor Alison Swift (geboren am 13. Dezember 1989) ist eine amerikanische Singer-Songwriterin,\n","# die für ihr autobiografisches Songwriting, ihre künstlerische Vielseitigkeit und\n","# ihren kulturellen Einfluss bekannt ist..."]},{"cell_type":"markdown","source":["Der `agent_scratchpad` ist ein entscheidender Bestandteil des Agenten-Frameworks, insbesondere bei der Verwendung von Werkzeugen (Tools):\n","\n","* **Arbeitsbereich für den Agenten:**\n","    * Der `agent_scratchpad` dient als temporärer Arbeitsbereich, in dem der Agent seine Überlegungen, Aktionen und Beobachtungen während der Ausführung speichert.\n","    * Es ist der Ort, an dem der Agent seine \"Denkschritte\" festhält, bevor er eine endgültige Antwort gibt.\n","* **Kommunikation mit dem Sprachmodell:**\n","    * Wenn der Agent ein Werkzeug aufruft, werden die Details des Aufrufs (z. B. der Name des Werkzeugs und die Eingabeparameter) in den `agent_scratchpad` geschrieben.\n","    * Nachdem das Werkzeug ausgeführt wurde, wird das Ergebnis (die \"Beobachtung\") ebenfalls in den `agent_scratchpad` geschrieben.\n","    * Das Sprachmodell verwendet den Inhalt des `agent_scratchpad`, um zu entscheiden, welche nächsten Schritte es unternehmen soll (z. B. ein weiteres Werkzeug aufrufen oder eine endgültige Antwort geben).\n","* **Debugging und Transparenz:**\n","    * Der `agent_scratchpad` kann auch für Debugging-Zwecke nützlich sein, da er Einblicke in die Denkprozesse des Agenten bietet.\n","    * Es macht die Aktionskette des Agenten transparenter.\n","\n","Im Kontext des Prompts:\n","\n","* `MessagesPlaceholder(variable_name=\"agent_scratchpad\")` fügt den Inhalt des `agent_scratchpad` in den Prompt ein, der an das Sprachmodell gesendet wird.\n","* Dadurch kann das Sprachmodell die vorherigen Aktionen und Beobachtungen des Agenten berücksichtigen, wenn es seine nächste Antwort generiert.\n","\n","Zusammengefasst ist der `agent_scratchpad` ein dynamischer Bereich, der die Kommunikation und das Gedächtnis des Agenten während der Ausführung von Werkzeugen ermöglicht."],"metadata":{"id":"Drk1eRjrHBhO"},"id":"Drk1eRjrHBhO"},{"cell_type":"markdown","id":"4275c095","metadata":{"id":"4275c095"},"source":["\n","# **2 <font color='orange'>|</font> Grundlegende Agentenarchitekturen**\n","---"]},{"cell_type":"markdown","source":["In diesem Abschnitt werden vier grundlegenden Agentenarchitekturen vorgestellt, die als Basis für komplexere Anwendungen dienen. Jede Architektur folgt einem eigenen Prinzip für Entscheidungsfindung und Handlung."],"metadata":{"id":"MnaBilDvATTM"},"id":"MnaBilDvATTM"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Komponenten von Agentenarchitekturen\n","</font></p>"],"metadata":{"id":"fE6OeSMG0XvQ"},"id":"fE6OeSMG0XvQ"},{"cell_type":"markdown","source":["\n","Die meisten modernen KI-Agentenarchitekturen bestehen aus mehreren Kernkomponenten:\n","\n","**1. Reasoning-Komponente**\n","\n","+ Verarbeitet Informationen und trifft Entscheidungen\n","+ Basiert typischerweise auf einem LLM\n","+ Kann explizite Gedankengänge (Chain-of-Thought) generieren\n","\n","**2. Aktionskomponente**\n","\n","+ Führt Aktionen in der Umgebung aus\n","+ Kann Tools aufrufen, APIs nutzen oder Systemaktionen auslösen\n","+ Übersetzt die Entscheidungen in konkrete Handlungen\n","\n","**3. Beobachtungskomponente**\n","\n","+ Nimmt Feedback aus der Umgebung wahr\n","+ Verarbeitet Ergebnisse vorheriger Aktionen\n","+ Liefert neue Informationen für den nächsten Reasoning-Schritt\n","\n","**4. Planungskomponente**\n","\n","+ Entwickelt Strategien für mehrschrittige Aufgaben\n","+ Zerlegt komplexe Ziele in Teilaufgaben\n","+ Kann kurz- und langfristige Pläne erstellen\n","\n","**5. Gedächtniskomponente**\n","\n","+ Speichert relevante Informationen\n","+ Behält den Kontext über mehrere Schritte hinweg\n","+ Ermöglicht Lernen aus vergangenen Erfahrungen\n","\n"],"metadata":{"id":"uqQp04YQ0Z6p"},"id":"uqQp04YQ0Z6p"},{"cell_type":"markdown","source":["## 2.1 ReAct"],"metadata":{"id":"1SLDBzC2Aedw"},"id":"1SLDBzC2Aedw"},{"cell_type":"markdown","source":["**Reasoning and Acting**"],"metadata":{"id":"FzOUzSVbAuHd"},"id":"FzOUzSVbAuHd"},{"cell_type":"markdown","source":["\n","\n","**Kernprinzip:** Zyklischer Ablauf von Denken, Handeln und Beobachten.\n","\n","**Charakteristika:**\n","\n","- Explizite Gedankenformulierung vor jeder Aktion\n","- Beobachtung der Ergebnisse nach jeder Aktion\n","- Iterative Anpassung basierend auf Beobachtungen\n","\n","**Reine ReAct-Implementierung:**"],"metadata":{"id":"-br8iZT6Agi-"},"id":"-br8iZT6Agi-"},{"cell_type":"code","execution_count":null,"id":"b93c3150","metadata":{"id":"b93c3150"},"outputs":[],"source":["from langchain import hub\n","from langchain.agents import create_react_agent, AgentExecutor\n","from langchain_openai import ChatOpenAI\n","from datetime import datetime\n","from IPython.display import display, Markdown\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n","\n","# Spezifischer ReAct-Prompt aus LangChain Hub\n","prompt = hub.pull(\"hwchase17/react\")\n","\n","# Bei der Erstellung des Agenten müssen wir sicherstellen, dass die Tools korrekt definiert sind\n","agent = create_react_agent(llm, tools, prompt)\n","\n","# Agent Executor zur Ausführung verwenden\n","agent_executor = AgentExecutor(\n","    agent=agent,\n","    tools=tools,\n","    verbose=True,\n","    max_iterations=10,  # Begrenzung der maximalen Iterationen\n","    early_stopping_method=\"force\"  # Erzwingen eines Stopps bei Erreichen der max_iterations\n",")\n","\n","# Aktuelles Datum für die Anfrage formatieren\n","aktuelles_datum_str = datetime.now().strftime(\"%d.%m.%Y\")\n","\n","# Typisches ReAct-Beispiel\n","query = f\"Wann war die Fußball-WM 2022 und wie viele Tage ist das her? Heute ist der {aktuelles_datum_str}.\"\n","result = agent_executor.invoke({\"input\": query})\n","\n","# Ergebnis anzeigen\n","display(Markdown(\"## 🏆 ReAct Agent Ergebnis\"))\n","display(Markdown(result[\"output\"]))\n","\n","# Beispielausgabe:\n","# Die Fußball-WM 2022 fand vom 20. November 2022 bis zum 18. Dezember 2022 statt,\n","# und es sind 815 Tage seitdem vergangen."]},{"cell_type":"markdown","id":"6f308473","metadata":{"id":"6f308473"},"source":["## 2.2 MRKL"]},{"cell_type":"markdown","source":["**Modular Reasoning, Knowledge and Language**"],"metadata":{"id":"kDX2E_nYApTP"},"id":"kDX2E_nYApTP"},{"cell_type":"markdown","source":["\n","\n","**Kernprinzip:** Kombination eines zentralen Sprachmodells mit spezialisierten Modulen (Tools).\n","\n","**Charakteristika:**\n","\n","- Modularität: Verschiedene Tools für spezialisierte Aufgaben\n","- Reasoning: Das Sprachmodell entscheidet, welches Tool zu verwenden ist\n","- Routing: Weiterleitung von Teilaufgaben an die passenden Tools\n","- Integration: Zusammenführung der Ergebnisse in eine kohärente Antwort\n","\n","**Reine MRKL-Implementierung:**"],"metadata":{"id":"AUh1xmDqAm12"},"id":"AUh1xmDqAm12"},{"cell_type":"code","execution_count":null,"id":"8dd5ff68","metadata":{"id":"8dd5ff68"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","from langchain.agents import AgentExecutor, create_react_agent\n","from langchain import hub\n","from IPython.display import display, Markdown\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n","\n","# Prompt von LangChain Hub laden\n","prompt = hub.pull(\"hwchase17/react\")\n","\n","# MRKL-Agent mit klar spezialisierten Modulen\n","agent = create_react_agent(llm, tools, prompt)\n","mrkl_agent = AgentExecutor(agent=agent, tools=tools, verbose=True)\n","\n","# Typisches MRKL-Beispiel mit verschiedenen Modulen\n","user_request = \"\"\"\n","Wie ist das Wetter in Berlin?\n","Gib mir eine kurze Zusammenfassung zur Geschichte Berlins und\n","berechne die Tage zwischen dem 01.01.2023 und dem 31.12.2023.\n","\"\"\"\n","\n","# Agent ausführen\n","response = mrkl_agent.invoke({\"input\": user_request})\n","\n","# Ergebnis anzeigen\n","display(Markdown(\"# 🧩 MRKL Agent Ergebnis\"))\n","display(Markdown(response[\"output\"]))\n","\n","# Beispielausgabe:\n","# Das Wetter in Berlin ist aktuell 8.45°C mit gebrochenen Wolken.\n","# Berlin hat eine reiche und komplexe Geschichte, die bis ins 13. Jahrhundert zurückreicht\n","# und sich über Kriege, Teilungen und Wiedervereinigungen erstreckt.\n","# Die Anzahl der Tage zwischen dem 01.01.2023 und dem 31.12.2023 beträgt 364 Tage."]},{"cell_type":"markdown","id":"1a1a7ab8","metadata":{"id":"1a1a7ab8"},"source":["## 2.3 Reflective Agents"]},{"cell_type":"markdown","source":["\n","\n","**Kernprinzip:** Selbstbewertung und -verbesserung durch kritische Analyse der eigenen Outputs.\n","\n","**Charakteristika:**\n","\n","- Self-criticism: Kritische Bewertung der eigenen Ausgaben\n","- Verbesserungszyklen: Mehrfache Iteration zur Qualitätsverbesserung\n","- Meta-Kognition: \"Denken über das Denken\"\n","\n","**Reine Reflective Agent-Implementierung:**"],"metadata":{"id":"mLAdxmXYA1gD"},"id":"mLAdxmXYA1gD"},{"cell_type":"code","execution_count":null,"id":"c0718a79","metadata":{"lines_to_next_cell":2,"id":"c0718a79"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","from langchain.prompts import PromptTemplate\n","from langchain.schema.output_parser import StrOutputParser\n","from IPython.display import display, Markdown\n","\n","# LLMs mit unterschiedlichen Einstellungen\n","answer_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)  # Kreativere Erstantwort\n","reflection_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)      # Kritische Bewertung\n","\n","# Prompts für die drei Phasen des Reflection-Prozesses\n","answer_prompt = PromptTemplate.from_template(\"Beantworte: {question}\")\n","reflection_prompt = PromptTemplate.from_template(\"\"\"\n","Bewerte:\n","Frage: {question}\n","Antwort: {answer}\n","\n","Fehler/Verbesserungen?\"\"\")\n","improvement_prompt = PromptTemplate.from_template(\"\"\"\n","Frage: {question}\n","Antwort: {answer}\n","Reflexion: {reflection}\n","\n","Verbesserte Antwort:\"\"\")\n","\n","# LCEL-Chains für die drei Phasen\n","answer_chain = answer_prompt | answer_llm | StrOutputParser()\n","reflection_chain = {\n","    \"question\": lambda x: x[\"question\"],\n","    \"answer\": lambda x: x[\"answer\"]\n","} | reflection_prompt | reflection_llm | StrOutputParser()\n","improvement_chain = {\n","    \"question\": lambda x: x[\"question\"],\n","    \"answer\": lambda x: x[\"initial_answer\"],\n","    \"reflection\": lambda x: x[\"reflection\"]\n","} | improvement_prompt | answer_llm | StrOutputParser()\n","\n","# Ausführung des Reflective Agent\n","question = \"Hauptursachen des Klimawandels?\"\n","initial_answer = answer_chain.invoke({\"question\": question})\n","reflection = reflection_chain.invoke({\"question\": question, \"answer\": initial_answer})\n","improved_answer = improvement_chain.invoke({\n","    \"question\": question,\n","    \"initial_answer\": initial_answer,\n","    \"reflection\": reflection\n","})\n","\n","# Ergebnisse anzeigen\n","display(Markdown(\"# 🔍 Reflective Agent Prozess\"))\n","display(Markdown(\"## Initiale Antwort:\"))\n","display(Markdown(initial_answer))\n","display(Markdown(\"## Selbstreflexion:\"))\n","display(Markdown(reflection))\n","display(Markdown(\"## Verbesserte Antwort:\"))\n","display(Markdown(improved_answer))\n","\n","# Beispielausgabe für verbesserte Antwort:\n","# Die Hauptursachen des Klimawandels lassen sich in zwei Kategorien einteilen: natürliche und\n","# menschengemachte Ursachen. Die wissenschaftliche Gemeinschaft ist sich einig, dass die aktuellen\n","# Veränderungen des Klimas überwiegend auf menschliche Aktivitäten zurückzuführen sind..."]},{"cell_type":"markdown","id":"a1c80b27","metadata":{"id":"a1c80b27"},"source":["## 2.4 Function-Calling"]},{"cell_type":"markdown","source":["\n","\n","**Kernprinzip:** Standardisierte Schnittstellen für den Aufruf externer Funktionen und Dienste.\n","\n","**Charakteristika:**\n","- Direkte Integration: Nahtlose Verbindung zu externen Funktionen\n","- Standardisierte Schnittstellen: Klare Definitionen für Ein- und Ausgaben\n","- Spezialisierte Aktionen: Fokus auf spezifische Funktionalitäten\n","\n","**Reine Function-Calling-Implementierung:**"],"metadata":{"id":"DXvAOoBqA6dz"},"id":"DXvAOoBqA6dz"},{"cell_type":"code","execution_count":null,"id":"1bd7e8ca","metadata":{"id":"1bd7e8ca"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","from langchain.agents import AgentExecutor, create_openai_functions_agent\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from IPython.display import display, Markdown\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n","\n","# Prompt für den Function-Calling Agent\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent, der Informationen über das Wetter und andere Fakten liefert.\"),\n","    (\"human\", \"{input}\"),\n","    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n","])\n","\n","# Function-Calling Agent (fokussiert auf direkten Funktionsaufruf)\n","function_agent = create_openai_functions_agent(llm, tools, prompt)\n","agent_executor = AgentExecutor.from_agent_and_tools(\n","    agent=function_agent,\n","    tools=tools,\n","    verbose=True\n",")\n","\n","# Typisches Function-Calling Beispiel\n","location = \"Berlin\"\n","response = agent_executor.invoke({\n","    \"input\": f\"Wie ist das Wetter in {location}? und was gibt es für allgemeine Informationen zu {location}?\",\n","})\n","\n","# Ergebnis anzeigen\n","display(Markdown(\"# 🌡️ Function-Calling Agent Ergebnis\"))\n","display(Markdown(response[\"output\"]))\n","\n","# Beispielausgabe:\n","# Das aktuelle Wetter in Berlin ist 8.38°C mit vereinzelten Wolken.\n","# Berlin ist die Hauptstadt Deutschlands und eine bedeutende kulturelle,\n","# politische und wirtschaftliche Metropole in Europa."]},{"cell_type":"markdown","source":["## 2.5 Fazit"],"metadata":{"id":"ULARoWo8yRyZ"},"id":"ULARoWo8yRyZ"},{"cell_type":"markdown","source":["  \n","Die Entwicklung von KI-Agentenarchitekturen hat in den letzten Jahren eine rasche Evolution durchlaufen:\n","\n","+ Frühe regelbasierte Agenten: Vor dem Aufkommen moderner LLMs basierten Agenten oft auf vordefinierten Regeln und einfachen Entscheidungsbäumen.\n","+ Tool-augmentierte Sprachmodelle: Mit der Entwicklung leistungsfähiger LLMs wurden erste Systeme geschaffen, die Sprachmodelle mit externen Tools verbinden.\n","+ Transparente Reasoning-Architekturen: Architekturen wie Chain-of-Thought und ReAct führten explizite Reasoning-Schritte ein, um die Entscheidungsfindung nachvollziehbarer zu machen.\n","+ Modulare Systeme: Ansätze wie MRKL ermöglichten die Integration spezialisierter Module für unterschiedliche Aufgabentypen.\n","+ Autonome Agentenarchitekturen: Neuere Systeme wie AutoGPT und BabyAGI führten zu selbstständigeren Agenten, die eigenständig Ziele verfolgen können.\n","+ Selbstreflektierende Agenten: Die neueste Generation von Agenten kann ihre eigenen Entscheidungen und Aktionen analysieren und verbessern."],"metadata":{"id":"zKJPmXS70qZF"},"id":"zKJPmXS70qZF"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Gegenüberstellung\n","</font></p>"],"metadata":{"id":"Q6PQ9K9zBZww"},"id":"Q6PQ9K9zBZww"},{"cell_type":"markdown","source":["| Aspekt | ReAct | MRKL | Reflective Agent | Function-Calling |\n","|--------|-------|------|------------------|------------------|\n","| Hauptfokus | Reasoning-Handlungs-Zyklus | Modulare Spezialisierung | Selbstverbesserung | Funktionsintegration |\n","| Arbeitsweise | Zyklischer Ablauf von Denken und Handeln | Routing zu spezialisierten Modulen | Mehrfache Iteration mit Selbstkritik | Direkte Ausführung externer Funktionen |\n","| Typischer Anwendungsfall | Mehrstufige Recherche, Problemlösung mit Begründung | Komplexe Fragen, die verschiedene Expertisen erfordern | Content-Erstellung, Code-Review, Fehlersuche | Wetterabfragen, Terminplanung, Datenbankabfragen |\n","| Flexibilität | Mittel | Hoch | Mittel | Begrenzt auf definierte Funktionen |\n","| Transparenz | Hoch (durch explizite Gedanken) | Mittel | Sehr hoch | Mittel |\n","| Komplexitätsbewältigung | Gut für sequentielle Probleme | Gut für diverse Problemtypen | Gut für komplizierte Aufgaben | Gut für definierte Aufgaben |\n","| Implementation | Prompting-Techniken | Modulare Architektur | Iteration und Feedback | API-Definitionen |"],"metadata":{"id":"vtbTCB0Lw-Xc"},"id":"vtbTCB0Lw-Xc"},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Kombinationsmöglichkeiten\n","</font></p>"],"metadata":{"id":"nVvMVIFF1FYe"},"id":"nVvMVIFF1FYe"},{"cell_type":"markdown","source":["Die Stärke moderner KI-Agenten liegt oft in der Kombination dieser Architekturen:\n","\n","- **ReAct + Function-Calling**: Verbindet strukturierte API-Aufrufe mit transparentem Reasoning\n","- **MRKL + Reflexion**: Modulare Systeme mit Selbstverbesserungsfähigkeit\n","- **Function-Calling + MRKL**: Standardisierte Schnittstellen für modulare Systeme\n"],"metadata":{"id":"BayCD4QL1Gf9"},"id":"BayCD4QL1Gf9"},{"cell_type":"markdown","id":"8fde7445","metadata":{"id":"8fde7445"},"source":["\n","# **3 <font color='orange'>|</font> Erweitere Konzepte**\n","---"]},{"cell_type":"markdown","source":["Nach der  Darstellung der grundlegenden Architekturen folgen nun erweiterte Konzepte, die auf diesen Architekturen aufbauen oder sie kombinieren."],"metadata":{"id":"UfiZ9JTGBy75"},"id":"UfiZ9JTGBy75"},{"cell_type":"markdown","source":["## 3.1 Einfache Planung"],"metadata":{"id":"DHZACro3CDvd"},"id":"DHZACro3CDvd"},{"cell_type":"markdown","source":["\n","\n","Planung und Zielverfolgung ist kein eigenständiger Architekturtyp, sondern ein Anwendungskonzept, das auf den grundlegenden Architekturen aufbaut.\n","\n","**Kombination:** Meist basierend auf ReAct oder Reflective Agents"],"metadata":{"id":"t6hNIqwvCMS6"},"id":"t6hNIqwvCMS6"},{"cell_type":"code","execution_count":null,"id":"f097e07f","metadata":{"id":"f097e07f"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import PromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from IPython.display import display, Markdown\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n","\n","# Chain-of-Thought Prompt erstellen\n","cot_prompt = PromptTemplate(\n","    input_variables=[\"problem\"],\n","    template=\"\"\"\n","    Zerlege das folgende Problem in kleinere Teilschritte und löse es Schritt für Schritt:\n","\n","    Problem: {problem}\n","\n","    Denke schrittweise:\n","    1. Welche Teilprobleme müssen gelöst werden?\n","    2. Wie löse ich jeden Teilschritt?\n","    3. Wie kombiniere ich die Teillösungen zur Gesamtlösung?\n","    \"\"\"\n",")\n","\n","# LCEL-Pipeline erstellen und ausführen\n","planning_chain = (\n","    {\"problem\": RunnablePassthrough()}\n","    | cot_prompt\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","# Beispielanwendung\n","complex_problem = \"\"\"\n","Berechne die Gesamtkosten für einen Einkauf von 3 Büchern zu je 12,99 €,\n","2 Notizbüchern zu je 4,50 € und einem Stift für 2,25 €,\n","wenn auf Bücher 7% Mehrwertsteuer und auf andere Artikel 19% Mehrwertsteuer anfällt.\n","Verwende für Formeln das Format $ Formel $.\n","\"\"\"\n","\n","solution = planning_chain.invoke(complex_problem)\n","\n","# Ergebnis anzeigen\n","display(Markdown(\"# 📝 Einfache Planung mit Chain-of-Thought\"))\n","display(Markdown(solution))\n","\n","# Beispielausgabe:\n","# Um die Gesamtkosten zu berechnen, muss ich die Kosten für die Bücher und die anderen\n","# Artikel separat berechnen, die Mehrwertsteuer hinzufügen und dann alles addieren..."]},{"cell_type":"markdown","id":"c1569b35","metadata":{"id":"c1569b35"},"source":["## 3.2 Fortgeschrittene Planung"]},{"cell_type":"markdown","source":["\n","\n","**Kombination:** Oft MRKL + Reflective Agents für hierarchische Planung"],"metadata":{"id":"rMPGRgPbCSa3"},"id":"rMPGRgPbCSa3"},{"cell_type":"code","execution_count":null,"id":"efb1d4bf","metadata":{"id":"efb1d4bf"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import PromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from IPython.display import display, Markdown\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n","\n","def create_planner_chain():\n","    \"\"\"Erstellt eine Chain für die hierarchische Planung.\"\"\"\n","    planer_prompt = PromptTemplate.from_template(\n","        \"\"\"\n","        Erstelle einen hierarchischen Plan, um das folgende Ziel zu erreichen:\n","\n","        Ziel: {objective}\n","\n","        Teile den Plan in:\n","        1. Strategische Ziele (hohe Ebene)\n","        2. Taktische Schritte (mittlere Ebene)\n","        3. Konkrete Aktionen (detaillierte Ebene)\n","\n","        Für jede konkrete Aktion gib an, welche Tools oder Ressourcen benötigt werden.\n","        \"\"\"\n","    )\n","    return ({\"objective\": RunnablePassthrough()} | planer_prompt | llm | StrOutputParser())\n","\n","def create_execution_chain():\n","    \"\"\"Erstellt eine Chain für die Anpassung des Plans basierend auf Feedback.\"\"\"\n","    execution_prompt = PromptTemplate.from_template(\n","        \"\"\"\n","        Hier ist der aktuelle Plan:\n","\n","        {plan}\n","\n","        Basierend auf dem folgenden Feedback, passe den Plan an:\n","\n","        Feedback: {feedback}\n","\n","        Überarbeiteter Plan:\n","        \"\"\"\n","    )\n","    return ({\"plan\": RunnablePassthrough(), \"feedback\": RunnablePassthrough()}\n","            | execution_prompt | llm | StrOutputParser())\n","\n","# Planungsketten erstellen\n","planner_chain = create_planner_chain()\n","execution_chain = create_execution_chain()\n","\n","# Beispiel ausführen\n","objective = \"Organisiere eine zweitägige Konferenz für 100 Teilnehmer zum Thema KI-Ethik\"\n","initial_plan = planner_chain.invoke(objective)\n","\n","# Feedback geben und Plan anpassen\n","feedback = \"Der Budgetrahmen wurde auf 15.000 € reduziert, und wir müssen die Konferenz auf einen Tag verkürzen.\"\n","revised_plan = execution_chain.invoke({\"plan\": initial_plan, \"feedback\": feedback})\n","\n","# Ergebnisse anzeigen\n","display(Markdown(\"# 📔 Hierarchische Planung\"))\n","display(Markdown(\"## Initialer Plan\"))\n","display(Markdown(initial_plan))\n","display(Markdown(\"## Überarbeiteter Plan\"))\n","display(Markdown(revised_plan))\n","\n","# Beispielausgabe für überarbeiteten Plan:\n","# # Überarbeiteter Plan für eintägige KI-Ethik Konferenz\n","#\n","# # Strategische Ziele\n","# 1. Durchführung einer gehaltvollen, komprimierten eintägigen Konferenz zum Thema KI-Ethik\n","# 2. Einhaltung des reduzierten Budgetrahmens von 15.000 €\n","# ..."]},{"cell_type":"markdown","id":"f8add875","metadata":{"id":"f8add875"},"source":["## 3.3 Zielorientierter Agent"]},{"cell_type":"markdown","source":["\n","\n","**Kombination:** Function-Calling + ReAct für zielgerichtete Aktionen"],"metadata":{"id":"3MDVgC3jCUTv"},"id":"3MDVgC3jCUTv"},{"cell_type":"code","execution_count":null,"id":"ee0da4fa","metadata":{"lines_to_next_cell":2,"id":"ee0da4fa"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","from langchain_core.tools import Tool\n","from langchain_community.utilities import SerpAPIWrapper\n","from langchain.agents import create_openai_functions_agent\n","from langchain.agents import AgentExecutor\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain.memory import ConversationBufferMemory\n","from IPython.display import display, Markdown\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n","\n","# Prompt erstellen mit ChatPromptTemplate\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Du bist ein hilfreicher Assistent, der Informationen recherchiert. \"\n","                \"Nutze die bereitgestellten Tools, um präzise und aktuelle Informationen zu finden.\"),\n","    MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n","    (\"human\", \"{input}\"),\n","    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n","])\n","\n","# Memory für Chat-Verlauf\n","memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n","\n","# Agent mit OpenAI Functions erstellen\n","agent = create_openai_functions_agent(llm, tools, prompt)\n","\n","# Agent-Executor konfigurieren\n","agent_executor = AgentExecutor(\n","    agent=agent,\n","    tools=tools,\n","    memory=memory,\n","    verbose=True,\n","    handle_parsing_errors=True\n",")\n","\n","# Komplexe Anfrage\n","complex_goal = \"\"\"\n","Finde heraus, wer der aktuelle Bundeskanzler von Deutschland ist,\n","wann er geboren wurde und erstelle eine kurze Zusammenfassung\n","seiner politischen Karriere.\n","\"\"\"\n","\n","# Agent ausführen\n","result = agent_executor.invoke({\"input\": complex_goal})\n","\n","# Ergebnis anzeigen\n","display(Markdown(\"# 🔎 Zielorientierter Agent\"))\n","display(Markdown(result[\"output\"]))\n","\n","# Beispielausgabe:\n","# Der aktuelle Bundeskanzler von Deutschland ist Olaf Scholz. Er wurde am 14. Juni 1958\n","# in Osnabrück geboren. Scholz ist Mitglied der SPD und hat vor seinem Amt als Bundeskanzler\n","# verschiedene politische Positionen inne gehabt..."]},{"cell_type":"markdown","id":"19b9a4b1","metadata":{"id":"19b9a4b1"},"source":["## 3.4 Kollaborative Analyse"]},{"cell_type":"markdown","source":["\n","\n","**Kombination:** Spezialisierte Agenten mit verschiedenen Architekturen"],"metadata":{"id":"0kUPFGQbCYjN"},"id":"0kUPFGQbCYjN"},{"cell_type":"code","execution_count":null,"id":"8b107657","metadata":{"id":"8b107657"},"outputs":[],"source":["from IPython.display import display, Markdown\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import PromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# LCEL-Pipelines mit unterschiedlichen Temperaturen und Rollen definieren\n","research_pipeline = PromptTemplate.from_template(\n","    \"\"\"Du bist ein Recherche-Spezialist. Sammle Informationen zum Thema: {topic}\"\"\"\n",") | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)  # MRKL-ähnlich für Informationssuche\n","\n","critique_pipeline = PromptTemplate.from_template(\n","    \"\"\"Du bist ein kritischer Analyst. Bewerte folgende Recherche: {research}\"\"\"\n",") | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  # Reflective-Agent für kritische Analyse\n","\n","synthesis_pipeline = PromptTemplate.from_template(\n","    \"\"\"Du bist ein Informationssynthetisierer. Erstelle eine Zusammenfassung:\n","    Recherche: {research}\n","    Kritik: {critique}\"\"\"\n",") | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)  # ReAct-ähnlich für Verarbeitung mehrerer Inputs\n","\n","decision_pipeline = PromptTemplate.from_template(\n","    \"\"\"Du bist ein Entscheidungsträger. Basierend auf: {synthesis}\n","    Beantworte: {question}\"\"\"\n",") | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  # Function-Calling-ähnlich für finale Entscheidung\n","\n","def collaborative_analysis(topic, question):\n","    \"\"\"Kollaborative Analyse mit LCEL-Pipelines.\"\"\"\n","    # Prozess ausführen\n","    research = research_pipeline.invoke({\"topic\": topic})\n","    display(Markdown(\"## 🔍 Recherche abgeschlossen\"))\n","\n","    critique = critique_pipeline.invoke({\"research\": research})\n","    display(Markdown(\"## 🌡️ Kritische Analyse abgeschlossen\"))\n","\n","    synthesis = synthesis_pipeline.invoke({\"research\": research, \"critique\": critique})\n","    display(Markdown(\"## ✨ Synthese abgeschlossen\"))\n","\n","    decision = decision_pipeline.invoke({\"synthesis\": synthesis, \"question\": question})\n","    display(Markdown(\"## 📔 Entscheidung getroffen\"))\n","\n","    return {\n","        \"research\": research,\n","        \"critique\": critique,\n","        \"synthesis\": synthesis,\n","        \"decision\": decision\n","    }\n","\n","# Beispielanwendung\n","topic = \"Künstliche Intelligenz in der Gesundheitsversorgung\"\n","question = \"Sollten Krankenhäuser KI-Systeme zur Diagnoseunterstützung einsetzen?\"\n","\n","# Multi-Agenten-Analyse ausführen\n","display(Markdown(\"# 👥 Multi-Agenten-System: Kollaborative Analyse\"))\n","results = collaborative_analysis(topic, question)\n","\n","# Finale Entscheidung anzeigen\n","display(Markdown(f\"# 📑 Finale Entscheidung\"))\n","display(Markdown(f\"{results['decision'].content}\"))\n","\n","# Beispielausgabe:\n","# Basierend auf der vorliegenden Synthese sollten Krankenhäuser KI-Systeme zur\n","# Diagnoseunterstützung einsetzen, jedoch unter bestimmten Bedingungen und mit\n","# angemessenen Vorsichtsmaßnahmen..."]},{"cell_type":"markdown","id":"525b252b","metadata":{"id":"525b252b"},"source":["## 3.5 Agent-Kommunikation"]},{"cell_type":"markdown","source":["\n","\n","**Kombination:** Agenten mit Function-Calling für standardisierte Kommunikation"],"metadata":{"id":"Kz9b7MTkCao9"},"id":"Kz9b7MTkCao9"},{"cell_type":"code","execution_count":null,"id":"00d5132f","metadata":{"id":"00d5132f"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain.memory import ConversationBufferMemory\n","from langchain.agents import AgentExecutor, create_openai_functions_agent\n","from langchain.tools import Tool\n","from IPython.display import Markdown, display\n","\n","class ModernAgent:\n","    \"\"\"Moderner Agent für Multi-Agenten-Kommunikation mit LangChain\"\"\"\n","    def __init__(self, name, role, llm, tools=tools):\n","        self.name = name\n","        self.role = role\n","        self.llm = llm\n","        self.memory = ConversationBufferMemory(return_messages=True)\n","\n","        # System-Prompt definieren\n","        system_prompt = f\"Du bist {self.name}, ein KI-Agent mit der Rolle: {self.role}\"\n","\n","        # Mindestens ein Tool bereitstellen (wird für alle Agenten benötigt)\n","        self.tools = tools\n","\n","        # Prompt-Template erstellen (mit agent_scratchpad für OpenAI Functions Agent)\n","        prompt = ChatPromptTemplate.from_messages([\n","            (\"system\", system_prompt),\n","            (\"human\", \"{input}\"),\n","            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n","        ])\n","\n","        # Agent erstellen\n","        self.agent = create_openai_functions_agent(llm=self.llm, tools=self.tools, prompt=prompt)\n","\n","        # Agent-Executor erstellen\n","        self.agent_executor = AgentExecutor(agent=self.agent, tools=self.tools, verbose=True)\n","\n","    def receive_message(self, message, sender):\n","        \"\"\"Speichert eine empfangene Nachricht im Gedächtnis.\"\"\"\n","        self.memory.chat_memory.add_message(HumanMessage(content=f\"{sender}: {message}\"))\n","\n","    def process_and_respond(self, query=None):\n","        \"\"\"Verarbeitet den Kontext und generiert eine Antwort mit dem Agent-Executor.\"\"\"\n","        # Kontext aus dem Speicher holen\n","        memory_variables = self.memory.load_memory_variables({})\n","        memory_messages = memory_variables.get(\"history\", [])\n","        memory_context = \"\\n\".join([f\"{msg.type}: {msg.content}\" for msg in memory_messages])\n","\n","        # Eingabe für den Agent erstellen\n","        input_text = f\"Kontext der bisherigen Konversation:\\n{memory_context}\\n\\n\"\n","        input_text += f\"Aufgabe/Frage: {query or 'Reagiere auf die letzte Nachricht.'}\"\n","\n","        # Antwort mit dem Agent-Executor generieren\n","        response = self.agent_executor.invoke({\"input\": input_text})\n","\n","        # Antwort zum Speicher hinzufügen\n","        self.memory.chat_memory.add_message(AIMessage(content=response[\"output\"]))\n","\n","        return response[\"output\"]\n","\n","# Multi-Agenten-System demonstrieren\n","display(Markdown(\"# 🤝 Multi-Agenten-System: Kommunikation zwischen Spezialisten\"))\n","\n","# LLM initialisieren\n","llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n","\n","# Agenten mit verschiedenen Rollen erstellen\n","researcher = ModernAgent(\"Forscher\", \"Sammle und analysiere Informationen zu einem Thema\", llm, tools=tools)\n","critic = ModernAgent(\"Kritiker\", \"Bewerte Informationen kritisch und identifiziere mögliche Fehler\", llm)\n","summarizer = ModernAgent(\"Zusammenfasser\", \"Erstelle prägnante Zusammenfassungen von Informationen\", llm)\n","\n","# Simulierte Interaktion zwischen Agenten\n","researcher.receive_message(\"Recherchiere die Vor- und Nachteile erneuerbarer Energien\", \"Benutzer\")\n","research_result = researcher.process_and_respond()\n","\n","critic.receive_message(research_result, \"Forscher\")\n","critique = critic.process_and_respond()\n","\n","summarizer.receive_message(research_result, \"Forscher\")\n","summarizer.receive_message(critique, \"Kritiker\")\n","final_summary = summarizer.process_and_respond(\"Erstelle eine ausgewogene Zusammenfassung über erneuerbare Energien\")\n","\n","# Anzeige der Ergebnisse\n","display(Markdown(f\"# 🔍 Forschungsergebnis\"))\n","display(Markdown(f\"{research_result}\\n\"))\n","display(Markdown(f\"# 📣 Kritik\"))\n","display(Markdown(f\"{critique}\\n\"))\n","display(Markdown(f\"# 📓 Endgültige Zusammenfassung\"))\n","display(Markdown(f\"{final_summary}\"))\n","\n","# Beispielausgabe:\n","# Erneuerbare Energien bieten zahlreiche Vorteile wie Nachhaltigkeit, reduzierte Umweltbelastung\n","# und wirtschaftliche Chancen, stehen jedoch vor Herausforderungen wie Intermittenz,\n","# Speicherbedarf und anfänglichen Investitionskosten..."]},{"cell_type":"markdown","metadata":{"id":"f7989ab6"},"source":["\n","# **4 <font color='orange'>|</font> Ethik & Verantwortung**\n","---"],"id":"f7989ab6"},{"cell_type":"markdown","source":["## 4.1 Autonomie und Kontrolle"],"metadata":{"id":"J0HxAUvUDXr1"},"id":"J0HxAUvUDXr1"},{"cell_type":"markdown","source":["\n","\n","**Herausforderung der Autonomie:**\n","\n","- Spannungsfeld zwischen Autonomie und Kontrolle bei KI-Agenten\n","- Risiken bei zu viel Handlungsfreiheit (Ressourcenverschwendung, unbeabsichtigte Aktionen)\n","- Notwendigkeit von \"Guardrails\" und Stopp-Mechanismen\n","\n","**Technische Lösungsansätze:**"],"metadata":{"id":"u7UE33D1Dv5F"},"id":"u7UE33D1Dv5F"},{"cell_type":"code","execution_count":null,"metadata":{"id":"8014b670"},"outputs":[],"source":["# Implementierung von Sicherheitsmechanismen\n","agent_executor = AgentExecutor(\n","    agent=agent,\n","    tools=tools,\n","    max_iterations=15,                # Verhindert Endlosschleifen\n","    early_stopping_method=\"force\",    # Erzwingt einen Stopp nach max_iterations\n","    max_execution_time=60,            # Zeitlimit in Sekunden\n","    handle_parsing_errors=True,       # Robustes Fehlerhandling\n","    tool_error_handling=\"continue\"    # Bei Tool-Fehlern weitermachen, statt abzubrechen\n",")"],"id":"8014b670"},{"cell_type":"markdown","metadata":{"id":"1f807c8f"},"source":["## 4.2 Verantwortung und Haftung"],"id":"1f807c8f"},{"cell_type":"markdown","source":["\n","\n","- Wer trägt die Verantwortung für Agenten-Entscheidungen?\n","- Haftungskette: Entwickler, Betreiber, Nutzer\n","- Rechtliche Rahmenbedingungen und ihre Lücken\n","- Dokumentationspflichten bei autonomen Systemen\n","\n"],"metadata":{"id":"5EQZxBfpDzoH"},"id":"5EQZxBfpDzoH"},{"cell_type":"markdown","source":["## 4.3 Verzerrungen und Fairness"],"metadata":{"id":"wao2oij-D2Gh"},"id":"wao2oij-D2Gh"},{"cell_type":"markdown","source":["\n","\n","**Systematische Verzerrungen in Agenten:**\n","\n","- Übertragung von Biases aus Trainingsdaten\n","- Verstärkung durch iterative Entscheidungsprozesse\n","- Besondere Risiken bei Multi-Agenten-Systemen (Echokammern)\n","\n","**Implementierung von Fairness-Checks:**"],"metadata":{"id":"fsQPgJeYD36S"},"id":"fsQPgJeYD36S"},{"cell_type":"code","source":["from IPython.display import display, Markdown\n","\n","from langchain_openai import ChatOpenAI\n","from langchain.prompts import PromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# LLM für Fairness-Prüfung initialisieren\n","fairness_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  # Niedrige Temperatur für objektivere Analyse\n","\n","# Prompt für die Bias-Prüfung definieren\n","fairness_prompt = PromptTemplate.from_template(\"\"\"\n","Überprüfe folgende Antwort auf mögliche Verzerrungen,\n","Stereotypen oder unfaire Behandlung bestimmter Gruppen:\n","\n","Frage: {question}\n","Antwort: {answer}\n","\n","Identifiziere jede Form von Bias oder Unfairness:\n","\"\"\")\n","\n","# Pipeline erstellen\n","fairness_check = fairness_prompt | fairness_llm | StrOutputParser()\n","\n","# Beispielfrage und -antwort definieren\n","question = \"Wer eignet sich am besten für Führungspositionen in der Technikbranche?\"\n","initial_answer = \"Männer mit technischem Hintergrund eignen sich am besten für Führungspositionen in der Technikbranche, da sie typischerweise analytischer denken und besser mit Stress umgehen können.\"\n","\n","# Fairness-Check durchführen\n","fairness_result = fairness_check.invoke({\n","    \"question\": question,\n","    \"answer\": initial_answer\n","})\n","\n","# Ergebnis ausgeben\n","display(Markdown(\"# ⚖️ Fairness-Prüfung\"))\n","display(Markdown(fairness_result))"],"metadata":{"id":"l0xxGAvOEItc"},"id":"l0xxGAvOEItc","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ad8a13b"},"outputs":[],"source":["# Reflective Agent mit Bias-Prüfung\n","fairness_prompt = PromptTemplate.from_template(\"\"\"\n","Überprüfe folgende Antwort auf mögliche Verzerrungen,\n","Stereotypen oder unfaire Behandlung bestimmter Gruppen:\n","\n","Frage: {question}\n","Antwort: {answer}\n","\n","Identifiziere jede Form von Bias oder Unfairness:\n","\"\"\")\n","\n","fairness_check = fairness_prompt | fairness_llm | StrOutputParser()\n","fairness_result = fairness_check.invoke({\n","    \"question\": question,\n","    \"answer\": initial_answer\n","})"],"id":"9ad8a13b"},{"cell_type":"markdown","metadata":{"id":"ebdbfb4f"},"source":["## 4.4 Ethische Grundprinzipien"],"id":"ebdbfb4f"},{"cell_type":"markdown","source":["\n","\n","- Transparenz: Nachvollziehbarkeit von Entscheidungen\n","- Menschliche Kontrolle: Stets die Möglichkeit des menschlichen Eingreifens\n","- Schadensvermeidung: Priorisierung der Sicherheit\n","- Datenschutz: Minimierung der Datenzugriffe\n","- Praktische Umsetzung dieser Prinzipien im Entwicklungsprozess\n","\n"],"metadata":{"id":"6XYE3Kb4EhVK"},"id":"6XYE3Kb4EhVK"},{"cell_type":"markdown","source":["# **5 <font color='orange'>|</font> Evaluation von Agents**\n","---"],"metadata":{"id":"LmaTvOQqEjJk"},"id":"LmaTvOQqEjJk"},{"cell_type":"markdown","source":["## 5.1 Evaluationsrahmenwerk"],"metadata":{"id":"I4RyxVMoEwAd"},"id":"I4RyxVMoEwAd"},{"cell_type":"markdown","source":["\n","\n","**Mehrdimensionales Bewertungssystem:**\n","\n","- Effektivität: Zielerreichungsgrad\n","- Effizienz: Ressourcenverbrauch (Zeit, API-Calls, Tokens)\n","- Robustheit: Verhalten bei unerwarteten Inputs\n","- Sicherheit: Vermeidung gefährlicher Aktionen\n","\n","**Implementierung einer Evaluationspipeline:**"],"metadata":{"id":"0S0DDDYvE5DM"},"id":"0S0DDDYvE5DM"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ef267e8f"},"outputs":[],"source":["from langchain.smith import RunEvalConfig, run_on_dataset\n","\n","# Vereinfachte Evaluationskonfiguration ohne TraceAnalyzer\n","eval_config = RunEvalConfig(\n","    evaluators=[\n","        \"qa\",                          # Qualität der Antworten\n","        \"context_relevance\",           # Relevanz verwendeter Informationen\n","        \"helpfulness\",                 # Nützlichkeit für den Nutzer\n","        \"correctness\",                 # Faktische Korrektheit\n","    ],\n","    custom_metrics={\n","        \"completed_in_time\": lambda run: run.execution_time < 30,\n","        \"used_appropriate_tools\": lambda run: analyze_tool_selection(run)\n","    }\n",")"],"id":"ef267e8f"},{"cell_type":"markdown","metadata":{"id":"19e68b06"},"source":["## 5.2 Quantitative Metriken"],"id":"19e68b06"},{"cell_type":"markdown","source":["\n","\n","- **Erfolgsrate:** Anteil erfolgreich gelöster Aufgaben\n","- **Effizienzmetriken:**\n","    - Anzahl der Reasoning-Schritte\n","    - Anzahl der Tool-Aufrufe\n","    - Token-Verbrauch\n","    - Ausführungszeit\n","- **Fehleranalyse:**\n","    - Typische Fehlermuster\n","    - Kritische Pfade bei Fehlschlägen\n","\n"],"metadata":{"id":"NulipbxXFiGP"},"id":"NulipbxXFiGP"},{"cell_type":"markdown","source":["## 5.3 Qualitative Evaluation"],"metadata":{"id":"VRj8PdIdFjs6"},"id":"VRj8PdIdFjs6"},{"cell_type":"markdown","source":["\n","\n","- Menschliche Bewertung der Agenten-Outputs\n","- Think-aloud Protokolle zur Bewertung des Reasoning\n","- A/B-Tests verschiedener Agentenarchitekturen\n","- Usability und Nutzererfahrung\n","\n"],"metadata":{"id":"reDzxgFmFn73"},"id":"reDzxgFmFn73"},{"cell_type":"markdown","source":["## 5.4 Kontinuierliche Verbesserung"],"metadata":{"id":"N4fBjKEdFpDy"},"id":"N4fBjKEdFpDy"},{"cell_type":"markdown","source":["# **A <font color='orange'>|</font> Aufgabe**\n","---"],"metadata":{"id":"Pzc1rzQlNV8J"},"id":"Pzc1rzQlNV8J"},{"cell_type":"markdown","source":["Die Aufgabestellungen unten bieten Anregungen, Sie können aber auch gerne eine andere Herausforderung angehen."],"metadata":{"id":"QQUImb-86GUw"},"id":"QQUImb-86GUw"},{"cell_type":"markdown","source":["Gegeben ist eine Datei, die eine Reihe von Gleichungen enthält.\n","Der Dateiname ist GenAI/02 data/gleichungen.txt\n","\n","**Gleichung:**    \n","41748459 - 87226336    \n","92995162 * 46769739    \n","61530438 * 56074589    \n","95329602 + 45418854    \n","412907 + 3731910    \n","...\n","\n","Verwenden Sie einen LangChain-Agenten mit einem Tool, um jede dieser Gleichungen zu berechnen, und senden Sie eine Datei ähnlich dieser:\n","\n","**Ergebnisse:**  \n","41748459 - 87226336 = 45477877   \n","92995162 * 46769739 = 4349359455002718   \n","61530438 * 56074589 = 3450294021839982   \n","95329602 + 45418854 = 140748456   \n","412907 + 3731910 = 4144817   \n","... ...\n","\n"],"metadata":{"id":"Y5y-OP9OemcW"},"id":"Y5y-OP9OemcW"},{"cell_type":"markdown","metadata":{"id":"3cd6645a"},"source":["Mit dieser Aufgabe wenden die Studierenden das Gelernte über Agentenarchitekturen praktisch an. Der Agent verwendet dabei Function-Calling, um:\n","1. Eine Datei zu lesen (Umgebungsinteraktion)\n","2. Mathematische Gleichungen zu identifizieren und zu berechnen\n","3. Die Ergebnisse in einer neuen Datei zu speichern\n","\n","Dies demonstriert die praktische Anwendung von Tools innerhalb einer Agentenarchitektur für eine reale Aufgabe."],"id":"3cd6645a"},{"cell_type":"markdown","source":["\n","\n","**Feedback-Schleife:**"],"metadata":{"id":"THh3mimaFqvn"},"id":"THh3mimaFqvn"},{"cell_type":"code","execution_count":null,"metadata":{"id":"68cfe45e"},"outputs":[],"source":["# Sammlung von Nutzerfeedback\n","def collect_feedback(agent_response, user_rating, user_comment):\n","    feedback_data = {\n","        \"input\": agent_response[\"input\"],\n","        \"output\": agent_response[\"output\"],\n","        \"rating\": user_rating,  # z.B. 1-5 Sterne\n","        \"comment\": user_comment,\n","        \"trace\": agent_response[\"intermediate_steps\"]\n","    }\n","    # Speichern für spätere Analyse und Modellverbesserung\n","    feedback_db.insert(feedback_data)\n","\n","    # Kritische Fehler sofort melden\n","    if user_rating < 2:\n","        alert_development_team(feedback_data)\n","\n","    return \"Feedback erfolgreich gespeichert\""],"id":"68cfe45e"}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"colab":{"provenance":[],"collapsed_sections":["099e73fa","4275c095","1SLDBzC2Aedw","6f308473","1a1a7ab8","a1c80b27","ULARoWo8yRyZ","8fde7445","DHZACro3CDvd","c1569b35","f8add875","19b9a4b1","525b252b","f7989ab6","J0HxAUvUDXr1","1f807c8f","wao2oij-D2Gh","ebdbfb4f","LmaTvOQqEjJk","I4RyxVMoEwAd","19e68b06","VRj8PdIdFjs6","N4fBjKEdFpDy","Pzc1rzQlNV8J"],"toc_visible":true},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}
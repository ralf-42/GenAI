{"cells":[{"cell_type":"markdown","id":"9bae79d8","metadata":{"id":"9bae79d8"},"source":["# M14a - Multimodales RAG Enhanced\n","\n","Dieses Notebook zeigt die Verwendung der erweiterten RAG-Funktionen mit automatischen Bildbeschreibungen.\n","\n","**Neue Features:**\n","- Automatische Bildbeschreibungen mit GPT-4o-mini\n","- Cross-References zwischen Text und Bildern\n","- Cross-Modal-Retrieval\n","- Hybride Suche √ºber Text und Bildbeschreibungen"]},{"cell_type":"markdown","source":[],"metadata":{"id":"dDk0IFHI6A4e"},"id":"dDk0IFHI6A4e"},{"cell_type":"code","execution_count":1,"metadata":{"id":"9c8df5ef","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760717162529,"user_tz":-120,"elapsed":21374,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"01a38032-e566-4afd-938c-2fb43e73825e"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úì OPENAI_API_KEY erfolgreich gesetzt\n","‚úì HF_TOKEN erfolgreich gesetzt\n","\n","Python Version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n","\n","Installierte LangChain-Bibliotheken:\n","langchain                                0.3.27\n","langchain-community                      0.3.31\n","langchain-core                           0.3.78\n","langchain-experimental                   0.3.4\n","langchain-ollama                         0.3.10\n","langchain-openai                         0.3.35\n","langchain-text-splitters                 0.3.11\n","\n","IP-Adresse: 35.227.3.113\n","Hostname: 113.3.227.35.bc.googleusercontent.com\n","Stadt: North Charleston\n","Region: South Carolina\n","Land: US\n","Koordinaten: 32.8546,-79.9748\n","Provider: AS396982 Google LLC\n","Postleitzahl: 29415\n","Zeitzone: America/New_York\n"]}],"source":["#@title üîß Umgebung einrichten{ display-mode: \"form\" }\n","!uv pip install --system -q git+https://github.com/ralf-42/Python_Modules\n","from genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint, install_packages\n","setup_api_keys(['OPENAI_API_KEY', 'HF_TOKEN'], create_globals=False)\n","print()\n","check_environment()\n","print()\n","get_ipinfo()"],"id":"9c8df5ef"},{"cell_type":"code","execution_count":2,"metadata":{"id":"4d2e699d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760717174365,"user_tz":-120,"elapsed":11800,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"6c0524e0-deba-4d5a-c5e8-ee1ccec3ad6a"},"outputs":[{"output_type":"stream","name":"stdout","text":["üîÑ Installiere markitdown[all]...\n","‚úÖ markitdown[all] erfolgreich installiert und importiert\n","üîÑ Installiere langchain_chroma...\n","‚úÖ langchain_chroma erfolgreich installiert und importiert\n"]}],"source":["#@title üõ†Ô∏è Installationen { display-mode: \"form\" }\n","install_packages([\n","    ('markitdown[all]', 'markitdown'),\n","    'langchain_chroma',\n","])"],"id":"4d2e699d"},{"cell_type":"code","execution_count":3,"metadata":{"id":"da4cf78d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760717178058,"user_tz":-120,"elapsed":3706,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"edf73e62-d60f-4bf5-8610-937064e63593"},"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  5324  100  5324    0     0  17934      0 --:--:-- --:--:-- --:--:-- 17986\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  7868  100  7868    0     0  46862      0 --:--:-- --:--:-- --:--:-- 47113\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 68184  100 68184    0     0   187k      0 --:--:-- --:--:-- --:--:--  187k\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 18743  100 18743    0     0  61269      0 --:--:-- --:--:-- --:--:-- 61452\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  1841  100  1841    0     0   5928      0 --:--:-- --:--:-- --:--:--  5938\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  338k  100  338k    0     0   609k      0 --:--:-- --:--:-- --:--:--  609k\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 2995k  100 2995k    0     0  6243k      0 --:--:-- --:--:-- --:--:-- 6253k\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  3419  100  3419    0     0  11867      0 --:--:-- --:--:-- --:--:-- 11912\n"]}],"source":["#@title üìÇ Dokumente & Bilder kopieren { display-mode: \"form\" }\n","!rm -rf files\n","!mkdir files\n","\n","# --- Texte\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02%20data/biografien_1.txt -o files/biografien_1.txt\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02%20data/biografien_2.md -o files/biografien_2.md\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02%20data/biografien_3.pdf -o files/biografien_3.pdf\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02%20data/biografien_4.docx -o files/biografien_4.docx\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02%20data/roboter.txt -o files/roboter.txt\n","\n","# --- Bilder\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02%20data/a_retro-futuristic_robot_dall_e.jpg -o files/a_retro-futuristic_robot_dall_e.jpg\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02%20data/hedra_cyborg.png -o files/hedra_cyborg.png\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02%20data/apfel.jpg -o files/apfel.jpg"],"id":"da4cf78d"},{"cell_type":"markdown","id":"96fccaf7","metadata":{"id":"96fccaf7"},"source":["## 2. Enhanced Funktionen laden"]},{"cell_type":"code","execution_count":20,"id":"96ea854d","metadata":{"id":"96ea854d","executionInfo":{"status":"ok","timestamp":1760717573812,"user_tz":-120,"elapsed":19,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}}},"outputs":[],"source":["import M14a_standalone as m14"]},{"cell_type":"code","source":["help(m14)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"QbEgroTk8Xoj","executionInfo":{"status":"ok","timestamp":1760717580161,"user_tz":-120,"elapsed":35,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"abec749d-0f58-41ec-d30a-5b05ba45de31"},"id":"QbEgroTk8Xoj","execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Help on module M14a_standalone:\n","\n","NAME\n","    M14a_standalone\n","\n","DESCRIPTION\n","    M14a - Standalone Multimodales RAG mit Bildbeschreibungen\n","    Vollst√§ndig eigenst√§ndige Version - ben√∂tigt KEIN M14.ipynb\n","\n","    Verwendung:\n","        from M14a_standalone import *\n","\n","        # System initialisieren\n","        rag = init_rag_system_enhanced()\n","\n","        # Verzeichnis verarbeiten\n","        process_directory(rag, './files', auto_describe_images=True)\n","\n","        # Suchen\n","        result = multimodal_search(rag, \"Roboter\")\n","\n","    Autor: Enhanced by Claude\n","    Datum: Oktober 2025\n","\n","CLASSES\n","    builtins.object\n","        RAGComponents\n","        RAGConfig\n","\n","    class RAGComponents(builtins.object)\n","     |  RAGComponents(text_embeddings: langchain_openai.embeddings.base.OpenAIEmbeddings, clip_model: sentence_transformers.SentenceTransformer.SentenceTransformer, llm: langchain_openai.chat_models.base.ChatOpenAI, vision_llm: langchain_openai.chat_models.base.ChatOpenAI, text_splitter: langchain_text_splitters.character.RecursiveCharacterTextSplitter, markitdown: markitdown._markitdown.MarkItDown, chroma_client: <function PersistentClient at 0x7b320c1b2840>, text_collection: langchain_chroma.vectorstores.Chroma, image_collection: <built-in function any>, config: M14a_standalone.RAGConfig) -> None\n","     |\n","     |  Container f√ºr alle RAG-System-Komponenten\n","     |\n","     |  Methods defined here:\n","     |\n","     |  __eq__(self, other)\n","     |      Return self==value.\n","     |\n","     |  __init__(self, text_embeddings: langchain_openai.embeddings.base.OpenAIEmbeddings, clip_model: sentence_transformers.SentenceTransformer.SentenceTransformer, llm: langchain_openai.chat_models.base.ChatOpenAI, vision_llm: langchain_openai.chat_models.base.ChatOpenAI, text_splitter: langchain_text_splitters.character.RecursiveCharacterTextSplitter, markitdown: markitdown._markitdown.MarkItDown, chroma_client: <function PersistentClient at 0x7b320c1b2840>, text_collection: langchain_chroma.vectorstores.Chroma, image_collection: <built-in function any>, config: M14a_standalone.RAGConfig) -> None\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |\n","     |  __repr__(self)\n","     |      Return repr(self).\n","     |\n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors defined here:\n","     |\n","     |  __dict__\n","     |      dictionary for instance variables\n","     |\n","     |  __weakref__\n","     |      list of weak references to the object\n","     |\n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |\n","     |  __annotations__ = {'chroma_client': <function PersistentClient>, 'clip...\n","     |\n","     |  __dataclass_fields__ = {'chroma_client': Field(name='chroma_client',ty...\n","     |\n","     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n","     |\n","     |  __hash__ = None\n","     |\n","     |  __match_args__ = ('text_embeddings', 'clip_model', 'llm', 'vision_llm'...\n","\n","    class RAGConfig(builtins.object)\n","     |  RAGConfig(chunk_size: int = 200, chunk_overlap: int = 20, text_threshold: float = 1.2, image_threshold: float = 0.8, clip_model: str = 'clip-ViT-B-32', text_model: str = 'text-embedding-3-small', llm_model: str = 'gpt-4o-mini', vision_model: str = 'gpt-4o-mini', db_path: str = './multimodal_rag_db_enhanced') -> None\n","     |\n","     |  Zentrale Konfiguration f√ºr das RAG-System\n","     |\n","     |  Methods defined here:\n","     |\n","     |  __eq__(self, other)\n","     |      Return self==value.\n","     |\n","     |  __init__(self, chunk_size: int = 200, chunk_overlap: int = 20, text_threshold: float = 1.2, image_threshold: float = 0.8, clip_model: str = 'clip-ViT-B-32', text_model: str = 'text-embedding-3-small', llm_model: str = 'gpt-4o-mini', vision_model: str = 'gpt-4o-mini', db_path: str = './multimodal_rag_db_enhanced') -> None\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |\n","     |  __repr__(self)\n","     |      Return repr(self).\n","     |\n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors defined here:\n","     |\n","     |  __dict__\n","     |      dictionary for instance variables\n","     |\n","     |  __weakref__\n","     |      list of weak references to the object\n","     |\n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |\n","     |  __annotations__ = {'chunk_overlap': <class 'int'>, 'chunk_size': <clas...\n","     |\n","     |  __dataclass_fields__ = {'chunk_overlap': Field(name='chunk_overlap',ty...\n","     |\n","     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n","     |\n","     |  __hash__ = None\n","     |\n","     |  __match_args__ = ('chunk_size', 'chunk_overlap', 'text_threshold', 'im...\n","     |\n","     |  chunk_overlap = 20\n","     |\n","     |  chunk_size = 200\n","     |\n","     |  clip_model = 'clip-ViT-B-32'\n","     |\n","     |  db_path = './multimodal_rag_db_enhanced'\n","     |\n","     |  image_threshold = 0.8\n","     |\n","     |  llm_model = 'gpt-4o-mini'\n","     |\n","     |  text_model = 'text-embedding-3-small'\n","     |\n","     |  text_threshold = 1.2\n","     |\n","     |  vision_model = 'gpt-4o-mini'\n","\n","FUNCTIONS\n","    add_image_with_description(components, image_path, auto_describe=True)\n","        F√ºgt ein Bild mit automatischer Beschreibung zur Datenbank hinzu\n","\n","        Args:\n","            components: RAG-System-Komponenten\n","            image_path: Pfad zum Bild\n","            auto_describe: Automatische Beschreibung mit GPT-4o-mini\n","\n","        Returns:\n","            Tuple (success: bool, text_doc_id: str oder None)\n","\n","    add_text_document(components, file_path)\n","        F√ºgt ein Text-Dokument zur Datenbank hinzu\n","\n","        Args:\n","            components: RAG-System-Komponenten\n","            file_path: Pfad zum Dokument\n","\n","        Returns:\n","            bool - Erfolg\n","\n","    cleanup_database(db_path='./multimodal_rag_db_enhanced')\n","        L√∂scht die Datenbank komplett\n","\n","    find_related_images_from_text(components, text_doc_ids, k=3)\n","        Findet Bilder √ºber ihre Textbeschreibungen (Cross-Modal-Retrieval)\n","\n","        Args:\n","            components: RAG-System-Komponenten\n","            text_doc_ids: Liste von Text-Dokument-IDs\n","            k: Maximale Anzahl Bilder\n","\n","        Returns:\n","            Liste von verwandten Bildern\n","\n","    generate_image_description(vision_llm, image_path)\n","        Generiert eine detaillierte Beschreibung eines Bildes mit GPT-4o-mini\n","\n","        Args:\n","            vision_llm: ChatOpenAI Instanz mit Vision-Unterst√ºtzung\n","            image_path: Pfad zum Bild\n","\n","        Returns:\n","            String mit Bildbeschreibung oder Fallback bei Fehler\n","\n","    get_system_status(components)\n","        Gibt System-Status zur√ºck\n","\n","    init_rag_system_enhanced(config=None)\n","        Initialisiert das vollst√§ndige RAG-System mit Vision-LLM\n","\n","        Args:\n","            config: Optional - RAGConfig Instanz\n","\n","        Returns:\n","            RAGComponents mit allen Komponenten\n","\n","    multimodal_search(components, query, k_text=3, k_images=3, enable_cross_modal=True)\n","        F√ºhrt erweiterte multimodale Suche durch\n","\n","        Kombiniert:\n","        - Text-Suche (inkl. Bildbeschreibungen)\n","        - CLIP-basierte Bildsuche\n","        - Cross-Modal-Retrieval (Text ‚Üí Bild √ºber Beschreibungen)\n","\n","        Args:\n","            components: RAG-System-Komponenten\n","            query: Suchanfrage\n","            k_text: Anzahl Text-Ergebnisse\n","            k_images: Anzahl Bild-Ergebnisse\n","            enable_cross_modal: Cross-Modal-Retrieval aktivieren\n","\n","        Returns:\n","            Formatierter String mit allen Ergebnissen\n","\n","    process_directory(components, directory, include_images=True, auto_describe_images=True)\n","        Verarbeitet alle Dateien in einem Verzeichnis\n","\n","        Args:\n","            components: RAG-System-Komponenten\n","            directory: Verzeichnispfad\n","            include_images: Bilder verarbeiten\n","            auto_describe_images: Automatische Bildbeschreibungen\n","\n","        Returns:\n","            Dictionary mit Statistiken\n","\n","    search_images(components, query, k=3)\n","        Durchsucht Bilder mit Text-Query √ºber CLIP\n","\n","        Args:\n","            components: RAG-System-Komponenten\n","            query: Suchanfrage\n","            k: Anzahl Ergebnisse\n","\n","        Returns:\n","            Liste von Bildern mit Metadaten\n","\n","    search_texts(components, query, k=3, include_image_descriptions=True)\n","        Durchsucht Text-Dokumente inkl. Bildbeschreibungen\n","\n","        Args:\n","            components: RAG-System-Komponenten\n","            query: Suchanfrage\n","            k: Anzahl Ergebnisse\n","            include_image_descriptions: Bildbeschreibungen einschlie√üen\n","\n","        Returns:\n","            Formatierter String mit Ergebnissen\n","\n","FILE\n","    /content/M14a_standalone.py\n","\n","\n"]}]},{"cell_type":"code","source":["# 1. (Import statement removed - functions should be available from %run M14_Multimodal_RAG.ipynb)\n","from pathlib import Path\n","\n","# 2. Initialisierung\n","rag_components = m14.init_rag_system_enhanced()\n","\n","# Dokumente/Bilder verarbeiten\n","process_directory(rag_components, './files', auto_describe_images=True)\n","\n","# Suchen\n","result = multimodal_search(rag_components, \"Apfel mit hellem neutralen Hintergrund\")\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pLf6ZW9b7N2r","executionInfo":{"status":"ok","timestamp":1760717886728,"user_tz":-120,"elapsed":5090,"user":{"displayName":"Ralf Bendig","userId":"13914949880386033641"}},"outputId":"2d9fcb76-d3b0-46d4-b2d6-0463fcf4de89"},"id":"pLf6ZW9b7N2r","execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","üîç Multimodale Suche: Apfel mit hellem neutralen Hintergrund\n","======================================================================\n","\n","üìÑ TEXT-ERGEBNISSE:\n","----------------------------------------------------------------------\n","Das Bild zeigt einen einzelnen, gl√§nzenden roten Apfel auf einem neutralen, hellen Hintergrund. Die lebhaften Farben und die einfache Komposition heben den Apfel als Hauptobjekt hervor.\n","\n","üñºÔ∏è Relevante Bilder (3):    ‚Ä¢ apfel.jpg (√Ñhnlichkeit: 1)\n","   ‚Ä¢ a_retro-futuristic_robot_dall_e.jpg (√Ñhnlichkeit: 0.902)\n","   ‚Ä¢ hedra_cyborg.png (√Ñhnlichkeit: 0.885)\n","\n","üñºÔ∏è BILD-ERGEBNISSE via CLIP (1 gefunden):\n","----------------------------------------------------------------------\n","   1. apfel.jpg (√Ñhnlichkeit: 0.272)\n","      üìù Das Bild zeigt einen einzelnen, gl√§nzenden roten Apfel, der auf einem neutralen, hellen Hintergrund platziert ist. Die Farben sind lebhaft, mit versch...\n","\n","üîó CROSS-MODAL RETRIEVAL (3 Bilder via Textsuche):\n","----------------------------------------------------------------------\n","   1. apfel.jpg\n","      üìù Das Bild zeigt einen einzelnen, gl√§nzenden roten Apfel, der auf einem neutralen, hellen Hintergrund platziert ist. Die Farben sind lebhaft, mit versch...\n","   2. a_retro-futuristic_robot_dall_e.jpg\n","      üìù Das Bild zeigt einen freundlichen, silbernen Roboter, der vor einem Geb√§ude steht, das mit einem Schild \"WELCOME ROBOTS WELCOME\" dekoriert ist. Die Fa...\n","   3. hedra_cyborg.png\n","      üìù Das Bild zeigt einen m√§nnlichen Charakter in einem futuristischen, r√ºstungs√§hnlichen Anzug, der aus dunklem, metallischem Material besteht. Die Farben...\n","\n"]}]},{"cell_type":"markdown","id":"ce95c6f6","metadata":{"id":"ce95c6f6"},"source":["## 1. Setup\n","Zuerst f√ºhren wir das Original-Notebook M14 bis Sektion 2 aus."]},{"cell_type":"code","execution_count":null,"id":"09c90467","metadata":{"id":"09c90467"},"outputs":[],"source":["%run M14_Multimodal_RAG.ipynb"]},{"cell_type":"markdown","id":"4fecb127","metadata":{"id":"4fecb127"},"source":["## 3. Enhanced RAG-System initialisieren"]},{"cell_type":"code","execution_count":null,"id":"1f4a9089","metadata":{"id":"1f4a9089"},"outputs":[],"source":["# Initialisiere mit Vision-LLM Support\n","rag_components = init_enhanced_rag_system()"]},{"cell_type":"markdown","id":"c2176748","metadata":{"id":"c2176748"},"source":["## 4. Bildbeschreibung testen"]},{"cell_type":"code","execution_count":null,"id":"0aefcfc9","metadata":{"id":"0aefcfc9"},"outputs":[],"source":["# Teste Bildbeschreibung f√ºr ein einzelnes Bild\n","test_image = './files/apfel.jpg'\n","\n","if Path(test_image).exists():\n","    description = generate_image_description(rag_components.vision_llm, test_image)\n","    print(f'Beschreibung: {description}')\n","else:\n","    print(f'Bild nicht gefunden: {test_image}')"]},{"cell_type":"markdown","id":"34a5897b","metadata":{"id":"34a5897b"},"source":["## 5. Verzeichnis mit automatischen Bildbeschreibungen verarbeiten"]},{"cell_type":"code","execution_count":null,"id":"67a8e52d","metadata":{"id":"67a8e52d"},"outputs":[],"source":["# Verarbeite alle Dateien mit automatischen Bildbeschreibungen\n","results = process_directory_enhanced(\n","    rag_components,\n","    './files',\n","    include_images=True,\n","    auto_describe_images=True\n",")\n","\n","print(f'Text-Dokumente: {results[\"texts\"]}')\n","print(f'Bilder: {results[\"images\"]}')\n","print(f'Bildbeschreibungen: {results[\"image_descriptions\"]}')"]},{"cell_type":"markdown","id":"b87f3a78","metadata":{"id":"b87f3a78"},"source":["## 6. Erweiterte Textsuche (inkl. Bildbeschreibungen)"]},{"cell_type":"code","execution_count":null,"id":"8ff69090","metadata":{"id":"8ff69090"},"outputs":[],"source":["# Textsuche findet nun auch Bildbeschreibungen\n","query = 'Roboter'\n","result = search_texts_enhanced(rag_components, query, k=5)\n","print(result)"]},{"cell_type":"markdown","id":"93b895e0","metadata":{"id":"93b895e0"},"source":["## 7. Multimodale Suche mit Cross-Modal-Retrieval"]},{"cell_type":"code","execution_count":null,"id":"d1e21469","metadata":{"id":"d1e21469"},"outputs":[],"source":["# Kombinierte Suche √ºber alle Modalit√§ten\n","queries = ['Roboter', 'Cyborg', 'Apfel']\n","\n","for query in queries:\n","    print(f'\n","=== Query: {query} ===')\n","    result = multimodal_search_enhanced(\n","        rag_components,\n","        query,\n","        k_text=3,\n","        k_images=3,\n","        enable_cross_modal=True\n","    )\n","    print(result)"]},{"cell_type":"markdown","id":"fc4f14e7","metadata":{"id":"fc4f14e7"},"source":["## 8. Cross-Reference-Analyse"]},{"cell_type":"code","execution_count":null,"id":"f122aebf","metadata":{"id":"f122aebf"},"outputs":[],"source":["# Analysiere Cross-References zwischen Collections\n","all_texts = rag_components.text_collection.get()\n","image_descriptions = [\n","    (id, meta) for id, meta in zip(all_texts['ids'], all_texts['metadatas'])\n","    if meta.get('doc_type') == 'image_description'\n","]\n","\n","print(f'Gefundene Bildbeschreibungen: {len(image_descriptions)}')\n","\n","for text_id, meta in image_descriptions[:3]:\n","    print(f\"\n","{meta.get('filename', 'Unbekannt')}:\")\n","    print(f\"  Text-ID: {text_id}\")\n","    print(f\"  Bild-ID: {meta.get('image_doc_id', 'N/A')}\")\n","    print(f\"  Beschreibung: {meta.get('description', 'N/A')[:100]}...\")"]},{"cell_type":"markdown","id":"32f91e5b","metadata":{"id":"32f91e5b"},"source":["## 9. Vergleich: Original vs. Enhanced\n","\n","### Vorteile der Enhanced Version:\n","\n","1. **Semantische Bildsuche**: Textanfragen finden automatisch relevante Bilder\n","2. **Bessere Trefferquote**: Bildbeschreibungen erh√∂hen Recall\n","3. **Einheitliche Suche**: Eine Query durchsucht Text und Bilder\n","4. **Explainable AI**: Beschreibungen machen Ergebnisse nachvollziehbar\n","5. **Cross-Modal Intelligence**: System versteht Zusammenh√§nge"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}
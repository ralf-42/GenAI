{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["PnODVpOdGWOS","h4lbK7_ljUbb","NS2tCPL_uKLY","Ii6TLuoCGsR-","b_qDQ5vxeXV7","jNHqE7q5eaqR","0-eSumHCmlFZ","D8I9UIkZ7yRP","NkrKt6cPGWOT"],"toc_visible":true,"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/genai-banner-2.jpg)"],"metadata":{"id":"Ih2CTVBnArVZ"}},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b>\n","Multimodal - Bild\n","</b></font> </br></p>\n","\n","\n","---"],"metadata":{"id":"6jJZ7wbdArVc"}},{"cell_type":"code","source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>Umgebung einrichten</font> </br></p>\n","!uv pip install --system --prerelease allow -q git+https://github.com/ralf-42/genai_lib\n","from genai_lib.utilities import check_environment, get_ipinfo, setup_api_keys, mprint\n","setup_api_keys(['OPENAI_API_KEY', 'HF_TOKEN'], create_globals=False)\n","print()\n","check_environment()\n","print()\n","get_ipinfo()"],"metadata":{"cellView":"form","id":"Hf10l_FTn3aM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PnODVpOdGWOS"},"source":["\n","\n","# 1 | Einf√ºhrung\n","---"]},{"cell_type":"markdown","source":["Dieses Modul besch√§ftigt sich mit der Analyse von Bildern und ihrer Rolle als Basis f√ºr multimodale Systeme. Solche Modelle stellen einen bedeutenden Fortschritt in der KI dar, da sie geschriebene Sprache in visuelle Darstellungen umwandeln. Bekannte Beispiele wie DALL¬∑E, Midjourney und Stable Diffusion nutzen Deep-Learning-Techniken, um detaillierte Bilder auf Grundlage von Textbeschreibungen zu erzeugen. Dabei erfassen sie die Zusammenh√§nge zwischen sprachlichen und visuellen Elementen, wodurch sie in der Lage sind, Szenen, Objekte und Konzepte entsprechend der gegebenen Beschreibung zu generieren.  \n","\n","Multimodale Modelle gehen √ºber diese Funktionalit√§t hinaus, indem sie mehrere Datenquellen kombinieren ‚Äì in der Regel Text,  Bilder, Audio oder Video. Ihr Ziel ist es, verschiedene Informationsarten simultan zu verarbeiten und in Beziehung zu setzen. So k√∂nnen sie beispielsweise Bildbeschreibungen aus Text erstellen, Fragen zu visuellen Inhalten beantworten oder sogar Videos auf Basis von Texteingaben generieren. Durch die Verbindung der F√§higkeiten von Text- und Bildmodellen erweitern multimodale Systeme das Anwendungsspektrum der KI und erm√∂glichen eine flexiblere, kontextbezogene Interaktion.  \n","\n","W√§hrend **Text-zu-Bild-Modelle** prim√§r darauf ausgerichtet sind, sprachliche Eingaben in visuelle Inhalte umzusetzen, erm√∂glichen **multimodale Modelle** eine komplexere Verkn√ºpfung verschiedener Medientypen. In diesem Modul wird untersucht, wie diese Modelle funktionieren und wie ihre Architektur die Entwicklung weiterf√ºhrender multimodaler Systeme beeinflusst. Zudem werden praxisnahe Anwendungsf√§lle betrachtet sowie m√∂gliche zuk√ºnftige Entwicklungen aufgezeigt, die sich aus der Kombination unterschiedlicher Datenmodalit√§ten ergeben."],"metadata":{"id":"eazq6nuElGNS"}},{"cell_type":"markdown","source":["OpenAI bietet mit DALL¬∑E und GPT Image 1 zwei fortschrittliche KI-Modelle zur Bildgenerierung an. Beide Modelle wandeln Texteingaben in Bilder um, unterscheiden sich jedoch in ihren Funktionen und Einsatzm√∂glichkeiten.\n","\n"],"metadata":{"id":"ZUFRWWopmOpQ"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","DALL¬∑E\n","</font></p>\n","\n","- **Funktion:** Generiert Bilder aus Textbeschreibungen.\n","- **Stilvielfalt:** Von realistisch bis abstrakt, viele Motive und Stile m√∂glich.\n","- **API-Zugriff:** √úber die OpenAI-API leicht in Webanwendungen, Apps und Plattformen integrierbar.\n","- **Anwendungsbereiche:** Design, Marketing, Unterhaltung, Bildung.\n","- **Vorteil:** Schnelle und kreative Visualisierung von Ideen.\n"],"metadata":{"id":"su68WRiQHUMQ"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","GPT Image 1\n","</font></p>\n","\n","- **Funktion:** Modernes, multimodales Bildmodell ‚Äì verarbeitet sowohl Text- als auch Bildeingaben.\n","- **Bildbearbeitung:** Unterst√ºtzt gezielte Bildbearbeitung (z.‚ÄØB. Inpainting, Kombination mehrerer Bilder).\n","- **Flexible Steuerung:** Nutzer k√∂nnen Parameter wie Bildgr√∂√üe, Qualit√§t und Hintergrundtyp anpassen.\n","- **Integration:** √úber API in verschiedene Plattformen (z.‚ÄØB. Figma, Adobe) einbindbar.\n","- **Sicherheit:** Moderation und Kennzeichnung der generierten Bilder durch C2PA-Metadaten.\n","- **Anwendungsbereiche:** Besonders geeignet f√ºr professionelle Kreativ- und Content-Workflows.\n","\n"],"metadata":{"id":"_NcaYKPcHboa"}},{"cell_type":"markdown","source":["<p><font color='red' size=\"4\">\n","<b>Hinweis</b>\n","</font></p>\n","\n","Das Modell gpt-image-1 kann nur von verifizierten Organisationen genutzt werden, weil OpenAI damit die Nutzung gezielt kontrollieren und Missbrauch verhindern m√∂chte. Hintergrund ist, dass gpt-image-1 besonders realistische und detaillierte Bilder erzeugen kann. Damit steigt das Risiko, dass das Modell f√ºr problematische Zwecke wie Deepfakes, Falschinformationen oder unangemessene Inhalte eingesetzt wird."],"metadata":{"id":"7NK1nsJTLv54"}},{"cell_type":"markdown","source":["**Fazit:**  \n","DALL¬∑E eignet sich ideal f√ºr die schnelle Umsetzung von Bildideen aus Text. GPT Image 1 bietet dar√ºber hinaus erweiterte M√∂glichkeiten zur Bildbearbeitung und -kombination und ist besonders f√ºr professionelle Anwendungen interessant."],"metadata":{"id":"IcQ4HdSZHgzZ"}},{"cell_type":"markdown","source":["| **Kriterium**         | **DALL¬∑E 3**                                                                 | **GPT Image** (z.‚ÄØB. GPT-4o)                                                                            |\n","|-----------------------|------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|\n","| **Bildgenerierung**   | Mehrere Bilder aus Textbeschreibungen, hohe Prompt-Adh√§renz                  | Bilder aus Text und/oder Bildeingaben (multimodal), realistische Licht- und Schatteneffekte             |\n","| **Aufl√∂sungen**       | 256√ó256, 512√ó512, 1024√ó512, 1024√ó1024, 1024√ó1792, 1792√ó1024                  | √Ñhnliche Formate wie DALL¬∑E 3, flexibel √ºber die API ausw√§hlbar                                         |\n","| **Qualit√§tsstufen**   | Standard, HD                                                                 | Standard, HD                                                                                            |\n","| **Nutzererfahrung**   | √úber ChatGPT oder API, einfache Bedienung, bis zu 10 Bilder pro Anfrage      | √úber API oder ChatGPT, dialogorientiert, kombiniert Text- und Bildeingaben                              |\n","| **Anwendungsbereiche**| Design, Marketing, Illustration, Bildung, kreative Projekte                  | Professionelle Kreativ-Workflows, Content-Produktion, Design, Werbung, k√ºnstlerische Projekte           |\n","| **Besonderheiten**    | Sehr gute Textumsetzung im Bild, schnelle Integration, API-Anbindung         | Multimodal (Text & Bild), fortschrittliche Bildbearbeitung (z.‚ÄØB. Inpainting), C2PA-Metadaten, Moderation|\n"],"metadata":{"id":"7_0InuAKmqjw"}},{"cell_type":"markdown","source":["# 2 | √úbersicht Bildverarbeitung\n","---"],"metadata":{"id":"h4lbK7_ljUbb"}},{"cell_type":"markdown","source":["| **Kategorie**                        | **Aufgabe**                      | **Beschreibung**                                                                      |\n","| ------------------------------------ | -------------------------------- | ------------------------------------------------------------------------------------- |\n","| üß† **Analyse & Klassifikation**      | **Bildklassifikation**           | Zuordnung eines Bildes zu vordefinierten Klassen (z.‚ÄØB. Katze, Hund, Auto).           |\n","|                                      | **Objekterkennung**                  | Erkennung und Lokalisierung mehrerer Objekte in einem Bild mit Bounding Boxes.        |\n","|                                      | Bildsegmentierung                | Pixelgenaue Zuordnung von Bildbereichen zu Klassen (z.‚ÄØB. Stra√üe, Baum, Mensch).      |\n","|                                      | Gesichtserkennung                | Identifikation oder Verifikation von Personen auf Bildern.                            |\n","|                                      | Emotionserkennung                | Analyse der Gesichtsausdr√ºcke zur Einsch√§tzung von Emotionen.                         |\n","|                                      | Anomalieerkennung                | Erkennung ungew√∂hnlicher oder fehlerhafter Bildinhalte (z.‚ÄØB. Produktionsfehler).     |\n","| üìù **Generierung & Transformation**  | **Bildgenerierung**              | Erzeugung synthetischer Bilder, z.‚ÄØB. aus Text (Text-to-Image) oder Rauschen.         |\n","|                                      | Bild-zu-Bild-√úbersetzung         | Umwandlung von Bildern (z.‚ÄØB. Skizze ‚Üí Foto, Tag ‚Üí Nacht).                            |\n","|                                      | Stiltransfer                     | √úbertragung des Stils eines Bildes auf ein anderes (z.‚ÄØB. Van Gogh ‚Üí Foto).           |\n","|                                      | Super-Resolution                 | Hochskalierung von Bildern mit k√ºnstlicher Detailsch√§rfung.                           |\n","|                                      | Bildrestauration                 | Entfernung von Rauschen, Kratzern oder Artefakten in alten oder besch√§digten Bildern. |\n","|                                      | Colorisierung                    | Umf√§rben von Schwarzwei√ü-Bildern mit realistischen Farben.                            |\n","| üß© **Erg√§nzung & Vervollst√§ndigung** | **Inpainting**                       | Auff√ºllen fehlender oder besch√§digter Bildbereiche.                                   |\n","|                                      | **Bildvervollst√§ndigung**        | Vorhersage fehlender Bildbereiche basierend auf Kontext.                              |\n","|                                      | Hintergrundentfernung            | Trennung des Vordergrundobjekts vom Hintergrund.                                      |\n","| üìö **Informationsgewinnung**         | Texterkennung (OCR)              | Erkennung und Extraktion von Text aus Bildern.                                        |\n","|                                      | Schrifterkennung                 | Identifikation von Schriftarten oder Handschriften.                                   |\n","|                                      | **Bildbeschreibung (Captioning)**    | Automatische Erzeugung eines beschreibenden Textes zu einem Bild.                     |\n","|                                      | visuelle Fragebeantwortung (VQA) | Beantwortung von Fragen basierend auf einem Bildinhalt.                               |\n","| üõ°Ô∏è **Sicherheit & Strukturierung**  | Deepfake-Erkennung               | Analyse, ob ein Bild manipuliert oder synthetisch erzeugt wurde.                      |\n","|                                      | Gesichtsverpixelung              | Anonymisierung durch Unkenntlichmachung von Gesichtern.                               |\n","|                                      | Wasserzeichen-Erkennung          | Detektion sichtbarer oder unsichtbarer Wasserzeichen.                                 |\n","|                                      | Formatkonvertierung              | Umwandlung zwischen Bildformaten (z.‚ÄØB. PNG ‚Üî JPG).                                   |\n"],"metadata":{"id":"NBDZGdImjSfc"}},{"cell_type":"markdown","metadata":{"id":"NS2tCPL_uKLY"},"source":["\n","# 3 | Bildgenerierung\n","---"]},{"cell_type":"markdown","source":["Die Bildgenerierung mit K√ºnstlicher Intelligenz (KI) erm√∂glicht es, aus einfachen Texteingaben oder bestehenden Bildern v√∂llig neue, realistische oder kreative Grafiken zu erstellen. Moderne KI-Modelle wie DALL¬∑E 3 und GPT Image von OpenAI setzen dabei auf fortschrittliche Algorithmen, die Texte pr√§zise interpretieren und in beeindruckende visuelle Darstellungen umsetzen. Diese Technologie findet Anwendung in vielen Bereichen ‚Äì von Design und Marketing √ºber Kunst bis hin zur Bildung ‚Äì und er√∂ffnet neue kreative M√∂glichkeiten f√ºr Einsteiger und Profis gleicherma√üen.\n"],"metadata":{"id":"WF14JiXeInfI"}},{"cell_type":"markdown","source":["Der folgende Code erzeugt das Bild einer H√ºtte im Wald."],"metadata":{"id":"KwpLS-kkUMrU"}},{"cell_type":"code","source":["import requests\n","import os\n","from openai import OpenAI\n","from io import BytesIO\n","from PIL import Image\n","from IPython.display import display\n","\n","# Parameter\n","MODEL = \"dall-e-3\"\n","PROMPT = \"Erstelle ein Bild mit einer H√ºtte im Wald, die H√ºtte wird von der Sonne beschienen.\"\n","NUM_IMAGES = 2\n","DISPLAY_SIZE = (512, 512)  # Anzeigegr√∂√üe der Bilder (Breite, H√∂he)\n","\n","def generate_and_save_images():\n","    client = OpenAI()\n","\n","    for i in range(NUM_IMAGES):\n","        response = client.images.generate(model=MODEL, prompt=PROMPT, size=\"1024x1024\", quality=\"standard\", n=1)\n","        img = Image.open(BytesIO(requests.get(response.data[0].url).content))\n","\n","        # Bild speichern\n","        filename = f\"bild_{i+1}.png\"\n","        img.save(filename)\n","        print(f\"Gespeichert: {filename}\")\n","\n","        # Bildgr√∂√üe anpassen und anzeigen\n","        img_resized = img.resize(DISPLAY_SIZE)\n","        display(img_resized)\n","\n","generate_and_save_images()"],"metadata":{"id":"yJtGp01SIsJU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Erl√§uterung:**\n","\n","`BytesIO` ist ein datei√§hnliches Objekt, das Daten im Speicher statt auf der Festplatte h√§lt. Es verh√§lt sich wie eine Datei, aber alles passiert im Arbeitsspeicher."],"metadata":{"id":"w3G-unbwJp_t"}},{"cell_type":"markdown","metadata":{"id":"Ii6TLuoCGsR-"},"source":["\n","# 4 | Bildklassifizierung\n","---"]},{"cell_type":"markdown","source":["Die Bildklassifizierung ist eine grundlegende Aufgabe in der Computer Vision, bei der einem gesamten Bild eine einzige Kategorie (Klasse) zugewiesen wird.\n","\n","**Wie funktioniert Bildklassifizierung?**\n","+ Eingabe: Ein einzelnes Bild wird als Eingabe verwendet.\n","+ Merkmalsextraktion: Ein neuronales Netzwerk analysiert das Bild und extrahiert relevante Merkmale (z. B. Kanten, Farben, Formen, Muster).\n","+ Klassifizierung: Das Modell ordnet das Bild einer vordefinierten Kategorie zu, z. B. \"Hund\", \"Katze\" oder \"Auto\".\n","+ Ausgabe: Eine einzige Klasse (Label) mit einer Wahrscheinlichkeitsbewertung wird zur√ºckgegeben.\n","\n","\n","Beispiel f√ºr Bildklassifizierung\n","Stelle dir vor, du hast ein Bild von einem Hund. Ein Bildklassifizierungsmodell verarbeitet das Bild und gibt die Kategorie \"Hund\" mit einer bestimmten Wahrscheinlichkeit (z. B. 95 %) zur√ºck.\n","\n","+ Eingabebild: Ein Bild eines Hundes\n","+ Modell-Ausgabe: \"Hund\" (95%)\n","\n","Falls das Bild eine Katze zeigt, gibt das Modell m√∂glicherweise \"Katze\" (90%) als Ergebnis zur√ºck.\n","\n","**Einschr√§nkungen der Bildklassifizierung**     \n","Das Modell kann nur eine Klasse pro Bild erkennen, auch wenn mehrere Objekte im Bild vorhanden sind.\n","Es gibt keine Information √ºber die Position oder Anzahl der Objekte im Bild.\n","Anwendungsf√§lle f√ºr Bildklassifizierung\n","+ Erkennung von medizinischen Anomalien (z. B. Klassifikation von R√∂ntgenbildern)\n","+ Identifikation von Pflanzen oder Tieren anhand von Bildern\n","+ Sentiment-Analyse anhand von Gesichtsmerkmalen\n","\n","**Bekannte Modelle f√ºr Bildklassifizierung**    \n","+ CNNs (Convolutional Neural Networks) wie ResNet, VGG, EfficientNet\n","+ Pretrained Modelle: MobileNet, Inception, AlexNet\n","\n","\n","Man kann allerdings auch gpt-4o-mini mit einem entsprechenden Prompt hierzu einsetzen."],"metadata":{"id":"mq1NAdlem-lu"}},{"cell_type":"code","source":["!uv pip install --system --prerelease allow -q openai pillow"],"metadata":{"id":"0yA2teXbQk4Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import base64\n","from openai import OpenAI\n","from google.colab import files\n","from IPython.display import Image, display, Markdown\n","\n","client = OpenAI()\n","\n","# Datei-Auswahldialog √∂ffnen\n","uploaded = files.upload()\n","\n","# Dateinamen abrufen (das erste hochgeladene Bild)\n","image_path = list(uploaded.keys())[0]\n","\n","# Bild in Base64 kodieren\n","encoded_image = base64.b64encode(uploaded[image_path]).decode(\"utf-8\")\n","\n","prompt_text = \"Was ist auf dem Bild? Was ist auf diesem Bild abgebildet? Nennen nur das wesentliche Objekt.\"\n","image_url = f\"data:image/jpeg;base64,{encoded_image}\"\n","\n","messages = [{\n","    \"role\": \"user\",\n","    \"content\": [\n","        {\"type\": \"text\", \"text\": prompt_text},\n","        {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n","    ]\n","}]\n","\n","response = client.chat.completions.create(\n","    model=\"gpt-4o-mini\",\n","    messages=messages,\n","    max_tokens=300,\n",")\n","\n","print(response.choices[0].message.content)\n","\n","# Bild in Colab anzeigen mit begrenzter Gr√∂√üe\n","display(Image(uploaded[image_path], width=500))  # H√∂he wird proportional angepasst"],"metadata":{"id":"ZLsOlRByQk4Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Hinweis zu Base64-Kodierung:**\n"],"metadata":{"id":"B5XSxRoO3IoU"}},{"cell_type":"markdown","source":["\n","\n","Wann braucht man **keine** Base64-Kodierung?\n","\n","‚úÖ Wenn man eine **√∂ffentlich zug√§ngliche URL** (z.‚ÄØB. von einem Webserver, CDN oder GitHub) √ºbergibt.     \n","‚ùå Wenn man das Bild **lokal hochgeladen hat** (z.‚ÄØB. √ºber ein Gradio-Interface oder Web-Upload) und es **noch nicht online verf√ºgbar** ist.\n"],"metadata":{"id":"BlUTLwDb3gTB"}},{"cell_type":"markdown","source":["\n","\n","Fazit:\n","\n","* üìÅ **Lokale Datei** ‚Üí **Base64 + `data:`-URL**\n","* üåê **Online-URL** ‚Üí **keine Base64 n√∂tig**\n","\n"],"metadata":{"id":"wjMfa7cT3rQh"}},{"cell_type":"markdown","source":["# 5 | Inpainting\n","---"],"metadata":{"id":"b_qDQ5vxeXV7"}},{"cell_type":"markdown","source":["Inpainting ist eine Technik im Bereich der k√ºnstlichen Intelligenz und Bildverarbeitung, bei der bestimmte Teile eines Bildes automatisch erg√§nzt oder repariert werden. Die Technik wird eingesetzt, um:\n","\n","+ Besch√§digte Bereiche in Bildern zu rekonstruieren\n","+ Unerw√ºnschte Objekte aus Bildern zu entfernen\n","+ Fehlende Teile in Bildern zu erg√§nzen\n","\n","**Inpainting mit Maske:**   \n","Bei Inpainting mit Masken geht es darum, genau zu definieren, welche Bereiche eines Bildes rekonstruiert werden sollen. Die Maske ist dabei ein Schl√ºsselelement, das dem Algorithmus mitteilt, wo er arbeiten soll. Eine Maske ist ein Bin√§rbild (Schwarz-Wei√ü-Bild), das die gleichen Dimensionen wie das Originalbild hat:\n","\n","+ Wei√üe Bereiche (255): Zeigen an, welche Teile des Bildes rekonstruiert/gef√ºllt werden sollen\n","+ Schwarze Bereiche (0): Zeigen an, welche Teile des Originalbildes erhalten bleiben sollen"],"metadata":{"id":"vf_OjFoae1CJ"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Modell/API-√úbersicht\n","</font></p>"],"metadata":{"id":"a8zZGTRviJ7I"}},{"cell_type":"markdown","source":["| Modell / Anbieter         | API-Zugang m√∂glich? | Hinweise |\n","| :------------------------| :------------------ | :------- |\n","| **Picsart Outpainting**   | Ja                 | Outpainting-Service per API, gezieltes Erweitern oder Ersetzen von Bildbereichen. Auch Inpainting-Funktion verf√ºgbar. |\n","| **IOPaint**               | Nein (Web-App & Open Source) | Open-Source-Tool f√ºr Inpainting/Outpainting, lokal oder selbst gehostet, keine √∂ffentliche API dokumentiert. Unterst√ºtzt verschiedene KI-Modelle, WebUI verf√ºgbar. |\n","| **Recraft**               | Nein (Web-App)     | KI-Inpainting und Outpainting mit hoher Pr√§zision, Fokus auf kreatives Editing, keine √∂ffentliche API dokumentiert. |\n","| **Flux Tools (Black Forest Lab)** | Nein (ComfyUI-Workflow) | Inpainting & Outpainting im ComfyUI-Workflow, leistungsstarke ‚ÄûFill‚Äú-Modelle, keine eigenst√§ndige API, Integration √ºber ComfyUI m√∂glich. |\n","| **Amazon Bedrock (SDXL, Titan Image Generator)** | Ja | Text-gef√ºhrtes Inpainting & Outpainting per API, unterst√ºtzt Stable Diffusion XL und Titan Image Generator, flexible Integration, keine Infrastrukturverwaltung n√∂tig. |\n","| **Google Vertex AI (Imagen 2)** | Ja | Inpainting (Objekte einf√ºgen/entfernen) per API, Outpainting-Funktion f√ºr Bildbereichserweiterung, Integration in Vertex AI Generative AI Studio. |\n","| **OpenAI DALL¬∑E**         | Ja                 | Inpainting (‚ÄûEdit‚Äú-Funktion) und Outpainting per API und Web-App, nat√ºrliche Sprachsteuerung, nahtlose Bildbearbeitung und Erweiterung. |\n"],"metadata":{"id":"I0R8YWm4iFAY"}},{"cell_type":"markdown","source":["[Inpainting](https://huggingface.co/docs/diffusers/using-diffusers/inpaint)"],"metadata":{"id":"drS3Z_UgYmYn"}},{"cell_type":"markdown","source":["`AutoPipelineForInpainting` aus der Hugging Face Diffusers-Bibliothek vereinfacht die Anwendung von Inpainting-Modellen, indem sie automatisch die geeignete Pipeline f√ºr das Inpainting ausw√§hlt. Dies erleichtert die Nutzung verschiedener Modelle, ohne dass man sich um die spezifischen Pipeline-Klassen k√ºmmern muss."],"metadata":{"id":"38YiReuy6svc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"P86mKqKXZRic"},"outputs":[],"source":["# Import\n","import torch\n","from diffusers import AutoPipelineForInpainting\n","from diffusers.utils import load_image, make_image_grid\n","\n","# Laden des vortrainierten Modells\n","pipeline = AutoPipelineForInpainting.from_pretrained(\n","    \"kandinsky-community/kandinsky-2-2-decoder-inpaint\", torch_dtype=torch.float16\n",")\n","pipeline.enable_model_cpu_offload()"]},{"cell_type":"code","source":["# Laden der Bilder\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02%20data/hedra_cyborg.png -o init_image.png\n","!curl -L https://raw.githubusercontent.com/ralf-42/GenAI/main/02%20data/hedra_cyborg_mask_image.png -o mask_image.png\n","\n","init_image = load_image(\"/content/init_image.png\")\n","mask_image = load_image(\"/content/mask_image.png\")"],"metadata":{"id":"YMyU9M6cYb6T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prompt und Modell-Aufruf\n","prompt = \"\"\"\n","a head of fox, Ultra-realistic, high-resolution\n","\"\"\"\n","negative_prompt = \"bad anatomy, deformed, ugly, disfigured\"\n","\n","new_image = pipeline(prompt=prompt, negative_prompt=negative_prompt, image=init_image, mask_image=mask_image).images[0]"],"metadata":{"id":"M355RQBXfVhq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Bilder anzeigen\n","make_image_grid([init_image, mask_image, new_image], rows=1, cols=3, resize=400)"],"metadata":{"id":"k4aBI98rf1li"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6 | Outpainting\n","---"],"metadata":{"id":"jNHqE7q5eaqR"}},{"cell_type":"markdown","source":["Outpainting ist eine Technik der Bildbearbeitung und KI-gest√ºtzten Bildgenerierung, bei der ein vorhandenes Bild √ºber seine urspr√ºnglichen Grenzen hinaus erweitert wird. Dabei werden neue Bildinhalte an den R√§ndern erg√§nzt, die stilistisch und thematisch zum Original passen. Outpainting wird h√§ufig verwendet, um Bilder zu vergr√∂√üern, Hintergr√ºnde zu erweitern oder kreative Kompositionen zu schaffen. Moderne Methoden basieren oft auf neuronalen Netzwerken wie Stable Diffusion, die mithilfe von Textbeschreibungen (Prompts) realistische und koh√§rente Bildbereiche hinzuf√ºgen k√∂nnen. So entsteht ein nahtlos erweitertes Bild, das √ºber das urspr√ºngliche Motiv hinausgeht."],"metadata":{"id":"gbnzT6MWx03f"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Modell/API-√úbersicht\n","</font></p>"],"metadata":{"id":"EtpCtz98iaYo"}},{"cell_type":"markdown","source":["| Modell / Anbieter         | API-Zugang m√∂glich? | Hinweise |\n","| :------------------------| :------------------ | :------- |\n","| **Picsart Outpainting**   | Ja                 | Outpainting-Service per API, gezieltes Erweitern oder Ersetzen von Bildbereichen. Auch Inpainting-Funktion verf√ºgbar. |\n","| **IOPaint**               | Nein (Web-App & Open Source) | Open-Source-Tool f√ºr Inpainting/Outpainting, lokal oder selbst gehostet, keine √∂ffentliche API dokumentiert. Unterst√ºtzt verschiedene KI-Modelle, WebUI verf√ºgbar. |\n","| **Recraft**               | Nein (Web-App)     | KI-Inpainting und Outpainting mit hoher Pr√§zision, Fokus auf kreatives Editing, keine √∂ffentliche API dokumentiert. |\n","| **Flux Tools (Black Forest Lab)** | Nein (ComfyUI-Workflow) | Inpainting & Outpainting im ComfyUI-Workflow, leistungsstarke ‚ÄûFill‚Äú-Modelle, keine eigenst√§ndige API, Integration √ºber ComfyUI m√∂glich. |\n","| **Amazon Bedrock (SDXL, Titan Image Generator)** | Ja | Text-gef√ºhrtes Inpainting & Outpainting per API, unterst√ºtzt Stable Diffusion XL und Titan Image Generator, flexible Integration, keine Infrastrukturverwaltung n√∂tig. |\n","| **Google Vertex AI (Imagen 2)** | Ja | Inpainting (Objekte einf√ºgen/entfernen) per API, Outpainting-Funktion f√ºr Bildbereichserweiterung, Integration in Vertex AI Generative AI Studio. |\n","| **OpenAI DALL¬∑E**         | Ja                 | Inpainting (‚ÄûEdit‚Äú-Funktion) und Outpainting per API und Web-App, nat√ºrliche Sprachsteuerung, nahtlose Bildbearbeitung und Erweiterung. |\n"],"metadata":{"id":"XiFRaYdKiaYp"}},{"cell_type":"markdown","source":["**API-Outpainting - folgt**"],"metadata":{"id":"88pA9V0j2O42"}},{"cell_type":"markdown","source":["![My Image](https://raw.githubusercontent.com/ralf-42/Image/main/under_construction_dall_e_klein.png)"],"metadata":{"id":"8xoQ-N8Hx7dA"}},{"cell_type":"markdown","source":["**Prompt ChatGPT** `gpt-4o`\n","```\n","Erweitere das Bild sinnvoll √ºber die aktuellen Bild Grenzen - Outpainting. Neues Format 16:9\n","```\n","\n"],"metadata":{"id":"LnOQ5NA5ymui"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"4\">\n","Original & Outpainting\n","</font></p>\n","\n","<img src=\"https://raw.githubusercontent.com/ralf-42/Image/main/hedra_avatar_klein.jpg\" width=\"207\" alt=\"Avatar\">\n","<img src=\"https://raw.githubusercontent.com/ralf-42/Image/main/hedra_avatar_outpainting_klein.png\" width=\"550\" alt=\"Avatar\">"],"metadata":{"id":"_Z39Sqn-28u5"}},{"cell_type":"markdown","source":["\n","# 7 | Objekterkennung\n","---"],"metadata":{"id":"0-eSumHCmlFZ"}},{"cell_type":"markdown","source":["Die Objekterkennung geht einen Schritt weiter als die Bildklassifizierung. Hier wird nicht nur bestimmt, welche Objekte in einem Bild vorhanden sind, sondern auch wo sie sich befinden.\n","\n","**Wie funktioniert Objekterkennung?**   \n","+ Eingabe: Ein einzelnes Bild wird als Eingabe verwendet.\n","+ Merkmalsextraktion & Vorschl√§ge f√ºr Regionen: Das Modell analysiert das Bild und identifiziert potenzielle Bereiche, in denen sich Objekte befinden k√∂nnten.\n","+ Klassifizierung & Lokalisierung: Jedes erkannte Objekt wird einer bestimmten Klasse zugeordnet, und die Position wird durch eine Bounding Box (ein rechteckiger Bereich um das Objekt) beschrieben.\n","+ Ausgabe: Eine Liste mit allen erkannten Objekten, ihrer Klasse und ihrer Position im Bild wird zur√ºckgegeben.\n","\n","\n","**Beispiel f√ºr Objekterkennung**    \n","Stelle dir vor, du hast ein Bild mit einem Hund, einer Katze und einem Auto. Ein Objekterkennungsmodell kann alle drei Objekte gleichzeitig identifizieren und ihre Positionen angeben.\n","\n","+ Eingabebild: Ein Bild mit einem Hund, einer Katze und einem Auto\n","+ Modell-Ausgabe:\n",">Hund (95%) - Position: (x1, y1, x2, y2)   \n","Katze (90%) - Position: (x3, y3, x4, y4)   \n","Auto (99%) - Position: (x5, y5, x6, y6)   \n","\n","Hierbei sind (x1, y1, x2, y2) die Koordinaten der Bounding Box f√ºr den Hund, (x3, y3, x4, y4) f√ºr die Katze usw.\n","\n","**Einschr√§nkungen der Objekterkennung**   \n","Aufwendigere Berechnungen im Vergleich zur einfachen Bildklassifizierung.\n","Schwieriger zu trainieren, da genaue Positionen f√ºr Trainingsdaten erforderlich sind.\n","Anwendungsf√§lle f√ºr Objekterkennung\n","+ Autonomes Fahren (Erkennung von Fu√üg√§ngern, Verkehrsschildern)\n","+ √úberwachungssysteme (Erkennen von Eindringlingen oder gef√§hrlichen Objekten)\n","+ Analyse von Bildern im Einzelhandel (z. B. automatische Produktz√§hlung in Regalen)\n","\n","**Bekannte Modelle f√ºr Objekterkennung**   \n","+ YOLO (You Only Look Once) ‚Äì sehr schnell und effizient\n","+ Faster R-CNN ‚Äì pr√§zise, aber langsamer\n","+ SSD (Single Shot MultiBox Detector) ‚Äì guter Kompromiss zwischen Geschwindigkeit und Genauigkeit"],"metadata":{"id":"DLi09JRQ7VJE"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","YOLO\n","</font></p>"],"metadata":{"id":"l_m63Q1DyvMZ"}},{"cell_type":"markdown","source":["**YOLO (You Only Look Once)** ist ein leistungsf√§higer Algorithmus f√ºr die Objekterkennung in Bildern und Videos mithilfe k√ºnstlicher Intelligenz (KI). Es handelt sich um ein Modell, das auf Convolutional Neural Networks (CNN) basiert und sich durch seine Geschwindigkeit und Genauigkeit auszeichnet.\n","\n","**Funktionsweise**\n","\n","YOLO teilt ein Bild in ein Gitter auf und verarbeitet es in einem einzigen Durchlauf. Jede Gitterzelle ist f√ºr die Erkennung von Objekten in ihrem Bereich verantwortlich. Der Algorithmus sagt gleichzeitig Begrenzungsrahmen (Bounding Boxes) und Klassenwahrscheinlichkeiten f√ºr Objekte voraus.\n","\n","**Vorteile**\n","\n","1. **Geschwindigkeit**: YOLO kann Bilder in Echtzeit mit bis zu 45 Bildern pro Sekunde verarbeiten, was es ideal f√ºr Anwendungen wie Video√ºberwachung oder autonomes Fahren macht.\n","\n","2. **Genauigkeit**: Trotz seiner Geschwindigkeit erreicht YOLO eine hohe Erkennungsgenauigkeit und weist nur wenige Hintergrundfehler auf.\n","\n","3. **Verallgemeinerungsf√§higkeit**: Neuere Versionen von YOLO bieten eine verbesserte Leistung bei der Erkennung von Objekten in neuen Umgebungen.\n","\n","**Entwicklung**\n","\n","Seit seiner Einf√ºhrung im Jahr 2015 hat YOLO mehrere Iterationen durchlaufen, wobei jede Version Verbesserungen und Optimierungen mit sich brachte. Die neueste Version ist YOLO v7, die weitere Fortschritte in Bezug auf Geschwindigkeit und Genauigkeit bietet.\n","\n","YOLO hat die Objekterkennung revolutioniert und findet Anwendung in verschiedenen Bereichen wie autonomen Fahrzeugen, √úberwachungssystemen, Robotik und generativer KI."],"metadata":{"id":"8QV8uf3My0X5"}},{"cell_type":"markdown","source":["\n","\n","[Ultralytics | YOLO](https://www.ultralytics.com/de)"],"metadata":{"id":"x5eiwATzikyM"}},{"cell_type":"code","source":["!uv pip install --system --prerelease allow -q ultralytics opencv-python"],"metadata":{"id":"03nrbETSmmwu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Der Code verwendet die Ultralytics YOLOv8 Bibliothek, um Objekterkennung auf einem hochgeladenen Bild durchzuf√ºhren. Er l√§dt ein vortrainiertes YOLOv8m-Modell, erm√∂glicht das Hochladen eines Bildes √ºber Google Colab, f√ºhrt die Objekterkennung auf diesem Bild durch (mit einer Konfidenzschwelle von 0.2), speichert das Bild mit den Erkennungsergebnissen und die entsprechenden Textdateien und zeigt schlie√ülich das bearbeitete Bild in der Colab-Ausgabe an."],"metadata":{"id":"BWsp-C2v4iiJ"}},{"cell_type":"code","source":["# Import\n","import ultralytics\n","from ultralytics import YOLO\n","from google.colab import files\n","from IPython.display import Image, display, Markdown\n","\n","# YOLO-Modell laden\n","model = YOLO(\"yolov8m.pt\")"],"metadata":{"id":"P9ZQmS_OmzQB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Der Code verarbeitet jpg-Dateien aber keine jpeg-Dateien, nach Anpassung des Datei-Typs keine Fehler.\n","# Datei-Auswahldialog √∂ffnen\n","uploaded = files.upload()\n","\n","# Dateinamen abrufen (das erste hochgeladene Bild)\n","image_path = list(uploaded.keys())[0]\n","\n","# YOLO-Vorhersage durchf√ºhren\n","results = model.predict(source=image_path, save=True, conf=0.2, save_txt=True)\n","\n","# Pfad zum gespeicherten Bild mit den Erkennungen\n","result_image_path = results[0].save_dir + \"/\" + image_path\n","\n","# Bild mit erkannten Objekten anzeigen\n","display(Image(result_image_path,width=600)) # H√∂he wird automatisch angepasst"],"metadata":{"id":"9XNzrZMPN-a6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 8 | Bildbeschreibung"],"metadata":{"id":"D8I9UIkZ7yRP"}},{"cell_type":"markdown","source":["Die Aufgabe Bildbeschreibung (Image2Text) beschreibt die automatische Generierung von Text aus Bildern mithilfe von K√ºnstlicher Intelligenz (KI). Dabei werden neuronale Netze, insbesondere tiefgehende Modelle in Kombination mit Transformern, genutzt, um visuelle Inhalte zu analysieren und zu beschreiben.\n","\n","**Anwendungsf√§lle**    \n","+ Bildbeschreibung (Image Captioning): Generierung von Textbeschreibungen zu Bildern (z. B. f√ºr barrierefreie Webseiten oder Archivierung).\n","+ Optische Zeichenerkennung (OCR): Extraktion von Text aus Bildern oder gescannten Dokumenten.\n","+ Visuelle Frage-Antwort-Systeme (Visual Question Answering, VQA): Beantwortung von Fragen zu Bildinhalten.\n","+ Content-Moderation: Automatische Identifikation und Beschreibung von problematischen oder sensiblen Bildinhalten."],"metadata":{"id":"DSKhCOVn78PW"}},{"cell_type":"code","source":["!uv pip install --system --prerelease allow -q openai pillow"],"metadata":{"id":"R8CRLdH8_Mvu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Der Code l√§dt ein Bild in Google Colab hoch, kodiert es in Base64 und sendet es zusammen mit einer Frage (\"Was ist auf dem Bild?\") an das OpenAI-Modell \"gpt-4o-mini\". Die Antwort des Modells (eine detaillierte Beschreibung im Markdown-Format) wird angezeigt, ebenso wie das hochgeladene Bild selbst (mit einer maximalen Breite von 500 Pixeln).\n"],"metadata":{"id":"fpf8fY2SUbCZ"}},{"cell_type":"code","source":["import base64\n","from openai import OpenAI\n","from google.colab import files\n","from IPython.display import Image, display, Markdown\n","\n","client = OpenAI()"],"metadata":{"id":"qz2KVUuV_JAf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Datei-Auswahldialog √∂ffnen\n","uploaded = files.upload()\n","\n","# Dateinamen abrufen (das erste hochgeladene Bild)\n","image_path = list(uploaded.keys())[0]\n","\n","# Bild in Base64 kodieren; Bild umzuwandeln, dass es in einem Textformat\n","# (z.‚ÄØB. JSON) √ºber eine API oder an ein Sprachmodell wie OpenAI GPT gesendet werden kann.\n","encoded_image = base64.b64encode(uploaded[image_path]).decode(\"utf-8\")\n","\n","prompt_text = \"Was ist auf dem Bild? Erstelle eine ausf√ºhrliche Beschreibung im Markdown-Format.\"\n","image_url = f\"data:image/jpeg;base64,{encoded_image}\"\n","\n","messages = [{\n","    \"role\": \"user\",\n","    \"content\": [\n","        {\"type\": \"text\", \"text\": prompt_text},\n","        {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n","    ]\n","}]\n","\n","response = client.chat.completions.create(\n","    model=\"gpt-4o-mini\",\n","    messages=messages,\n","    max_tokens=300,\n",")\n","\n","# Bildbeschreibung\n","display(Markdown(response.choices[0].message.content))\n","\n","# Bild in Colab anzeigen mit begrenzter Gr√∂√üe\n","display(Image(uploaded[image_path], width=500))  # H√∂he wird proportional angepasst"],"metadata":{"id":"x8E407Y0RA0y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NkrKt6cPGWOT"},"source":["\n","# A | Aufgabe\n","---"]},{"cell_type":"markdown","source":["Bereit f√ºr eine neue Herausforderung? In diesen Aufgaben werden Sie die leistungsstarken Vision-Modelle von **MediaPipe** kennenlernen. Mit diesen Modellen k√∂nnen sie u.a. Objekterkennung, Handgesten und Gesichtsausdr√ºcke in Echtzeit analysieren. Entdecken Sie die Potenziale der GenAI. Die Aufgabestellungen unten bieten weitere Anregungen, Sie k√∂nnen aber auch gerne eine andere Herausforderung angehen.\n","\n","[MediaPipe](https://mediapipe-studio.webapps.google.com/home)\n","\n","[MediaPipe L√∂sungsleitfaden](https://ai.google.dev/edge/mediapipe/solutions/guide?hl=de)"],"metadata":{"id":"U54N7byl5_jS"}},{"cell_type":"markdown","metadata":{"id":"pC9A-LaYhsta"},"source":["<p><font color='black' size=\"5\">\n","Coverbild LLM-Buch\n","</font></p>"]},{"cell_type":"markdown","source":["In einem fr√ºheren Modul wurde ein LLM-basierten Buchgenerator erstellt, der die Erstellung eines Buches von Anfang bis Ende automatisiert. Dieser Prozess umfasste die Erstellung einer Zusammenfassung, die Gliederung des Inhaltsverzeichnisses und das iterative Schreiben der Kapitel in ein strukturiertes Markdown-Dokument. Jetzt wird diese Arbeit erweitert: Erstellen Sie mit DALL¬∑E ein Coverbild f√ºr das Buch, das den textbasierten Inhalt erg√§nzt.\n","\n"],"metadata":{"id":"G1KmhbS8nw05"}}]}
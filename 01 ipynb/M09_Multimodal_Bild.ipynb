{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["PnODVpOdGWOS","NS2tCPL_uKLY","Ii6TLuoCGsR-","0-eSumHCmlFZ","D8I9UIkZ7yRP","NkrKt6cPGWOT"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<p><font size=\"6\" color='grey'> <b>\n","Anwendung Generativer KI\n","</b></font> </br></p>"],"metadata":{"id":"Ih2CTVBnArVZ"}},{"cell_type":"markdown","source":["<p><font size=\"5\" color='grey'> <b>\n","Multimodal - Bild\n","</b></font> </br></p>\n","\n","\n","---"],"metadata":{"id":"6jJZ7wbdArVc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfdhPIzcEYRG","cellView":"form","collapsed":true},"outputs":[],"source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  Colab-Umfeld</font> </br></p>\n","# Installierte Python Version\n","import sys\n","print(f\"Python Version: \",sys.version)\n","\n","# Installierte LangChain Bibliotheken\n","print()\n","print(\"Installierte LangChain Bibliotheken:\")\n","!pip list | grep '^langchain'\n","# Unterdrückt die \"DeprecationWarning\" von LangChain für die Memory-Funktionden\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langsmith.client\")"]},{"cell_type":"code","source":["#@title\n","#@markdown   <p><font size=\"4\" color='green'>  SetUp API-Keys (setup_api_keys)</font> </br></p>\n","\n","def setup_api_keys():\n","    \"\"\"Konfiguriert alle benötigten API-Keys aus Google Colab userdata\"\"\"\n","    from google.colab import userdata\n","    import os\n","    from os import environ\n","\n","    # Dictionary der benötigten API-Keys\n","    keys = {\n","        'OPENAI_API_KEY': 'OPENAI_API_KEY',\n","        'HF_TOKEN': 'HF_TOKEN',\n","        # Weitere Keys bei Bedarf\n","    }\n","\n","    # Keys in Umgebungsvariablen setzen\n","    for env_var, key_name in keys.items():\n","        environ[env_var] = userdata.get(key_name)\n","\n","    return {k: environ[k] for k in keys.keys()}\n","\n","# Verwendung\n","all_keys = setup_api_keys()\n","# Bei Bedarf einzelne Keys direkt zugreifen\n","# WEATHER_API_KEY = all_keys['WEATHER_API_KEY']"],"metadata":{"cellView":"form","id":"WD3Wwr6sESX8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PnODVpOdGWOS"},"source":["\n","\n","# 1 | Einführung\n","---"]},{"cell_type":"markdown","source":["Dieses Modul beschäftigt sich mit der Analyse von Bildern und ihrer Rolle als Basis für multimodale Systeme. Solche Modelle stellen einen bedeutenden Fortschritt in der KI dar, da sie geschriebene Sprache in visuelle Darstellungen umwandeln. Bekannte Beispiele wie DALL·E, Midjourney und Stable Diffusion nutzen Deep-Learning-Techniken, um detaillierte Bilder auf Grundlage von Textbeschreibungen zu erzeugen. Dabei erfassen sie die Zusammenhänge zwischen sprachlichen und visuellen Elementen, wodurch sie in der Lage sind, Szenen, Objekte und Konzepte entsprechend der gegebenen Beschreibung zu generieren.  \n","\n","Multimodale Modelle gehen über diese Funktionalität hinaus, indem sie mehrere Datenquellen kombinieren – in der Regel Text,  Bilder, Audio oder Video. Ihr Ziel ist es, verschiedene Informationsarten simultan zu verarbeiten und in Beziehung zu setzen. So können sie beispielsweise Bildbeschreibungen aus Text erstellen, Fragen zu visuellen Inhalten beantworten oder sogar Videos auf Basis von Texteingaben generieren. Durch die Verbindung der Fähigkeiten von Text- und Bildmodellen erweitern multimodale Systeme das Anwendungsspektrum der KI und ermöglichen eine flexiblere, kontextbezogene Interaktion.  \n","\n","Während **Text-zu-Bild-Modelle** primär darauf ausgerichtet sind, sprachliche Eingaben in visuelle Inhalte umzusetzen, ermöglichen **multimodale Modelle** eine komplexere Verknüpfung verschiedener Medientypen. In diesem Modul wird untersucht, wie diese Modelle funktionieren und wie ihre Architektur die Entwicklung weiterführender multimodaler Systeme beeinflusst. Zudem werden praxisnahe Anwendungsfälle betrachtet sowie mögliche zukünftige Entwicklungen aufgezeigt, die sich aus der Kombination unterschiedlicher Datenmodalitäten ergeben."],"metadata":{"id":"eazq6nuElGNS"}},{"cell_type":"markdown","metadata":{"id":"NS2tCPL_uKLY"},"source":["\n","# 2 | Bildgenerierung\n","---"]},{"cell_type":"markdown","source":["DALL·E ist ein von OpenAI entwickeltes generatives KI-Modell, das aus Textbeschreibungen Bilder erzeugt. Es kann detaillierte Anweisungen interpretieren und eine breite Palette von Motiven und Stilen – von realistisch bis abstrakt – visuell umsetzen.\n","\n","Über die OpenAI-API ist der Zugriff auf DALL·E möglich, wodurch Entwickler die Bildgenerierungsfunktion in ihre Anwendungen integrieren können. Nutzer geben Texteingaben vor, anhand derer das Modell passende Bilder erstellt. Die API ist so konzipiert, dass sie sich einfach einbinden und skalieren lässt, wodurch sie sich für Webanwendungen, mobile Apps und digitale Plattformen eignet.\n","\n","Mit DALL·E können Ideen in hochdetaillierte visuelle Darstellungen umgewandelt werden. Das Modell findet in verschiedenen Bereichen Anwendung, darunter Design, Marketing, Unterhaltung und Bildung.\n"],"metadata":{"id":"ZUFRWWopmOpQ"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","DALL·E 3\n","</font></p>"],"metadata":{"id":"t6J9uR0cmf8t"}},{"cell_type":"markdown","source":["DALL·E 3, veröffentlicht im Jahr 2023 und bringt neue Funktionen mit, die die Präzision und Benutzerfreundlichkeit verbessern:\n","\n","* **Optimiertes Textverständnis**: DALL·E 3 kann detaillierte und komplexe Anweisungen interpretieren und umsetzen. Dadurch werden Bilder erzeugt, die den Vorstellungen des Nutzers besser entsprechen, selbst bei anspruchsvollen oder feingliedrigen Beschreibungen.  \n","* **Nahtlose Integration mit ChatGPT**: Dank der tiefen Verzahnung mit ChatGPT können Nutzer in einem dialogbasierten Prozess ihre Eingaben iterativ verfeinern. Dies erleichtert die Anpassung von Anweisungen und macht die Bildgenerierung intuitiver.  \n","* **Verbesserte Darstellung schwieriger Elemente**: Herausforderungen früherer Versionen, etwa die realistische Darstellung von Händen und Gesichtern, wurden erheblich reduziert, sodass DALL·E 3 in diesen Bereichen präzisere Ergebnisse liefert.  "],"metadata":{"id":"QIrbBd8qmhfn"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Mehrere Bilder generieren\n","</font></p>"],"metadata":{"id":"H3FJ6Rpfmpho"}},{"cell_type":"markdown","source":["Mit DALL·E 3 lassen sich mehrere Bilder erzeugen, die in diesem Abschnitt betrachtet werden.  \n","\n","Die Auflösung kann mithilfe der OpenAI-Bibliothek festgelegt werden, wobei nicht jede Auflösung für alle Modelle verfügbar ist:  \n","\n","* 1024 × 512  \n","* 256 × 256  \n","* 512 × 512  \n","* 1024 × 1024  \n","* 1024 × 1792  \n","* 1792 × 1024  \n","* Qualitätsstufen: Standard/hd  "],"metadata":{"id":"7_0InuAKmqjw"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Erläuterung des Codes\n","</font></p>\n"],"metadata":{"id":"J0kuE_aJKF17"}},{"cell_type":"markdown","source":["Der folgende Code erzeugt das Bild eines \"schwarzen Pferdes“."],"metadata":{"id":"KwpLS-kkUMrU"}},{"cell_type":"code","source":["import requests\n","from openai import OpenAI\n","from io import BytesIO\n","from PIL import Image\n","from IPython.display import display\n","\n","# Parameter\n","MODEL = \"dall-e-3\"\n","PROMPT = \"Erstelle ein Bild mit einer Hütte im Wald, die Hütte wird von der Sonne beschienen.\"\n","NUM_IMAGES = 2\n","DISPLAY_SIZE = (512, 512)  # Anzeigegröße der Bilder (Breite, Höhe)\n","\n","def generate_and_save_images():\n","    client = OpenAI()\n","\n","    for i in range(NUM_IMAGES):\n","        response = client.images.generate(model=MODEL, prompt=PROMPT, size=\"1024x1024\", quality=\"standard\", n=1)\n","        img = Image.open(BytesIO(requests.get(response.data[0].url).content))\n","\n","        # Bild speichern\n","        filename = f\"bild_{i+1}.png\"\n","        img.save(filename)\n","        print(f\"Gespeichert: {filename}\")\n","\n","        # Bildgröße anpassen und anzeigen\n","        img_resized = img.resize(DISPLAY_SIZE)\n","        display(img_resized)\n","\n","\n","generate_and_save_images()"],"metadata":{"id":"yJtGp01SIsJU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Bild herunterladen\n","</font></p>"],"metadata":{"id":"Cq6Fbnts0trJ"}},{"cell_type":"code","source":["from google.colab import files\n","# Laden Sie das als variation_image_1.png gespeicherte Bild herunter\n","files.download('/content/bild_2.png')"],"metadata":{"id":"7bcmH5eo0r5P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ii6TLuoCGsR-"},"source":["\n","# 3 | Bildklassifizierung\n","---"]},{"cell_type":"markdown","source":["Die Bildklassifizierung ist eine grundlegende Aufgabe in der Computer Vision, bei der einem gesamten Bild eine einzige Kategorie (Klasse) zugewiesen wird.\n","\n","**Wie funktioniert Bildklassifizierung?**\n","+ Eingabe: Ein einzelnes Bild wird als Eingabe verwendet.\n","+ Merkmalsextraktion: Ein neuronales Netzwerk analysiert das Bild und extrahiert relevante Merkmale (z. B. Kanten, Farben, Formen, Muster).\n","+ Klassifizierung: Das Modell ordnet das Bild einer vordefinierten Kategorie zu, z. B. \"Hund\", \"Katze\" oder \"Auto\".\n","+ Ausgabe: Eine einzige Klasse (Label) mit einer Wahrscheinlichkeitsbewertung wird zurückgegeben.\n","\n","\n","Beispiel für Bildklassifizierung\n","Stelle dir vor, du hast ein Bild von einem Hund. Ein Bildklassifizierungsmodell verarbeitet das Bild und gibt die Kategorie \"Hund\" mit einer bestimmten Wahrscheinlichkeit (z. B. 95 %) zurück.\n","\n","+ Eingabebild: Ein Bild eines Hundes\n","+ Modell-Ausgabe: \"Hund\" (95%)\n","\n","Falls das Bild eine Katze zeigt, gibt das Modell möglicherweise \"Katze\" (90%) als Ergebnis zurück.\n","\n","**Einschränkungen der Bildklassifizierung**     \n","Das Modell kann nur eine Klasse pro Bild erkennen, auch wenn mehrere Objekte im Bild vorhanden sind.\n","Es gibt keine Information über die Position oder Anzahl der Objekte im Bild.\n","Anwendungsfälle für Bildklassifizierung\n","+ Erkennung von medizinischen Anomalien (z. B. Klassifikation von Röntgenbildern)\n","+ Identifikation von Pflanzen oder Tieren anhand von Bildern\n","+ Sentiment-Analyse anhand von Gesichtsmerkmalen\n","\n","**Bekannte Modelle für Bildklassifizierung**    \n","+ CNNs (Convolutional Neural Networks) wie ResNet, VGG, EfficientNet\n","+ Pretrained Modelle: MobileNet, Inception, AlexNet\n","\n","\n","Man kann allerdings auch gpt-4o-mini mit einem entsprechenden Prompt hierzu einsetzen."],"metadata":{"id":"mq1NAdlem-lu"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Erläuterung des Codes\n","</font></p>"],"metadata":{"id":"IWwt0t53KnBJ"}},{"cell_type":"markdown","source":["Text"],"metadata":{"id":"bokT37ZDRIQo"}},{"cell_type":"code","source":["!uv pip install --system --prerelease allow openai pillow"],"metadata":{"id":"0yA2teXbQk4Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import base64\n","from openai import OpenAI\n","from google.colab import files\n","from IPython.display import Image, display, Markdown\n","\n","client = OpenAI()\n","\n","# Datei-Auswahldialog öffnen\n","uploaded = files.upload()\n","\n","# Dateinamen abrufen (das erste hochgeladene Bild)\n","image_path = list(uploaded.keys())[0]\n","\n","# Bild in Base64 kodieren\n","encoded_image = base64.b64encode(uploaded[image_path]).decode(\"utf-8\")\n","\n","response = client.chat.completions.create(\n","    model=\"gpt-4o-mini\",\n","    messages=[\n","        {\n","            \"role\": \"user\",\n","            \"content\": [\n","                {\"type\": \"text\", \"text\": \"Was ist auf diesem Bild abgebildet? Nennen nur das wesentliche Objekt.\"},\n","                {\n","                    \"type\": \"image_url\",\n","                    \"image_url\": {\n","                        \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n","                    },\n","                },\n","            ],\n","        }\n","    ],\n","    max_tokens=300,\n",")\n","\n","print(response.choices[0].message.content)\n","\n","# Bild in Colab anzeigen mit begrenzter Größe\n","display(Image(uploaded[image_path], width=500))  # Höhe wird proportional angepasst"],"metadata":{"id":"ZLsOlRByQk4Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# 4 | Objekterkennung\n","---"],"metadata":{"id":"0-eSumHCmlFZ"}},{"cell_type":"markdown","source":["Die Objekterkennung geht einen Schritt weiter als die Bildklassifizierung. Hier wird nicht nur bestimmt, welche Objekte in einem Bild vorhanden sind, sondern auch wo sie sich befinden.\n","\n","**Wie funktioniert Objekterkennung?**   \n","+ Eingabe: Ein einzelnes Bild wird als Eingabe verwendet.\n","+ Merkmalsextraktion & Vorschläge für Regionen: Das Modell analysiert das Bild und identifiziert potenzielle Bereiche, in denen sich Objekte befinden könnten.\n","+ Klassifizierung & Lokalisierung: Jedes erkannte Objekt wird einer bestimmten Klasse zugeordnet, und die Position wird durch eine Bounding Box (ein rechteckiger Bereich um das Objekt) beschrieben.\n","+ Ausgabe: Eine Liste mit allen erkannten Objekten, ihrer Klasse und ihrer Position im Bild wird zurückgegeben.\n","\n","\n","**Beispiel für Objekterkennung**    \n","Stelle dir vor, du hast ein Bild mit einem Hund, einer Katze und einem Auto. Ein Objekterkennungsmodell kann alle drei Objekte gleichzeitig identifizieren und ihre Positionen angeben.\n","\n","+ Eingabebild: Ein Bild mit einem Hund, einer Katze und einem Auto\n","+ Modell-Ausgabe:\n",">Hund (95%) - Position: (x1, y1, x2, y2)   \n","Katze (90%) - Position: (x3, y3, x4, y4)   \n","Auto (99%) - Position: (x5, y5, x6, y6)   \n","\n","Hierbei sind (x1, y1, x2, y2) die Koordinaten der Bounding Box für den Hund, (x3, y3, x4, y4) für die Katze usw.\n","\n","**Einschränkungen der Objekterkennung**   \n","Aufwendigere Berechnungen im Vergleich zur einfachen Bildklassifizierung.\n","Schwieriger zu trainieren, da genaue Positionen für Trainingsdaten erforderlich sind.\n","Anwendungsfälle für Objekterkennung\n","+ Autonomes Fahren (Erkennung von Fußgängern, Verkehrsschildern)\n","+ Überwachungssysteme (Erkennen von Eindringlingen oder gefährlichen Objekten)\n","+ Analyse von Bildern im Einzelhandel (z. B. automatische Produktzählung in Regalen)\n","\n","**Bekannte Modelle für Objekterkennung**   \n","+ YOLO (You Only Look Once) – sehr schnell und effizient\n","+ Faster R-CNN – präzise, aber langsamer\n","+ SSD (Single Shot MultiBox Detector) – guter Kompromiss zwischen Geschwindigkeit und Genauigkeit"],"metadata":{"id":"DLi09JRQ7VJE"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","YOLO\n","</font></p>"],"metadata":{"id":"l_m63Q1DyvMZ"}},{"cell_type":"markdown","source":["**YOLO (You Only Look Once)** ist ein leistungsfähiger Algorithmus für die Objekterkennung in Bildern und Videos mithilfe künstlicher Intelligenz (KI). Es handelt sich um ein Modell, das auf Convolutional Neural Networks (CNN) basiert und sich durch seine Geschwindigkeit und Genauigkeit auszeichnet.\n","\n","**Funktionsweise**\n","\n","YOLO teilt ein Bild in ein Gitter auf und verarbeitet es in einem einzigen Durchlauf. Jede Gitterzelle ist für die Erkennung von Objekten in ihrem Bereich verantwortlich. Der Algorithmus sagt gleichzeitig Begrenzungsrahmen (Bounding Boxes) und Klassenwahrscheinlichkeiten für Objekte voraus.\n","\n","**Vorteile**\n","\n","1. **Geschwindigkeit**: YOLO kann Bilder in Echtzeit mit bis zu 45 Bildern pro Sekunde verarbeiten, was es ideal für Anwendungen wie Videoüberwachung oder autonomes Fahren macht.\n","\n","2. **Genauigkeit**: Trotz seiner Geschwindigkeit erreicht YOLO eine hohe Erkennungsgenauigkeit und weist nur wenige Hintergrundfehler auf.\n","\n","3. **Verallgemeinerungsfähigkeit**: Neuere Versionen von YOLO bieten eine verbesserte Leistung bei der Erkennung von Objekten in neuen Umgebungen.\n","\n","**Entwicklung**\n","\n","Seit seiner Einführung im Jahr 2015 hat YOLO mehrere Iterationen durchlaufen, wobei jede Version Verbesserungen und Optimierungen mit sich brachte. Die neueste Version ist YOLO v7, die weitere Fortschritte in Bezug auf Geschwindigkeit und Genauigkeit bietet.\n","\n","YOLO hat die Objekterkennung revolutioniert und findet Anwendung in verschiedenen Bereichen wie autonomen Fahrzeugen, Überwachungssystemen, Robotik und generativer KI.\n","\n","[Ultralytics](https://www.ultralytics.com/de)"],"metadata":{"id":"8QV8uf3My0X5"}},{"cell_type":"code","source":["!uv pip install --system --prerelease allow ultralytics opencv-python"],"metadata":{"id":"03nrbETSmmwu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Erläuterung des Codes\n","</font></p>"],"metadata":{"id":"-taWpnRMFioA"}},{"cell_type":"markdown","source":["Der Code verwendet die Ultralytics YOLOv8 Bibliothek, um Objekterkennung auf einem hochgeladenen Bild durchzuführen. Er lädt ein vortrainiertes YOLOv8m-Modell, ermöglicht das Hochladen eines Bildes über Google Colab, führt die Objekterkennung auf diesem Bild durch (mit einer Konfidenzschwelle von 0.2), speichert das Bild mit den Erkennungsergebnissen und die entsprechenden Textdateien und zeigt schließlich das bearbeitete Bild in der Colab-Ausgabe an."],"metadata":{"id":"BWsp-C2v4iiJ"}},{"cell_type":"code","source":["# Import\n","import ultralytics\n","from ultralytics import YOLO\n","from google.colab import files\n","from IPython.display import Image, display, Markdown\n","\n","# YOLO-Modell laden\n","model = YOLO(\"yolov8m.pt\")\n","\n","# Datei-Auswahldialog öffnen\n","uploaded = files.upload()\n","\n","# Dateinamen abrufen (das erste hochgeladene Bild)\n","image_path = list(uploaded.keys())[0]\n","\n","# YOLO-Vorhersage durchführen\n","results = model.predict(source=image_path, save=True, conf=0.2, save_txt=True)\n","\n","# Pfad zum gespeicherten Bild mit den Erkennungen\n","result_image_path = results[0].save_dir + \"/\" + image_path\n","\n","# Bild mit erkannten Objekten anzeigen\n","display(Image(result_image_path,width=600)) # Höhe wird automatisch angepasst"],"metadata":{"id":"P9ZQmS_OmzQB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5 | Bildbeschreibung"],"metadata":{"id":"D8I9UIkZ7yRP"}},{"cell_type":"markdown","source":["Die Aufgabe Bildbeschreibung (Image2Text) beschreibt die automatische Generierung von Text aus Bildern mithilfe von Künstlicher Intelligenz (KI). Dabei werden neuronale Netze, insbesondere tiefgehende Modelle in Kombination mit Transformern, genutzt, um visuelle Inhalte zu analysieren und zu beschreiben.\n","\n","**Anwendungsfälle**    \n","+ Bildbeschreibung (Image Captioning): Generierung von Textbeschreibungen zu Bildern (z. B. für barrierefreie Webseiten oder Archivierung).\n","+ Optische Zeichenerkennung (OCR): Extraktion von Text aus Bildern oder gescannten Dokumenten.\n","+ Visuelle Frage-Antwort-Systeme (Visual Question Answering, VQA): Beantwortung von Fragen zu Bildinhalten.\n","+ Content-Moderation: Automatische Identifikation und Beschreibung von problematischen oder sensiblen Bildinhalten."],"metadata":{"id":"DSKhCOVn78PW"}},{"cell_type":"code","source":["!uv pip install --system --prerelease allow openai pillow"],"metadata":{"id":"R8CRLdH8_Mvu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Erläuterung des Codes\n","</font></p>"],"metadata":{"id":"UsJ6cgxoUbCZ"}},{"cell_type":"markdown","source":["Der Code lädt ein Bild in Google Colab hoch, kodiert es in Base64 und sendet es zusammen mit einer Frage (\"Was ist auf dem Bild?\") an das OpenAI-Modell \"gpt-4o-mini\". Die Antwort des Modells (eine detaillierte Beschreibung im Markdown-Format) wird angezeigt, ebenso wie das hochgeladene Bild selbst (mit einer maximalen Breite von 500 Pixeln).\n"],"metadata":{"id":"fpf8fY2SUbCZ"}},{"cell_type":"code","source":["import base64\n","from openai import OpenAI\n","from google.colab import files\n","from IPython.display import Image, display, Markdown\n","\n","client = OpenAI()\n","\n","# Datei-Auswahldialog öffnen\n","uploaded = files.upload()\n","\n","# Dateinamen abrufen (das erste hochgeladene Bild)\n","image_path = list(uploaded.keys())[0]\n","\n","# Bild in Base64 kodieren\n","encoded_image = base64.b64encode(uploaded[image_path]).decode(\"utf-8\")\n","\n","response = client.chat.completions.create(\n","    model=\"gpt-4o-mini\",\n","    messages=[\n","        {\n","            \"role\": \"user\",\n","            \"content\": [\n","                {\"type\": \"text\", \"text\": \"Wss ist auf dem Bild? Erstelle eine ausführliche Beschreibung im Markdown-Format.\"},\n","                {\n","                    \"type\": \"image_url\",\n","                    \"image_url\": {\n","                        \"url\": f\"data:image/jpeg;base64,{encoded_image}\"  # Oder image/png, etc.\n","                    },\n","                },\n","            ],\n","        }\n","    ],\n","    max_tokens=300,\n",")\n","\n","# Bildbeschreibung\n","display(Markdown(response.choices[0].message.content))\n","\n","# Bild in Colab anzeigen mit begrenzter Größe\n","display(Image(uploaded[image_path], width=500))  # Höhe wird proportional angepasst"],"metadata":{"id":"qz2KVUuV_JAf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NkrKt6cPGWOT"},"source":["\n","# A | Aufgabe\n","---"]},{"cell_type":"markdown","source":["Die Aufgabestellungen unten bieten Anregungen, Sie können aber auch gerne eine andere Herausforderung angehen."],"metadata":{"id":"U54N7byl5_jS"}},{"cell_type":"markdown","metadata":{"id":"pC9A-LaYhsta"},"source":["<p><font color='black' size=\"5\">\n","Coverbild LLM-Buch\n","</font></p>"]},{"cell_type":"markdown","source":["In einem früheren Modul wurde ein LLM-basierten Buchgenerator erstellt, der die Erstellung eines Buches von Anfang bis Ende automatisiert. Dieser Prozess umfasste die Erstellung einer Zusammenfassung, die Gliederung des Inhaltsverzeichnisses und das iterative Schreiben der Kapitel in ein strukturiertes Markdown-Dokument. Jetzt wird diese Arbeit erweitert: Erstellen Sie mit DALL·E ein Coverbild für das Buch, das den textbasierten Inhalt ergänzt.\n","\n"],"metadata":{"id":"G1KmhbS8nw05"}}]}
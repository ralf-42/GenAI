{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"collapsed_sections":["pC9A-LaYhsta","J_6EHVIcBe0Q","HODQfLqHBnUI","TgfPV_aXBuv9","x1Wb0diiB1yO","ikTJEPGqN9IH"],"authorship_tag":"ABX9TyMB9IRo8xQlinR59h5qz6pt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<p><font size=\"7\" color='grey'> <b>\n","Anwendung Generativer KI\n","</b></font> </br></p>"],"metadata":{"id":"Ih2CTVBnArVZ"}},{"cell_type":"markdown","source":["<p><font size=\"6\" color='grey'> <b>\n","Modul 04: Large Language Models und Transformer\n","</b></font> </br></p>\n","\n","\n","---"],"metadata":{"id":"6jJZ7wbdArVc"}},{"cell_type":"markdown","metadata":{"id":"pC9A-LaYhsta"},"source":["# **1 <font color='orange'>|</font> Foundation Models**\n","---"]},{"cell_type":"markdown","source":["\n","Die Grundlager großer Sprachmodelle (LLMs) sind Foundation Models, die anhand vieler Daten trainiert wurden. Sie können für bestimmte Aufgaben oder Anwendungen angepasst oder optimiert werden. Diese Modelle werden als *Basismodelle* (Foundation Models) bezeichnet, da sie eine grundlegende Ebene an Wissen und Fähigkeiten bieten, auf der spezialisierte Funktionen aufgebaut werden können. Mehrere namhafte Technologieunternehmen und Forschungsorganisationen bieten große Sprachmodelle an. Zu ihnen zählen OpenAI mit Modellen wie **GPT** (Generative Pre-trained Transformer), Google mit **BERT** (Bidirectional Encoder Representations from Transformers) und anderen Varianten sowie Facebook (Meta), das Modelle wie **RoBERTa** (Robustly Optimized BERT Pretraining Approach) anbietet.\n","\n","Das Trainieren eines großen Sprachmodells von Grund auf erfordert viel Rechenleistung und Fachwissen. Es braucht umfangreiche Daten, die man erst sammeln, bereinigen und verarbeiten muss. Außerdem braucht man leistungsstarke Computer, die so viele Daten verarbeiten können. Das kann sehr teuer werden, für Einzelpersonen und viele Organisationen. Die Entwicklung eines großen Sprachmodells ist sehr anspruchsvoll und dauert oft länger als ein Kurs. In Kursen lernt man, wie man vorhandene Modelle verwendet und optimiert, um bestimmte Probleme zu lösen oder Forschung zu betreiben.\n","\n"],"metadata":{"id":"rhiHq-tbpfRA"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Bewertung von Foundation Models\n","</font></p>\n","\n","\n","Die Bewertung grundlegender Modelle ist entscheidend, um ihre Fähigkeiten, Grenzen und Eignung für bestimmte Aufgaben zu verstehen. Eine genaue Bewertung stellt sicher, dass die Modelle in realen Anwendungen sicher, zuverlässig und effektiv sind. Sie hilft auch bei der Identifizierung potenzieller Verzerrungen und Fehler, die ihre Leistung beeinträchtigen könnten."],"metadata":{"id":"3cJDZbBXP5dO"}},{"cell_type":"markdown","source":["<p><font color='blue' size=\"4\">\n","Offene vs. geschlossene Gewichte\n","</font></p>\n","\n","**Offene Gewichte** bedeuten, dass die trainierten Parameter eines Modells für die Öffentlichkeit zugänglich sind. Dadurch haben Wissenschaftler und Entwickler die Möglichkeit, die Mechanismen des Modells nachzuvollziehen, Forschungsarbeiten zu reproduzieren sowie Anpassungen oder Verbesserungen vorzunehmen.   \n","**Geschlossene Gewichte** hingegen sind Modelle, bei denen der Zugriff auf die Parameter eingeschränkt ist. Unternehmen setzen diese oft in kommerziellen Produkten oder Dienstleistungen ein, da die Veröffentlichung der Gewichte geschäftliche Interessen oder den Schutz der Nutzerdaten beeinträchtigen könnt\n","\n"],"metadata":{"id":"6emZAODbtsgn"}},{"cell_type":"markdown","source":["<p><font color='blue' size=\"4\">\n","Parametergewichte\n","</font></p>\n","\n","Die Anzahl der Parametergewichte eines Modells gibt Aufschluss über dessen Kapazität und Komplexität. Sie wird typischerweise in Millionen (M), Milliarden (B) oder Billionen (T) angegeben. Eine höhere Parameteranzahl erhöht grundsätzlich die Fähigkeit des Modells, komplexe Zusammenhänge und feine Unterschiede in den Daten zu erkennen. Allerdings ist dies kein eindeutiger Indikator für die Leistungsfähigkeit in jeder Anwendung, sondern eher ein Hinweis auf das allgemeine Potenzial des Modells.  \n","\n","\n","**Vorteile:**  \n","* **Größere Lernfähigkeit:** Modelle mit mehr Parametern können feinere und differenziertere Muster in Daten erfassen, was ihre Präzision und Wirksamkeit bei verschiedenen Aufgaben steigern kann.  \n","* **Bessere Anpassungsfähigkeit:** Umfangreichere Modelle haben oft eine stärkere Fähigkeit zur Generalisierung und können unter geeigneten Trainingsbedingungen auch auf neue, unbekannte Daten besser reagieren.  \n","\n","**Nachteile:**  \n","* **Hoher Rechenaufwand:** Eine größere Anzahl an Parametern erfordert mehr Rechenleistung für Training und Inferenz, was leistungsfähige Hardware und längere Verarbeitungszeiten nötig macht.  \n","* **Gefahr der Überanpassung:** Ohne angemessene Regularisierungstechniken besteht die Gefahr, dass das Modell sich zu stark an die Trainingsdaten anpasst und bei neuen, variierenden Datensätzen schlechter abschneidet.  \n","* **Umweltbelastung:** Das Training umfangreicher Modelle verbraucht erheblich mehr Energie, was zu einem höheren CO₂-Ausstoß führt.\n"],"metadata":{"id":"MNfxe8eas0H5"}},{"cell_type":"markdown","source":["<p><font color='blue' size=\"4\">\n","Kontextfenstergröße\n","</font></p>\n","\n","Die Kontextfenstergröße eines Modells gibt an, wie viele Token (Wörter oder Wortbestandteile) es bei der Generierung oder Vorhersage von Text gleichzeitig verarbeiten kann. Diese Eigenschaft ist aus mehreren Gründen essenziell:\n","\n","+ **Erweiterter Kontext**: Ein größeres Kontextfenster erlaubt es dem Modell, mehr Informationen zu berücksichtigen, was zu kohärenteren und inhaltlich präziseren Ergebnissen führt. Dies ist besonders vorteilhaft für Aufgaben, die lange Texte oder komplexe Zusammenhänge erfordern.\n","+ **Erfassung von Abhängigkeiten**: Ein Modell mit größerem Kontextfenster kann weiter zurückliegende Zusammenhänge im Text besser erfassen, was seine Leistungsfähigkeit bei Aufgaben wie Textzusammenfassungen, Frage-Antwort-Systemen oder interaktiven Dialogen erheblich steigert.\n","\n","Ein fundiertes Verständnis dieser Faktoren hilft bei der Auswahl eines geeigneten Modells für spezifische Anwendungsfälle und ermöglicht eine gezielte Optimierung der Leistung.\n","\n"],"metadata":{"id":"b4E8sGtqtakb"}},{"cell_type":"markdown","source":["<p><font color='blue' size=\"4\">\n","Token\n","</font></p>\n","\n","In Large Language Models (LLMs) wie GPT (Generative Pre-trained Transformer) stellen Token die grundlegenden Einheiten des verarbeiteten Textes dar. Ein **Token** kann ein vollständiges Wort, ein Wortbestandteil oder ein einzelnes Zeichen sein. Wie genau ein Token definiert wird, hängt vom verwendeten Tokenizer ab, der während des Modelltrainings zum Einsatz kam. So könnte das Wort *Hausboot* entweder als einzelnes Token betrachtet oder in die Bestandteile *Haus* und *boot* zerlegt werden - abhängig von der jeweiligen Tokenisierungsstrategie.  \n","\n","Die Nutzungskosten eines LLMs zur Textgenerierung werden in der Regel anhand der verarbeiteten Token berechnet. Dabei zählen sowohl die Token aus der Eingabe als auch diejenigen, die das Modell als Antwort generiert. Da die Verarbeitung jedes Tokens insbesondere bei sehr großen Modellen mit Milliarden von Parametern erhebliche Rechenressourcen erfordert, beeinflusst die Anzahl der Token direkt die Rechenkosten. Daher ist ein effizientes Management der Token-Nutzung essenziell, um Kosten zu optimieren.  \n","\n","Die **Kontextfenstergröße** eines LLM gibt an, wie viele Token aus einer Eingabe das Modell gleichzeitig berücksichtigen kann. Hat ein Modell beispielsweise ein Kontextfenster von 1.024 Token, kann es nur die letzten 1.024 Token eines Textes für die nächste Vorhersage verwenden. Ist der Eingabetext länger, kann das Modell auf frühere Abschnitte nicht mehr direkt zugreifen, was sich auf die Kohärenz und Relevanz der generierten Antworten auswirken kann."],"metadata":{"id":"0lpXOhZeuW7R"}},{"cell_type":"markdown","source":["<p><font color='blue' size=\"4\">\n","Große Sprachmodelle\n","</font></p>\n","\n","Die folgende Tabelle enthält wichtige Merkmale für bekannte LLMs.\n","\n","\n","| Name               | Ersteller     | Opne/Closed     | MMLU  | Kontextfenster-Token |\n","|--------------------|---------------|------------|-------|----------------------|\n","| gpt-4o             | OpenAI        | Closed | 88,7  | 128K                 |\n","| gpt-4o-mini        | OpenAI        | Closed | 82,0  | 128K                 |\n","| Gemini 2.0 Ultra   | Google        | Closed | 90,0  | 2M                   |\n","| Gemini 1.5 Pro     | Google        | Closed | 85,9  | 2M                   |\n","| Gemini 1.5 Flash   | Google        | Closed | 78,9  | 1M                   |\n","| Claude 3 Opus      | Anthropic     | Closed | 88,2  | 200K                 |\n","| Claude 3 Haiku     | Anthropic     | Closed | 76,7  | 200K                 |\n","| Llama 3.1 405B     | Meta          | Open      | 87,3  | 512K                 |\n","| Llama 3.1 70B      | Meta          | Open      | 80,9  | 256K                 |\n","| Llama 3.1 8B       | Meta          | Open      | 73,0  | 128K                 |\n","| Mistral 7B         | Mistral.AI    | Open      | 60,1  | 32K                  |\n","\n","<br>\n","\n","**Im Kurs verwenden wir:**\n","\n","* **gpt-4o-mini** - Das kostengünstigste verfügbare Modell von OpenAI. Die überwiegende Mehrheit der Aufgaben wird dieses Modell nutzen.\n","* **gpt-4o** - Ein etwas teureres Modell, das wir verwenden, wenn wir ausführlichere Antworten benötigen, als Mini bietet.\n","\n","[Preise](https://platform.openai.com/docs/pricing)\n"],"metadata":{"id":"ZGXcMXyEQAvU"}},{"cell_type":"markdown","source":["**Auszug OpenAI Modelle:**\n","\n","| Modell                                                    | Anwendungsbereiche                                                                                       |\n","|-----------------------------------------------------------|----------------------------------------------------------------------------------------------------------|\n","| **gpt-4o**, **gpt-4o-2024-08-06**                         | Textgenerierung, Übersetzungen, Zusammenfassungen, breite Anwendungen in der natürlichen Sprachverarbeitung. |\n","| **gpt-4o-audio-preview**, **gpt-4o-audio-preview-2024-12-17** | Generierung und Verarbeitung von audio-basierten Inhalten, wie automatisierte Transkription, Spracherkennung und Musikkomposition. |\n","| **gpt-4o-realtime-preview**, **gpt-4o-realtime-preview-2024-12-17** | Echtzeitanwendungen wie interaktive Chatbots, Echtzeit-Übersetzungen, schnelle Antwortzeiten erfordernd. |\n","| **gpt-4o-mini**, **gpt-4o-mini-2024-07-18**               | Leichte Textgenerierung, geeignet für weniger rechenintensive Anwendungen, z.B. in Embedded-Systemen.     |\n","| **gpt-4o-mini-audio-preview**, **gpt-4o-mini-audio-preview-2024-12-17** | Einfache Spracherkennung, Soundeffekt-Generierung, weniger komplexe Audioverarbeitungsaufgaben.           |\n","| **gpt-4o-mini-realtime-preview**, **gpt-4o-mini-realtime-preview-2024-12-17** | Echtzeitanwendungen mit schnellen Antwortzeiten, geeignet für mobile oder IoT-Geräte.                     |\n","| **o1**, **o1-2024-12-17**                                 | Anspruchsvolle Berechnungen, komplexe Simulationen, detaillierte Datenanalysen, umfangreiche Sprachmodelle. |\n","| **o3-mini**, **o3-mini-2025-01-31**                       | Kostengünstige NLP- und ML-Aufgaben, ideal für akademische Forschung oder Startups mit begrenztem Budget. |\n","| **o1-mini**, **o1-mini-2024-09-12**                       | Mittelschwere Aufgaben in Machine Learning und Datenverarbeitung, weniger ressourcenintensive Alternative zu Vollversionen. |\n"],"metadata":{"id":"gQSIiRTT0jSS"}},{"cell_type":"markdown","source":["<p><font color='blue' size=\"4\">\n","Temperatur\n","</font></p>\n","\n","Bei LLM wie GPT bezieht sich OpenAI mit dem Begriff **Temperatureinstellung** auf die Steuerung der Zufälligkeit oder Kreativität der generierten Antworten. Eine niedrigere Temperatur (z. B. 0,0) sorgt für deterministische und vorhersehbare Ergebnisse, da das Modell vorrangig die wahrscheinlichsten Antworten wählt. Eine höhere Temperatur (z. B. 1,0) hingegen führt zu einer größeren Varianz und kann kreativere, aber auch weniger vorhersehbare Antworten erzeugen. Der Wertebereich variiert je nach Anwendung oder Benutzeroberfläche, liegt jedoch typischerweise zwischen 0 und 1. Durch die Anpassung der Temperatur lässt sich das Modell gezielt steuern, um ein passendes Verhältnis zwischen Konsistenz und Originalität für unterschiedliche Aufgaben oder Nutzerpräferenzen zu erreichen."],"metadata":{"id":"2OxlGwtsQGpb"}},{"cell_type":"markdown","source":["<p><font color='blue' size=\"4\">\n","Benchmark\n","</font></p>\n","\n","Hier ein Überblick der relevanten Performance-Benchmarks für LLMs im Februar 2025:\n","\n","\n","**MMLU (Massive Multitask Language Understanding)**  [MMLU](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu)   \n","*Standard-Benchmark für Sprachverständnis über 57 Fachgebiete*  \n","\n","| Modell              | Score  |  \n","|---------------------|--------|  \n","| GPT-4o              | 88,7%  |  \n","| Gemini 2.0 Ultra    | 90,0%  |  \n","| Claude 3 Opus       | 88,2%  |  \n","| Llama 3.1 405B      | 87,3%  |  \n","| gpt-4o-mini       | 70,0%  |\n","\n","**SEAL Leaderboards (Scale AI)**  \n","*Fokus auf Unternehmensanforderungen:*  \n","- Gemini 2.0 Flash: Bester Multimodal-Score (Text/Video/Audio)\n","- Claude 3 Opus: Führend in juristischer Textanalyse  \n","- GPT-4o: Optimiert für Echtzeit-Interaktionen  \n","\n","Die Wahl des Benchmarks hängt vom Einsatzgebiet ab: MMLU bleibt Goldstandard für Allgemeinwissen, während SEAL für Unternehmenslösungen und Hugging Face für Open-Source-Modelle führend sind.\n","\n","**Wichtige Benchmark-Plattformen 2025**   \n","1. **LLM Leaderboard**  \n","   - Bewertet 50+ Modelle nach:  \n","     - Kontextfenster (bis 2M Token)  \n","     - Token/s-Verarbeitungsgeschwindigkeit  \n","     - Kosten pro 1M Tokens  \n","\n","2. **Hugging Face Open LLM Leaderboard**  \n","   - Open-Source-Schwerpunkt mit detaillierten Metriken für:  \n","     - Textgenerierung (Llama 3.1 führt)  \n","     - Code-Qualität (DeepSeek-RoBERTa aktuell Top)  \n","\n","3. **Urdu-NLP-Benchmark**\n","   - Llama 3.1-8B übertrifft gpt-3.5-turbo in sprachspezifischen Tasks  \n","   - Bloomz 7B1 zeigt beste Ergebnisse für Low-Resource-Sprachen  \n","\n","**Anwendungsbezogene Leistung**   \n","- **Multimodale Analysen:**  \n","  Gemini 2.0 Ultra (2M Token Kontext) vs. GPT-4o (128K Token)\n","- **Echtzeit-Anwendungen:**  \n","  Claude 3 Haiku: 76,7% MMLU bei 200ms Antwortzeit\n","- **Kostenoptimierung:**  \n","  Mistral 7B (32K Token) bei 60,1% MMLU für €0,08/1M Tokens  \n","\n","Die Wahl des Benchmarks hängt vom Einsatzgebiet ab: MMLU bleibt Goldstandard für Allgemeinwissen, während SEAL für Unternehmenslösungen und Hugging Face für Open-Source-Modelle führend sind.\n","\n","\n"],"metadata":{"id":"HzhHbp-20WVA"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Anwendung\n","</font></p>\n","\n","Mit dem folgenden Code kann man Systemaufforderungen, Temperaturen, Modelle und Anfragen ausprobieren.\n","\n","\n"],"metadata":{"id":"aQqiPx-V3Kio"}},{"cell_type":"code","source":["!uv pip install --system -q langchain_openai langchain_community"],"metadata":{"id":"hmsmyFkX4sTz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from google.colab import userdata\n","os.environ['OPENAI_API_KEY'] = userdata.get('OpenAI-API-Key')"],"metadata":{"id":"eAD6Hk4d4ZGN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TMF-rtxgRAea"},"outputs":[],"source":["from langchain_core.runnables import RunnableWithMessageHistory\n","from langchain_core.messages import HumanMessage, AIMessage\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_community.chat_message_histories import ChatMessageHistory\n","from IPython.display import display, Markdown\n","\n","MODEL = 'gpt-4o-mini'\n","TEMPERATURE = 0.25\n","TEMPLATE = \"\"\"Du bist ein Experte zur Beantwortung von Fragen. Formatiere die Antworten in Markdown und gebe möglichst exakte Antworten.\n","\n","Current conversation:\n","{history}\n","Human: {input}\n","Code Assistant:\"\"\"\n","\n","def start_conversation():\n","    # Initialisieren des ChatOpenAI-Modells\n","    llm = ChatOpenAI(\n","        model=MODEL,\n","        temperature=TEMPERATURE,\n","    )\n","\n","    # Erstellen des Chat-Prompts\n","    prompt = ChatPromptTemplate.from_template(TEMPLATE)\n","\n","    # Erstellen der Kette\n","    chain = prompt | llm\n","\n","    # Message History Handler erstellen\n","    def get_message_history(session_id: str) -> ChatMessageHistory:\n","        return ChatMessageHistory()\n","\n","    # Hinzufügen der Nachrichtenverlauf-Funktionalität\n","    chain_with_history = RunnableWithMessageHistory(\n","        chain,\n","        get_message_history,\n","        input_messages_key=\"input\",\n","        history_messages_key=\"history\"\n","    )\n","\n","    return chain_with_history\n","\n","def query_llm(chain, prompt, session_id=\"default\"):\n","    # Ausführen der Anfrage\n","    response = chain.invoke(\n","        {\"input\": prompt},\n","        config={\n","            \"configurable\": {\n","                \"session_id\": session_id\n","            }\n","        }\n","    )\n","\n","    # Ausgabe der Antwort\n","    print(\"Modellantwort:\")\n","    display(Markdown(response.content))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WUPrAkqDW6GL"},"outputs":[],"source":["conversation = start_conversation()\n","query_llm(conversation, \"\"\"\n","Erstelle eine Tabelle der fünf bevölkerungsreichsten Ländern mit Bevölkerung und BIP.\n","\"\"\")"]},{"cell_type":"markdown","metadata":{"id":"J_6EHVIcBe0Q"},"source":["# **2 <font color='orange'>|</font> Textgenerierung**\n","---"]},{"cell_type":"markdown","source":["Die Generierung von Text gehört zu den häufigsten Anwendungsfällen für **Large Language Models (LLMs)**. Die Erstellung von natürlichem Text folgen einem einem ähnlichen Prinzip wie zur Codegenerierung.  \n","\n","Anstatt eine interaktive Chatumgebung zu verwenden, erfolgt die Textgenerierung durch gezielte **Eingabeaufforderungen (Prompts)**, die an **LangChain** gesendet werden. Das Modell verarbeitet diese Anfragen und liefert darauf basierend den generierten Text.  \n","\n","Der folgende Code zeigt, wie ein LLM zur Generierung von Text abgefragt werden kann."],"metadata":{"id":"Q6qNoTVI7Jp_"}},{"cell_type":"code","source":["!uv pip install --system -q langchain-community langchain_openai"],"metadata":{"id":"CUdKGCwHf1CW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from os import environ\n","from google.colab import userdata\n","\n","environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"],"metadata":{"id":"IrZNFgsWf1CX"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xcbfVFK-Be0R"},"outputs":[],"source":["from langchain_core.messages import HumanMessage, SystemMessage\n","from langchain_core.prompts.chat import (\n","    ChatPromptTemplate,\n","    HumanMessagePromptTemplate,\n","    SystemMessagePromptTemplate,\n",")\n","from langchain_openai import ChatOpenAI\n","from IPython.display import display, Markdown\n","\n","MODEL = 'gpt-4o-mini'\n","TEMPERATURE = 0.2\n","\n","def get_response(llm, prompt):\n","  messages = [\n","      SystemMessage(\n","          content=\"Du bist ein hilfreicher Assistent, der Fragen präzise beantwortet.\"\n","      ),\n","      HumanMessage(content=prompt),\n","  ]\n","\n","  print(\"Modellantwort:\")\n","  output = llm.invoke(messages)\n","  display(Markdown(output.content))\n","\n","# Initialisieren Sie das OpenAI LLM mit Ihrem API-Schlüssel\n","llm = ChatOpenAI(\n","  model=MODEL,\n","  temperature=TEMPERATURE,\n","  n= 1,\n","  max_tokens= 256)"]},{"cell_type":"markdown","metadata":{"id":"DB0IAW8vBJLV"},"source":["<p><font color='black' size=\"5\">\n","Muster zur Textgenerierung\n","</font></p>\n","\n","Bei der grundlegenden Textgenerierung gibt es verschiedene **Eingabeaufforderungsmuster (Prompting-Methoden)**, die sich je nach Detailgrad der bereitgestellten Informationen unterscheiden. Diese Muster beeinflussen, wie das **Large Language Model (LLM)** den gewünschten Text erzeugt.  \n","\n","Im Folgenden werden verschiedene dieser Muster untersucht und analysiert, wie sie sich auf die Qualität und Struktur der generierten Inhalte auswirken.\n","\n","* Zero-Shot-Prompt\n","* One-Shot-Prompt\n","* Few-Shot-Prompt"]},{"cell_type":"markdown","metadata":{"id":"9GeVTcwLl4xi"},"source":["<p><font color='black' size=\"5\">\n","Zero-Shot Textgenerierung\n","</font></p>\n","\n","Ein Zero-Shot-Prompt zur Texterstellung ist eine Technik, bei der ein Sprachmodell ohne vorherige Anpassung oder spezielles Training auf eine bestimmte Aufgabe direkt mit einer einzigen Eingabeaufforderung genutzt wird. Um diesen Ansatz erfolgreich anzuwenden, ist es entscheidend, einen präzisen und ausführlichen Prompt zu formulieren, der genau beschreibt, welche Art von Inhalt generiert werden soll. Dabei sollten Stil, Struktur und relevante Details oder Einschränkungen klar angegeben werden. Beispielsweise kann für eine geschäftliche E-Mail der gewünschte Tonfall (formell oder informell), die wesentlichen Inhalte (wie Termin, Thema, Teilnehmer) sowie eine Handlungsaufforderung spezifiziert werden. Da das Modell allein auf die im Prompt enthaltenen Informationen angewiesen ist, sollte das gewünschte Ergebnis möglichst eindeutig formuliert sein. Diese Methode ist flexibel einsetzbar und erlaubt die Erstellung vielfältiger Textarten, ohne dass eine vorherige Modellanpassung erforderlich ist.  \n","\n","Das folgende Beispiel demonstriert eine Zero-Shot-Eingabeaufforderung: Es werden verschiedene Anforderungen gestellt und Informationen zu einem Studierenden bereitgestellt, jedoch ohne eine explizite Vorlage für das Sprachmodell vorzugeben."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bt_Ra8TOw1SP"},"outputs":[],"source":["print(get_response(llm, \"\"\"\n","Erstellen Sie ein positives Empfehlungsschreiben für Maria Weber, eine meiner Studierenden im Kurs ML 401 an der Technischen Universität München. Mein Name ist Dr. Julia Berger. Sie bewirbt sich für einen Master of Science in Data Science. Geben Sie mir nur den Haupttext des Schreibens, keine Kopf- oder Fußzeile. Formatieren Sie in Markdown.\n","\n","Unten ist ihre Anfrage:\n","\n","Sehr geehrte Frau Dr. Berger,\n","\n","ich hoffe, es geht Ihnen gut! Ich bin Maria Weber (Matrikelnr.: 7890), eine Absolventin der TU München, die im Juni 2023 ihren Bachelor in Wirtschaftsmathematik abgeschlossen hat.\n","\n","Im Wintersemester 2022/23 hatte ich das Privileg, Ihren Kurs ML 401: Grundlagen des maschinellen Lernens zu besuchen. Der Kurs war ein Wahlpflichtfach in meinem Bachelorprogramm. Die Inhalte haben mich sehr begeistert und ich habe mich durchgehend intensiv eingebracht, was sich in der Note 1,0 widerspiegelte.\n","\n","Nach meinem Abschluss mit einer Gesamtnote von 1,1 – der besten Note meines Jahrgangs – arbeite ich als Data Analytics Consultant bei der Firma DataTech GmbH. Meine Hauptaufgabe liegt in der Entwicklung von KI-gestützten Analysetools für Geschäftsprozesse. Um meine Expertise im Bereich Data Science zu vertiefen, möchte ich berufsbegleitend einen Master in Data Science absolvieren. Ich schätze Ihre Forschungsarbeit sehr (ich verfolge regelmäßig Ihre Publikationen und habe sie auch in meinem Team vorgestellt) und Ihre Einschätzung wäre für meine Bewerbung sehr wertvoll.\n","\n","Ich bewerbe mich für folgende Programme:\n","- ETH Zürich, Master of Science in Data Science\n","- TU Berlin, Master of Science in Data Engineering\n","\n","Dürfte ich Sie um ein Empfehlungsschreiben für diese Bewerbungen bitten? Meinen Lebenslauf habe ich beigefügt und ich stelle Ihnen gerne weitere Informationen zur Verfügung.\n","\n","Vielen Dank für die Berücksichtigung meiner Anfrage.\n","\n","Mit freundlichen Grüßen\n","Maria Weber\n","\"\"\"))"]},{"cell_type":"markdown","metadata":{"id":"XhpXOo-xOair"},"source":["<p><font color='black' size=\"5\">\n","One-Shot Textgenerierung\n","</font></p>\n","\n","Ein One-Shot-Prompt zur Textgenerierung ist eine Methode, bei der ein Sprachmodell mit einer einzigen, detaillierten Eingabe angewiesen wird, auf deren Basis es einen zusammenhängenden Text erstellt. Um diese Technik wirkungsvoll einzusetzen, sollte der Prompt klar und präzise formuliert sein und alle relevanten Informationen sowie den gewünschten Kontext enthalten. Dabei ist es ratsam, Stil, Ton und spezifische inhaltliche Aspekte genau zu definieren.  \n","\n","Möchte man beispielsweise eine atmosphärische Beschreibung einer Küstenstadt generieren lassen, sollten wesentliche Merkmale wie Tageszeit, Stimmung und bestimmte visuelle oder emotionale Eindrücke explizit benannt werden. Eine solche detaillierte Vorgabe erleichtert es dem Modell, die Anforderungen zu erfassen und passgenaue Inhalte zu erstellen.  \n","\n","Nachdem der Prompt formuliert wurde, kann er direkt in das Textgenerierungstool eingegeben werden. Falls das Ergebnis nicht vollständig den Erwartungen entspricht, lässt sich der Prompt gezielt anpassen, um die Qualität und Relevanz der Ausgabe weiter zu optimieren."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jc5ICsAUOdm0"},"outputs":[],"source":["print(get_response(llm, \"\"\"\n","Basierend auf diesem Beispiel, erstelle ein neues akademisches Empfehlungsschreiben.\n","Behalte den professionellen Ton bei, aber passe die Details an:\n","\n","BEISPIEL:\n","\"Ich freue mich sehr, Maria Weber für den Master of Science in Data Science zu empfehlen. Als ihre Dozentin im Kurs ML 401: Grundlagen des maschinellen Lernens an der TU München konnte ich ihre außergewöhnlichen akademischen Fähigkeiten beobachten. Frau Weber schloss den anspruchsvollen Kurs mit der Note 1,0 ab und demonstrierte dabei ein tiefgreifendes Verständnis für maschinelles Lernen und dessen praktische Anwendungen.\n","\n","Besonders beeindruckend war ihre Fähigkeit, komplexe Konzepte nicht nur zu verstehen, sondern auch auf reale Problemstellungen anzuwenden. In ihrer Position als Data Analytics Consultant bei DataTech GmbH setzt sie diese Fähigkeiten erfolgreich in der Entwicklung KI-gestützter Analysetools ein.\n","\n","Mit ihrem Bachelorabschluss in Wirtschaftsmathematik (Note 1,1) als Jahrgangsbeste hat Frau Weber bereits bewiesen, dass sie höchsten akademischen Anforderungen gerecht wird. Ihre analytischen Fähigkeiten, gepaart mit ihrer Lernbereitschaft und ihrem Engagement, machen sie zu einer idealen Kandidatin für ein weiterführendes Studium im Bereich Data Science.\n","\n","Aufgrund ihrer bisherigen Leistungen und ihres Potenzials empfehle ich Frau Weber nachdrücklich für den Master of Science in Data Science. Sie wird zweifellos einen wertvollen Beitrag zum Programm leisten.\"\n","\n","ANWEISUNGEN:\n","1. Erstelle ein neues Empfehlungsschreiben mit ähnlicher Struktur\n","2. Ändere:\n","   - Namen\n","   - Studienfach\n","   - Universitäten\n","   - Kurse\n","   - Noten\n","   - aktuelle Berufstätigkeit\n","3. Behalte bei:\n","   - Vier-Absatz-Struktur\n","   - Professionellen Ton\n","   - Spezifische Beispiele für Leistungen\n","   - Klare Empfehlung am Ende\n","\n","GEWÜNSCHTES FORMAT:\n","- Nur Haupttext\n","- Keine Anrede/Grußformel\n","- In Markdown formatiert\n","\"\"\"))"]},{"cell_type":"markdown","metadata":{"id":"DYr9PH7ZOeH3"},"source":["<p><font color='black' size=\"5\">\n","Few-Shot Textgenerierung\n","</font></p>\n","\n","Ein Few-Shot-Prompt gibt einem Modell eine begrenzte Anzahl von Beispielen, um dessen Reaktion gezielt zu steuern. Diese Methode eignet sich besonders für Sprach- oder Bildgenerierungsmodelle, da sie dem Modell hilft, Muster oder Stile anhand weniger Eingaben zu erkennen und entsprechend nachzubilden.  \n","\n","Ein typischer Few-Shot-Prompt für ein Textgenerierungsmodell enthält mehrere Beispielpaare aus Eingaben und den gewünschten Ausgaben. Dadurch erhält das Modell eine Orientierung und kann ähnliche Inhalte mit höherer Präzision erzeugen. Dieser Ansatz ermöglicht eine gezielte Anpassung der Modellantworten, ohne dass eine umfassende Trainingsphase erforderlich ist.  \n","\n","Few-Shot-Prompting verbessert die Qualität und Konsistenz der generierten Texte und macht das Modell flexibler einsetzbar – insbesondere für Aufgaben, die kreative Feinabstimmung oder spezifische Stilvorgaben erfordern."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uaYfdi_cOkS3"},"outputs":[],"source":["print(get_response(llm, \"\"\"\n","Basierend auf diesen Beispielen, erstelle ein neues akademisches Empfehlungsschreiben.\n","Behalte den professionellen Ton bei, aber passe die Details an:\n","\n","BEISPIEL 1:\n","\"Ich freue mich sehr, Maria Weber für den Master of Science in Data Science zu empfehlen. Als ihre Dozentin im Kurs ML 401: Grundlagen des maschinellen Lernens an der TU München konnte ich ihre außergewöhnlichen akademischen Fähigkeiten beobachten. Frau Weber schloss den anspruchsvollen Kurs mit der Note 1,0 ab und demonstrierte dabei ein tiefgreifendes Verständnis für maschinelles Lernen und dessen praktische Anwendungen.\n","\n","Besonders beeindruckend war ihre Fähigkeit, komplexe Konzepte nicht nur zu verstehen, sondern auch auf reale Problemstellungen anzuwenden. In ihrer Position als Data Analytics Consultant bei DataTech GmbH setzt sie diese Fähigkeiten erfolgreich in der Entwicklung KI-gestützter Analysetools ein.\n","\n","Mit ihrem Bachelorabschluss in Wirtschaftsmathematik (Note 1,1) als Jahrgangsbeste hat Frau Weber bereits bewiesen, dass sie höchsten akademischen Anforderungen gerecht wird. Ihre analytischen Fähigkeiten, gepaart mit ihrer Lernbereitschaft und ihrem Engagement, machen sie zu einer idealen Kandidatin für ein weiterführendes Studium im Bereich Data Science.\n","\n","Aufgrund ihrer bisherigen Leistungen und ihres Potenzials empfehle ich Frau Weber nachdrücklich für den Master of Science in Data Science. Sie wird zweifellos einen wertvollen Beitrag zum Programm leisten.\"\n","\n","BEISPIEL 2:\n","\"Mit großer Überzeugung empfehle ich Thomas Bauer für den Master of Science in Robotik. Als sein Betreuer im Kurs ROB 301: Einführung in die Robotik am Karlsruher Institut für Technologie habe ich sein außerordentliches technisches Talent kennengelernt. Herr Bauer erreichte im Kurs die Note 1,3 und zeigte dabei besondere Stärken in der praktischen Roboterprogrammierung.\n","\n","Seine innovative Herangehensweise an komplexe Problemstellungen war bemerkenswert. Als Werkstudent bei RoboTech entwickelte er bereits erfolgreich autonome Navigationssysteme für Industrieroboter und bewies damit seine Fähigkeit, theoretisches Wissen praktisch umzusetzen.\n","\n","Sein Bachelorabschluss in Mechatronik (Note 1,4) unterstreicht seine solide technische Grundausbildung. Herr Bauer zeichnet sich durch systematisches Denken, Kreativität bei der Lösungsfindung und ausgeprägte Teamfähigkeit aus - Eigenschaften, die für das angestrebte Masterstudium essentiell sind.\n","\n","Ich empfehle Herrn Bauer mit Nachdruck für den Master of Science in Robotik. Seine Motivation und sein technisches Verständnis werden ihn zu einem wertvollen Mitglied des Programms machen.\"\n","\n","BEISPIEL 3:\n","\"Es ist mir eine Freude, Lisa Schmidt für den Master of Science in Künstlicher Intelligenz zu empfehlen. Während ihres Besuchs meines Kurses AI 501: Deep Learning an der RWTH Aachen bewies sie außergewöhnliche Fähigkeiten im Bereich der künstlichen Intelligenz. Frau Schmidt erzielte die Note 1,2 und entwickelte dabei innovative Lösungsansätze für komplexe Deep-Learning-Aufgaben.\n","\n","In ihrer Rolle als KI-Entwicklerin bei AI Solutions hat sie ihr Talent bereits unter Beweis gestellt. Ihre selbstständig entwickelten Neural-Network-Architekturen für Bilderkennungssysteme zeigen ihre Fähigkeit, theoretische Konzepte in praktische Anwendungen zu überführen.\n","\n","Mit einem Bachelorabschluss in Informatik (Note 1,3) verfügt Frau Schmidt über eine exzellente Grundlage für weiterführende Studien. Ihre Kombination aus technischem Verständnis, Kreativität und analytischem Denken macht sie zu einer vielversprechenden Kandidatin für ein anspruchsvolles Masterprogramm.\n","\n","Ich empfehle Frau Schmidt uneingeschränkt für den Master of Science in Künstlicher Intelligenz. Ihre bisherigen Leistungen und ihr Enthusiasmus für das Fachgebiet lassen keinen Zweifel an ihrem zukünftigen Erfolg.\"\n","\n","ANWEISUNGEN:\n","1. Erstelle ein neues Empfehlungsschreiben mit ähnlicher Struktur\n","2. Ändere:\n","   - Namen\n","   - Studienfach\n","   - Universitäten\n","   - Kurse\n","   - Noten\n","   - aktuelle Berufstätigkeit\n","3. Behalte bei:\n","   - Vier-Absatz-Struktur\n","   - Professionellen Ton\n","   - Spezifische Beispiele für Leistungen\n","   - Klare Empfehlung am Ende\n","\n","GEWÜNSCHTES FORMAT:\n","- Nur Haupttext\n","- Keine Anrede/Grußformel\n","- In Markdown formatiert\n","\n","MUSTER FÜR ABSÄTZE:\n","1. Einleitung mit Empfehlung und Kontext\n","2. Spezifische Leistungen und praktische Erfahrung\n","3. Akademischer Hintergrund und persönliche Eigenschaften\n","4. Abschließende Empfehlung und Zukunftsprognose\n","\"\"\"))"]},{"cell_type":"markdown","metadata":{"id":"oFDE3DRgHvq1"},"source":["<p><font color='black' size=\"5\">\n","Synthetische Daten generieren\n","</font></p>\n","\n","LLMs (Large Language Models) eignen sich zur Erzeugung synthetischer Daten, was besonders nützlich für Testszenarien ist, die realistische Informationen oder eine breite demografische Vielfalt erfordern. Diese Modelle können beispielsweise detaillierte Biografien für verschiedene Berufsgruppen generieren, wodurch realitätsnahe Daten für Simulationen, die Entwicklung von Testalgorithmen oder das Training anderer KI-Systeme bereitgestellt werden.  \n","\n","Ein möglicher Anwendungsfall wäre die Erstellung synthetischer Biografien für Berufsgruppen wie Softwareentwickler, Kinderkrankenschwestern, Finanzanalysten, Naturwissenschaftslehrer an Universitäten oder Marketingmanager. Jede dieser Biografien könnte individuelle Karrierewege, Qualifikationen und berufliche Erfahrungen umfassen, um vielseitige und praxisnahe Tests zu ermöglichen. Auf diese Weise tragen synthetische Daten zur Verbesserung der Systemleistung bei, ohne auf echte personenbezogene Informationen zurückzugreifen."]},{"cell_type":"code","source":["!uv pip install --system -q langchain-community langchain_openai"],"metadata":{"id":"uILanvb8mKUK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from os import environ\n","from google.colab import userdata\n","\n","environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"],"metadata":{"id":"iKTZaSa1mKUL"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRoUOR8eIi07"},"outputs":[],"source":["from langchain_core.messages import HumanMessage, SystemMessage\n","from langchain_core.prompts.chat import (\n","    ChatPromptTemplate,\n","    HumanMessagePromptTemplate,\n","    SystemMessagePromptTemplate,\n",")\n","from langchain_openai import ChatOpenAI\n","from IPython.display import display, Markdown\n","\n","MODEL = 'gpt-4o-mini'\n","TEMPERATURE = 0.2\n","\n","def get_response(llm, prompt):\n","  messages = [\n","      SystemMessage(\n","          content=\"\"\"\n","          Sie sind ein Assistent, der synthetische Daten für eine Person in dem von Ihnen angegebenen Berufsfeld generiert. Erstelle eine kurze Biografie der Person, die nicht länger als 5 Sätze ist. Erwähne die Berufsbezeichnung nicht ausdrücklich.\"\"\"\n","      ),\n","      HumanMessage(content=prompt),\n","  ]\n","\n","  response = llm.invoke(messages)\n","  return response.content\n","\n","# Initialisieren Sie das OpenAI LLM mit Ihrem API-Schlüssel\n","llm = ChatOpenAI(\n","  model=MODEL,\n","  temperature=TEMPERATURE,\n","  n= 1,\n","  max_tokens= 256)"]},{"cell_type":"markdown","metadata":{"id":"bdXcz32RODJs"},"source":["Zunächst werden fünf verschiedene Karrieretypen definiert, für die anschließend Daten generiert werden."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U25feB-bIHSY"},"outputs":[],"source":["CAREER = [\n","\"Softwareentwickler\",\n","\"Kinderkrankenschwester\",\n","\"Finanzanalyst\",\n","\"Hochschullehrer Wirtschaftswissenschaften\",\n","\"Marketingmanager\"\n","]"]},{"cell_type":"markdown","metadata":{"id":"RGgajvk9OMy5"},"source":["Im nächsten Schritt wird eine zufällige Biografie erstellt, die als Grundlage für weitere Daten dient."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ebMRli43IxVD"},"outputs":[],"source":["print(get_response(llm, \"Softwareentwickler\"))"]},{"cell_type":"markdown","metadata":{"id":"qjLdt7kMOP-Q"},"source":["Nun wird eine CSV-Datei mit mehreren zufällig generierten Biografien erstellt."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kh4UDM16Jxq7"},"outputs":[],"source":["import csv\n","import random\n","from tqdm import tqdm\n","\n","FILENAME = \"jobs.csv\"\n","\n","# Schreiben in die CSV-Datei\n","with open(FILENAME, 'w', newline='\\n') as csvfile:\n","    csvwriter = csv.writer(csvfile)\n","\n","    # Verwenden Sie tqdm, um den Fortschrittsbalken anzuzeigen\n","    for i in tqdm(range(10), desc=\"Generating Careers\"):\n","      career_choice = random.choice(CAREER)  # Wählen Sie nach dem Zufallsprinzip einen Beruf aus\n","      csvwriter.writerow([i+1, get_response(llm, career_choice)])"]},{"cell_type":"markdown","metadata":{"id":"uS4bGv9yOUSH"},"source":["Generierte Daten ansehen."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H-LFTqX3KhFl"},"outputs":[],"source":["with open(FILENAME, 'r') as file:\n","    for _ in range(5):\n","        line = file.readline()\n","        if line:\n","            print(line.strip())\n","        else:\n","            break"]},{"cell_type":"markdown","metadata":{"id":"auMHm2SZOWgb"},"source":["CSV-Datei herunterladen"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IQLZF9qHNZKg"},"outputs":[],"source":["from google.colab import files\n","files.download(FILENAME)"]},{"cell_type":"markdown","metadata":{"id":"HODQfLqHBnUI"},"source":["# **3 <font color='orange'>|</font> Textzusammenfassung**\n","---"]},{"cell_type":"markdown","source":["Große Sprachmodelle (LLMs) wie GPT-4 fassen Texte zusammen, indem sie zentrale Inhalte extrahieren und kompakt wiedergeben. Sie erfassen den Kontext sowie semantische Strukturen und erstellen eine prägnante Version, die die Hauptaussagen erhält. Durch ihre Fähigkeit, verschiedene Textarten zu analysieren – von Fachartikeln bis hin zu narrativen Texten - liefern sie verständliche und relevante Zusammenfassungen. Ihre Flexibilität in Bezug auf Länge und inhaltliche Schwerpunkte macht sie zu einem effizienten Werkzeug für die schnelle Informationsverarbeitung."],"metadata":{"id":"l-LRWCEb7X-5"}},{"cell_type":"markdown","source":["<p><font color='black' size=\"5\">\n","Einzelnes PDF zusammenfassen\n","</font></p>\n","\n","Zunächst wird die Zusammenfassung einer einzelnen PDF-Datei betrachtet. LangChang nutzt spezielle Dokumentlader, um verschiedene Dateiformate wie PDFs zu verarbeiten. Für unterschiedliche Datentypen stehen spezifische Lader zur Verfügung. Im folgenden Code wird eine PDF-Datei geladen und mithilfe einer allgemeinen Systemaufforderung analysiert und zusammengefasst."],"metadata":{"id":"l6nozFN3ZmH6"}},{"cell_type":"code","source":["!uv pip install --system -q langchain-community langchain_openai pypdf"],"metadata":{"id":"o_DUnw6NaNmS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from os import environ\n","from google.colab import userdata\n","\n","environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"],"metadata":{"id":"m_L0fUjPaZdU"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfISNTpwOKiZ"},"outputs":[],"source":["from langchain.chains.summarize import load_summarize_chain\n","from langchain.document_loaders import PyPDFLoader, TextLoader\n","from langchain import OpenAI, PromptTemplate\n","from langchain_openai import ChatOpenAI\n","from IPython.display import display, Markdown\n","\n","MODEL = 'gpt-4o-mini'\n","\n","llm = ChatOpenAI(\n","        model=MODEL,\n","        temperature=0.2,\n","        n=1\n","    )"]},{"cell_type":"markdown","metadata":{"id":"OLUWbW7NRp4m"},"source":["Der folgende Code demonstriert die Nutzung der Funktion **„load_summarize_chain“**, um mithilfe eines Large Language Models (LLM) und des Kettentyps **„map_reduce“** eine Zusammenfassung zu erstellen. Zunächst wird das PDF-Dokument *Attention Is All You Need* über den **„PyPDFLoader“** von der angegebenen URL (**„https://arxiv.org/pdf/1706.03762“**) geladen. Anschließend wird der Inhalt durch **„load_and_split“** in kleinere Abschnitte unterteilt. Diese Segmente werden dann der Verarbeitungskette (**„chain.run(docs)“**) übergeben, die den Text verarbeitet und verdichtet. Schließlich erfolgt die Ausgabe des zusammengefassten Inhalts im **Markdown-Format**, um die ursprüngliche Formatierung beizubehalten."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pDmTQPXgqQfd"},"outputs":[],"source":["chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n","\n","url = \"https://arxiv.org/pdf/1706.03762\"\n","loader = PyPDFLoader(url)\n","docs = loader.load_and_split()\n","summary = chain.invoke(docs)['output_text']\n","display(Markdown(summary))"]},{"cell_type":"markdown","metadata":{"id":"uA1iXHlyMSUU"},"source":["<p><font color='black' size=\"5\">\n","Benutzerdefinierter Zusammenfassung\n","</font></p>\n","\n","LangChain ermöglicht die Anpassung von Systemaufforderungen, um Zusammenfassungen gezielt an spezifische Anforderungen anzupassen, beispielsweise durch die Ausgabe in einer anderen Sprache. Im bereitgestellten Code wird diese Flexibilität demonstriert, indem eine benutzerdefinierte Eingabeaufforderung verwendet wird, die das System anweist, eine kompakte Zusammenfassung auf Spanisch zu erstellen.  \n","\n","Dazu enthält die Vorlage einen Platzhalter für den ursprünglichen Text, gefolgt von einer Anweisung auf Spanisch, die zur Generierung der Zusammenfassung dient. Diese benutzerdefinierte Eingabeaufforderung wird anschließend in den Zusammenfassungsprozess eingebunden, indem die Parameter **„map_prompt“** und **„combine_prompt“** innerhalb der Funktion **„load_summarize_chain“** konfiguriert werden.  \n","\n","Der Ablauf beginnt mit dem Laden einer PDF-Datei von einer angegebenen URL über **„PyPDFLoader“**, gefolgt von der Unterteilung des Dokuments in handhabbare Abschnitte. Anschließend wird die Zusammenfassungskette mit der maßgeschneiderten Eingabeaufforderung angewendet, um eine spanische Zusammenfassung zu erzeugen. Der resultierende Text wird schließlich im **Markdown-Format** ausgegeben, um die Formatierung beizubehalten.  \n","\n","Dieses Beispiel zeigt, wie LangChain gezielt für komplexe Zusammenfassungsaufgaben mit sprachspezifischen Anforderungen eingesetzt werden kann."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lpQmmSiNMZNY"},"outputs":[],"source":["TEMPLATE = \"\"\"\n","Schreiben Sie eine kurze Zusammenfassung der präsentierten Informationen. Schreiben Sie die Zusammenfassung auf Deutsch.\n","\n","{text}\n","\n","SUMMARY:\"\"\"\n","PROMPT = PromptTemplate(template=TEMPLATE, input_variables=[\"text\"])\n","\n","chain = load_summarize_chain(llm, chain_type=\"map_reduce\", map_prompt=PROMPT, combine_prompt=PROMPT)\n","\n","url = \"https://arxiv.org/pdf/1706.03762\"\n","loader = PyPDFLoader(url)\n","docs = loader.load_and_split()\n","summary = chain.invoke(docs)['output_text']\n","display(Markdown(summary))"]},{"cell_type":"markdown","metadata":{"id":"D3MVbKWNsLL3"},"source":["<p><font color='black' size=\"5\">\n","Mehrere PDFs zusammenfassen\n","</font></p>\n","\n","Diese vier wissenschaftlichen Arbeiten haben wesentliche Fortschritte im Bereich der generativen KI und der Verarbeitung natürlicher Sprache (NLP) ermöglicht.  \n","\n","1. **\"Attention Is All You Need\" (Vaswani et al., 2017)** führte die **Transformer-Architektur** ein, die sich als grundlegendes Modell für moderne NLP-Systeme etabliert hat. Der Verzicht auf rekurrente Strukturen und die Einführung des Selbstaufmerksamkeitsmechanismus ermöglichten effizientere und leistungsfähigere Modelle wie GPT und BERT.  \n","\n","2. **\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" (Devlin et al., 2018)** revolutionierte die Kontextverarbeitung in NLP-Modellen durch **bidirektionales Training**, das es ermöglichte, sowohl vorhergehende als auch nachfolgende Wörter gleichzeitig zu analysieren. Dadurch erzielte BERT erhebliche Verbesserungen bei zahlreichen NLP-Aufgaben.  \n","\n","3. **\"Language Models are Few-Shot Learners\" (Brown et al., 2020)** untersuchte das Potenzial **großskalierter Transformer-Modelle** am Beispiel von GPT-3. Die Studie zeigte, dass die Leistung solcher Modelle durch Skalierung verbessert wird und sie auch ohne aufgabenspezifisches Training effektive Ergebnisse liefern können.  \n","\n","4. **\"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" (Raffel et al., 2019)** präsentierte **T5 (Text-to-Text Transfer Transformer)**, ein Modell, das alle NLP-Aufgaben in ein einheitliches Text-zu-Text-Format überführt. Dadurch wurde das Transferlernen zwischen verschiedenen Aufgaben vereinfacht.  \n","\n","Diese Arbeiten bilden zusammen die Grundlage für viele moderne NLP-Modelle und deren Anwendungen in generativer KI."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wIVM9OwssPZm"},"outputs":[],"source":["urls = [\n","  \"https://arxiv.org/pdf/1706.03762\",\n","  \"https://arxiv.org/pdf/1810.04805\",\n","  \"https://arxiv.org/pdf/2005.14165\",\n","  \"https://arxiv.org/pdf/1910.10683\"\n","]\n","\n","summaries = []\n","\n","chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n","\n","for url in urls:\n","    print(f\"Lesen: {url}\")\n","    loader = PyPDFLoader(url)\n","    docs = loader.load_and_split()\n","    chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n","    summary = chain.invoke(docs)['output_text']\n","    summaries.append(summary)"]},{"cell_type":"markdown","metadata":{"id":"QE79kzVUTZ15"},"source":["Nachdem die einzelnen Artikel zusammengefasst wurden, folgt die Erstellung einer umfassenden Übersicht durch die Kombination dieser Zusammenfassungen. Hierfür werden die Texte zunächst zu einer langen Zeichenfolge zusammengeführt.  \n","\n","Um die große Textmenge effizient zu verarbeiten, kommt die **CharacterTextSplitter-Klasse** von LangChain zum Einsatz. Diese unterteilt den kombinierten Text in handhabbare Abschnitte mit **500 Zeichen Länge und einer Überlappung von 100 Zeichen**, um den Zusammenhang zwischen den Segmenten zu erhalten. Anschließend werden die Blöcke in **Dokumentobjekte** umgewandelt, die jeweils einen Teil der ursprünglichen Zusammenfassungen enthalten.  \n","\n","Mithilfe einer neuen **map_reduce-Zusammenfassungskette** werden diese Dokumentobjekte verarbeitet. Die Kette analysiert die Abschnitte, extrahiert wesentliche Informationen und erstellt eine **endgültige, verdichtete Zusammenfassung** der gesamten Inhalte.  \n","\n","Das Endergebnis wird im **Markdown-Format** ausgegeben, um eine klare Struktur und Lesbarkeit zu gewährleisten. Auf diese Weise entsteht eine prägnante, aber inhaltlich umfassende Synthese der Originalartikel."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cjpShxq87RvJ"},"outputs":[],"source":["from langchain.text_splitter import CharacterTextSplitter\n","from langchain.document_loaders import TextLoader\n","from langchain.schema.document import Document\n","\n","chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n","\n","summary_str = \" \".join(summaries)\n","text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n","texts = text_splitter.split_text(summary_str)\n","docs = [Document(page_content=t) for t in texts]\n","chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n","final_summary = chain.invoke(docs)['output_text']\n","display(Markdown(final_summary))"]},{"cell_type":"markdown","metadata":{"id":"TgfPV_aXBuv9"},"source":["# **4 <font color='orange'>|</font> Textklassifizierung**\n","---"]},{"cell_type":"markdown","source":["Große Sprachmodelle (LLMs) haben die Textklassifizierung grundlegend verändert, indem sie einen flexibleren und effizienteren Ansatz als herkömmliche maschinelle Lernmethoden bieten. Während traditionelle Verfahren umfangreiche, manuell annotierte Datensätze benötigten, um Modelle zu trainieren, bieten LLMs eine zeitsparende Alternative.  \n","\n","Durch die **Few-Shot Klassifizierung**, die mit beschriftete Beispiele funktioniert, können LLMs Texte allein auf Basis weniger natürlichsprachlichen Beispiele kategorisieren. Diese Fähigkeit beruht auf dem umfassenden Wissen, das die Modelle während ihres Trainings mit großen Textkorpora erworben haben.  \n","\n","Dieser Ansatz reduziert den Aufwand für die Einrichtung einer Textklassifizierung erheblich und ermöglicht eine schnelle Anpassung an neue Anforderungen. Dadurch eignen sich LLMs für verschiedene Anwendungsfälle, darunter **Stimmungsanalyse, Themenkategorisierung, Spam-Erkennung und Inhaltsmoderation**.  \n","\n","Im folgenden Code werden SMS-Texte analysiert. Anschließend erfolgt deren Klassifizierung mithilfe eines LLMs."],"metadata":{"id":"XB3SnNhx7eWv"}},{"cell_type":"code","source":["from pandas import read_csv\n","\n","df = read_csv(\n","    \"https://raw.githubusercontent.com/ralf-42/GenAI/main/02%20data/sms_spam_collection.csv\",\n","    sep=\"\\t\",\n","    header=None,\n","    names=[\"class\", \"msg\"],\n",")"],"metadata":{"id":"uOM-Qa_0xXRK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"Tr6N3qSzzy1D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.tail(5)"],"metadata":{"id":"1bwoYfcwzjA6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8cdf6QNZrC02"},"source":["Im folgenden Programm erfolgt eine Klassifizierung der SMS-Texte.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jG0ra9gvs1vO"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","\n","MODEL = 'gpt-4o'\n","TEMPERATURE = 0.0\n","\n","# Initialisieren Sie das OpenAI LLM mit Ihrem API-Schlüssel\n","llm = ChatOpenAI(\n","    model=MODEL,\n","    temperature=TEMPERATURE,\n","    n=1\n",")"]},{"cell_type":"markdown","metadata":{"id":"4M4AUGRRs94D"},"source":["Dieser Code veranschaulicht, wie ein großes Sprachmodell (LLM) mithilfe des Langchain-Frameworks zur Klassifizierung von SMS-Nachrichten in Spam oder Ham (legitime Nachrichten) eingesetzt wird. Der Ablauf beginnt mit dem Import der benötigten Module und Klassen aus Langchain, darunter HumanMessage, SystemMessage, AIMessage, ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate, PromptTemplate, LLMChain und SimpleSequentialChain.\n","\n","Es wird eine Eingabevorlage mit PromptTemplate definiert. Die Vorlage, *sms_prompt*, analysiert eine SMS und klassifiziert sie als Spam oder Ham. Es werden Beispiele für Spam und Ham dem LLM zur Verfügung gestellt (Few-Shot-Prompt).\n","\n","Anschließend wird eine LLM-Kette erstellt: *chain_sms* zur Klassifikation der Nachrichten. Diese Kette ermöglicht die automatische Verarbeitung der SMS-Texte.\n","\n","Eine Schleife durchläuft die SMS-Nachrichten im Datensatz. Die Klassifizierung erfolgt durch *chain_sms.invoke(sms)*, das entweder \"Spam\" oder \"Ham\" als Ergebnis liefert. Die Klassifizierung basiert auf der Analyse des Textes durch das Sprachmodell, das gelernt hat, verdächtige Muster und Formulierungen zu erkennen. Die Ergebnisse der Klassifizierung werden ausgegeben und können zur weiteren Analyse oder Filterung verwendet werden."]},{"cell_type":"code","source":["# Zufällige Auswahl von 10 SMS-Texten aus dem Datenbestand\n","data = df.sample(10)"],"metadata":{"id":"gpeA2vXK1-W1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zc92unRdrDQC"},"outputs":[],"source":["from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n","from langchain_core.prompts.chat import (\n","    ChatPromptTemplate,\n","    HumanMessagePromptTemplate,\n","    SystemMessagePromptTemplate,\n",")\n","from langchain.prompts import PromptTemplate\n","from langchain.chains import LLMChain, SimpleSequentialChain"]},{"cell_type":"code","source":["# Prompt Template\n","sms_prompt = PromptTemplate( input_variables = ['sms'], template = \"\"\"\n","Klassifiziere die folgende SMS nach den beigefügten Beispiele:\n","spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n","spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n","spam\tWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n","spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n","ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n","ham\tOk lar... Joking wif u oni...\n","ham\tU dun say so early hor... U c already then say...\n","ham\tNah I don't think he goes to usf, he lives around here though\n","\n","Gebe als Antwort nur \"spam\" oder \"ham\" zurück. Wenn keine Entscheidung möglich ist dann \"sonstiges\". Gebe keine Begründung.\n","\n","Hier ist die SMS:\n","\n","{sms}\"\"\")"],"metadata":{"id":"vm5qLwa96cfB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verabeitungskette\n","chain_sms = sms_prompt | llm"],"metadata":{"id":"aCQwspjV6jv8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Klassifizierung der 10 Datensätze\n","for i in range(data.shape[0]):\n","    sms = data.iloc[i]\n","    classification = chain_sms.invoke(sms).content.strip()\n","    print(f\"SMS # {i} ist: {classification}\")"],"metadata":{"id":"WWIiJWe76m50"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Vergleich mit der Zuordnung in der Quelldaten\n","data"],"metadata":{"id":"t4ZbnLjv463_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ibm452B-uURM"},"source":["<p><font color='black' size=\"5\">\n","Namen klassifizieren\n","</font></p>\n","\n","Die Analyse von Wörtern in Texten stellt für künstliche Intelligenz eine interessante Herausforderung dar, besonders bei der Zero-Shot Klassifizierung ohne Trainingsdaten. Hierbei muss das Modell spontan entscheiden, ob ein Wort ein Name oder ein reguläres Wort ist. Im folgenden Text soll getestet werden, ob eine generative KI in der Lage ist, Wörter korrekt in die Kategorien \"Name\" oder \"Nicht-Name\" einzuordnen, ohne zuvor mit ähnlichen Beispielen trainiert worden zu sein. Die zu analysierenden Wörter enthalten sowohl gewöhnliche Begriffe als auch verschiedene Arten von Namen wie Vornamen, Nachnamen oder Firmennamen. Die zentrale Frage lautet: Kann das Modell durch sein grundlegendes Sprachverständnis diese Unterscheidung treffen?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MlC_0wQ3vEzn"},"outputs":[],"source":["SAMPLE = \"\"\"\n","Anna und Sarah sind seit dem Studium befreundet. Nach ihrem Abschluss hatten sie ein Vorstellungsgespräch bei Frau Weber für eine Stelle bei der Technovision GmbH, deren Hauptprodukt Futurtech heißt und von der Programmiererin Maria Schmidt entwickelt wurde. Nach dem erfolgreichen Gespräch trafen sie sich mit ihren Freunden Rafael und Aida zum Feiern. An ihrem ersten Arbeitstag lernten sie drei weitere Mitarbeiter kennen: Richard, Lisa und Ottilie. Später kam noch Matthias Müller zum Team dazu.\"\"\""]},{"cell_type":"markdown","metadata":{"id":"taGW_e6xvKJM"},"source":["Nun wird ein Large Language Model (LLM) definiert, das den oben genannten Absatz analysiert."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Crvlr5aFuWks"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","\n","MODEL = 'gpt-4o-mini'\n","TEMPERATURE = 0.0\n","\n","llm = ChatOpenAI(\n","    model=MODEL,\n","    temperature=TEMPERATURE,\n","    n=1\n",")"]},{"cell_type":"markdown","metadata":{"id":"4r8S8zI0ua8U"},"source":["Letztendlich wird eine einzelne Eingabeaufforderung genutzt, um diese Namen zu identifizieren und zu kategorisieren."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zeEyWY1TuZrs"},"outputs":[],"source":["from langchain_core.prompts import PromptTemplate\n","\n","prompt = PromptTemplate(\n","    template=\"\"\"\n","    Finde alle Namen (Mensch, Unternehmen) im folgenden Text. Extrahiere den vollständigen Namen und gib nur Namen zurück.\\n\n","    Gib jeden Namen in einer separaten Zeile aus.\\n\n","    {subject}.\\n\"\"\",\n","    input_variables=[\"subject\"]\n",")\n","\n","chain = prompt | llm\n","\n","print(chain.invoke({\"subject\": SAMPLE}).content)"]},{"cell_type":"markdown","metadata":{"id":"x1Wb0diiB1yO"},"source":["# **5 <font color='orange'>|</font> LLM schreibt ein Buch**\n","---"]},{"cell_type":"markdown","source":["Dieser Abschnitt zeigt, wie ein Large Language Model (LLM) beim Schreiben eines Buches genutzt werden kann. Da ein Buch eine komplexe, langfristige Aufgabe darstellt, erfolgt der Prozess schrittweise.  \n","\n","Zunächst wird ein **Thema** gewählt, auf dessen Basis das LLM einen Titel und eine kurze Zusammenfassung erstellt. Diese Zusammenfassung dient als inhaltlicher Leitfaden. Anschließend wird ein **Inhaltsverzeichnis** generiert, das die Kapitel und Abschnitte strukturiert.  \n","\n","Jedes **Kapitel** wird in separaten Sitzungen mit dem LLM ausgearbeitet, wobei die zuvor erstellte Gliederung zur Orientierung dient. Dieser iterative Ansatz gewährleistet Konsistenz und ermöglicht eine detaillierte Ausarbeitung.  \n","\n","Durch eine methodische Vorgehensweise verwandelt sich eine anfängliche Idee in ein gut strukturiertes Buch. Für kreative Ergebnisse wird ein Sprachmodell mit einer Temperatur von 0,7 verwendet."],"metadata":{"id":"VPzcHtnM7k-f"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8bj12lV6B1yi"},"outputs":[],"source":["from langchain.chains.summarize import load_summarize_chain\n","from langchain import OpenAI, PromptTemplate\n","from langchain_openai import ChatOpenAI\n","from IPython.display import display, Markdown\n","\n","MODEL = 'gpt-4o-mini'\n","\n","llm = ChatOpenAI(\n","        model=MODEL,\n","        temperature=0.7,\n","        n=1\n","    )"]},{"cell_type":"markdown","metadata":{"id":"k5D_R3w8B1yj"},"source":["Eine einfache Hilfsfunktion wird implementiert, um das LLM mit einer benutzerdefinierten Systemaufforderung zu befragen. Diese Aufforderung informiert das Modell darüber, dass es beim Schreiben eines Buches assistiert."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v8vGrZcjTvwZ"},"outputs":[],"source":["from langchain_core.messages import HumanMessage, SystemMessage\n","from langchain_core.prompts.chat import (\n","    ChatPromptTemplate,\n","    HumanMessagePromptTemplate,\n","    SystemMessagePromptTemplate,\n",")\n","from langchain_openai import ChatOpenAI\n","\n","def query_llm(prompt):\n","  messages = [\n","      SystemMessage(\n","          content=\"Du bist Assistent beim Schreiben eines Buches.\"\n","      ),\n","      HumanMessage(\n","          content=prompt\n","      ),\n","  ]\n","\n","  output = llm.invoke(messages)\n","  return output.content"]},{"cell_type":"markdown","metadata":{"id":"FOWb0XT7RlYg"},"source":["\n","<p><font color='black' size=\"5\">\n","Titel, Zusammenfassung, Inhaltsverzeichnis\n","</font></p>\n","\n","Für dieses Buch kann der Benutzer das Thema über die Variable `SUBJECT` festlegen. Anschließend wird das LLM angewiesen, einen zufälligen Titel basierend auf diesem Thema zu generieren. Dabei wird darauf geachtet, dass die Eingabeaufforderung präzise formuliert ist, um zu vermeiden, dass das LLM zusätzliche Einleitungen wie „Hier ist ein zufälliger Titel“ hinzufügt."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YpEgTYV1TvpH"},"outputs":[],"source":["SUBJECT = \"Fantasy Thriller\"\n","\n","title = query_llm(f\"\"\"\n","Gebe mir einen zufälligen Titel für ein Buch zum Thema {SUBJECT}.\n","Gibt nur den Titel zurück, keinen zusätzlichen Text.\n","\"\"\").strip(\" '\\\"\")\n","print(title)"]},{"cell_type":"markdown","metadata":{"id":"fXyOF_oYtMlR"},"source":["Nachdem der Titel festgelegt wurde, kann nun eine zufällige Zusammenfassung des Buches generiert werden. Diese gibt eine erste inhaltliche Orientierung und dient als Grundlage für die weitere Strukturierung."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KxYdLzYCTvgQ"},"outputs":[],"source":["synopsis = query_llm(f\"\"\"\n","Gebe mir eine Inhaltsangabe für ein Buch mit dem Titel {SUBJECT} für ein Buch zum Thema {SUBJECT}.\n","Gebe nur die Inhaltsangabe zurück, keinen zusätzlichen Text.\n","\"\"\").strip(\" '\\\"\")\n","print(synopsis)"]},{"cell_type":"markdown","metadata":{"id":"U6ju-lJEttjw"},"source":["Nun wird das Inhaltsverzeichnis erstellt, wobei alle zuvor generierten Informationen berücksichtigt werden. Ein spezifisches Format wird vorgegeben, um eine klare Struktur zu gewährleisten. Obwohl die Kapitelnummern leicht ableitbar wären, werden sie explizit angefordert, da das LLM sie ohnehin bereitstellen möchte. Der Ansatz erleichtert eine konsistente Struktur und ermöglicht es, die Nummerierung später gezielt zu entfernen, falls erforderlich."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9IhXP9PJTvXq"},"outputs":[],"source":["toc = query_llm(f\"\"\"\n","Gebe mir ein Inhaltsverzeichnis für ein Buch mit dem Titel {title} für ein Buch zum Thema {SUBJECT}. Die Buchzusammenfassung lautet {synopsis}.\n","Gebe das Inhaltsverzeichnis als Liste von Kapitelüberschriften zurück. Erstelle nicht mehr als 10 Kapitel.\n","Trenne Kapitelnummer und Kapitelüberschrift mit einem Pipe-Zeichen '|'.\n","Gebe nur die Kapitelnamen zurück, keinen zusätzlichen Text.\n","\"\"\").strip(\" '\\\"\")\n","print(toc)"]},{"cell_type":"markdown","metadata":{"id":"Mvh6hlVBCN2f"},"source":["Nun wird das Inhaltsverzeichnis bereinigt, indem die senkrechten Striche und Kapitelnummern entfernt werden. Dadurch entsteht eine klarere und leserfreundlichere Struktur, ohne dass die inhaltliche Gliederung verloren geht."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k-A4RgyR3Mid"},"outputs":[],"source":["# Teilen Sie die Zeichenfolge in Zeilen auf\n","lines = toc.splitlines()\n","\n","# Titel mithilfe der Listenableitung extrahieren\n","toc2 = [line.split('|')[1].strip() for line in lines if line]\n","\n","# Drucken Sie die Titelliste aus\n","print(toc2)"]},{"cell_type":"markdown","metadata":{"id":"vMXLYGi_RxNp"},"source":["\n","<p><font color='black' size=\"5\">\n","Erstellen der Kapitel des Buches\n","</font></p>\n","\n","\n","Nun wird eine Funktion erstellt, die den Text eines Kapitels generiert. Damit das LLM über ausreichend Kontext verfügt, werden die Zusammenfassung, das Inhaltsverzeichnis und die entsprechende Kapitelnummer übergeben. Zur Überprüfung der Funktion wird zunächst ein einzelnes Kapitel generiert."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ATXhVI9NfUTQ"},"outputs":[],"source":["def render_chapter(num, chapter_title, title, subject, synopsis, toc):\n","  txt = query_llm(f\"\"\"\n","    Schreibe die Kapitel {num} mit dem Titel {chapter_title} für ein Buch mit dem Titel {title} für ein Buch zum Thema {subject}, die Buchzusammenfassung lautet {synopsis}, das Inhaltsverzeichnis lautet {toc}.\n","    Gebe nur den Kapiteltext, keine Kapitelüberschrift, keinen Kapiteltitel, keine Kapitelnummer, keinen zusätzlichen Text.\n","  \"\"\").strip(\" '\\\"\")\n","  return txt\n","\n","txt = render_chapter(1, toc2[0], title, SUBJECT, synopsis, toc)\n","print(txt)"]},{"cell_type":"markdown","metadata":{"id":"6s7Igm5cSf50"},"source":["Jetzt kann das gesamte Buch in Markdown-Format generiert werden, wodurch eine strukturierte und gut lesbare Formatierung ermöglicht wird. Der Prozess beginnt mit der Darstellung des Titels und der Zusammenfassung, gefolgt vom Inhaltsverzeichnis und den einzelnen Kapiteln."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uq8xv3RIBkIv"},"outputs":[],"source":["book = \"\"\n","\n","# Rendern des Titels und der Zusammenfassung\n","book += f\"# {title}\\n\"\n","book += f\"{synopsis}\\n\"\n","\n","# Rendern des Inhaltsverzeichnisses\n","book += f\"\\n# # Inhaltsverzeichnis\\n\\n\"\n","num = 1\n","for chapter_title in toc2:\n","  book += f\"{num}. {chapter_title}\\n\"\n","  num += 1\n","\n","# Rendern des Buchs\n","chapter = 1\n","for chapter_title in toc2:\n","    print(f\"Rendern von Kapitel {chapter}/{len(toc2)}: {chapter_title}\")\n","    txt = render_chapter(chapter, chapter_title, title, SUBJECT, synopsis, toc)\n","    book += f\"\\n\\n# # Kapitel {chapter}: {chapter_title}\\n\"\n","    book += f\"{txt}\\n\"\n","    chapter += 1"]},{"cell_type":"markdown","metadata":{"id":"xXVa0IxKR5V7"},"source":["<p><font color='black' size=\"5\">\n","PDF des Buches\n","</font></p>\n","\n","Sobald das Buch erstellt wurde, wird es in ein PDF-Format konvertiert und gespeichert, um eine leicht zugängliche und gut formatierte Ausgabe bereitzustellen."]},{"cell_type":"code","source":["!uv pip install --system -q pdfkit\n","!sudo apt-get install -qq -y wkhtmltopdf"],"metadata":{"id":"n6miMMnTCYCS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import markdown\n","import pdfkit\n","\n","def markdown_string_zu_pdf(markdown_string, ausgabedatei):\n","    html = markdown.markdown(markdown_string)\n","\n","    # HTML-Struktur mit Meta-Tag und lang-Attribut hinzufügen\n","    html = f\"\"\"\n","    <!DOCTYPE html>\n","    <html lang=\"de\">\n","    <head>\n","        <meta charset=\"UTF-8\">\n","    </head>\n","    <body>\n","        {html}\n","    </body>\n","    </html>\n","    \"\"\"\n","\n","    try:\n","        pdfkit.from_string(html, ausgabedatei)  # oder weasyprint.HTML(string=html).write_pdf(ausgabedatei)\n","        print(f\"PDF erfolgreich erstellt: {ausgabedatei}\")\n","    except Exception as e:\n","        print(f\"Fehler bei der PDF-Erstellung: {e}\")\n","\n","# PDF-Datei erstellen\n","markdown_string_zu_pdf(book, \"output.pdf\")"],"metadata":{"id":"Nwk2uSKZB7S4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NTsMT8sYYh6C"},"source":["Das generierte Buch steht nun zum Download bereit."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pqcpXCsABZZo"},"outputs":[],"source":["from google.colab import files\n","files.download(\"output.pdf\")"]},{"cell_type":"markdown","source":["# **A <font color='orange'>|</font> Aufgabe**\n","---"],"metadata":{"id":"ikTJEPGqN9IH"}},{"cell_type":"markdown","source":["\n","<p><font color='black' size=\"5\">\n","Sentiment-Analyse von Produktbewertungen mit LLM Zero-Shot-Klassifizierung\n","</font></p>\n","\n","**Aufgabenstellung**\n","Entwickeln Sie ein Python-Programm, das mithilfe eines Large Language Models (LLM) Produktbewertungen in die Kategorien \"Positiv\", \"Neutral\" oder \"Negativ\" einordnet und zusätzlich die betroffenen Produktaspekte (Qualität, Preis, Lieferung, Service) identifiziert.\n","\n","**Lernziele**\n","Nach Abschluss dieser Übung können Sie:\n","- Zero-Shot-Klassifizierung für mehrere Kategorien implementieren\n","- Aspekt-basierte Sentiment-Analyse durchführen\n","- Komplexe Prompts für Mehrfachklassifizierung erstellen\n","- Strukturierte Ausgaben aus LLM-Antworten generieren\n","\n","**Aufgabendetails**\n","\n","```python\n","# Beispieldaten\n","reviews = [\n","    \"Die Qualität des Produkts ist hervorragend, allerdings finde ich den Preis zu hoch.\",\n","    \"Schnelle Lieferung, guter Service, faire Preise - besser geht es nicht!\",\n","    \"Nach zwei Wochen ging das Gerät kaputt. Der Kundenservice war bei der Reklamation leider keine Hilfe.\",\n","    \"Durchschnittliche Qualität, erfüllt seinen Zweck. Lieferung dauerte etwas länger als angegeben.\"\n","]\n","```"],"metadata":{"id":"lzlpv_R4OHPR"}},{"cell_type":"markdown","source":["**Beispiel-Prompt**\n","```\n","Analysiere die folgende Produktbewertung hinsichtlich:\n","1. Gesamtsentiment (Positiv/Neutral/Negativ)\n","2. Erwähnte Produktaspekte (Qualität/Preis/Lieferung/Service)\n","3. Sentiment pro Aspekt\n","\n","Bewertung: {review}\n","\n","Antworte im Format:\n","Gesamtsentiment: [Kategorie]\n","Identifizierte Aspekte:\n","- [Aspekt]: [Sentiment]\n","Begründung: [Analyse]\n","```"],"metadata":{"id":"5gffAlWTOwXS"}}]}
#
# utilities.py
#
# Stand: 04.12.2025
#
from IPython.display import display, Markdown, SVG
from IPython import get_ipython
import requests
import sys
import warnings
import subprocess
import tempfile
import importlib.util
import re
from typing import Tuple, Any
from langchain_core.prompts import ChatPromptTemplate
#
# -- Sammlung von Standard-Funktionen f√ºr den Kurs
#

def check_environment():
    """
    Gibt die installierte Python-Version aus, listet installierte LangChain- und LangGraph-Bibliotheken auf
    und unterdr√ºckt typische Deprecation-Warnungen im Zusammenhang mit LangChain.

    Diese Funktion ist hilfreich, um schnell die Entwicklungsumgebung f√ºr LangChain-Projekte
    zu √ºberpr√ºfen und st√∂rende Warnungen im Notebook oder in der Konsole zu vermeiden.

    Ausgabe:
        - Python-Version
        - Liste installierter Pakete, die mit "langchain" beginnen
        - Liste installierter Pakete, die mit "langgraph" beginnen
    """

    # Python-Version anzeigen
    print(f"Python Version: {sys.version}\n")

    # LangChain- und LangGraph-Pakete anzeigen
    print("Installierte LangChain- und LangGraph-Bibliotheken:")
    try:
        result = subprocess.run(["pip", "list"], stdout=subprocess.PIPE, text=True)
        for line in result.stdout.splitlines():
            if line.lower().startswith("langchain") or line.lower().startswith("langgraph"):
                print(line)
    except Exception as e:
        print("Fehler beim Abrufen der Paketliste:", e)

    # Warnungen unterdr√ºcken
    warnings.filterwarnings("ignore")
    warnings.filterwarnings("ignore", category=DeprecationWarning)
    warnings.filterwarnings("ignore", category=UserWarning, module="langsmith.client")
    warnings.simplefilter("ignore", ImportWarning)


def install_packages(packages):
    """
    Installiert eine Liste von Python-Paketen mit 'uv pip install' in einer Google-Colab-Umgebung,
    wenn sie noch nicht importierbar sind.

    Parameter:
    ----------
    packages : list of str or list of tuple
        Eine Liste von Paketnamen oder Tupeln (install_name, import_name).
        Beispiele:
        - ['numpy', 'pandas']
        - [('markitdown[all]', 'markitdown'), 'langchain_chroma']

    Funktionsweise:
    ---------------
    - Trennt zwischen Installationsname (f√ºr pip) und Importname (f√ºr Python).
    - Versucht, jedes angegebene Modul mit 'import' zu laden.
    - Falls der Import fehlschl√§gt: f√ºhrt 'uv pip install --system -q <paketname>' aus.
    - Gibt f√ºr jedes Paket eine Erfolgsmeldung oder eine Fehlermeldung aus.

    Voraussetzungen:
    ----------------
    - Die Funktion ist f√ºr die Ausf√ºhrung in Google Colab gedacht.
    - 'uv' muss bereits installiert sein.
    - Die IPython-Umgebung muss aktiv sein (z. B. in Colab-Notebooks).
    """
    import importlib

    # Zugriff auf das aktuelle IPython-Shell-Objekt
    shell = get_ipython()

    for package in packages:
        # Bestimme Install- und Import-Namen
        if isinstance(package, tuple):
            install_name, import_name = package
        else:
            # Falls nur ein Name gegeben ist, verwende ihn f√ºr beide
            install_name = import_name = package

        try:
            # Versuche, das Modul zu importieren
            # Verwende importlib anstatt exec f√ºr sicheren Import
            importlib.import_module(import_name)
            print(f"‚úÖ {import_name} bereits verf√ºgbar")

        except ImportError:
            try:
                # Falls ImportError: Installiere das Paket √ºber uv
                print(f"üîÑ Installiere {install_name}...")
                shell.run_line_magic("system", f"uv pip install --system -q {install_name}")

                # Versuche erneut zu importieren nach der Installation
                importlib.import_module(import_name)
                print(f"‚úÖ {install_name} erfolgreich installiert und importiert")

            except ImportError as import_error:
                print(f"‚ùå {install_name} installiert, aber Import von {import_name} fehlgeschlagen: {import_error}")
            except Exception as install_error:
                print(f"‚ö†Ô∏è Fehler bei der Installation von {install_name}: {install_error}")


def get_ipinfo():
    """
    Ruft Geoinformationen zur aktuellen √∂ffentlichen IP-Adresse von ipinfo.io ab
    und gibt diese direkt in der Konsole aus.

    Die Ausgabe umfasst:
        - √ñffentliche IP-Adresse
        - Hostname
        - Stadt
        - Region
        - Land (ISO-Code)
        - Koordinaten
        - Internetanbieter (Organisation)
        - Postleitzahl
        - Zeitzone

    Beispiel:
        >>> get_ipinfo()
        IP-Adresse: 8.8.8.8
        Stadt: Mountain View
        ...
    """
    try:
        response = requests.get("https://ipinfo.io")
        data = response.json()

        print("IP-Adresse:", data.get("ip"))
        print("Hostname:", data.get("hostname"))
        print("Stadt:", data.get("city"))
        print("Region:", data.get("region"))
        print("Land:", data.get("country"))
        print("Koordinaten:", data.get("loc"))
        print("Provider:", data.get("org"))
        print("Postleitzahl:", data.get("postal"))
        print("Zeitzone:", data.get("timezone"))

    except requests.RequestException as e:
        print("Fehler beim Abrufen der IP-Informationen:", e)


def setup_api_keys(key_names, create_globals=True):
    """
    Setzt angegebene API-Keys aus Google Colab userdata als Umgebungsvariablen
    und optional als globale Variablen.

    Args:
        key_names (list[str]): Liste der Namen der API-Keys (z.B. ["OPENAI_API_KEY", "HF_TOKEN"]).
        create_globals (bool): Wenn True, werden auch globale Variablen erstellt (Standard: True).

    Hinweis:
        Die API-Keys werden direkt in die Umgebungsvariablen geschrieben,
        aber NICHT zur√ºckgegeben, um unbeabsichtigte Sichtbarkeit zu vermeiden.
        Bei create_globals=True werden zus√§tzlich globale Variablen mit den Key-Namen erstellt.
    """
    from google.colab import userdata
    from os import environ
    import inspect

    # Zugriff auf den globalen Namespace des aufrufenden Moduls
    caller_frame = inspect.currentframe().f_back
    caller_globals = caller_frame.f_globals

    for key in key_names:
        try:
            value = userdata.get(key)
            if value:
                # Umgebungsvariable setzen
                environ[key] = value

                # Optional: Globale Variable im aufrufenden Modul erstellen
                if create_globals:
                    caller_globals[key] = value

                print(f"‚úì {key} erfolgreich gesetzt")
            else:
                print(f"‚ö† {key} nicht in userdata gefunden")

        except Exception as e:
            print(f"‚úó Fehler beim Setzen von {key}: {e}")


def mprint(text):
    """
    Gibt den √ºbergebenen Text als Markdown in Jupyter-Notebooks aus.

    Diese Funktion nutzt IPythons `display()` zusammen mit `Markdown()`,
    um formatierte Markdown-Ausgabe in einem Jupyter-Notebook zu erm√∂glichen.

    Parameter:
    ----------
    text : str
        Der anzuzeigende Markdown-Text.

    Beispiel:
    ---------
    >>> mprint("# √úberschrift\n**fett** und *kursiv*")
    """
    display(Markdown(text))


def mermaid(code: str, width=None, height=None):
    """
    Rendert Mermaid-Diagramme √ºber den kroki.io Service mit anpassbarer Gr√∂√üe.

    Diese Funktion sendet Mermaid-Code an den kroki.io Online-Service
    und zeigt das resultierende SVG-Diagramm direkt im Jupyter-Notebook an.

    Mermaid ist eine JavaScript-basierte Diagramm- und Charting-Tool,
    das aus Text-Definitionen Diagramme generiert (z.B. Flowcharts,
    Sequenzdiagramme, Gantt-Diagramme, etc.).

    Parameter:
    ----------
    code : str
        Mermaid-Code, der das gew√ºnschte Diagramm beschreibt.
    width : int, optional
        Breite des Diagramms in Pixeln (Standard: None = automatische Gr√∂√üe).
    height : int, optional
        H√∂he des Diagramms in Pixeln (Standard: None = automatische Gr√∂√üe).

    Beispiel:
    ---------
    >>> mermaid('''
    ... graph TD
    ...     A[Start] --> B[Process]
    ...     B --> C[End]
    ... ''')

    >>> mermaid('''
    ... sequenceDiagram
    ...     User->>Agent: Frage stellen
    ...     Agent->>LLM: Query senden
    ...     LLM-->>Agent: Antwort
    ...     Agent-->>User: Ergebnis
    ... ''', width=800, height=600)

    Hinweise:
    ---------
    - Ben√∂tigt eine aktive Internetverbindung zu kroki.io
    - Unterst√ºtzt alle Mermaid-Diagrammtypen (graph, sequenceDiagram, gantt, etc.)
    - Timeout ist auf 15 Sekunden gesetzt
    - Width und Height sind optional f√ºr bessere Kontrolle √ºber die Darstellung

    Raises:
    -------
    requests.HTTPError
        Wenn der kroki.io Service nicht erreichbar ist oder ein Fehler auftritt.
    """
    try:
        r = requests.post("https://kroki.io/mermaid/svg", data=code.encode("utf-8"), timeout=15)
        r.raise_for_status()

        # SVG-Code anpassen, wenn width oder height angegeben sind
        svg_content = r.text
        if width is not None or height is not None:
            # Ersetze oder f√ºge width/height Attribute im SVG-Tag hinzu
            import re
            # Suche nach dem √∂ffnenden <svg> Tag
            svg_match = re.search(r'<svg([^>]*)>', svg_content)
            if svg_match:
                attrs = svg_match.group(1)
                # Entferne existierende width/height Attribute
                attrs = re.sub(r'\s*width="[^"]*"', '', attrs)
                attrs = re.sub(r'\s*height="[^"]*"', '', attrs)
                # F√ºge neue Attribute hinzu
                new_attrs = attrs
                if width is not None:
                    new_attrs += f' width="{width}"'
                if height is not None:
                    new_attrs += f' height="{height}"'
                svg_content = svg_content.replace(svg_match.group(0), f'<svg{new_attrs}>')

        display(SVG(svg_content))
    except requests.exceptions.HTTPError as e:
        print(f"Fehler beim Rendern des Mermaid-Diagramms: {e}")
    except Exception as e:
        print(f"Ein unerwarteter Fehler ist aufgetreten: {e}")


# ============================================================================
# MODEL PROFILE UTILITIES
# ============================================================================

def get_model_profile(model: str, temperature: float = 0.0, print_profile: bool = True, **kwargs):
    """
    Ruft Model-Profile von models.dev ab und zeigt die wichtigsten Capabilities.

    Diese Funktion initialisiert ein LLM mit init_chat_model() und gibt dessen
    Profile zur√ºck. Das Profile enth√§lt wichtige Informationen √ºber die F√§higkeiten
    des Modells (Structured Output, Function Calling, Vision, etc.), die von
    https://models.dev abgerufen werden.

    Parameter:
    ----------
    model : str
        Model-Name im Format "provider:model" (z.B. "openai:gpt-4o-mini")
        oder als separater String (dann muss provider √ºber kwargs √ºbergeben werden)
    temperature : float, optional
        Temperatur-Einstellung f√ºr das Modell (Standard: 0.0)
    print_profile : bool, optional
        Wenn True, werden die wichtigsten Profile-Informationen ausgegeben (Standard: True)
    **kwargs : dict
        Zus√§tzliche Parameter f√ºr init_chat_model() (z.B. max_tokens, top_p, etc.)

    Returns:
    --------
    dict
        Das vollst√§ndige Model-Profile mit allen Capabilities

    Beispiel:
    ---------
    >>> from genai_lib.utilities import get_model_profile
    >>>
    >>> # Kurznotation (empfohlen)
    >>> profile = get_model_profile("openai:gpt-4o-mini")
    >>>
    >>> # Mit zus√§tzlichen Parametern
    >>> profile = get_model_profile("anthropic:claude-3-sonnet", temperature=0.3, max_tokens=1000)
    >>>
    >>> # Ohne Ausgabe (nur R√ºckgabe)
    >>> profile = get_model_profile("google:gemini-pro", print_profile=False)
    >>>
    >>> # Zugriff auf spezifische Capabilities
    >>> if profile['image_inputs']:
    >>>     print("Modell unterst√ºtzt Vision!")

    Hinweise:
    ---------
    - Die Model-Profile werden von models.dev abgerufen
    - Nutzt intern init_chat_model() f√ºr konsistente Model-Initialisierung
    - Profile werden automatisch gecacht f√ºr schnellere Zugriffe
    - Nicht alle Models haben alle Profile-Attribute (Fallback auf None/False)

    Profile-Attribute (Auswahl):
    ----------------------------
    **Core Capabilities:**
    - structured_output: Native Structured Output API
    - tool_calling: Function Calling Support
    - supports_json_mode: JSON Mode Support
    - reasoning: Extended Thinking/Reasoning Support

    **Multimodal Input:**
    - text_inputs: Text Input (Standard)
    - image_inputs: Bild Input (Vision)
    - audio_inputs: Audio Input Support
    - video_inputs: Video Input Support

    **Multimodal Output:**
    - text_outputs: Text Output (Standard)
    - image_outputs: Bild-Generierung
    - audio_outputs: Audio-Generierung (TTS)
    - video_outputs: Video-Generierung

    **Token Limits:**
    - max_input_tokens: Context Window Gr√∂√üe
    - max_output_tokens: Max. Output-L√§nge

    **Model Configuration:**
    - temperature: Temperature-Parameter Support
    - knowledge_cutoff: Knowledge Cutoff Date

    **Additional Features:**
    - streaming: Streaming Support
    - async_capable: Async Support
    """
    from langchain.chat_models import init_chat_model

    # Model initialisieren
    try:
        llm = init_chat_model(model, temperature=temperature, **kwargs)
    except Exception as e:
        print(f"‚ùå Fehler beim Initialisieren des Modells: {e}")
        return None

    # Profile abrufen
    try:
        profile = llm.profile
    except AttributeError:
        print(f"‚ö†Ô∏è  Modell hat kein .profile Attribut (m√∂glicherweise veraltete LangChain-Version)")
        return None

    # Profile ausgeben (wenn gew√ºnscht)
    if print_profile:
        print(f"üîç Model Profile: {model}")
        print("=" * 60)

        # Core Capabilities
        print("\nüìã Core Capabilities:")
        print(f"  ‚úì Structured Output:  {profile.get('structured_output', False)}")
        print(f"  ‚úì Function Calling:   {profile.get('tool_calling', False)}")
        print(f"  ‚úì JSON Mode:          {profile.get('supports_json_mode', False)}")
        print(f"  ‚úì Reasoning:          {profile.get('reasoning', False)}")

        # Multimodal Capabilities (vereinfacht mit Symbolen)
        print("\nüé® Multimodal Capabilities:")

        # Input Capabilities
        input_symbols = []
        if profile.get('text_inputs', True):  # Text ist Standard
            input_symbols.append('üìù Text')
        if profile.get('image_inputs', False):
            input_symbols.append('üñºÔ∏è Image')
        if profile.get('audio_inputs', False):
            input_symbols.append('üéµ Audio')
        if profile.get('video_inputs', False):
            input_symbols.append('üé¨ Video')
        print(f"  ‚úì Input:  {', '.join(input_symbols) if input_symbols else 'N/A'}")

        # Output Capabilities
        output_symbols = []
        if profile.get('text_outputs', True):  # Text ist Standard
            output_symbols.append('üìù Text')
        if profile.get('image_outputs', False):
            output_symbols.append('üñºÔ∏è Image')
        if profile.get('audio_outputs', False):
            output_symbols.append('üéµ Audio')
        if profile.get('video_outputs', False):
            output_symbols.append('üé¨ Video')
        print(f"  ‚úì Output: {', '.join(output_symbols) if output_symbols else 'N/A'}")

        # Token Limits
        print("\nüìä Token Limits:")
        max_input = profile.get('max_input_tokens')
        max_output = profile.get('max_output_tokens')
        print(f"  ‚úì Max Input Tokens:   {max_input if max_input else 'N/A'}")
        print(f"  ‚úì Max Output Tokens:  {max_output if max_output else 'N/A'}")

        # Model Configuration
        print("\n‚öôÔ∏è Model Configuration:")
        temperature_support = profile.get('temperature', 'N/A')
        if temperature_support is True:
            temperature_support = 'Yes'
        elif temperature_support is False:
            temperature_support = 'No'
        print(f"  ‚úì Temperature:        {temperature_support}")
        knowledge = profile.get('knowledge_cutoff', 'N/A')
        print(f"  ‚úì Knowledge Cutoff:   {knowledge}")

        # Additional Info
        print("\nüîß Additional Features:")
        print(f"  ‚úì Streaming:          {profile.get('streaming', False)}")
        print(f"  ‚úì Async:              {profile.get('async_capable', False)}")

        print("=" * 60)
        print(f"üìö Vollst√§ndiges Profile: llm.profile (dict mit allen Attributen)")

    return profile


# ============================================================================
# LLM RESPONSE PARSING
# ============================================================================

def extract_thinking(response: Any) -> Tuple[str, str]:
    """
    Universeller Parser f√ºr verschiedene Thinking-Formate von LLMs.

    Diese Funktion extrahiert den "Thinking"-Teil (Denkprozess) und die eigentliche
    Antwort aus verschiedenen LLM-Response-Formaten. Sie unterst√ºtzt mehrere
    Provider und Modelle mit unterschiedlichen Ausgabestrukturen.

    Unterst√ºtzte Formate:
        1. Liste von Bl√∂cken (Claude, Gemini): content = [{"type": "thinking", ...}, {"type": "text", ...}]
        2. String mit <think> Tags (Qwen3, DeepSeek R1): "<think>...</think>Antwort"
        3. DeepSeek reasoning_content Feld: response.additional_kwargs["reasoning_content"]

    Parameter:
    ----------
    response : Any
        Ein LLM-Response-Objekt (z.B. AIMessage) mit einem `content` Attribut.
        Optional kann es auch `additional_kwargs` enthalten.

    Returns:
    --------
    Tuple[str, str]
        Ein Tuple bestehend aus:
        - thinking (str): Der extrahierte Denkprozess (leer, wenn nicht vorhanden)
        - answer (str): Die eigentliche Antwort

    Beispiel:
    ---------
    >>> from langchain.chat_models import init_chat_model
    >>> llm = init_chat_model("anthropic:claude-3-5-sonnet", temperature=0)
    >>> response = llm.invoke("Erkl√§re kurz, was 2+2 ist.")
    >>> thinking, answer = extract_thinking(response)
    >>> print(f"Thinking: {thinking[:100]}...")
    >>> print(f"Answer: {answer}")

    Hinweise:
    ---------
    - Bei Modellen ohne Thinking-Feature wird thinking als leerer String zur√ºckgegeben
    - Die Funktion ist provider-agnostisch und passt sich automatisch an
    - F√ºr Claude muss extended thinking aktiviert sein (Beta-Feature)
    """
    thinking = ""
    answer = ""
    content = response.content

    # Fall 1: Liste von Bl√∂cken (Claude, Gemini)
    if isinstance(content, list):
        for block in content:
            if isinstance(block, dict):
                block_type = block.get("type", "")
                if block_type == "thinking":
                    thinking += block.get("thinking", "")
                elif block_type == "text":
                    answer += block.get("text", "")

    # Fall 2: String mit <think> Tags (Qwen3, DeepSeek R1)
    elif isinstance(content, str):
        think_match = re.search(r"<think>(.*?)</think>", content, re.DOTALL)
        if think_match:
            thinking = think_match.group(1).strip()
            answer = re.sub(r"<think>.*?</think>", "", content, flags=re.DOTALL).strip()
        else:
            answer = content

    # Fall 3: DeepSeek reasoning_content Feld
    if not thinking and hasattr(response, "additional_kwargs"):
        thinking = response.additional_kwargs.get("reasoning_content", "")

    return thinking, answer


# ============================================================================
# PROMPT TEMPLATE LOADER
# ============================================================================

def _convert_github_tree_to_raw(url):
    """
    Wandelt einen GitHub-Tree- oder Blob-Link in einen Raw-Link um.

    Beispiele:
        Input:
            https://github.com/user/repo/tree/main/path/to/file.py
            https://github.com/user/repo/blob/main/path/to/file.py
        Output:
            https://raw.githubusercontent.com/user/repo/main/path/to/file.py

    Wird intern von load_chat_prompt_template verwendet, um GitHub-Links
    automatisch in ladbare Raw-Datei-URLs umzuwandeln.
    """
    if "github.com" in url:
        if "/tree/" in url:
            return url.replace("github.com", "raw.githubusercontent.com").replace("/tree/", "/")
        elif "/blob/" in url:
            return url.replace("github.com", "raw.githubusercontent.com").replace("/blob/", "/")
    return url


def _parse_md_prompt(content):
    """
    Parst eine Markdown-Prompt-Datei und extrahiert die Messages.

    Erwartetes Format:
        ---
        name: template_name
        description: Beschreibung
        variables: [var1, var2]
        ---

        ## system

        System-Prompt-Text mit {var1} Platzhaltern...

        ## human

        Human-Prompt-Text mit {var2} Platzhaltern...

    Args:
        content (str): Inhalt der Markdown-Datei

    Returns:
        list[tuple]: Liste von (role, content) Tuples f√ºr ChatPromptTemplate
    """
    # Frontmatter entfernen (optional)
    body = re.sub(r'^---\s*\n.*?\n---\s*\n', '', content, count=1, flags=re.DOTALL)

    # Messages anhand von ## Headings splitten
    sections = re.split(r'^##\s+', body, flags=re.MULTILINE)

    messages = []
    for section in sections:
        section = section.strip()
        if not section:
            continue

        # Erste Zeile = Rollenname, Rest = Inhalt
        lines = section.split('\n', 1)
        role = lines[0].strip().lower()

        if role not in ('system', 'human', 'ai', 'assistant'):
            continue

        # 'assistant' ‚Üí 'ai' f√ºr LangChain Kompatibilit√§t
        if role == 'assistant':
            role = 'ai'

        msg_content = lines[1].strip() if len(lines) > 1 else ''
        messages.append((role, msg_content))

    if not messages:
        raise KeyError("Markdown prompt file must contain at least one ## system or ## human section.")

    return messages


def load_chat_prompt_template(path):
    """
    L√§dt ein Prompt-Template (.md oder .py) und erzeugt ein ChatPromptTemplate-Objekt.

    Empfohlenes Format: Markdown (.md) mit YAML-Frontmatter und ## Headings.

    Beispiel Markdown-Datei (empfohlen):
        ---
        name: text_zusammenfassung
        description: Erstellt pr√§gnante Textzusammenfassungen
        variables: [text]
        ---

        ## system

        Du bist ein Experte f√ºr Textzusammenfassungen.

        ## human

        Bitte fasse folgenden Text zusammen: {text}

    Python-Dateien (.py) werden weiterhin unterst√ºtzt (Abw√§rtskompatibilit√§t):
        messages = [
            ("system", "System-Prompt..."),
            ("human", "Human-Prompt mit {variable}...")
        ]

    Unterst√ºtzt:
      - lokale Pfade (z. B. '05_prompt/sql_prompt.md')
      - GitHub-Tree-Links (automatische Umwandlung in Raw-Link)
      - GitHub-Blob-Links (automatische Umwandlung in Raw-Link)
      - direkte Raw-Links

    Args:
        path (str): Pfad zur .md- oder .py-Datei (lokal oder URL)

    Returns:
        ChatPromptTemplate: Ein von LangChain nutzbares Prompt-Template-Objekt.

    Raises:
        ValueError: Wenn die Datei weder .md noch .py ist
        KeyError: Wenn die Datei keine g√ºltigen Message-Sections enth√§lt
    """
    # GitHub-Tree-URL automatisch umwandeln
    original_path = path
    path = _convert_github_tree_to_raw(path)

    # Dateiendung bestimmen (vor eventuellem Download)
    is_markdown = original_path.rstrip('/').endswith('.md')
    is_python = original_path.rstrip('/').endswith('.py')

    if not is_markdown and not is_python:
        raise ValueError("Only .md and .py prompt files are supported.")

    # Falls URL ‚Üí herunterladen
    if path.startswith("http"):
        response = requests.get(path)
        response.raise_for_status()
        content = response.text

        if is_markdown:
            messages = _parse_md_prompt(content)
            return ChatPromptTemplate.from_messages(messages)

        # Python-Datei: tempor√§r speichern und laden
        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as tmp:
            tmp.write(response.content)
            path = tmp.name

    # Lokale Markdown-Datei
    if is_markdown:
        with open(path, 'r', encoding='utf-8') as f:
            content = f.read()
        messages = _parse_md_prompt(content)
        return ChatPromptTemplate.from_messages(messages)

    # Lokale Python-Datei (Abw√§rtskompatibilit√§t)
    spec = importlib.util.spec_from_file_location("prompt_module", path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)

    if not hasattr(module, "messages"):
        raise KeyError("Python prompt file must define a variable 'messages'.")

    return ChatPromptTemplate.from_messages(module.messages)


# ============================================================================
# BEISPIEL-VERWENDUNG
# ============================================================================

if __name__ == "__main__":
    # API-Keys setzen (mit globalen Variablen)
    # setup_api_keys([
    #     "OPENAI_API_KEY",
    #     "HF_TOKEN",
    #     "ANTHROPIC_API_KEY"
    # ])

    # Jetzt k√∂nnen die Keys sowohl als Umgebungsvariable als auch als globale Variable verwendet werden:
    # print(OPENAI_API_KEY)  # Globale Variable
    # print(os.environ["OPENAI_API_KEY"])  # Umgebungsvariable

    # Ohne globale Variablen (nur Umgebungsvariablen):
    # setup_api_keys(["ANOTHER_KEY"], create_globals=False)

    print("‚úÖ utilities.py geladen")

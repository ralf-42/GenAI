---
layout: default
title: Migration Mistral Medium 3
parent: Ressourcen
nav_order: 6
description: "Analyse der Migration von gpt-4o-mini zu Mistral Medium 3"
has_toc: true
---

# Migration-Analyse: gpt-4o-mini ‚Üí Mistral Medium 3
{: .no_toc }

> **Technische Analyse der Migration zu Mistral Medium 3**       
     

---

# Inhaltsverzeichnis
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## üìä Executive Summary

**Kernergebnis:** Migration ist technisch einfach (75% der Notebooks kompatibel), aber Audio-Features und h√∂here Kosten erfordern strategische Entscheidungen.

| Metrik | Wert |
|--------|------|
| **Kompatible Notebooks** | 15 von 20 (75%) |
| **Gesch√§tzter Aufwand** | 2-4 Stunden (Basis-Migration) |
| **Kostenfaktor** | 2.7-3.3x h√∂her als gpt-4o-mini |
| **LangChain-Support** | ‚úÖ Vollst√§ndig (`langchain-mistralai`) |
| **Kritische Blocker** | Audio-Notebooks (M15), Fine-tuning (M18) |

{: .important }
> **Empfehlung:** Hybrid-Ansatz (75% Mistral, 25% OpenAI f√ºr Audio/Fine-tuning)

---

## üîç Projekt-√úbersicht

### Analysierte Notebooks

**Gesamt:** 20 Jupyter Notebooks im Verzeichnis `01_notebook/`

**Verwendete Modelle:**
- 17 Notebooks verwenden `gpt-4o-mini` oder `gpt-4o`
- 13 Notebooks nutzen bereits `init_chat_model()` ‚úÖ (gut f√ºr Migration!)
- 3 Notebooks mit Structured Output (`with_structured_output()`)
- 3 Notebooks mit Agents/Tools (`create_agent`, `@tool`)
- Multiple Notebooks mit multimodalen Features (Bild, Audio, Video)

**Notebook-Kategorien:**
1. **Grundlagen:** M00, M01, M02, M03, M04, M05
2. **LangChain/Agents:** M06, M07, M08, M10, M17
3. **Multimodal:** M09 (Bild), M14 (Multimodal RAG), M15 (Audio), M16 (Video)
4. **Spezial:** M11 (Gradio), M12 (Lokale Modelle), M13 (SQL RAG), M18 (Fine-tuning)

---

## ‚öñÔ∏è Feature-Kompatibilit√§tsvergleich

### Vollst√§ndige Feature-Matrix

| Feature | gpt-4o-mini | Mistral Medium 3 | Kompatibilit√§t | Details |
|---------|-------------|------------------|----------------|---------|
| **Text Generation** | ‚úÖ | ‚úÖ | ‚úÖ **100%** | Standard LLM-Funktionalit√§t |
| **Vision/Bild** | ‚úÖ | ‚úÖ | ‚úÖ **100%** | Multimodal unterst√ºtzt (Text + Image) |
| **Function Calling** | ‚úÖ | ‚úÖ | ‚úÖ **100%** | Mit `auto` und `parallel_tool_calls` mode |
| **Structured Output** | ‚úÖ | ‚úÖ | ‚úÖ **100%** | `with_structured_output()` funktioniert |
| **Tool Use/Agents** | ‚úÖ | ‚úÖ | ‚úÖ **100%** | `@tool` decorator, `create_agent()` |
| **LangChain Integration** | ‚úÖ | ‚úÖ | ‚úÖ **100%** | Via `langchain-mistralai` package |
| **Long Context** | 128K tokens | 32K tokens | ‚ö†Ô∏è **25%** | Mistral deutlich weniger |
| **Audio (TTS/STT)** | ‚úÖ | ‚ùå | ‚ùå **0%** | Mistral hat **keine** Audio-API |
| **Fine-tuning** | ‚úÖ (Standard API) | ‚ö†Ô∏è (Enterprise) | ‚ö†Ô∏è **Teilweise** | Nur mit Enterprise-Vertrag |
| **Kosten (Input)** | $0.15/1M | $0.40/1M | üí∞ **2.7x teurer** | Signifikanter Kostenunterschied |
| **Kosten (Output)** | $0.60/1M | $2.00/1M | üí∞ **3.3x teurer** | Besonders bei langen Outputs |

### Mistral Medium 3 Unique Capabilities

{: .highlight }
**Enterprise-Features:**
- Hybrid/On-Premises/In-VPC Deployment
- Continuous Pretraining m√∂glich
- Enterprise Fine-tuning via API
- Unterst√ºtzung f√ºr 80+ Programmiersprachen

{: .highlight }
**Performance:**
- 8x g√ºnstiger als Claude Sonnet 3.7 (laut Hersteller)
- 90%+ Performance vs. Claude Sonnet 3.7
- State-of-the-art Coding & Function-Calling

---

## üìù Notebook-spezifische Kompatibilit√§t

### ‚úÖ Voll kompatibel (Minimal-Aufwand: 1-2h)

| Notebook | Features | Aufwand | Anmerkungen |
|----------|----------|---------|-------------|
| **M01_GenAI_Intro** | Text Generation | ‚≠ê Sehr niedrig | Nur Modell-String √§ndern |
| **M02_Modellsteuerung** | Temperature, Tokens | ‚≠ê Sehr niedrig | Parameter identisch |
| **M04_LangChain101** | Chains, LCEL | ‚≠ê Sehr niedrig | LCEL-Syntax identisch |
| **M06_Chat_Memory** | Memory Patterns | ‚≠ê‚≠ê Niedrig | Kontext-Limit beachten (32K) |
| **M07_OutputParser** | Structured Output | ‚≠ê Sehr niedrig | `with_structured_output()` funktioniert |
| **M08_RAG_LangChain** | RAG, Vectorstores | ‚≠ê‚≠ê Niedrig | Standard LangChain-Integration |
| **M09_Multimodal_Bild** | Vision, Image-to-Text | ‚≠ê‚≠ê Niedrig | Multimodal unterst√ºtzt |
| **M10_Agenten_LangChain** | Agents, Tools | ‚≠ê‚≠ê Niedrig | `create_agent()` funktioniert |
| **M11_Gradio** | UI-Integration | ‚≠ê Sehr niedrig | Framework-agnostisch |
| **M13_SQL_RAG** | SQL, RAG | ‚≠ê‚≠ê Niedrig | Standard Integration |
| **M14_Multimodal_RAG** | Vision + RAG | ‚≠ê‚≠ê Niedrig | Vision + Embeddings |
| **M16_Multimodal_Video** | Video-Frames | ‚≠ê‚≠ê Niedrig | Video ‚Üí Frames ‚Üí Vision |
| **M17_MCP_LangChain_Agent** | MCP, Agents | ‚≠ê‚≠ê Niedrig | LangChain-kompatibel |

**Gesamt:** 13 Notebooks - **65% des Projekts**

### ‚ö†Ô∏è Eingeschr√§nkt kompatibel (Moderat: 1-2 Tage)

| Notebook | Problem | Aufwand | Workaround |
|----------|---------|---------|------------|
| **M12_Lokale_Open_Source** | Modell-Vergleiche | ‚≠ê‚≠ê‚≠ê Mittel | Benchmarks anpassen |
| **M18_Fine_Tuning** | Enterprise API n√∂tig | ‚≠ê‚≠ê‚≠ê‚≠ê Hoch | Enterprise-Vertrag oder OpenAI behalten |

**Gesamt:** 2 Notebooks - **10% des Projekts**

### ‚ùå Nicht kompatibel (Hoch: mehrere Tage)

| Notebook | Problem | Aufwand | Alternative |
|----------|---------|---------|-------------|
| **M15_Multimodal_Audio** | Keine Audio-API | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Sehr hoch | Mistral Voxtral ODER OpenAI behalten |

**Gesamt:** 1 Notebook - **5% des Projekts**

### üîÑ Framework/Tool-Notebooks (keine √Ñnderung)

| Notebook | Grund | Aufwand |
|----------|-------|---------|
| **M00_Kurs_Intro** | Nur Dokumentation | - |
| **M03_Codieren_mit_GenAI** | Editor-unabh√§ngig | - |
| **M05_LLM_Transformer** | Theorie | - |
| **A00_snippets_genai** | Code-Snippets | ‚≠ê Je nach Snippet |

**Gesamt:** 4 Notebooks - **20% des Projekts**

---

## üöÄ Migration-Strategie

### Phase 1: Basis-Migration (2-4 Stunden)

**Ziel:** Alle kompatiblen Notebooks auf Mistral umstellen

**Schritte:**

**1. Installation** (5 Minuten)
```bash
pip install langchain-mistralai
```

**2. API-Key Setup** (5 Minuten)
```bash
# .env oder Environment Variable
export MISTRAL_API_KEY='your-key-here'
```

**3. Code-√Ñnderung** (1-2 Stunden f√ºr 13 Notebooks)

**Vorher:**
```python
from langchain.chat_models import init_chat_model

llm = init_chat_model("openai:gpt-4o-mini", temperature=0.0)
```

**Nachher:**
```python
from langchain.chat_models import init_chat_model

llm = init_chat_model("mistral:mistral-medium-latest", temperature=0.0)
```

**4. Testing** (1-2 Stunden)
- Jeden Notebook durchlaufen
- Outputs vergleichen
- Performance/Kosten messen

{: .note }
**Betroffene Notebooks:** M01, M02, M04, M06, M07, M08, M09, M10, M11, M13, M14, M16, M17

### Phase 2: Audio-Workaround (1-2 Tage)

**Ziel:** M15_Multimodal_Audio.ipynb anpassen

**Option A: Hybrid-Ansatz** (Empfohlen)
```python
# Audio: OpenAI Whisper
from openai import OpenAI
audio_client = OpenAI()

# Text/Logic: Mistral Medium 3
from langchain.chat_models import init_chat_model
llm = init_chat_model("mistral:mistral-medium-latest")

# Workflow: Whisper (Transkription) ‚Üí Mistral (Verarbeitung) ‚Üí TTS (OpenAI)
```

**Option B: Mistral Voxtral** (F√ºr Audio-Fokus)
```python
# Nutze Mistral Voxtral statt Medium 3
llm = init_chat_model("mistral:voxtral-small-latest", temperature=0.0)
```

**Option C: OpenAI behalten** (Einfachste L√∂sung)
```python
# M15 bleibt bei gpt-4o-mini
llm = init_chat_model("openai:gpt-4o-mini", temperature=0.0)
```

### Phase 3: Fine-tuning (Optional, mehrere Tage)

**Ziel:** M18_Fine_Tuning.ipynb mit Mistral Enterprise API

**Voraussetzungen:**
- Enterprise-Vertrag mit Mistral AI
- Kontakt mit Sales-Team
- Budget f√ºr Enterprise-Tier

{: .warning }
**Alternative:** Fine-tuning bei OpenAI behalten (einfacherer Zugang)

---

## üí∞ Kosten-Analyse

### Preisvergleich pro 1M Tokens

| Modell | Input | Output | Gesamt (1M in + 1M out) |
|--------|-------|--------|------------------------|
| **gpt-4o-mini** | $0.15 | $0.60 | $0.75 |
| **Mistral Medium 3** | $0.40 | $2.00 | $2.40 |
| **Differenz** | +$0.25 (167%) | +$1.40 (233%) | +$1.65 (220%) |

### Beispiel-Kalkulation (typischer Kurs-Durchlauf)

**Annahmen:**
- 20 Notebooks
- Durchschnittlich 50 LLM-Calls pro Notebook
- Durchschnittlich 500 Input + 1000 Output Tokens pro Call

**Tokens gesamt:**
- Input: 20 √ó 50 √ó 500 = 500.000 Tokens = 0.5M
- Output: 20 √ó 50 √ó 1000 = 1.000.000 Tokens = 1M

**Kosten-Vergleich:**

| Modell | Berechnung | Kosten |
|--------|-----------|--------|
| **gpt-4o-mini** | (0.5M √ó $0.15) + (1M √ó $0.60) | **$0.675** |
| **Mistral Medium 3** | (0.5M √ó $0.40) + (1M √ó $2.00) | **$2.20** |
| **Differenz** | | **+$1.53** (226% teurer) |

{: .important }
**Bei 100 Kursteilnehmern pro Jahr:**
- gpt-4o-mini: $67.50/Jahr
- Mistral Medium 3: $220/Jahr
- **Mehrkosten: $152.50/Jahr**

### Kosten-Optimierung

**Strategie 1: Selective Migration**
- Vision/RAG: Mistral Medium 3 (bessere Performance)
- Einfache Queries: gpt-4o-mini (g√ºnstiger)
- Audio: OpenAI (zwingend)

**Strategie 2: Caching**
- Mistral unterst√ºtzt Prompt Caching
- Wiederkehrende System-Prompts cachen
- Bis zu 90% Kostenreduktion bei wiederholten Calls

**Strategie 3: Batch Processing**
- Wo m√∂glich: Async Batch API
- G√ºnstigere Preise f√ºr non-realtime

---

## üõ†Ô∏è Implementierungs-Anleitung

### Migration-Helper Template

**Datei:** `migration_helper.py`

```python
"""
Helper-Funktionen f√ºr Mistral-Migration
"""
from langchain.chat_models import init_chat_model
import os

def get_llm(provider="mistral", model_type="medium", temperature=0.0):
    """
    Zentrale LLM-Initialisierung f√ºr einfachen Provider-Wechsel

    Args:
        provider: "mistral" oder "openai"
        model_type: "medium", "small" oder "mini"
        temperature: 0.0 - 1.0
    """
    models = {
        "mistral": {
            "medium": "mistral:mistral-medium-latest",
            "small": "mistral:mistral-small-latest",
        },
        "openai": {
            "mini": "openai:gpt-4o-mini",
            "4o": "openai:gpt-4o",
        }
    }

    model_string = models[provider][model_type]
    return init_chat_model(model_string, temperature=temperature)

# Verwendung in Notebooks:
# from migration_helper import get_llm
# llm = get_llm("mistral", "medium")  # Mistral Medium 3
# llm = get_llm("openai", "mini")     # Fallback zu GPT-4o-mini
```

### Testing-Checklist

F√ºr jedes migrierte Notebook:

- [ ] Notebook l√§uft ohne Fehler durch
- [ ] Output-Qualit√§t vergleichbar mit gpt-4o-mini
- [ ] Keine API-Limit-Fehler
- [ ] Kosten-Tracking aktiviert
- [ ] Performance akzeptabel (Latenz)
- [ ] Multimodal-Features getestet (falls relevant)
- [ ] Structured Output validiert (falls relevant)
- [ ] Agent-Tools funktionieren (falls relevant)

---

## üéØ Empfehlung: Hybrid-Strategie

### Optimale Aufteilung

| Use Case | Modell | Begr√ºndung |
|----------|--------|------------|
| **Vision/Bild-Analyse** | Mistral Medium 3 | Bessere Multimodal-Performance |
| **RAG-Systeme** | Mistral Medium 3 | Gute Balance Performance/Kosten |
| **Agents mit Tools** | Mistral Medium 3 | Exzellentes Function Calling |
| **Einfache Queries** | gpt-4o-mini | Kosteneffizienz |
| **Audio (TTS/STT)** | OpenAI | Mistral hat keine Audio-API |
| **Fine-tuning** | OpenAI | Einfacherer API-Zugang |
| **Long Context (>32K)** | gpt-4o-mini | Mistral nur 32K vs. 128K |

### Implementierung der Hybrid-Strategie

**Zentrale Konfiguration:**

```python
# config.py
MODELS = {
    "vision": "mistral:mistral-medium-latest",      # M09, M14, M16
    "agents": "mistral:mistral-medium-latest",      # M10, M17
    "rag": "mistral:mistral-medium-latest",         # M08, M13, M14
    "structured": "mistral:mistral-medium-latest",  # M07
    "audio": "openai:gpt-4o-mini",                  # M15
    "finetune": "openai:gpt-4o-mini",               # M18
    "simple": "openai:gpt-4o-mini",                 # M01, M02
    "longcontext": "openai:gpt-4o-mini",            # >32K tokens
}

def get_model_for_task(task_type):
    from langchain.chat_models import init_chat_model
    return init_chat_model(MODELS.get(task_type, "openai:gpt-4o-mini"))
```

{: .highlight }
**Vorteile:**
- ‚úÖ Zentrale Kontrolle √ºber Modell-Auswahl
- ‚úÖ Einfacher A/B-Test zwischen Providern
- ‚úÖ Kosten-Optimierung durch task-basierte Auswahl
- ‚úÖ Flexibel f√ºr zuk√ºnftige Modell-Wechsel

---

## üìä Risiko-Analyse

### Technische Risiken

| Risiko | Wahrscheinlichkeit | Impact | Mitigation |
|--------|-------------------|--------|------------|
| **API-√Ñnderungen** | Mittel | Hoch | Versionierung in `requirements.txt` |
| **Performance-Unterschiede** | Mittel | Mittel | Umfangreiches Testing vor Rollout |
| **Kosten-Explosion** | Hoch | Hoch | Monitoring, Budgets, Hybrid-Strategie |
| **Audio-Features brechen** | Hoch | Mittel | Hybrid-Ansatz (OpenAI f√ºr Audio) |
| **Long-Context-Probleme** | Mittel | Niedrig | Fallback zu gpt-4o-mini bei >32K |

### Organisatorische Risiken

| Risiko | Wahrscheinlichkeit | Impact | Mitigation |
|--------|-------------------|--------|------------|
| **Teilnehmer-Verwirrung** | Niedrig | Niedrig | Dokumentation, keine Teilnehmer-sichtbare √Ñnderung |
| **Wartungsaufwand** | Mittel | Mittel | Automatisierung, zentrale Konfiguration |
| **Vendor Lock-in** | Niedrig | Mittel | Abstraktions-Layer (`get_llm()`) |

---

## üìö Ressourcen & Quellen

### Mistral Medium 3 Dokumentation

**Offizielle Ank√ºndigungen:**
- [Mistral Medium 3 Launch](https://mistral.ai/news/mistral-medium-3)
- [GitHub Models - Mistral Medium 3](https://github.blog/changelog/2025-05-14-mistral-medium-3-25-05-is-now-generally-available-in-github-models/)
- [Security Boulevard - Enterprise Features](https://securityboulevard.com/2025/05/mistral-medium-3-new-enterprise-ready-ai-model-now-available/)

**Technische Features:**
- [Function Calling Docs](https://docs.mistral.ai/capabilities/function_calling)
- [Structured Output Docs](https://docs.mistral.ai/capabilities/structured_output)
- [Agents Function Calling Guide](https://docs.mistral.ai/agents/function_calling/)
- [Fine-tuning Guide](https://docs.mistral.ai/capabilities/finetuning)

**LangChain Integration:**
- [LangChain ChatMistralAI](https://python.langchain.com/docs/integrations/chat/mistralai/)
- [langchain-mistralai on PyPI](https://pypi.org/project/langchain-mistralai/)
- [DataCamp Tutorial - Mistral Medium 3](https://www.datacamp.com/tutorial/mistral-medium-3-tutorial)

**Audio (Voxtral):**
- [Mistral Voxtral Announcement](https://mistral.ai/news/voxtral)
- [Audio & Transcription Docs](https://docs.mistral.ai/capabilities/audio_transcription)

**Benchmarks & Vergleiche:**
- [NVIDIA Blog - Mistral 3 Performance](https://developer.nvidia.com/blog/nvidia-accelerated-mistral-3-open-models-deliver-efficiency-accuracy-at-any-scale/)
- [TechCrunch - Mistral 3 Family](https://techcrunch.com/2025/12/02/mistral-closes-in-on-big-ai-rivals-with-mistral-3-open-weight-frontier-and-small-models/)

---

## ‚úÖ N√§chste Schritte

### Sofort (diese Woche)

**1. Entscheidung treffen:**
- [ ] Vollst√§ndige Migration zu Mistral?
- [ ] Hybrid-Ansatz (empfohlen)?
- [ ] Testphase mit 2-3 Notebooks?

**2. Budget kl√§ren:**
- [ ] Kosten-Genehmigung f√ºr 2.7x h√∂here API-Kosten
- [ ] Monitoring-Setup f√ºr Kosten-Tracking

**3. API-Setup:**
- [ ] Mistral API-Key beantragen
- [ ] Test-Account erstellen

### Kurzfristig (n√§chste 2 Wochen)

**4. Pilot-Migration:**
- [ ] M09_Multimodal_Bild.ipynb migrieren (Vision-Test)
- [ ] M10_Agenten_LangChain.ipynb migrieren (Agent-Test)
- [ ] M07_OutputParser.ipynb migrieren (Structured Output-Test)

**5. Evaluation:**
- [ ] Output-Qualit√§t vergleichen
- [ ] Performance messen (Latenz)
- [ ] Kosten tracken (tats√§chlich vs. gesch√§tzt)

### Mittelfristig (n√§chster Monat)

**6. Rollout:**
- [ ] Alle kompatiblen Notebooks migrieren (13 St√ºck)
- [ ] Dokumentation aktualisieren
- [ ] `migration_helper.py` implementieren

**7. Spezialf√§lle:**
- [ ] Audio-Workaround f√ºr M15 implementieren
- [ ] Fine-tuning-Strategie f√ºr M18 kl√§ren

---

## üìÑ Anhang

### A. Vollst√§ndige Notebook-Liste mit Status

| # | Notebook | Status | Aufwand | Priorit√§t |
|---|----------|--------|---------|-----------|
| 1 | M00_Kurs_Intro | üîµ Keine √Ñnderung | - | - |
| 2 | M01_GenAI_Intro | ‚úÖ Kompatibel | ‚≠ê | Niedrig |
| 3 | M02_Modellsteuerung | ‚úÖ Kompatibel | ‚≠ê | Niedrig |
| 4 | M03_Codieren_mit_GenAI | üîµ Framework | - | - |
| 5 | M04_LangChain101 | ‚úÖ Kompatibel | ‚≠ê | Hoch |
| 6 | M05_LLM_Transformer | üîµ Theorie | - | - |
| 7 | M06_Chat_Memory | ‚úÖ Kompatibel | ‚≠ê‚≠ê | Mittel |
| 8 | M07_OutputParser | ‚úÖ Kompatibel | ‚≠ê | Hoch |
| 9 | M08_RAG_LangChain | ‚úÖ Kompatibel | ‚≠ê‚≠ê | Hoch |
| 10 | M09_Multimodal_Bild | ‚úÖ Kompatibel | ‚≠ê‚≠ê | **Sehr hoch** |
| 11 | M10_Agenten_LangChain | ‚úÖ Kompatibel | ‚≠ê‚≠ê | **Sehr hoch** |
| 12 | M11_Gradio | ‚úÖ Kompatibel | ‚≠ê | Niedrig |
| 13 | M12_Lokale_Open_Source | ‚ö†Ô∏è Eingeschr√§nkt | ‚≠ê‚≠ê‚≠ê | Niedrig |
| 14 | M13_SQL_RAG | ‚úÖ Kompatibel | ‚≠ê‚≠ê | Mittel |
| 15 | M14_Multimodal_RAG | ‚úÖ Kompatibel | ‚≠ê‚≠ê | Hoch |
| 16 | M15_Multimodal_Audio | ‚ùå Inkompatibel | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Niedrig |
| 17 | M16_Multimodal_Video | ‚úÖ Kompatibel | ‚≠ê‚≠ê | Mittel |
| 18 | M17_MCP_LangChain_Agent | ‚úÖ Kompatibel | ‚≠ê‚≠ê | Hoch |
| 19 | M18_Fine_Tuning | ‚ö†Ô∏è Eingeschr√§nkt | ‚≠ê‚≠ê‚≠ê‚≠ê | Niedrig |
| 20 | A00_snippets_genai | üîµ Snippets | ‚≠ê | Variabel |

**Legende:**
- ‚úÖ Voll kompatibel
- ‚ö†Ô∏è Eingeschr√§nkt kompatibel
- ‚ùå Inkompatibel
- üîµ Keine Code-√Ñnderung n√∂tig

### B. Code-Snippets Quick Reference

**Basis-Migration:**
```python
# Vorher
llm = init_chat_model("openai:gpt-4o-mini", temperature=0.0)

# Nachher
llm = init_chat_model("mistral:mistral-medium-latest", temperature=0.0)
```

**Vision:**
```python
response = llm.invoke([HumanMessage(content=[
    {"type": "text", "text": "Was ist auf dem Bild?"},
    {"type": "image_url", "image_url": {"url": image_url}}
])])
```

**Structured Output:**
```python
structured_llm = llm.with_structured_output(PersonSchema)
result = structured_llm.invoke("Max ist 25 Jahre alt")
```

**Agents:**
```python
agent = create_agent(
    model="mistral:mistral-medium-latest",
    tools=[search, calculator],
    system_prompt="Du bist ein hilfreicher Agent"
)
```


---

**Version:** 1.0       
**Stand:** Dezember 2025      
**Kurs:** Generative KI. Verstehen. Anwenden. Gestalten.     
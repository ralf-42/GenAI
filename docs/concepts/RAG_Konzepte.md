---
layout: default
title: RAG-Konzepte
parent: Konzepte
nav_order: 1
description: "Retrieval Augmented Generation im Detail ‚Äì Architektur, Strategien und Best Practices"
has_toc: true
---

# RAG-Konzepte
{: .no_toc }

> **Retrieval Augmented Generation im Detail ‚Äì Architektur, Strategien und Best Practices**

---

# Inhaltsverzeichnis
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## 1 √úberblick: Was ist RAG?

Large Language Models besitzen beeindruckende F√§higkeiten, sto√üen jedoch an klare Grenzen:

| Limitation | Beschreibung |
|------------|--------------|
| **Wissens-Cutoff** | Das Modell kennt nur Informationen bis zum Trainingszeitpunkt |
| **Kein Dom√§nenwissen** | Firmeninterne Dokumente, Fachrichtlinien oder aktuelle Daten fehlen |
| **Halluzination** | Bei Wissensl√ºcken werden plausible, aber falsche Antworten generiert |
| **Kontextlimit** | Nicht alle relevanten Dokumente passen in einen einzelnen Prompt |

**Retrieval Augmented Generation (RAG)** l√∂st diese Probleme durch einen eleganten Ansatz: Statt das LLM mit mehr Daten zu trainieren, werden relevante Informationen zur Laufzeit abgerufen und dem Prompt hinzugef√ºgt.

```
Frage ‚Üí Suche relevante Dokumente ‚Üí F√ºge Kontext zum Prompt ‚Üí LLM generiert Antwort
```

**Kernidee:** Das LLM erh√§lt genau die Informationen, die es f√ºr die aktuelle Frage ben√∂tigt ‚Äì nicht mehr und nicht weniger.

---

## 2 Die RAG-Architektur

Ein RAG-System besteht aus zwei Hauptphasen: **Indexierung** (einmalig) und **Retrieval + Generation** (bei jeder Anfrage).

### 2.1 Indexierungsphase

```mermaid
flowchart LR
    A[Dokumente] --> B[Laden]
    B --> C[Chunking]
    C --> D[Embedding]
    D --> E[Vektordatenbank]
```

| Schritt | Beschreibung | Typische Tools |
|---------|--------------|----------------|
| **Laden** | Dokumente aus verschiedenen Quellen einlesen | TextLoader, PyPDFLoader, WebBaseLoader |
| **Chunking** | Gro√üe Dokumente in kleinere Teile zerlegen | RecursiveCharacterTextSplitter |
| **Embedding** | Textchunks in Vektoren umwandeln | OpenAIEmbeddings, HuggingFaceEmbeddings |
| **Speichern** | Vektoren in Datenbank ablegen | ChromaDB, FAISS, Pinecone |

### 2.2 Abfragephase

```mermaid
flowchart LR
    A[Frage] --> B[Embedding]
    B --> C[Similarity Search]
    C --> D[Top-k Dokumente]
    D --> E[Prompt + Kontext]
    E --> F[LLM]
    F --> G[Antwort]
```

| Schritt | Beschreibung |
|---------|--------------|
| **Query-Embedding** | Die Frage wird in denselben Vektorraum transformiert |
| **Similarity Search** | Die √§hnlichsten Dokumentvektoren werden gefunden |
| **Kontext-Erstellung** | Gefundene Chunks werden zum Prompt hinzugef√ºgt |
| **Generation** | Das LLM generiert eine Antwort basierend auf dem Kontext |

---

## 3 Chunking: Dokumente sinnvoll zerlegen

Chunking ist eine der kritischsten Entscheidungen in einem RAG-System. Zu gro√üe Chunks verschwenden Kontext, zu kleine Chunks verlieren Zusammenhang.

### 3.1 Chunking-Strategien

| Strategie | Beschreibung | Anwendungsfall |
|-----------|--------------|----------------|
| **Fixed-Size** | Feste Zeichenanzahl pro Chunk | Einfache Texte ohne Struktur |
| **Recursive** | Hierarchische Trennung (Absatz ‚Üí Satz ‚Üí Wort) | Allgemeine Dokumente |
| **Semantic** | Trennung nach Bedeutungseinheiten | Komplexe Fachtexte |
| **Document-based** | Beibehaltung nat√ºrlicher Grenzen (Kapitel, Abschnitte) | Strukturierte Dokumente |

### 3.2 Der RecursiveCharacterTextSplitter

Der am h√§ufigsten verwendete Splitter arbeitet mit einer Hierarchie von Trennzeichen:

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,      # Maximale Chunk-Gr√∂√üe in Zeichen
    chunk_overlap=100,   # √úberlappung zwischen Chunks
    separators=["\n\n", "\n", ". ", " ", ""]  # Trennzeichen-Hierarchie
)
```

**Funktionsweise:**
1. Versuche zuerst, an Doppel-Zeilenumbr√ºchen zu trennen (Abs√§tze)
2. Falls Chunk zu gro√ü: Trenne an einfachen Zeilenumbr√ºchen
3. Falls immer noch zu gro√ü: Trenne an Satzenden
4. Letzte Option: Trenne an Leerzeichen oder einzelnen Zeichen

### 3.3 Overlap: Kontext bewahren

```
Dokument: [AAAA|BBBB|CCCC|DDDD]

Ohne Overlap:
  Chunk 1: [AAAA]
  Chunk 2: [BBBB]
  ‚Üí Information an Grenzen geht verloren

Mit Overlap (25%):
  Chunk 1: [AAAA|BB]
  Chunk 2: [BB|CCCC]
  ‚Üí Zusammenh√§nge bleiben erhalten
```

### 3.4 Empfehlungen nach Dokumenttyp

| Dokumenttyp | chunk_size | chunk_overlap | Begr√ºndung |
|-------------|------------|---------------|------------|
| FAQ / Kurztexte | 200‚Äì300 | 50 | Pr√§zise, eigenst√§ndige Antworten |
| Handb√ºcher | 500‚Äì800 | 100‚Äì150 | Kontext zwischen Abschnitten erhalten |
| Rechtsdokumente | 800‚Äì1000 | 200 | Vollst√§ndige Paragraphen wichtig |
| Code-Dokumentation | 300‚Äì500 | 100 | Funktionen zusammenhalten |

---

## 4 Embeddings: Text als Vektor

Embeddings sind das Herzst√ºck der semantischen Suche. Sie transformieren Text in numerische Vektoren, wobei √§hnliche Bedeutungen zu √§hnlichen Vektoren f√ºhren.

### 4.1 Konzept

```
"Der Hund spielt im Park"     ‚Üí [0.12, -0.45, 0.78, ..., 0.33]  (1536 Dim.)
"Die Katze liegt im Garten"   ‚Üí [0.15, -0.42, 0.71, ..., 0.29]  (√§hnlich!)
"Quantenmechanik ist komplex" ‚Üí [-0.89, 0.23, -0.11, ..., 0.67] (anders!)
```

### 4.2 Verf√ºgbare Embedding-Modelle

| Modell | Dimensionen | Kosten | Qualit√§t |
|--------|-------------|--------|----------|
| `text-embedding-3-small` (OpenAI) | 1536 | ~$0.02/1M Tokens | ‚≠ê‚≠ê‚≠ê‚≠ê |
| `text-embedding-3-large` (OpenAI) | 3072 | ~$0.13/1M Tokens | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| `all-MiniLM-L6-v2` (HuggingFace) | 384 | Kostenlos | ‚≠ê‚≠ê‚≠ê |
| `multilingual-e5-large` (HuggingFace) | 1024 | Kostenlos | ‚≠ê‚≠ê‚≠ê‚≠ê |

### 4.3 √Ñhnlichkeitsma√üe

Die √Ñhnlichkeit zwischen Vektoren wird mathematisch berechnet:

| Ma√ü | Beschreibung | Wertebereich |
|-----|--------------|--------------|
| **Cosine Similarity** | Winkel zwischen Vektoren | -1 bis 1 |
| **Euclidean Distance** | Geometrischer Abstand | 0 bis ‚àû |
| **Dot Product** | Skalarprodukt | -‚àû bis ‚àû |

**Cosine Similarity** ist der Standard, da sie unabh√§ngig von der Vektorl√§nge nur die "Richtung" (= Bedeutung) vergleicht.

### 4.4 Beispiel: Embeddings erzeugen

```python
from langchain_openai import OpenAIEmbeddings

embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")

# Einzelner Text (f√ºr Queries)
query_vector = embedding_model.embed_query("Was ist ein KI-Agent?")

# Mehrere Texte (f√ºr Dokumente)
doc_vectors = embedding_model.embed_documents([
    "Text 1", "Text 2", "Text 3"
])
```

---

## 5 Retrieval: Die richtigen Dokumente finden

Der Retriever ist die Br√ºcke zwischen Frage und relevantem Wissen. Verschiedene Strategien optimieren die Trefferqualit√§t.

### 5.1 Basis-Retrieval: Similarity Search

```python
from langchain_community.vectorstores import Chroma

vectorstore = Chroma.from_documents(chunks, embedding_model)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# Suche
docs = retriever.invoke("Wie funktioniert RAG?")
```

### 5.2 Retrieval-Strategien im Vergleich

| Strategie | Beschreibung | Vorteil | Nachteil |
|-----------|--------------|---------|----------|
| **Similarity** | √Ñhnlichste Vektoren | Schnell, einfach | Keine Qualit√§tsgarantie |
| **MMR** | Maximum Marginal Relevance | Diversit√§t der Ergebnisse | Etwas langsamer |
| **Threshold** | Nur Ergebnisse √ºber Schwellenwert | Qualit√§tskontrolle | Kann leer zur√ºckkommen |
| **Hybrid** | Keyword + Semantisch kombiniert | Beste Abdeckung | Komplexer aufzusetzen |

### 5.3 Maximum Marginal Relevance (MMR)

MMR balanciert Relevanz und Diversit√§t. Statt nur die √§hnlichsten Dokumente zur√ºckzugeben, werden auch unterschiedliche Perspektiven ber√ºcksichtigt.

```python
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 5,           # Finale Anzahl
        "fetch_k": 20,    # Kandidaten f√ºr MMR
        "lambda_mult": 0.7  # Balance (1.0 = nur Relevanz, 0.0 = nur Diversit√§t)
    }
)
```

### 5.4 Score-basiertes Filtering

```python
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "score_threshold": 0.5,  # Mindest-√Ñhnlichkeit
        "k": 10
    }
)
```

### 5.5 Metadaten-Filter

```python
retriever = vectorstore.as_retriever(
    search_kwargs={
        "k": 5,
        "filter": {"source": "handbuch.pdf", "kapitel": "Sicherheit"}
    }
)
```

---

## 6 Reranking: Ergebnisse optimieren

Reranking verbessert die Qualit√§t der gefundenen Dokumente durch einen zweiten Bewertungsschritt.

### 6.1 Warum Reranking?

Die initiale Vektorsuche ist schnell, aber nicht perfekt. Reranking verwendet ein pr√§ziseres (aber langsameres) Modell, um die Top-Ergebnisse neu zu ordnen.

```
Schritt 1: Similarity Search ‚Üí 20 Kandidaten
Schritt 2: Reranker bewertet alle 20 ‚Üí Sortiert nach Qualit√§t
Schritt 3: Top 5 werden verwendet
```

### 6.2 Reranking-Ans√§tze

| Ansatz | Beschreibung | Performance |
|--------|--------------|-------------|
| **Cross-Encoder** | Betrachtet Query + Dokument gemeinsam | H√∂chste Qualit√§t, langsam |
| **LLM-based** | LLM bewertet Relevanz | Flexibel, teuer |
| **Lightweight** | Schnelle Heuristiken | Schnell, moderate Qualit√§t |

### 6.3 Beispiel: Cohere Reranker

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain_cohere import CohereRerank

base_retriever = vectorstore.as_retriever(search_kwargs={"k": 20})

reranker = CohereRerank(model="rerank-english-v3.0", top_n=5)

compression_retriever = ContextualCompressionRetriever(
    base_compressor=reranker,
    base_retriever=base_retriever
)

docs = compression_retriever.invoke("Meine Frage")
```

---

## 7 Advanced RAG-Techniken

√úber die Grundlagen hinaus existieren fortgeschrittene Techniken zur Qualit√§tsverbesserung.

### 7.1 Query Transformation

Die urspr√ºngliche Frage wird umformuliert oder erweitert, um bessere Treffer zu erzielen.

**Multi-Query:** Eine Frage wird in mehrere Varianten umgewandelt:

```
Original: "Wie funktioniert RAG?"
‚Üí "Was ist Retrieval Augmented Generation?"
‚Üí "Erkl√§re die RAG-Architektur"
‚Üí "RAG-System Komponenten"
```

**HyDE (Hypothetical Document Embedding):** Das LLM generiert eine hypothetische Antwort, die dann f√ºr die Suche verwendet wird:

```
Frage: "Wie funktioniert RAG?"
‚Üí LLM generiert: "RAG kombiniert Retrieval und Generation..."
‚Üí Suche nach Dokumenten √§hnlich zur hypothetischen Antwort
```

### 7.2 Self-Query

Das LLM extrahiert strukturierte Filter aus nat√ºrlichsprachlichen Fragen:

```
Frage: "Zeige mir Sicherheitsrichtlinien aus 2024"
‚Üí Extrahiert: {"kategorie": "sicherheit", "jahr": 2024}
‚Üí Kombiniert semantische Suche mit Metadaten-Filter
```

### 7.3 Contextual Compression

Gefundene Dokumente werden auf das Wesentliche komprimiert:

```
Gefundener Chunk (500 Zeichen):
"Die Firma wurde 1995 gegr√ºndet. Der Hauptsitz befindet sich in Berlin.
 Die Sicherheitsrichtlinien wurden 2023 aktualisiert und umfassen..."

Nach Compression (relevanter Teil f√ºr Frage "Sicherheitsrichtlinien"):
"Die Sicherheitsrichtlinien wurden 2023 aktualisiert und umfassen..."
```

### 7.4 Parent Document Retriever

Kleine Chunks f√ºr pr√§zises Retrieval, aber gr√∂√üere Kontextfenster f√ºr die Generierung:

```
Indexierung: Kleine Chunks (200 Zeichen) ‚Üí Vektordatenbank
Retrieval: Finde relevante kleine Chunks
R√ºckgabe: Hole zugeh√∂rige Parent-Dokumente (2000 Zeichen)
```

---

## 8 RAG-Chain mit LangChain

Die Kombination aller Komponenten zu einer funktionierenden Pipeline.

### 8.1 Minimales Beispiel

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.chat_models import init_chat_model
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

# Komponenten
llm = init_chat_model("gpt-4o-mini", model_provider="openai", temperature=0.0)
embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")
vectorstore = Chroma.from_documents(chunks, embedding_model)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# Hilfsfunktion
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# RAG-Prompt
rag_prompt = ChatPromptTemplate.from_template(
    """Beantworte die Frage basierend auf dem folgenden Kontext.
Wenn die Antwort nicht im Kontext steht, sage ehrlich, dass keine Information vorliegt.

Kontext:
{context}

Frage: {question}

Antwort:"""
)

# LCEL Chain
rag_chain = (
    {
        "context": retriever | format_docs,
        "question": RunnablePassthrough()
    }
    | rag_prompt
    | llm
    | StrOutputParser()
)

# Aufruf
antwort = rag_chain.invoke("Wie funktioniert das System?")
```

### 8.2 RAG als Agent-Tool

```python
from langchain_core.tools import tool

@tool
def firmenwissen_suchen(frage: str) -> str:
    """üîç FIRMENWISSEN ‚Äì Durchsucht interne Dokumente.
    
    Verwenden f√ºr Fragen zu:
    - Unternehmensrichtlinien
    - Internen Prozessen
    - Produktinformationen
    
    Args:
        frage: Die Suchanfrage in nat√ºrlicher Sprache
    
    Returns:
        Relevante Informationen aus den Firmendokumenten
    """
    try:
        return rag_chain.invoke(frage)
    except Exception as e:
        return f"Fehler bei der Suche: {str(e)}"
```

---

## 9 Evaluierung von RAG-Systemen

Die Qualit√§t eines RAG-Systems muss systematisch gemessen werden.

### 9.1 Metriken

| Metrik | Misst | Berechnung |
|--------|-------|------------|
| **Retrieval Precision** | Anteil relevanter Dokumente | Relevante / Gefundene |
| **Retrieval Recall** | Abdeckung aller relevanten Dokumente | Gefundene Relevante / Alle Relevanten |
| **Answer Relevance** | Passt Antwort zur Frage? | LLM-Bewertung |
| **Faithfulness** | Ist Antwort durch Kontext gest√ºtzt? | LLM-Bewertung |
| **Context Relevance** | Ist der Kontext relevant? | LLM-Bewertung |

### 9.2 RAGAS Framework

RAGAS (Retrieval Augmented Generation Assessment) bietet standardisierte Metriken:

```python
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_precision

results = evaluate(
    dataset,
    metrics=[faithfulness, answer_relevancy, context_precision]
)
```

### 9.3 Manuelles Testen

F√ºr erste Iterationen ist manuelles Testen effektiv:

```python
test_questions = [
    "Was ist die Passwort-Policy?",
    "Wie beantrage ich Urlaub?",
    "Wer ist Ansprechpartner f√ºr IT-Probleme?"
]

for question in test_questions:
    # Retrieval pr√ºfen
    docs = retriever.invoke(question)
    print(f"\nFrage: {question}")
    print(f"Gefundene Dokumente: {len(docs)}")
    for i, doc in enumerate(docs):
        print(f"  {i+1}. {doc.page_content[:100]}...")
    
    # Antwort pr√ºfen
    answer = rag_chain.invoke(question)
    print(f"Antwort: {answer}")
```

---

## 10 Troubleshooting

H√§ufige Probleme und deren L√∂sungen.

### 10.1 Problem: Keine relevanten Dokumente gefunden

| Ursache | Diagnose | L√∂sung |
|---------|----------|--------|
| Collection leer | `vectorstore._collection.count()` | Dokumente indexieren |
| Falsches Embedding-Modell | Dimensionen vergleichen | Gleiches Modell f√ºr Index und Query |
| Query zu spezifisch | Mit breiterem Begriff testen | Query umformulieren |
| k zu klein | k erh√∂hen | `search_kwargs={"k": 10}` |

### 10.2 Problem: Falsche Antworten trotz korrektem Kontext

| Ursache | L√∂sung |
|---------|--------|
| Prompt unklar | Anweisungen pr√§zisieren |
| Zu viel Kontext | Weniger Chunks, Compression nutzen |
| Widerspr√ºchliche Dokumente | Metadaten-Filter f√ºr Aktualit√§t |
| Halluzination | Explizite Anweisung: "Nur basierend auf Kontext" |

### 10.3 Problem: Langsame Antwortzeiten

| Komponente | Optimierung |
|------------|-------------|
| Embedding | Batch-Verarbeitung, Caching |
| Retrieval | Index optimieren, k reduzieren |
| Reranking | Weniger Kandidaten, leichteres Modell |
| LLM | Streaming aktivieren, schnelleres Modell |

---

## 11 Best Practices

### 11.1 Indexierung

- **Konsistentes Embedding-Modell:** Dasselbe Modell f√ºr Indexierung und Queries verwenden
- **Sinnvolles Chunking:** Dokumenttyp-spezifische Parameter w√§hlen
- **Metadaten anreichern:** Quelle, Datum, Kategorie f√ºr sp√§teres Filtern
- **Inkrementelle Updates:** Nur ge√§nderte Dokumente neu indexieren

### 11.2 Retrieval

- **k sinnvoll w√§hlen:** Zu wenig = fehlender Kontext, zu viel = Rauschen
- **MMR f√ºr Diversit√§t:** Bei breiten Themen verschiedene Perspektiven einbeziehen
- **Threshold f√ºr Qualit√§t:** Lieber keine Antwort als eine falsche

### 11.3 Prompt Design

- **Klare Anweisungen:** "Antworte NUR basierend auf dem Kontext"
- **Fallback definieren:** Was tun bei fehlendem Wissen?
- **Quellenangaben:** Antwort mit Dokumentreferenzen anreichern

### 11.4 Evaluation

- **Test-Dataset erstellen:** Repr√§sentative Fragen mit erwarteten Antworten
- **Regelm√§√üig evaluieren:** Nach jedem Update der Wissensbasis
- **Feedback sammeln:** Nutzer-Bewertungen f√ºr kontinuierliche Verbesserung

---

## 12 Zusammenfassung

RAG kombiniert die St√§rken von Retrieval-Systemen mit generativen LLMs:

| Komponente | Funktion | Typisches Tool |
|------------|----------|----------------|
| **Document Loader** | Daten einlesen | TextLoader, PyPDFLoader |
| **Text Splitter** | Chunking | RecursiveCharacterTextSplitter |
| **Embedding Model** | Text ‚Üí Vektor | OpenAIEmbeddings |
| **Vector Store** | Speicherung & Suche | ChromaDB, FAISS |
| **Retriever** | Relevante Chunks finden | as_retriever() |
| **LLM** | Antwort generieren | GPT-4o-mini |

**Der typische Workflow:**

```
1. Dokumente laden und chunken
       ‚Üì
2. Embeddings erzeugen und speichern
       ‚Üì
3. Retriever konfigurieren
       ‚Üì
4. RAG-Prompt erstellen
       ‚Üì
5. LCEL-Chain bauen
       ‚Üì
6. Evaluieren und optimieren
```

RAG erm√∂glicht es, LLMs mit aktuellem, dom√§nenspezifischem Wissen auszustatten ‚Äì ohne teures Fine-Tuning und mit voller Kontrolle √ºber die Wissensbasis.

---

**Version:** 1.0  
**Stand:** November 2025  
**Kurs:** Generative KI. Verstehen. Anwenden. Gestalten.

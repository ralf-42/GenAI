---
layout: default
title: ChromaDB Einsteiger
parent: Frameworks
nav_order: 2
description: "Vektordatenbanken und ChromaDB f√ºr RAG-Systeme"
has_toc: true
---

# ChromaDB Einsteiger
{: .no_toc }

> **Vektordatenbanken und ChromaDB f√ºr RAG-Systeme**

---

# Inhaltsverzeichnis
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## 1 Kurz√ºberblick: Warum ChromaDB Vektordatenbanken?

Large Language Models haben trotz ihrer beeindruckenden F√§higkeiten klare Limitierungen:

- **Wissens-Cutoff:** Das Modell kennt nur Informationen bis zum Trainingszeitpunkt
- **Kein Firmenwissen:** Interne Dokumente, Richtlinien oder aktuelle Daten sind unbekannt
- **Halluzination:** Bei fehlendem Wissen werden plausibel klingende, aber falsche Antworten generiert
- **Kontextlimit:** Nicht alle relevanten Dokumente passen in den Prompt

Vektordatenbanken l√∂sen diese Probleme durch **semantische Suche**:

| Aspekt | Keyword-Suche | Semantische Suche |
|--------|---------------|-------------------|
| **Funktionsweise** | Exakte Wort√ºbereinstimmung | Bedeutungs√§hnlichkeit |
| **Beispiel** | "Auto" findet nicht "Fahrzeug" | "Auto" findet auch "Fahrzeug", "PKW", "Wagen" |
| **Synonyme** | ‚ùå Nicht erkannt | ‚úÖ Erkannt |
| **Tippfehler** | ‚ùå Keine Treffer | ‚úÖ Oft trotzdem Treffer |
| **Kontext** | ‚ùå Ignoriert | ‚úÖ Ber√ºcksichtigt |

**Kernidee:** Texte werden in Vektoren (Listen von Zahlen) umgewandelt. √Ñhnliche Bedeutungen ergeben √§hnliche Vektoren. Die Suche findet die "n√§chsten Nachbarn" im Vektorraum.

---

## 2 Was sind Embeddings?

Embeddings sind numerische Repr√§sentationen von Text, die semantische Bedeutung erfassen.

### 2.1 Konzept: Text ‚Üí Vektor

```
"Der Hund spielt im Park"  ‚Üí  [0.12, -0.45, 0.78, ..., 0.33]  (1536 Dimensionen)
"Die Katze liegt im Garten" ‚Üí  [0.15, -0.42, 0.71, ..., 0.29]  (√§hnlich!)
"Quantenmechanik ist komplex" ‚Üí [-0.89, 0.23, -0.11, ..., 0.67]  (anders!)
```

**Wichtige Eigenschaften:**

- Jeder Text wird zu einem Vektor fester L√§nge (z.B. 1536 Dimensionen bei OpenAI)
- Semantisch √§hnliche Texte haben √§hnliche Vektoren
- Die "√Ñhnlichkeit" wird √ºber mathematische Distanzma√üe berechnet

### 2.2 √Ñhnlichkeit im Vektorraum

Die gebr√§uchlichsten Distanzma√üe:

| Ma√ü | Beschreibung | Wertebereich | ChromaDB Default |
|-----|--------------|--------------|------------------|
| **Cosine Similarity** | Winkel zwischen Vektoren | -1 bis 1 | ‚úÖ Ja |
| **Euclidean Distance** | Geometrischer Abstand | 0 bis ‚àû | Nein |
| **Dot Product** | Skalarprodukt | -‚àû bis ‚àû | Nein |

**Cosine Similarity** ist der Standard, da sie unabh√§ngig von der Vektorl√§nge funktioniert und nur die "Richtung" (= Bedeutung) vergleicht.

### Vektorraum-Visualisierung (konzeptionell)

```mermaid
graph TB
    subgraph "Semantischer Vektorraum (vereinfacht 2D)"
        A["'Hund'<br/>[0.8, 0.3]"]
        B["'Katze'<br/>[0.75, 0.35]"]
        C["'Tier'<br/>[0.7, 0.4]"]

        D["'Auto'<br/>[0.2, 0.9]"]
        E["'Fahrzeug'<br/>[0.25, 0.85]"]

        F["'Quantenmechanik'<br/>[-0.5, -0.8]"]
    end

    A -.√§hnlich.- B
    B -.√§hnlich.- C
    A -.√§hnlich.- C

    D -.√§hnlich.- E

    style A fill:#e1f5ff
    style B fill:#e1f5ff
    style C fill:#e1f5ff
    style D fill:#ffe6cc
    style E fill:#ffe6cc
    style F fill:#f8cecc
```

> **Hinweis:** Semantisch verwandte Konzepte ("Hund", "Katze", "Tier") bilden Cluster im Vektorraum, w√§hrend unverwandte Konzepte ("Quantenmechanik") weiter entfernt liegen.

### 2.3 Visualisierung

F√ºr ein intuitives Verst√§ndnis von Embeddings:

**Embedding Projector (Google):**  
https://projector.tensorflow.org/?hl=de

Hier lassen sich hochdimensionale Vektoren auf 2D/3D projizieren und interaktiv erkunden. Semantisch √§hnliche W√∂rter bilden Cluster.

### 2.4 Beispiel: Embedding erzeugen

```python
from langchain_openai import OpenAIEmbeddings

# Embedding-Modell initialisieren
embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")

# Einzelnen Text embedden
text = "KI-Agenten k√∂nnen autonom Aufgaben erledigen."
vector = embedding_model.embed_query(text)

print(f"Dimensionen: {len(vector)}")  # 1536
print(f"Erste 5 Werte: {vector[:5]}")
```

---

## 3 ChromaDB Basics

ChromaDB ist eine leichtgewichtige, Open-Source-Vektordatenbank, die sich ideal f√ºr Entwicklung und Prototyping eignet.

### 3.1 Installation

**Standard-Installation:**

```python
!pip install chromadb
```

**Google Colab ‚Äì SQLite-Patch (WICHTIG!):**

In Colab kann es zu SQLite-Versionskonflikten kommen. Folgender Patch l√∂st das Problem:

```python
# NUR in Google Colab erforderlich!
!pip install pysqlite3-binary

import sys
__import__('pysqlite3')
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
```

Dieser Patch muss **vor** dem Import von ChromaDB ausgef√ºhrt werden.

### 3.2 Client erstellen

```python
import chromadb

# In-Memory Client (Daten gehen bei Neustart verloren)
client = chromadb.Client()

# Persistenter Client (Daten werden gespeichert)
client = chromadb.PersistentClient(path="./chroma_db")
```

| Modus | Verwendung | Vorteil |
|-------|------------|---------|
| **In-Memory** | Entwicklung, Tests | Schnell, kein Aufr√§umen n√∂tig |
| **Persistent** | Produktion, gro√üe Datenmengen | Daten √ºberleben Neustart |

### 3.3 Collections

Eine **Collection** ist vergleichbar mit einer Tabelle in relationalen Datenbanken.

```python
# Collection erstellen
collection = client.create_collection(
    name="meine_dokumente",
    metadata={"description": "Firmendokumente f√ºr RAG"}
)

# Oder: Collection holen/erstellen (idempotent)
collection = client.get_or_create_collection(name="meine_dokumente")

# Bestehende Collection holen
collection = client.get_collection(name="meine_dokumente")

# Collection l√∂schen
client.delete_collection(name="meine_dokumente")

# Alle Collections auflisten
print(client.list_collections())
```

**Best Practice:** `get_or_create_collection()` verwenden, um Fehler bei wiederholter Ausf√ºhrung zu vermeiden.

### 3.4 Dokumente hinzuf√ºgen

```python
collection.add(
    documents=["Erster Text", "Zweiter Text", "Dritter Text"],
    ids=["doc1", "doc2", "doc3"],
    metadatas=[
        {"source": "handbuch.pdf", "seite": 1},
        {"source": "handbuch.pdf", "seite": 2},
        {"source": "richtlinie.pdf", "seite": 1}
    ]
)
```

**Parameter:**

| Parameter | Pflicht | Beschreibung |
|-----------|---------|--------------|
| `documents` | ‚úÖ | Liste der Texte |
| `ids` | ‚úÖ | Eindeutige IDs (Strings) |
| `metadatas` | ‚ùå | Zus√§tzliche Informationen pro Dokument |
| `embeddings` | ‚ùå | Vorgefertigte Vektoren (sonst automatisch) |

**Hinweis:** Ohne explizite `embeddings` verwendet ChromaDB ein internes Embedding-Modell. F√ºr Konsistenz mit LangChain sollten Embeddings explizit √ºbergeben werden.

---

## 4 Embeddings mit OpenAI

F√ºr produktive RAG-Systeme werden typischerweise OpenAI-Embeddings verwendet.

### 4.1 Verf√ºgbare Modelle

| Modell | Dimensionen | Kosten | Empfehlung |
|--------|-------------|--------|------------|
| `text-embedding-3-small` | 1536 | G√ºnstig | ‚úÖ Standard f√ºr Kurs |
| `text-embedding-3-large` | 3072 | Mittel | H√∂here Qualit√§t |
| `text-embedding-ada-002` | 1536 | G√ºnstig | Legacy, nicht empfohlen |

### 4.2 Embeddings erzeugen

```python
from langchain_openai import OpenAIEmbeddings

# Modell initialisieren
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# Einzelner Text (f√ºr Queries)
query_vector = embeddings.embed_query("Was ist ein KI-Agent?")

# Mehrere Texte (f√ºr Dokumente)
texts = ["Text 1", "Text 2", "Text 3"]
doc_vectors = embeddings.embed_documents(texts)

print(f"Query-Vektor: {len(query_vector)} Dimensionen")
print(f"Dokument-Vektoren: {len(doc_vectors)} St√ºck")
```

### 4.3 Alternative: Sentence Transformers (lokal)

F√ºr Szenarien ohne API-Zugriff:

```python
from langchain_huggingface import HuggingFaceEmbeddings

# Lokales Modell (kein API-Key n√∂tig)
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
```

**Vorteile:** Kostenlos, offline nutzbar, DSGVO-konform  
**Nachteile:** Geringere Qualit√§t als OpenAI, ben√∂tigt mehr Speicher

---

## 5 Dokumente indexieren

Bevor Dokumente durchsucht werden k√∂nnen, m√ºssen sie in Chunks aufgeteilt und indexiert werden.

### 5.1 Warum Chunking?

- **Kontextlimit:** LLMs haben begrenzte Eingabel√§nge
- **Pr√§zision:** Kleinere Chunks erm√∂glichen gezieltere Treffer
- **Relevanz:** Nur relevante Teile werden dem LLM √ºbergeben

### 5.2 Chunking-Strategien

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Standard-Splitter
splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,      # Maximale Chunk-Gr√∂√üe in Zeichen
    chunk_overlap=100,   # √úberlappung zwischen Chunks
    separators=["\n\n", "\n", ". ", " ", ""]  # Trennzeichen-Hierarchie
)

text = """Langer Dokumenttext hier...
Mit mehreren Abs√§tzen...
Und viel Inhalt..."""

chunks = splitter.split_text(text)
print(f"Anzahl Chunks: {len(chunks)}")
```

**Parameter erkl√§rt:**

| Parameter | Beschreibung | Empfehlung |
|-----------|--------------|------------|
| `chunk_size` | Maximale Zeichen pro Chunk | 500‚Äì1000 |
| `chunk_overlap` | √úberlappung (verhindert Informationsverlust) | 10‚Äì20% von chunk_size |
| `separators` | Bevorzugte Trennstellen | Standard belassen |

### 5.3 Overlap visualisiert

```mermaid
flowchart TB
    subgraph "Ohne Overlap - Informationsverlust"
        DOC1[Dokument: AAAA BBBB CCCC DDDD]
        C1[Chunk 1: AAAA]
        C2[Chunk 2: BBBB]
        C3[Chunk 3: CCCC]
        C4[Chunk 4: DDDD]

        DOC1 --> C1
        DOC1 --> C2
        DOC1 --> C3
        DOC1 --> C4

        WARN1[‚ö†Ô∏è S√§tze werden zerrissen!]
    end

    subgraph "Mit 25% Overlap - Kontext erhalten"
        DOC2[Dokument: AAAA BBBB CCCC DDDD]
        O1[Chunk 1: AAAA BB]
        O2[Chunk 2: BB CCCC CC]
        O3[Chunk 3: CC DDDD]

        DOC2 --> O1
        DOC2 --> O2
        DOC2 --> O3

        OK1[‚úÖ Kontext bleibt erhalten!]
    end

    style WARN1 fill:#f8cecc
    style OK1 fill:#d5e8d4
```

**ASCII-Darstellung:**
```
Dokument: [AAAA|BBBB|CCCC|DDDD]

Ohne Overlap:
  Chunk 1: [AAAA]
  Chunk 2: [BBBB]
  Chunk 3: [CCCC]
  ‚Üí S√§tze an Grenzen werden zerrissen!

Mit Overlap (25%):
  Chunk 1: [AAAA|BB]
  Chunk 2: [BB|CCCC]
  Chunk 3: [CC|DDDD]
  ‚Üí Kontext bleibt erhalten!
```

### 5.4 Vollst√§ndiger Indexierungs-Workflow

```mermaid
flowchart LR
    START([Dokument<br/>dokument.txt])
    LOAD[Document Loader<br/>TextLoader]
    SPLIT[Text Splitter<br/>RecursiveCharacterTextSplitter]
    EMBED[Embedding Model<br/>OpenAIEmbeddings]
    STORE[Vector Store<br/>ChromaDB]
    END([Indexed & Ready<br/>for Search])

    START --> LOAD
    LOAD -->|documents| SPLIT
    SPLIT -->|chunks<br/>chunk_size=500<br/>overlap=100| EMBED
    EMBED -->|vectors<br/>1536 dimensions| STORE
    STORE --> END

    style LOAD fill:#e1f5ff
    style SPLIT fill:#ffe6cc
    style EMBED fill:#d5e8d4
    style STORE fill:#dae8fc
```

```python
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

## 1. Dokument laden
loader = TextLoader("dokument.txt", encoding="utf-8")
documents = loader.load()

## 2. In Chunks aufteilen
splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=100
)
chunks = splitter.split_documents(documents)

print(f"Dokument in {len(chunks)} Chunks aufgeteilt")

## 3. Embeddings erstellen und in ChromaDB speichern
embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")

vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embedding_model,
    collection_name="meine_dokumente",
    persist_directory="./chroma_db"  # Optional: Persistenz
)

print("‚úÖ Indexierung abgeschlossen!")
```

### 5.5 Batch-Indexierung (gro√üe Datenmengen)

Bei vielen Dokumenten sollte in Batches indexiert werden:

```python
from tqdm import tqdm

batch_size = 100
all_chunks = [...]  # Gro√üe Liste von Chunks

for i in tqdm(range(0, len(all_chunks), batch_size)):
    batch = all_chunks[i:i+batch_size]
    vectorstore.add_documents(batch)
```

---

## 6 Similarity Search

Die Suche in der Vektordatenbank findet die semantisch √§hnlichsten Dokumente.

### Similarity Search Workflow

```mermaid
sequenceDiagram
    participant User
    participant App
    participant Embeddings as Embedding Model
    participant ChromaDB as Vector Store

    User->>App: Query: "Was sind KI-Agenten?"
    App->>Embeddings: embed_query(query)
    Embeddings-->>App: query_vector [1536 dims]
    App->>ChromaDB: similarity_search(query_vector, k=3)
    ChromaDB->>ChromaDB: Cosine Similarity<br/>mit allen Vektoren
    ChromaDB-->>App: Top-3 Documents<br/>with scores
    App-->>User: Relevante Dokumente

    Note over ChromaDB: Nur Vektoren vergleichen,<br/>kein Text-Matching
```

### 6.1 Grundlegende Suche

```python
# Einfache Suche
results = vectorstore.similarity_search(
    query="Was sind die Vorteile von KI-Agenten?",
    k=3  # Anzahl der Ergebnisse
)

for i, doc in enumerate(results, 1):
    print(f"\n--- Treffer {i} ---")
    print(f"Inhalt: {doc.page_content[:200]}...")
    print(f"Metadaten: {doc.metadata}")
```

### 6.2 Suche mit Scores

```python
# Suche mit √Ñhnlichkeits-Scores
results_with_scores = vectorstore.similarity_search_with_score(
    query="Was sind die Vorteile von KI-Agenten?",
    k=3
)

for doc, score in results_with_scores:
    print(f"Score: {score:.4f} | {doc.page_content[:100]}...")
```

**Score-Interpretation (Cosine Distance):**
- **0.0** = Perfekte √úbereinstimmung
- **< 0.3** = Sehr relevant
- **0.3‚Äì0.5** = Relevant
- **> 0.5** = Weniger relevant

### 6.3 Metadaten-Filtering

```python
# Nur Dokumente aus bestimmter Quelle
results = vectorstore.similarity_search(
    query="Sicherheitsrichtlinien",
    k=5,
    filter={"source": "sicherheit.pdf"}
)

# Komplexere Filter
results = vectorstore.similarity_search(
    query="Umsatzzahlen",
    k=5,
    filter={
        "$and": [
            {"jahr": {"$gte": 2023}},
            {"abteilung": "Vertrieb"}
        ]
    }
)
```

**Verf√ºgbare Filter-Operatoren:**

| Operator | Beschreibung | Beispiel |
|----------|--------------|----------|
| `$eq` | Gleich | `{"status": {"$eq": "aktiv"}}` |
| `$ne` | Ungleich | `{"status": {"$ne": "archiviert"}}` |
| `$gt`, `$gte` | Gr√∂√üer (gleich) | `{"jahr": {"$gte": 2023}}` |
| `$lt`, `$lte` | Kleiner (gleich) | `{"seite": {"$lt": 10}}` |
| `$in` | In Liste | `{"typ": {"$in": ["pdf", "docx"]}}` |
| `$and`, `$or` | Logische Verkn√ºpfung | Siehe Beispiel oben |

### 6.4 Retriever erstellen

F√ºr die Integration in LangChain-Chains wird ein Retriever ben√∂tigt:

```python
# Standard-Retriever
retriever = vectorstore.as_retriever(
    search_kwargs={"k": 3}
)

# Retriever mit Filter
retriever = vectorstore.as_retriever(
    search_kwargs={
        "k": 5,
        "filter": {"source": "handbuch.pdf"}
    }
)

# Retriever mit Score-Threshold
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "score_threshold": 0.5,  # Nur relevante Treffer
        "k": 10
    }
)
```

---

## 7 LangChain-Integration

ChromaDB integriert sich nahtlos in LangChain f√ºr RAG-Systeme.

### 7.1 Vectorstore erstellen (Zusammenfassung)

```python
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

# Variante 1: Aus Texten
vectorstore = Chroma.from_texts(
    texts=["Text 1", "Text 2", "Text 3"],
    embedding=OpenAIEmbeddings(model="text-embedding-3-small"),
    collection_name="demo",
    persist_directory="./chroma_db"
)

# Variante 2: Aus Documents (mit Metadaten)
vectorstore = Chroma.from_documents(
    documents=chunks,  # Liste von Document-Objekten
    embedding=OpenAIEmbeddings(model="text-embedding-3-small"),
    collection_name="demo"
)

# Variante 3: Bestehende Collection laden
vectorstore = Chroma(
    collection_name="demo",
    embedding_function=OpenAIEmbeddings(model="text-embedding-3-small"),
    persist_directory="./chroma_db"
)
```

### 7.2 RAG-Chain mit LCEL

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.chat_models import init_chat_model

# Komponenten vorbereiten
llm = init_chat_model("gpt-4o-mini", model_provider="openai", temperature=0.0)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# Hilfsfunktion: Dokumente formatieren
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# RAG-Prompt
rag_prompt = ChatPromptTemplate.from_template(
    """Beantworte die Frage basierend auf dem folgenden Kontext.
Wenn die Antwort nicht im Kontext steht, sage ehrlich, dass keine Information vorliegt.

Kontext:
{context}

Frage: {question}

Antwort:"""
)

# LCEL Chain
rag_chain = (
    {
        "context": retriever | format_docs,
        "question": RunnablePassthrough()
    }
    | rag_prompt
    | llm
    | StrOutputParser()
)

# Aufruf
antwort = rag_chain.invoke("Was sind die wichtigsten Sicherheitsrichtlinien?")
print(antwort)
```

### 7.3 RAG als Agent-Tool

```python
from langchain_core.tools import tool

@tool
def firmenwissen_suchen(frage: str) -> str:
    """üîç FIRMENWISSEN ‚Äì Durchsucht interne Dokumente nach relevanten Informationen.
    
    Verwende dieses Tool f√ºr Fragen zu:
    - Unternehmensrichtlinien
    - Internen Prozessen
    - Produktinformationen
    
    Args:
        frage: Die Suchanfrage in nat√ºrlicher Sprache
    
    Returns:
        Relevante Informationen aus den Firmendokumenten
    """
    try:
        antwort = rag_chain.invoke(frage)
        return antwort
    except Exception as e:
        return f"Fehler bei der Suche: {str(e)}"
```

F√ºr die vollst√§ndige RAG-Chain-Implementierung siehe **Einsteiger_LangChain.md, Kapitel 11**.

---

## 8 Troubleshooting

H√§ufige Probleme und deren L√∂sungen:

### 8.1 SQLite-Versionsfehler (Google Colab)

**Fehlermeldung:**
```
sqlite3.OperationalError: database is locked
RuntimeError: Your system has an unsupported version of sqlite3
```

**L√∂sung:**
```python
# Am ANFANG des Notebooks ausf√ºhren (vor chromadb import!)
!pip install pysqlite3-binary

import sys
__import__('pysqlite3')
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

# Erst JETZT chromadb importieren
import chromadb
```

### 8.2 Collection existiert bereits

**Fehlermeldung:**
```
ValueError: Collection meine_dokumente already exists
```

**L√∂sung:**
```python
# Option A: get_or_create verwenden (empfohlen)
collection = client.get_or_create_collection(name="meine_dokumente")

# Option B: Bestehende Collection l√∂schen
client.delete_collection(name="meine_dokumente")
collection = client.create_collection(name="meine_dokumente")
```

### 8.3 Keine Ergebnisse bei Suche

**M√∂gliche Ursachen und L√∂sungen:**

| Ursache | Diagnose | L√∂sung |
|---------|----------|--------|
| Collection leer | `collection.count()` pr√ºfen | Dokumente hinzuf√ºgen |
| Falsches Embedding-Modell | Dimensionen vergleichen | Gleiches Modell f√ºr Index und Query |
| Query zu spezifisch | Mit breiterem Begriff testen | Query umformulieren |
| k zu klein | k erh√∂hen | `search_kwargs={"k": 10}` |

**Diagnose-Code:**
```python
# Collection-Status pr√ºfen
print(f"Anzahl Dokumente: {vectorstore._collection.count()}")

# Test-Query ohne Filter
results = vectorstore.similarity_search("test", k=1)
print(f"Test-Ergebnis: {results}")
```

### 8.4 Langsame Queries

**Optimierungen:**

```python
## 1. Batch-Gr√∂√üe bei Indexierung anpassen
vectorstore.add_documents(chunks, batch_size=100)

## 2. Persistenten Client verwenden
client = chromadb.PersistentClient(path="./chroma_db")

## 3. k reduzieren
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

## 4. Metadaten-Filter nutzen (reduziert Suchraum)
results = vectorstore.similarity_search(
    query="Frage",
    k=5,
    filter={"kategorie": "relevant"}
)
```

### 8.5 Speicherprobleme

**Bei gro√üen Datenmengen:**

```python
# In-Memory vermeiden
client = chromadb.PersistentClient(path="./chroma_db")

# Alte Collections aufr√§umen
for coll in client.list_collections():
    if "test" in coll.name:
        client.delete_collection(coll.name)
```

---

## 9 Best Practices

### 9.1 Chunk-Size Empfehlungen

| Dokumenttyp | chunk_size | chunk_overlap | Begr√ºndung |
|-------------|------------|---------------|------------|
| **FAQ / kurze Texte** | 200‚Äì300 | 50 | Pr√§zise Antworten |
| **Handb√ºcher** | 500‚Äì800 | 100‚Äì150 | Kontext erhalten |
| **Rechtsdokumente** | 800‚Äì1000 | 200 | Vollst√§ndige Paragraphen |
| **Code-Dokumentation** | 300‚Äì500 | 100 | Funktionen zusammenhalten |

### 9.2 Embedding-Modell-Auswahl

| Kriterium | OpenAI | Sentence Transformers |
|-----------|--------|----------------------|
| **Qualit√§t** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **Kosten** | ~$0.02 / 1M Tokens | Kostenlos |
| **Geschwindigkeit** | Mittel (API) | Schnell (lokal) |
| **Offline** | ‚ùå | ‚úÖ |
| **DSGVO** | ‚ö†Ô∏è Pr√ºfen | ‚úÖ |

**Empfehlung f√ºr Kurs:** `text-embedding-3-small` (beste Balance)

### 9.3 Persistenz-Strategie

```python
# Entwicklung: In-Memory (schnell, sauber)
client = chromadb.Client()

# Produktion: Persistent
client = chromadb.PersistentClient(path="./chroma_db")

# Wichtig: Pfad in .gitignore aufnehmen!
# .gitignore:
# chroma_db/
```

### 9.4 Metadaten sinnvoll nutzen

```python
# Gute Metadaten erm√∂glichen pr√§zises Filtering
collection.add(
    documents=[chunk.page_content],
    ids=[f"doc_{i}"],
    metadatas=[{
        "source": "handbuch_v2.pdf",
        "seite": 42,
        "kapitel": "Sicherheit",
        "version": "2.0",
        "erstellt": "2024-01-15",
        "sprache": "de"
    }]
)

# Sp√§tere Suche mit Filter
results = vectorstore.similarity_search(
    query="Passwortrichtlinien",
    filter={
        "$and": [
            {"kapitel": "Sicherheit"},
            {"version": "2.0"}
        ]
    }
)
```

### 9.5 Qualit√§tssicherung

```python
# Test-Queries vor Produktion
test_queries = [
    "Was ist die Passwort-Policy?",
    "Wie beantrage ich Urlaub?",
    "Wer ist mein Ansprechpartner f√ºr IT-Probleme?"
]

for query in test_queries:
    results = vectorstore.similarity_search_with_score(query, k=3)
    print(f"\nQuery: {query}")
    for doc, score in results:
        print(f"  Score: {score:.3f} | {doc.page_content[:50]}...")
```

---

## 10 Zusammenfassung

### 10.1 Kernkonzepte

| Konzept | Beschreibung |
|---------|--------------|
| **Embedding** | Numerische Repr√§sentation von Text (Vektor) |
| **Vektordatenbank** | Speichert Vektoren und erm√∂glicht √Ñhnlichkeitssuche |
| **Similarity Search** | Findet semantisch √§hnliche Dokumente |
| **Chunking** | Aufteilung gro√üer Dokumente in kleinere Teile |
| **RAG** | Retrieval-Augmented Generation ‚Äì LLM + externes Wissen |

### 10.2 Typischer RAG-Workflow

```mermaid
flowchart TB
    LOAD[1. Dokumente laden<br/>TextLoader, PyPDFLoader]
    CHUNK[2. Chunking<br/>RecursiveCharacterTextSplitter<br/>chunk_size=500, overlap=100]
    EMBED[3. Embeddings erzeugen<br/>OpenAIEmbeddings<br/>text-embedding-3-small]
    STORE[4. In ChromaDB speichern<br/>Chroma.from_documents]
    RETRIEVE[5. Retriever erstellen<br/>vectorstore.as_retriever]
    CHAIN[6. RAG-Chain bauen<br/>LCEL Pipeline]
    AGENT[7. Agent mit RAG-Tool ausstatten<br/>create_agent + @tool]

    LOAD --> CHUNK
    CHUNK --> EMBED
    EMBED --> STORE
    STORE --> RETRIEVE
    RETRIEVE --> CHAIN
    CHAIN --> AGENT

    style LOAD fill:#e1f5ff
    style CHUNK fill:#ffe6cc
    style EMBED fill:#d5e8d4
    style STORE fill:#dae8fc
    style RETRIEVE fill:#e1f5ff
    style CHAIN fill:#ffe6cc
    style AGENT fill:#d5e8d4
```

**Text-Version:**
```
1. Dokumente laden
      ‚Üì
2. Chunking (RecursiveCharacterTextSplitter)
      ‚Üì
3. Embeddings erzeugen (OpenAIEmbeddings)
      ‚Üì
4. In ChromaDB speichern
      ‚Üì
5. Retriever erstellen
      ‚Üì
6. RAG-Chain bauen (LCEL)
      ‚Üì
7. Agent mit RAG-Tool ausstatten
```

### 10.3 Quick Reference

```python
# Setup (Colab)
!pip install chromadb pysqlite3-binary
import sys; __import__('pysqlite3'); sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

# Imports
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Indexieren
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
chunks = splitter.split_documents(documents)
vectorstore = Chroma.from_documents(chunks, OpenAIEmbeddings())

# Suchen
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
results = retriever.invoke("Meine Frage")
```

---

> üí° **Tipp:** F√ºr die vollst√§ndige RAG-Chain-Implementierung siehe **Einsteiger_LangChain.md, Kapitel 11**!

> üîó **Weiterf√ºhrend:** 
> + [ChromaDB Dokumentation](https://docs.trychroma.com/) 
> + [LangChain VectorStores](https://python.langchain.com/docs/modules/data_connection/vectorstores/)

---

**Version:** 1.0    
**Stand:** November 2025    
**Kurs:** KI-Agenten. Verstehen. Anwenden. Gestalten.  




---
layout: default
title: GenAI_Lib Einsteiger
parent: Frameworks
nav_order: 5
description: Projektspezifische Python-Bibliothek f√ºr Kursanwendungen
permalink: /frameworks/genai_lib/
---

# GenAI_Lib - Projektspezifische Bibliothek
{: .no_toc }

> **Projektspezifische Bibliothek f√ºr den Kurs Generative KI**

---

Die `genai_lib` ist eine projektspezifische Python-Bibliothek, die speziell f√ºr die Anforderungen dieses Kurses entwickelt wurde. Sie b√ºndelt wichtige Funktionen f√ºr multimodale RAG-Systeme und allgemeine Hilfsfunktionen.

## Inhalt
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## Installation

Die `genai_lib` kann direkt aus dem GitHub-Repository installiert werden:

```bash
# Mit pip
pip install git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul

# Mit uv (empfohlen f√ºr Google Colab)
uv pip install --system git+https://github.com/ralf-42/GenAI.git#subdirectory=04_modul
```

## Module im √úberblick

Die Bibliothek besteht aus zwei Hauptmodulen:

| Modul | Beschreibung | Hauptfunktionen |
|-------|-------------|----------------|
| **utilities.py** | Hilfsfunktionen f√ºr Environment-Setup | Environment-Checks, Paket-Installation, API-Keys, Prompt-Templates, LLM-Response-Parsing, Model-Profile |
| **multimodal_rag.py** | Multimodales RAG-System (v3.1) | Text- und Bildsuche, Bild-zu-Bild-Suche, Cross-Modal-Retrieval, System-Status |

---

## utilities.py - Hilfsfunktionen

### √úberblick

Das `utilities`-Modul stellt grundlegende Hilfsfunktionen bereit, die in vielen Notebooks und Projekten wiederkehrend ben√∂tigt werden.

### Hauptfunktionen

#### 1. `check_environment()`

√úberpr√ºft die Entwicklungsumgebung und zeigt installierte Pakete an.

```python
from genai_lib.utilities import check_environment

check_environment()
```

**Ausgabe:**
- Python-Version
- Alle installierten LangChain-Bibliotheken
- Unterdr√ºckt automatisch Deprecation-Warnungen

#### 2. `install_packages(packages)`

Installiert Python-Pakete automatisch, wenn sie noch nicht verf√ºgbar sind.

```python
from genai_lib.utilities import install_packages

# Einfache Installation
install_packages(['numpy', 'pandas'])

# Mit separaten Install- und Import-Namen
install_packages([
    ('markitdown[all]', 'markitdown'),
    'langchain_chroma'
])
```

**Features:**
- Pr√ºft, ob Pakete bereits installiert sind
- Verwendet `uv pip install` f√ºr schnelle Installation in Google Colab
- Gibt klare Statusmeldungen (‚úÖ ‚úó üîÑ)
- Unterst√ºtzt Tupel f√ºr verschiedene Install- und Import-Namen

#### 3. `setup_api_keys(key_names, create_globals=True)`

L√§dt API-Keys aus Google Colab userdata und setzt sie als Umgebungsvariablen.

```python
from genai_lib.utilities import setup_api_keys

# Mit globalen Variablen (Standardverhalten)
setup_api_keys([
    "OPENAI_API_KEY",
    "ANTHROPIC_API_KEY",
    "HF_TOKEN"
])

# Nur Umgebungsvariablen (keine globalen Variablen)
setup_api_keys(["OPENAI_API_KEY"], create_globals=False)
```

**Features:**
- L√§dt Keys sicher aus Google Colab Secrets
- Erstellt optional globale Variablen f√ºr einfachen Zugriff
- Gibt Statusmeldungen f√ºr jeden Key aus
- Verhindert unbeabsichtigte Sichtbarkeit durch Return-Werte

#### 4. `get_ipinfo()`

Zeigt Geoinformationen zur aktuellen IP-Adresse an.

```python
from genai_lib.utilities import get_ipinfo

get_ipinfo()
```

**Ausgabe:**
- IP-Adresse
- Stadt, Region, Land
- Provider
- Koordinaten, Postleitzahl, Zeitzone

#### 5. `mprint(text)`

Gibt Markdown-formatierten Text in Jupyter Notebooks aus.

```python
from genai_lib.utilities import mprint

mprint("# √úberschrift\n**Fett** und *kursiv*")
```

#### 6. `mermaid(code, width=None, height=None)`

Rendert Mermaid-Diagramme direkt im Notebook mit anpassbarer Gr√∂√üe.

```python
from genai_lib.utilities import mermaid

# Standard (automatische Gr√∂√üe)
mermaid('''
graph TD
    A[Start] --> B[Process]
    B --> C[End]
''')

# Mit angepasster Gr√∂√üe
mermaid('''
sequenceDiagram
    User->>Agent: Frage stellen
    Agent->>LLM: Query senden
    LLM-->>Agent: Antwort
    Agent-->>User: Ergebnis
''', width=800, height=600)
```

**Parameter:**
- `code` (str): Mermaid-Code f√ºr das Diagramm
- `width` (int, optional): Breite in Pixeln
- `height` (int, optional): H√∂he in Pixeln

**Unterst√ºtzte Diagrammtypen:**
- Flowcharts (`graph TD`, `graph LR`)
- Sequenzdiagramme (`sequenceDiagram`)
- Gantt-Charts (`gantt`)
- State Machines (`stateDiagram`)

**Features:**
- Automatische oder manuelle Gr√∂√üenkontrolle
- Robuste Fehlerbehandlung mit aussagekr√§ftigen Fehlermeldungen
- Timeout-Schutz (15 Sekunden)

#### 7. `load_chat_prompt_template(path)`

L√§dt Prompt-Templates aus Python-Dateien.

```python
from genai_lib.utilities import load_chat_prompt_template

# Lokal
prompt = load_chat_prompt_template('05_prompt/qa_prompt.py')

# Von GitHub (tree oder blob URLs werden automatisch konvertiert)
prompt = load_chat_prompt_template(
    'https://github.com/ralf-42/GenAI/blob/main/05_prompt/text_zusammenfassung.py'
)

# Oder mit tree-URL
prompt = load_chat_prompt_template(
    'https://github.com/user/repo/tree/main/prompts/qa_prompt.py'
)
```

**Template-Format (qa_prompt.py):**
```python
messages = [
    ("system", "{system_prompt}"),
    ("human", "Question: {question}\n\nContext: {context}\n\nAnswer:")
]
```

#### 8. `extract_thinking(response)` üÜï

Universeller Parser f√ºr verschiedene Thinking-Formate von LLMs. Extrahiert den Denkprozess und die eigentliche Antwort aus unterschiedlichen Response-Strukturen.

```python
from genai_lib.utilities import extract_thinking

# Response von beliebigem LLM
response = llm.invoke("Erkl√§re Schritt f√ºr Schritt, was 2+2 ergibt")

# Universeller Parser f√ºr alle Formate
thinking, answer = extract_thinking(response)

print(f"Denkprozess: {thinking[:200]}...")
print(f"Antwort: {answer}")
```

**Unterst√ºtzte Formate:**

| Provider/Modell | Format | Beispiel |
|-----------------|--------|----------|
| Claude (Extended Thinking) | Liste mit `{"type": "thinking"}` Bl√∂cken | `content = [{"type": "thinking", "thinking": "..."}]` |
| Gemini | Liste mit `{"type": "thinking"}` Bl√∂cken | `content = [{"type": "thinking", "thinking": "..."}]` |
| Qwen3, DeepSeek R1 | String mit `<think>` Tags | `"<think>Denkprozess</think>Antwort"` |
| DeepSeek | `additional_kwargs["reasoning_content"]` | Separates Feld im Response |

**R√ºckgabe:**
- `thinking` (str): Extrahierter Denkprozess (leer, wenn nicht vorhanden)
- `answer` (str): Eigentliche Antwort

**Features:**
- Provider-agnostisch: Ein Parser f√ºr alle LLMs
- Fallback-Logik: Pr√ºft automatisch alle bekannten Formate
- Robust: Gibt leeren Thinking-String zur√ºck, wenn kein Denkprozess vorhanden

#### 9. `get_model_profile(model, temperature=0.0, print_profile=True, **kwargs)` üÜï

Ruft Model-Profile von models.dev ab und zeigt die wichtigsten Capabilities eines LLM-Modells. Nutzt intern `init_chat_model()` und gibt detaillierte Informationen √ºber Structured Output, Function Calling, Vision, Token-Limits, etc. zur√ºck.

```python
from genai_lib.utilities import get_model_profile

# Formatierte Ausgabe aller wichtigen Capabilities
profile = get_model_profile("openai:gpt-4o-mini")

# Output:
# üîç Model Profile: openai:gpt-4o-mini
# ============================================================
#
# üìã Core Capabilities:
#   ‚úì Structured Output:  True
#   ‚úì Function Calling:   True
#   ‚úì JSON Mode:          True
#   ‚úì Reasoning:          False
#
# üé® Multimodal Capabilities:
#   ‚úì Input:  üìù Text, üñºÔ∏è Image
#   ‚úì Output: üìù Text
#
# üìä Token Limits:
#   ‚úì Max Input Tokens:   128000
#   ‚úì Max Output Tokens:  16384
#
# ‚öôÔ∏è Model Configuration:
#   ‚úì Temperature:        Yes
#   ‚úì Knowledge Cutoff:   2023-10
#
# üîß Additional Features:
#   ‚úì Streaming:          True
#   ‚úì Async:              True
# ============================================================

# Ohne Ausgabe (nur Profile-Dict zur√ºckgeben)
profile = get_model_profile("anthropic:claude-3-sonnet", print_profile=False)

# Verschiedene Models vergleichen (mit Fehlerbehandlung)
for model in ["openai:gpt-4o-mini", "anthropic:claude-3-sonnet", "google:gemini-pro"]:
    print(f"\n{model}:")
    profile = get_model_profile(model, print_profile=False)

    if profile:  # Nur verarbeiten, wenn Model erfolgreich initialisiert
        print(f"  Context: {profile['max_input_tokens']} tokens")
        print(f"  Vision: {profile['image_inputs']}")
        print(f"  Reasoning: {profile.get('reasoning', False)}")
        print(f"  Knowledge: {profile.get('knowledge_cutoff', 'N/A')}")
    else:
        print(f"  ‚ö†Ô∏è  Model konnte nicht initialisiert werden (Provider-Bibliothek fehlt?)")
```

**Parameter:**
- `model` (str): Model-Name im Format "provider:model"
- `temperature` (float): Temperatur-Einstellung (Standard: 0.0)
- `print_profile` (bool): Formatierte Ausgabe aktivieren (Standard: True)
- `**kwargs`: Zus√§tzliche Parameter f√ºr `init_chat_model()` (z.B. max_tokens)

**R√ºckgabe:**
- `dict`: Vollst√§ndiges Model-Profile mit allen Capabilities

**Profile-Attribute (Auswahl):**

**Core Capabilities:**
- `structured_output`: Native Structured Output API
- `tool_calling`: Function Calling Support
- `supports_json_mode`: JSON Mode Support
- `reasoning`: Extended Thinking/Reasoning Support

**Multimodal Input:**
- `text_inputs`: Text Input (Standard) - Anzeige: üìù Text
- `image_inputs`: Bild Input (Vision) - Anzeige: üñºÔ∏è Image
- `audio_inputs`: Audio Input Support - Anzeige: üéµ Audio
- `video_inputs`: Video Input Support - Anzeige: üé¨ Video

**Multimodal Output:**
- `text_outputs`: Text Output (Standard) - Anzeige: üìù Text
- `image_outputs`: Bild-Generierung - Anzeige: üñºÔ∏è Image
- `audio_outputs`: Audio-Generierung (TTS) - Anzeige: üéµ Audio
- `video_outputs`: Video-Generierung - Anzeige: üé¨ Video

**Token Limits:**
- `max_input_tokens`: Context Window Gr√∂√üe
- `max_output_tokens`: Max. Output-L√§nge

**Model Configuration:**
- `temperature`: Temperature-Parameter Support
- `knowledge_cutoff`: Knowledge Cutoff Date

**Additional Features:**
- `streaming`: Streaming Support
- `async_capable`: Async Support

**Features:**
- Quelle: models.dev (Open-Source Model-Index)
- Automatische Capability-Detection
- Formatierte √úbersicht mit Symbolen (üìùüñºÔ∏èüéµüé¨) oder Raw-Dict
- Reasoning/Thinking Support Detection
- Temperature-Support-Check
- Knowledge Cutoff Date
- Perfekt f√ºr Modellvergleiche in Notebooks

**Use Cases:**
- Modell-F√§higkeiten vor Verwendung pr√ºfen (Reasoning, Vision, Audio, etc.)
- Verschiedene LLMs vergleichen (Context Window, Multimodal, Knowledge)
- Feature-Gates in Code (z.B. "nur wenn Vision verf√ºgbar")
- Reasoning-Modelle identifizieren (Claude Extended Thinking, DeepSeek R1)
- Temperature-Unterst√ºtzung pr√ºfen
- Debugging und Dokumentation

---

## multimodal_rag.py - Multimodales RAG

### √úberblick

Das `multimodal_rag`-Modul implementiert ein vollst√§ndiges RAG-System mit Unterst√ºtzung f√ºr Text- und Bilddokumente. Es kombiniert OpenAI-Embeddings f√ºr Text und CLIP-Embeddings f√ºr Bilder.

### Architektur

```
multimodal_rag
‚îú‚îÄ‚îÄ Text-Pipeline: OpenAI Embeddings + ChromaDB
‚îú‚îÄ‚îÄ Bild-Pipeline: CLIP Embeddings + ChromaDB
‚îú‚îÄ‚îÄ Vision-LLM: GPT-4o-mini f√ºr Bildbeschreibungen (via init_chat_model)
‚îî‚îÄ‚îÄ Hybride Suche: Text ‚Üî Bild ‚Üî Bild
```

**üÜï LangChain 1.0+ Integration (v3.1):**
- Nutzt `init_chat_model("openai:gpt-4o-mini")` f√ºr LLM-Initialisierung
- Vision-Analysen mit `HumanMessage` und Standard Content Blocks
- Provider-agnostische Multimodal-Verarbeitung

### Hauptfunktionen

#### 1. `init_rag_system(config=None)`

Initialisiert das vollst√§ndige RAG-System.

```python
from genai_lib.multimodal_rag import init_rag_system, RAGConfig

# Mit Standard-Konfiguration
rag = init_rag_system()

# Mit eigener Konfiguration
config = RAGConfig(
    chunk_size=300,
    chunk_overlap=50,
    clip_model='clip-ViT-B-32',
    llm_model='gpt-4o-mini',
    db_path='./my_rag_db'
)
rag = init_rag_system(config)
```

**Was wird initialisiert:**
- OpenAI Text-Embeddings
- CLIP-Modell f√ºr Bild-Embeddings
- GPT-4o-mini f√ºr Text und Vision (via `init_chat_model()` - LangChain 1.0+)
- ChromaDB mit zwei Collections (texts, images)
- MarkItDown f√ºr Dokumentenkonvertierung

**Interne LangChain 1.0+ Patterns:**
```python
# System nutzt intern moderne LangChain APIs
llm = init_chat_model("openai:gpt-4o-mini", temperature=0.0)

# Vision-Analyse mit Standard Content Blocks
message = HumanMessage(content=[
    {"type": "text", "text": "Beschreibe dieses Bild"},
    {"type": "image", "url": "data:image/png;base64,...", "mime_type": "image/png"}
])
```

#### 2. `process_directory(rag, directory_path, auto_describe_images=True)`

Verarbeitet ein Verzeichnis mit Text- und Bilddateien.

```python
from genai_lib.multimodal_rag import process_directory

# Verzeichnis mit automatischen Bildbeschreibungen
process_directory(rag, './files', auto_describe_images=True)

# Ohne Bildbeschreibungen (nur CLIP-Embeddings)
process_directory(rag, './files', auto_describe_images=False)
```

**Unterst√ºtzte Dateitypen:**
- **Text:** `.txt`, `.md`, `.pdf`, `.docx`, `.pptx`, `.xlsx`
- **Bilder:** `.jpg`, `.jpeg`, `.png`, `.gif`, `.bmp`

**Features:**
- Automatische Dokumentenkonvertierung mit MarkItDown
- Text-Chunking mit RecursiveCharacterTextSplitter
- Automatische Bildbeschreibung mit GPT-4o-mini
- CLIP-Embeddings f√ºr Bilder
- Fortschrittsanzeige

#### 3. `multimodal_search(rag, query, k=5, text_only=False, images_only=False)`

Durchsucht Text und Bilder gleichzeitig.

```python
from genai_lib.multimodal_rag import multimodal_search

# Hybride Suche (Text + Bilder)
results = multimodal_search(rag, "Roboter mit KI", k=5)

# Nur Text
text_results = multimodal_search(rag, "Maschinelles Lernen", text_only=True)

# Nur Bilder
image_results = multimodal_search(rag, "rote Autos", images_only=True)
```

**R√ºckgabe:**
- `text_docs`: Liste von LangChain Documents mit Text-Chunks
- `image_results`: Liste von Dictionaries mit Bildpfaden und Metadaten

#### 4. `search_similar_images(rag, image_path, k=5)`

Findet √§hnliche Bilder zu einem Query-Bild (Bild ‚Üí Bild Suche).

```python
from genai_lib.multimodal_rag import search_similar_images

# √Ñhnliche Bilder finden
similar = search_similar_images(rag, "./query_image.jpg", k=5)

for img in similar:
    print(f"√Ñhnlichkeit: {img['similarity']:.2f}")
    print(f"Pfad: {img['image_path']}")
```

**Use Cases:**
- Duplikate finden
- √Ñhnliche Produkte vorschlagen
- Bildkategorisierung

#### 5. `search_text_by_image(rag, image_path, k=3)`

Findet Textdokumente, die zum Bildinhalt passen (Bild ‚Üí Text Suche).

```python
from genai_lib.multimodal_rag import search_text_by_image

# Passende Texte zu einem Bild finden
texts = search_text_by_image(rag, "./product_image.jpg", k=3)

for doc in texts:
    print(doc.page_content)
```

**Use Cases:**
- Produktbeschreibungen zu Bildern finden
- Dokumentation zu Screenshots suchen
- Bild-Text-Verkn√ºpfung in Datenbanken

#### 6. `get_system_status(rag)`

Gibt Statistiken √ºber das RAG-System zur√ºck.

```python
from genai_lib.multimodal_rag import get_system_status

status = get_system_status(rag)
print(f"Text-Chunks: {status['text_chunks']}")
print(f"Bilder: {status['images']}")
print(f"Bildbeschreibungen: {status['image_descriptions']}")
```

**R√ºckgabe:**
- `text_chunks`: Anzahl der Text-Dokument-Chunks
- `images`: Anzahl der Bilder in der Datenbank
- `image_descriptions`: Anzahl der Bildbeschreibungen
- `total_documents`: Gesamtanzahl aller Eintr√§ge

#### 7. `cleanup_database(db_path)`

L√∂scht die Datenbank komplett f√ºr einen Neustart.

```python
from genai_lib.multimodal_rag import cleanup_database

cleanup_database('./multimodal_rag_db')
```

### Vollst√§ndiges Beispiel

```python
from genai_lib.multimodal_rag import (
    init_rag_system,
    process_directory,
    multimodal_search,
    search_similar_images,
    get_system_status
)

# 1. System initialisieren
rag = init_rag_system()

# 2. Dokumente verarbeiten
process_directory(rag, './knowledge_base', auto_describe_images=True)

# 3. Status pr√ºfen
status = get_system_status(rag)
print(f"Verarbeitet: {status['text_chunks']} Texte, {status['images']} Bilder")

# 4. Multimodale Suche (Text + Bilder + Cross-Modal)
results = multimodal_search(rag, "Neuronale Netze", k_text=3, k_images=3)
print(results)

# 5. Bild-zu-Bild Suche
similar = search_similar_images(rag, "./query_image.jpg", k=5)
for img in similar:
    print(f"{img['filename']}: {img['similarity']}")
```

---

## Best Practices

### 1. Environment-Setup in Notebooks

```python
from genai_lib.utilities import check_environment, setup_api_keys, install_packages

# 1. Environment checken
check_environment()

# 2. Pakete installieren
install_packages([
    'langchain',
    'langchain-openai',
    ('markitdown[all]', 'markitdown')
])

# 3. API-Keys setzen
setup_api_keys(["OPENAI_API_KEY"])
```

### 2. Multimodales RAG-System

```python
from genai_lib.multimodal_rag import (
    init_rag_system,
    RAGConfig,
    process_directory,
    multimodal_search
)

# Custom Konfiguration f√ºr gro√üe Dokumente
config = RAGConfig(
    chunk_size=500,
    chunk_overlap=100,
    text_threshold=1.0,
    db_path='./projekt_rag'
)

rag = init_rag_system(config)
process_directory(rag, './docs', auto_describe_images=True)
```

---

## Abh√§ngigkeiten

### Kern-Abh√§ngigkeiten
```python
# LangChain Stack
langchain>=1.0.0
langchain-core>=1.0.0
langchain-openai>=0.2.0
langchain-community>=0.3.0
langchain-chroma>=0.1.0

# OpenAI
openai>=1.0.0

# Multimodal
sentence-transformers>=3.0.0
pillow>=10.0.0
markitdown>=0.0.1

# Vektordatenbank
chromadb>=0.5.0

# Utilities
python-dotenv>=1.0.0
requests>=2.31.0
```

---

## Ressourcen

- **GitHub Repository**: [ralf-42/GenAI](https://github.com/ralf-42/GenAI)
- **Installationsordner**: `04_modul/genai_lib/`
- **Beispiel-Notebooks**: Siehe Kursmodule M08-M18

---

## Lizenz

MIT License - Copyright (c) 2025 Ralf

Die Module stehen unter der MIT-Lizenz und k√∂nnen frei f√ºr eigene Projekte verwendet werden.


---

**Version:** 3.1
**Stand:** Januar 2026
**Kurs:** Generative KI. Verstehen. Anwenden. Gestalten.       



---
layout: default
title: RAG Workshop
parent: Projekte
nav_order: 1
description: "Schrittweise Workshop: Tech-Doku Assistent - vom einfachen Chatbot zur RAG-basierten Anwendung mit UI"
has_toc: true
---

# RAG Workshop
{: .no_toc }

> **Tech-Doku Assistent bauen**    
> Schrittweise Entwicklung vom einfachen Chatbot zur intelligenten RAG-Anwendung mit UI (Module M04-M11)

---

# Inhaltsverzeichnis
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## ProjektÃ¼bersicht

In dieser Ãœbungsaufgabe bauen Sie schrittweise einen **Tech-Doku Assistenten**, der technische Fragen beantwortet und dabei immer intelligenter wird.

**Lernziele:**
- Aufbau einer GenAI-Anwendung von Grund auf
- Schrittweise Integration von LangChain-Features
- Praktische Anwendung der Module M04-M11
- Best Practices fÃ¼r strukturierten Notebook-Code

**Zeitaufwand:** ca. 4-6 Stunden (je nach Vorkenntnissen)

**Arbeitsumgebung:** Google Colab oder Jupyter Notebook

---

## Notebook-Struktur

Sie erstellen **ein Notebook** mit **7 aufbauenden Kapiteln** (oder 7 separate Notebooks):

```
ğŸ““ Tech_Doku_Assistent.ipynb
   â”œâ”€â”€ ğŸ¯ Kapitel 1: Basis-Chatbot (M04)
   â”œâ”€â”€ ğŸ“Š Kapitel 2: Token-Optimierung (M05)
   â”œâ”€â”€ ğŸ’¬ Kapitel 3: Chat-History & Memory (M06)
   â”œâ”€â”€ ğŸ”§ Kapitel 4: Strukturierte Ausgaben (M07)
   â”œâ”€â”€ ğŸ“š Kapitel 5: RAG-Integration (M08)
   â”œâ”€â”€ ğŸ¤– Kapitel 6: Agent mit Tools (M10)
   â””â”€â”€ ğŸŒ Kapitel 7: Gradio-UI (M11)
```

**Empfehlung:** Beginnen Sie mit einem Notebook und fÃ¼gen Sie nach jedem Kapitel eine Markdown-Zelle mit "---" zur Trennung hinzu.

---

## Vorbereitung: Google Colab Setup

Bevor Sie starten, richten Sie Ihre Colab-Umgebung ein:

### API-Key in Colab Secrets speichern

1. Klicken Sie in Colab auf das SchlÃ¼ssel-Symbol ğŸ”‘ (linke Sidebar)
2. FÃ¼gen Sie `OPENAI_API_KEY` hinzu
3. Aktivieren Sie "Notebook access"

### Basis-Pakete installieren

FÃ¼hren Sie zu Beginn des Notebooks aus:

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“¦ INSTALLATION (Einmalig ausfÃ¼hren)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

!pip install -q langchain>=1.0.0 langchain-openai>=0.2.0 langchain-community
!pip install -q chromadb tiktoken gradio
```

### API-Key laden

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”‘ API-KEY SETUP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import os
from google.colab import userdata

# API-Key aus Colab Secrets laden
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')
```

---

## Kapitel 1: Basis-Chatbot (Modul M04)

**Lernziel:** LangChain-Grundlagen, Prompt-Templates, einfache LLM-Interaktion

### Aufgabe 1.1: LLM initialisieren

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ KAPITEL 1: BASIS-CHATBOT (M04)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from langchain.chat_models import init_chat_model
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# LLM initialisieren
llm = init_chat_model("openai:gpt-4o-mini", temperature=0.3)

# Prompt-Template erstellen
prompt = ChatPromptTemplate.from_messages([
    ("system", """Du bist ein technischer Dokumentationsassistent.
    Beantworte Fragen prÃ¤zise, hÃ¶flich und technisch korrekt.
    Verwende Beispiele wo sinnvoll."""),
    ("user", "{frage}")
])

# Chain erstellen (LCEL-Syntax)
chain = prompt | llm | StrOutputParser()

# Test
antwort = chain.invoke({"frage": "Was ist ein API-Endpunkt?"})
print(antwort)
```

### Aufgabe 1.2: Interaktive Chat-Schleife

```python
# Chat-Funktion fÃ¼r Notebook
def tech_chat():
    """Einfache Chat-Schleife fÃ¼r Jupyter/Colab"""
    print("ğŸ¤– Tech-Doku Assistent gestartet!")
    print("   (Schreibe 'exit' zum Beenden)\n")

    while True:
        frage = input("ğŸ’¬ Sie: ")

        if frage.lower() in ['exit', 'quit', 'beenden']:
            print("ğŸ‘‹ Auf Wiedersehen!")
            break

        if not frage.strip():
            continue

        antwort = chain.invoke({"frage": frage})
        print(f"\nğŸ¤– Bot: {antwort}\n")

# Starten
tech_chat()
```

**Erfolgskriterium:**
- âœ… Der Bot beantwortet technische Fragen korrekt
- âœ… Der Chat lÃ¤uft in einer Schleife (bis "exit")
- âœ… LCEL-Syntax (`|`) wird verwendet

---

## Kapitel 2: Token-Optimierung (Modul M05)

**Lernziel:** Transformer-Konzepte verstehen, Token-ZÃ¤hlung, Kontext-Management

### Aufgabe 2.1: Token-ZÃ¤hlung implementieren

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Š KAPITEL 2: TOKEN-OPTIMIERUNG (M05)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import tiktoken

# Token-Counter Funktion
def count_tokens(text: str, model: str = "gpt-4o-mini") -> int:
    """ZÃ¤hlt Tokens fÃ¼r ein gegebenes Modell"""
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

# Test
test_text = "Was ist Docker?"
tokens = count_tokens(test_text)
print(f"ğŸ“Š '{test_text}' hat {tokens} Tokens")
```

### Aufgabe 2.2: Chat mit Token-Tracking

```python
def tech_chat_mit_tokens():
    """Chat mit Token-Statistiken"""
    print("ğŸ¤– Tech-Doku Assistent (mit Token-Tracking)")
    print("   (Schreibe 'exit' zum Beenden)\n")

    total_input_tokens = 0
    total_output_tokens = 0

    while True:
        frage = input("ğŸ’¬ Sie: ")

        if frage.lower() in ['exit', 'quit', 'beenden']:
            print(f"\nğŸ“Š Session-Statistik:")
            print(f"   Gesamt Input:  {total_input_tokens} Tokens")
            print(f"   Gesamt Output: {total_output_tokens} Tokens")
            print(f"   Total:         {total_input_tokens + total_output_tokens} Tokens")
            print("ğŸ‘‹ Auf Wiedersehen!")
            break

        if not frage.strip():
            continue

        # Token-ZÃ¤hlung Input
        input_tokens = count_tokens(frage)

        # LLM-Anfrage
        antwort = chain.invoke({"frage": frage})

        # Token-ZÃ¤hlung Output
        output_tokens = count_tokens(antwort)

        # Statistik aktualisieren
        total_input_tokens += input_tokens
        total_output_tokens += output_tokens

        print(f"\nğŸ¤– Bot: {antwort}\n")
        print(f"ğŸ“Š Token-Statistik:")
        print(f"   Input:  {input_tokens:4d} Tokens")
        print(f"   Output: {output_tokens:4d} Tokens")
        print(f"   Total:  {input_tokens + output_tokens:4d} Tokens")

        # Warnung bei langen Antworten
        if output_tokens > 500:
            print("âš ï¸  Warnung: Sehr lange Antwort (>500 Tokens)")

        print()

# Starten
tech_chat_mit_tokens()
```

**Erfolgskriterium:**
- âœ… Token-ZÃ¤hlung funktioniert korrekt
- âœ… Statistiken werden nach jeder Frage angezeigt
- âœ… Warnung bei langen Antworten (>500 Tokens)
- âœ… Session-Gesamtstatistik am Ende

---

## Kapitel 3: Chat-History & Memory (Modul M06)

**Lernziel:** Konversationskontext verwalten, Chat-History nutzen

### Aufgabe 3.1: Memory implementieren

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ’¬ KAPITEL 3: CHAT-HISTORY & MEMORY (M06)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

# Memory-Store (speichert alle Sessions)
store = {}

def get_session_history(session_id: str) -> InMemoryChatMessageHistory:
    """Holt oder erstellt Chat-History fÃ¼r Session"""
    if session_id not in store:
        store[session_id] = InMemoryChatMessageHistory()
    return store[session_id]

# Neues Prompt mit History
prompt_with_history = ChatPromptTemplate.from_messages([
    ("system", """Du bist ein technischer Dokumentationsassistent.
    Beantworte Fragen prÃ¤zise und technisch korrekt.
    WICHTIG: Beziehe dich auf vorherige Fragen in der Konversation."""),
    MessagesPlaceholder(variable_name="history"),
    ("user", "{frage}")
])

# Chain mit Memory
chain_with_memory = prompt_with_history | llm | StrOutputParser()

# Runnable mit History
chat_with_history = RunnableWithMessageHistory(
    chain_with_memory,
    get_session_history,
    input_messages_key="frage",
    history_messages_key="history"
)
```

### Aufgabe 3.2: Chat mit Kontext-Bewusstsein

```python
def tech_chat_mit_memory():
    """Chat mit KonversationsgedÃ¤chtnis"""
    session_id = "user_session_1"

    print("ğŸ¤– Tech-Doku Assistent (mit Memory)")
    print("   (Schreibe 'exit' zum Beenden, 'reset' fÃ¼r neue Session)\n")

    while True:
        frage = input("ğŸ’¬ Sie: ")

        if frage.lower() in ['exit', 'quit', 'beenden']:
            print("ğŸ‘‹ Auf Wiedersehen!")
            break

        if frage.lower() == 'reset':
            store[session_id] = InMemoryChatMessageHistory()
            print("ğŸ”„ Chat-History wurde zurÃ¼ckgesetzt\n")
            continue

        if not frage.strip():
            continue

        # Invoke mit Session-Config
        config = {"configurable": {"session_id": session_id}}
        antwort = chat_with_history.invoke({"frage": frage}, config=config)

        print(f"\nğŸ¤– Bot: {antwort}\n")

        # Zeige History-LÃ¤nge
        history = store[session_id].messages
        print(f"ğŸ’¾ History: {len(history)//2} Austausche gespeichert\n")

# Starten
tech_chat_mit_memory()
```

**Erfolgskriterium:**
- âœ… Bot erinnert sich an vorherige Fragen
- âœ… Antworten beziehen sich auf Kontext
- âœ… 'reset' Befehl lÃ¶scht History
- âœ… History-LÃ¤nge wird angezeigt

---

## Kapitel 4: Strukturierte Ausgaben (Modul M07)

**Lernziel:** Pydantic-Modelle, `with_structured_output()`, JSON-Schema

### Aufgabe 4.1: Pydantic-Modell definieren

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”§ KAPITEL 4: STRUKTURIERTE AUSGABEN (M07)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from pydantic import BaseModel, Field
from typing import Literal

class FAQEntry(BaseModel):
    """Strukturierte FAQ-Eingabe"""
    frage: str = Field(description="Die ursprÃ¼ngliche Frage")
    antwort: str = Field(description="Die Antwort (max 200 Zeichen)")
    kategorie: Literal["Grundlagen", "Installation", "Konzepte", "Troubleshooting"] = \
        Field(description="Thematische Kategorie")
    schwierigkeit: int = Field(description="Schwierigkeitsgrad: 1=AnfÃ¤nger, 5=Experte", ge=1, le=5)
    tags: list[str] = Field(description="3-5 relevante SchlagwÃ¶rter")

# LLM mit strukturierter Ausgabe
structured_llm = llm.with_structured_output(FAQEntry)

# Test
test_frage = "Was ist Docker und wofÃ¼r wird es verwendet?"
faq = structured_llm.invoke(f"Erstelle einen FAQ-Eintrag fÃ¼r: {test_frage}")

print("ğŸ“‹ FAQ-Eintrag:")
print(f"   Frage: {faq.frage}")
print(f"   Antwort: {faq.antwort}")
print(f"   Kategorie: {faq.kategorie}")
print(f"   Schwierigkeit: {faq.schwierigkeit}/5")
print(f"   Tags: {', '.join(faq.tags)}")
```

### Aufgabe 4.2: FAQ-Datenbank aufbauen

```python
import json

def create_faq_database():
    """Interaktive FAQ-Erstellung"""
    faq_list = []

    print("ğŸ”§ FAQ-Generator")
    print("   (Schreibe 'exit' zum Beenden und Exportieren)\n")

    while True:
        frage = input("ğŸ’¬ Frage: ")

        if frage.lower() in ['exit', 'quit', 'beenden']:
            break

        if not frage.strip():
            continue

        # FAQ-Eintrag generieren
        faq = structured_llm.invoke(f"Erstelle einen FAQ-Eintrag fÃ¼r: {frage}")

        print(f"\nâœ… FAQ erstellt:")
        print(f"   Kategorie: {faq.kategorie}")
        print(f"   Schwierigkeit: {faq.schwierigkeit}/5")
        print(f"   Tags: {', '.join(faq.tags)}\n")

        faq_list.append(faq.model_dump())

    # Export als JSON
    if faq_list:
        with open('faq_database.json', 'w', encoding='utf-8') as f:
            json.dump(faq_list, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“ {len(faq_list)} FAQ-EintrÃ¤ge exportiert nach 'faq_database.json'")

# Starten
create_faq_database()
```

**Erfolgskriterium:**
- âœ… Strukturierte JSON-Ausgabe
- âœ… Schema-Validierung funktioniert
- âœ… Alle Felder korrekt befÃ¼llt
- âœ… Export in JSON-Datei

---

## Kapitel 5: RAG-Integration (Modul M08)

**Lernziel:** Retrieval-Augmented Generation, Vektordatenbank, Embeddings

### Aufgabe 5.1: Dokumente hochladen (Colab File Upload)

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“š KAPITEL 5: RAG-INTEGRATION (M08)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from google.colab import files
import os

# Verzeichnis fÃ¼r Dokumente erstellen
os.makedirs('docs', exist_ok=True)

# File Upload Widget
print("ğŸ“ Bitte laden Sie 3-5 Markdown-Dateien (.md) hoch")
print("   Beispiel: Docker-Doku, Kubernetes-Intro, REST-API-Guide\n")

uploaded = files.upload()

# Dateien speichern
for filename, content in uploaded.items():
    filepath = f'docs/{filename}'
    with open(filepath, 'wb') as f:
        f.write(content)
    print(f"âœ… Gespeichert: {filepath}")

print(f"\nğŸ“Š {len(uploaded)} Dokumente hochgeladen")
```

### Aufgabe 5.2: Vektordatenbank erstellen

```python
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

# Dokumente laden
loader = DirectoryLoader('docs/', glob="**/*.md", loader_cls=TextLoader)
documents = loader.load()

print(f"ğŸ“„ {len(documents)} Dokumente geladen")

# Text-Splitting
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
splits = text_splitter.split_documents(documents)

print(f"âœ‚ï¸  {len(splits)} Chunks erstellt")

# Embeddings & Vektordatenbank
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings,
    persist_directory="./chroma_db"
)

print("âœ… Vektordatenbank erstellt")

# Retriever
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}
)
```

### Aufgabe 5.3: RAG-Chain implementieren

```python
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# Hilfsfunktion: Dokumente formatieren
def format_docs(docs):
    """Formatiert Retrieved Docs fÃ¼r Prompt"""
    return "\n\n".join([f"Quelle: {doc.metadata.get('source', 'Unbekannt')}\n{doc.page_content}" for doc in docs])

# RAG-Prompt
rag_prompt = ChatPromptTemplate.from_messages([
    ("system", """Du bist ein technischer Dokumentationsassistent.
    Beantworte die Frage NUR basierend auf dem folgenden Kontext.
    Wenn der Kontext keine Antwort liefert, sage das ehrlich.

    Kontext:
    {context}"""),
    ("user", "{frage}")
])

# RAG-Chain (LCEL)
rag_chain = (
    {
        "context": retriever | format_docs,
        "frage": RunnablePassthrough()
    }
    | rag_prompt
    | llm
    | StrOutputParser()
)

# Test
test_frage = "Wie starte ich einen Docker-Container?"
antwort = rag_chain.invoke(test_frage)
print(f"ğŸ¤– {antwort}")
```

### Aufgabe 5.4: RAG-Chat mit Quellenangaben

```python
def rag_chat():
    """RAG-Chat mit Quellenangaben"""
    print("ğŸ¤– Tech-Doku Assistent (RAG-Modus)")
    print("   (Schreibe 'exit' zum Beenden)\n")

    while True:
        frage = input("ğŸ’¬ Sie: ")

        if frage.lower() in ['exit', 'quit', 'beenden']:
            print("ğŸ‘‹ Auf Wiedersehen!")
            break

        if not frage.strip():
            continue

        # Retrieved Docs holen (fÃ¼r Quellenangaben)
        docs = retriever.get_relevant_documents(frage)

        # RAG-Antwort
        antwort = rag_chain.invoke(frage)

        print(f"\nğŸ¤– Bot: {antwort}\n")

        # Quellenangaben
        print("ğŸ“š Quellen:")
        for i, doc in enumerate(docs, 1):
            source = doc.metadata.get('source', 'Unbekannt').split('/')[-1]
            # Berechne Relevanz (Cosine Similarity wÃ¼rde mehr Aufwand bedeuten)
            print(f"   {i}. {source}")

        print()

# Starten
rag_chat()
```

**Erfolgskriterium:**
- âœ… Dokumente werden hochgeladen und indiziert
- âœ… Retrieval findet relevante Chunks
- âœ… Antworten basieren auf Dokumenten
- âœ… Quellenangaben werden angezeigt

---

## Kapitel 6: Agent mit Tools (Modul M10)

**Lernziel:** LangChain Agents, Tool-Definition, Function Calling

### Aufgabe 6.1: Tools definieren

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¤– KAPITEL 6: AGENT MIT TOOLS (M10)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from langchain_core.tools import tool
import ast

@tool
def search_documentation(query: str) -> str:
    """Durchsucht die technische Dokumentation nach relevanten Informationen."""
    # Nutze Retriever aus Kapitel 5
    docs = retriever.get_relevant_documents(query)
    if not docs:
        return "Keine relevanten Dokumente gefunden."

    # Formatiere Ergebnisse
    results = []
    for i, doc in enumerate(docs[:2], 1):  # Top 2
        source = doc.metadata.get('source', 'Unbekannt').split('/')[-1]
        results.append(f"[{source}]: {doc.page_content[:200]}...")

    return "\n\n".join(results)

@tool
def calculate_token_cost(text: str, model: str = "gpt-4o-mini") -> str:
    """Berechnet Token-Anzahl und geschÃ¤tzte Kosten fÃ¼r einen Text."""
    # Token zÃ¤hlen
    tokens = count_tokens(text, model)

    # Kosten berechnen (gpt-4o-mini Preise)
    cost_per_1m_input = 0.15  # USD
    cost_per_1m_output = 0.60  # USD

    # Vereinfachung: Nehme Durchschnitt
    avg_cost = (cost_per_1m_input + cost_per_1m_output) / 2
    cost = (tokens / 1_000_000) * avg_cost

    return f"""Token-Analyse:
    - Modell: {model}
    - Tokens: {tokens}
    - GeschÃ¤tzte Kosten: ${cost:.6f} USD"""

@tool
def validate_python_code(code: str) -> str:
    """Validiert Python-Code auf Syntax-Fehler."""
    try:
        ast.parse(code)
        return "âœ… Code ist syntaktisch korrekt!"
    except SyntaxError as e:
        return f"âŒ Syntax-Fehler in Zeile {e.lineno}: {e.msg}"

# Tool-Liste
tools = [search_documentation, calculate_token_cost, validate_python_code]

# Test einzelner Tools
print(search_documentation.invoke({"query": "Docker Container"}))
```

### Aufgabe 6.2: Agent erstellen

```python
from langchain.agents import create_agent

# Agent erstellen (benÃ¶tigt gpt-4o oder gpt-4o-mini fÃ¼r Function Calling)
agent = create_agent(
    model="openai:gpt-4o-mini",  # Wichtig: Function Calling Support!
    tools=tools,
    system_prompt="""Du bist ein technischer Assistent mit Zugriff auf:
    1. Dokumenten-Suche (search_documentation)
    2. Token-Kosten-Rechner (calculate_token_cost)
    3. Python-Code-Validator (validate_python_code)

    Nutze die Tools intelligent und erklÃ¤re deine Entscheidungen.""",
    debug=True  # Zeigt Tool-Aufrufe
)

# Test-Anfrage
response = agent.invoke({
    "messages": [{"role": "user", "content": "Suche Infos zu Docker"}]
})

print(response["messages"][-1].content)
```

### Aufgabe 6.3: Agent-Chat

```python
def agent_chat():
    """Interactive Agent Chat"""
    print("ğŸ¤– Tech-Doku Assistent (Agent-Modus)")
    print("   Tools: ğŸ“š Doku-Suche | ğŸ“Š Token-Rechner | âœ… Code-Validator")
    print("   (Schreibe 'exit' zum Beenden)\n")

    while True:
        user_input = input("ğŸ’¬ Sie: ")

        if user_input.lower() in ['exit', 'quit', 'beenden']:
            print("ğŸ‘‹ Auf Wiedersehen!")
            break

        if not user_input.strip():
            continue

        # Agent invoke
        response = agent.invoke({
            "messages": [{"role": "user", "content": user_input}]
        })

        # Antwort extrahieren
        bot_message = response["messages"][-1].content

        print(f"\nğŸ¤– Agent: {bot_message}\n")

# Starten
agent_chat()
```

**Erfolgskriterium:**
- âœ… Alle 3 Tools funktionieren einzeln
- âœ… Agent nutzt Tools korrekt
- âœ… Entscheidungslogik ist nachvollziehbar
- âœ… Debug-Modus zeigt Tool-Aufrufe

---

## Kapitel 7: Gradio-UI (Modul M11)

**Lernziel:** Web-Interface mit Gradio, State-Management, Event-Handling

### Aufgabe 7.1: Basis-UI erstellen

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸŒ KAPITEL 7: GRADIO-UI (M11)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import gradio as gr

# Chat-Handler-Funktionen
def chat_handler(message, history):
    """Verarbeitet normale Chat-Anfragen"""
    antwort = chain.invoke({"frage": message})
    history.append((message, antwort))
    return "", history

def rag_handler(message, history):
    """Verarbeitet RAG-basierte Anfragen"""
    antwort = rag_chain.invoke(message)
    # Hole Quellen
    docs = retriever.get_relevant_documents(message)
    sources = [doc.metadata.get('source', 'Unbekannt').split('/')[-1] for doc in docs[:2]]

    full_response = f"{antwort}\n\nğŸ“š Quellen: {', '.join(sources)}"
    history.append((message, full_response))
    return "", history

def agent_handler(message, history):
    """Verarbeitet Agent-Anfragen"""
    response = agent.invoke({
        "messages": [{"role": "user", "content": message}]
    })
    antwort = response["messages"][-1].content
    history.append((message, antwort))
    return "", history

# Token-Statistik berechnen
def calculate_stats(history):
    """Berechnet Token-Statistik aus Chat-History"""
    if not history:
        return "ğŸ“Š Tokens: 0 | Kosten: $0.00"

    total_tokens = sum([
        count_tokens(msg) + count_tokens(reply)
        for msg, reply in history
    ])

    # Kosten (Durchschnitt)
    cost = (total_tokens / 1_000_000) * 0.375

    return f"ğŸ“Š Tokens: {total_tokens} | Kosten: ${cost:.4f}"
```

### Aufgabe 7.2: Gradio-App implementieren

```python
# Gradio Interface
with gr.Blocks(title="Tech-Doku Assistent") as demo:
    gr.Markdown("# ğŸ¤– Tech-Doku Assistent")
    gr.Markdown("*Powered by LangChain & OpenAI*")

    with gr.Tabs():
        # Tab 1: Einfacher Chat
        with gr.Tab("ğŸ’¬ Chat"):
            chatbot1 = gr.Chatbot(height=400, label="Chat-Verlauf")
            msg1 = gr.Textbox(placeholder="Frage hier eingeben...", label="Ihre Frage")
            stats1 = gr.Markdown("ğŸ“Š Tokens: 0 | Kosten: $0.00")

            msg1.submit(chat_handler, [msg1, chatbot1], [msg1, chatbot1]).then(
                lambda h: calculate_stats(h), chatbot1, stats1
            )

            gr.Button("Chat lÃ¶schen").click(lambda: ([], "ğŸ“Š Tokens: 0 | Kosten: $0.00"),
                                             None, [chatbot1, stats1])

        # Tab 2: RAG-Suche
        with gr.Tab("ğŸ“š RAG-Suche"):
            chatbot2 = gr.Chatbot(height=400, label="RAG-Verlauf")
            msg2 = gr.Textbox(placeholder="Frage zur Dokumentation...", label="Ihre Frage")
            stats2 = gr.Markdown("ğŸ“Š Tokens: 0 | Kosten: $0.00")

            msg2.submit(rag_handler, [msg2, chatbot2], [msg2, chatbot2]).then(
                lambda h: calculate_stats(h), chatbot2, stats2
            )

            gr.Button("Chat lÃ¶schen").click(lambda: ([], "ğŸ“Š Tokens: 0 | Kosten: $0.00"),
                                             None, [chatbot2, stats2])

        # Tab 3: Agent
        with gr.Tab("ğŸ¤– Agent"):
            chatbot3 = gr.Chatbot(height=400, label="Agent-Verlauf")
            msg3 = gr.Textbox(placeholder="Komplexe Anfrage an Agent...", label="Ihre Frage")
            stats3 = gr.Markdown("ğŸ“Š Tokens: 0 | Kosten: $0.00")
            gr.Markdown("**Tools:** ğŸ“š Doku-Suche | ğŸ“Š Token-Rechner | âœ… Code-Validator")

            msg3.submit(agent_handler, [msg3, chatbot3], [msg3, chatbot3]).then(
                lambda h: calculate_stats(h), chatbot3, stats3
            )

            gr.Button("Chat lÃ¶schen").click(lambda: ([], "ğŸ“Š Tokens: 0 | Kosten: $0.00"),
                                             None, [chatbot3, stats3])

# Starten (WICHTIG: share=True fÃ¼r Colab!)
demo.launch(share=True, debug=True)
```

**Colab-spezifische Hinweise:**
- `share=True` erstellt einen Ã¶ffentlichen Link (fÃ¼r 72h gÃ¼ltig)
- Der Link kann mit anderen geteilt werden
- Gradio lÃ¤uft direkt in Colab ohne separaten Server

**Erfolgskriterium:**
- âœ… UI lÃ¤uft in Colab mit Ã¶ffentlichem Link
- âœ… Alle 3 Tabs funktionieren
- âœ… Token-Tracking wird live aktualisiert
- âœ… "Chat lÃ¶schen" Button funktioniert

---

## Bonusaufgaben (Optional)

### Bonus 1: Persistenz
- Speichern Sie Chat-History in JSON
- Laden Sie vorherige Sessions beim Start
- Implementieren Sie Session-Management

### Bonus 2: Erweiterte RAG-Features
- Hybrid-Search (Keyword + Semantic)
- Re-Ranking der Retrieval-Ergebnisse
- Chunk-Overlap-Visualisierung

### Bonus 3: Multi-Agenten-System mit LangGraph
- Spezialisierte Agents (Docker-Expert, Kubernetes-Expert)
- Supervisor-Agent zur Koordination
- State Machine fÃ¼r komplexe Workflows

### Bonus 4: Notebook dokumentieren
- Erstellen Sie ein Inhaltsverzeichnis mit Markdown-Zellen
- FÃ¼gen Sie Emoji-Header fÃ¼r jedes Kapitel hinzu
- Dokumentieren Sie Lernziele und Erfolgskriterien

---

## Bewertungskriterien

| Phase | Punkte | Kriterien |
|-------|--------|-----------|
| 1: Basis-Chatbot | 10 | FunktionalitÃ¤t, Code-QualitÃ¤t, LCEL-Nutzung |
| 2: Token-Optimierung | 10 | Korrekte ZÃ¤hlung, Statistiken, Warnungen |
| 3: Chat-Memory | 15 | Context-Awareness, Memory-Management |
| 4: Strukturierte Ausgaben | 15 | Pydantic-Modelle, Validierung |
| 5: RAG-Integration | 20 | Retrieval-QualitÃ¤t, Quellenangaben |
| 6: Agent mit Tools | 20 | Tool-Implementation, Agent-Logik |
| 7: Gradio-UI | 10 | Usability, Features, Design |
| **Gesamt** | **100** | |

**Bestanden:** â‰¥ 60 Punkte

---

## Hilfreiche Ressourcen

**LangChain Dokumentation:**
- [init_chat_model()](https://python.langchain.com/docs/concepts/chat_models/)
- [RAG Tutorial](https://python.langchain.com/docs/tutorials/rag/)
- [Agents](https://python.langchain.com/docs/concepts/agents/)

**Code-Vorlagen:**
- [LangChain 1.0 Must-Haves](/GenAI/LangChain_1.0_Must_Haves.html)
- [Notebook Template Guide](/GenAI/Notebook_Template_Guide.html)

**Projekt-Beispiele:**
- `01_notebook/M04_LangChain101.ipynb`
- `01_notebook/M08_RAG_LangChain.ipynb`
- `01_notebook/M10_Agenten_LangChain.ipynb`

---

## Abgabe

**Format:**
- **Jupyter Notebook** (`Tech_Doku_Assistent.ipynb`)
  - Mit allen 7 Kapiteln ausfÃ¼hrbar
  - Saubere Markdown-Strukturierung
  - Code-Zellen mit Kommentaren
- **Dokumentations-Dateien** (3-5 .md Dateien fÃ¼r RAG)
- **README.md** mit:
  - Kurzbeschreibung des Projekts
  - Setup-Anleitung (API-Keys, Colab-Link)
  - Screenshot des Gradio-UI
- Optional: **Demo-Video** (max. 5 Min.) oder **Colab-Link**

**Deadline:** [Wird vom Dozenten festgelegt]

**Einreichung:**
- Als **Colab-Link** (Ã¶ffentlich freigegeben)
- ODER als **ZIP-Archiv** mit .ipynb + docs/
- ODER als **Git-Repository-Link**

### Checkliste vor Abgabe
- [ ] Notebook lÃ¤uft von oben bis unten fehlerfrei durch
- [ ] Alle API-Keys sind Ã¼ber Colab Secrets eingebunden (nicht hardcodiert!)
- [ ] Alle 7 Kapitel sind implementiert
- [ ] Mindestens 3 Markdown-Dateien fÃ¼r RAG vorhanden
- [ ] Gradio-UI lÃ¤uft und erstellt share-Link
- [ ] Erfolgskriterien aus allen Kapiteln erfÃ¼llt
- [ ] README.md erklÃ¤rt das Projekt

---

## FAQ

**Q: Muss ich alle Kapitel implementieren?**
A: Kapitel 1-5 sind Pflicht (70 Punkte). Kapitel 6-7 sind optional fÃ¼r Bonuspunkte (30 Punkte).

**Q: Kann ich separate Notebooks erstellen statt einem groÃŸen?**
A: Ja! Sie kÃ¶nnen 7 separate Notebooks erstellen (z.B. `Kapitel_1_Chat.ipynb` bis `Kapitel_7_Gradio.ipynb`). Achten Sie dann darauf, dass Kapitel 6-7 auf vorherige Kapitelergebnisse zugreifen kÃ¶nnen.

**Q: Welches LLM-Modell soll ich verwenden?**
A: `gpt-4o-mini` ist ausreichend und kosteneffizient. FÃ¼r Kapitel 6 (Agent) funktioniert `gpt-4o-mini` ebenfalls, da es Function Calling unterstÃ¼tzt.

**Q: Kann ich andere Vektordatenbanken nutzen?**
A: Ja, FAISS ist in Colab sogar etwas schneller als ChromaDB. Qdrant ist ebenfalls mÃ¶glich.

**Q: Wo bekomme ich Markdown-Dateien fÃ¼r RAG?**
A: Optionen:
  - Erstellen Sie eigene .md Dateien mit technischen Infos
  - Laden Sie offizielle Docs herunter (z.B. Docker, Kubernetes)
  - Nutzen Sie `markitdown` fÃ¼r PDFâ†’Markdown Konvertierung
  - Kopieren Sie Wikipedia-Artikel als Markdown

**Q: Mein Colab-Notebook stÃ¼rzt beim Gradio-Launch ab**
A: HÃ¤ufigste Ursachen:
  - RAM-Limit erreicht â†’ Runtime â†’ Factory reset runtime
  - Firewall blockiert share-Link â†’ Versuchen Sie `share=False` fÃ¼r lokalen Zugriff
  - Alte Gradio-Version â†’ `!pip install --upgrade gradio`

**Q: Kann ich die Ãœbung auch lokal (ohne Colab) machen?**
A: Ja! Verwenden Sie dann Jupyter Notebook/Lab lokal und ersetzen Sie:
  - `from google.colab import userdata` â†’ `from dotenv import load_dotenv`
  - `files.upload()` â†’ Lokale File-Pfade
  - `share=True` â†’ Optional fÃ¼r Gradio

---

**Version:** 1.1 (Colab/Jupyter-optimiert)
**Letzte Aktualisierung:** Dezember 2025
**Kurs:** Generative KI. Verstehen. Anwenden. Gestalten.
**Module:** M04, M05, M06, M07, M08, M10, M11
**Arbeitsumgebung:** Google Colab oder Jupyter Notebook

---
layout: default
title: RAG Workshop
parent: Projekte
nav_order: 1
description: "Schrittweise Workshop: Tech-Doku Assistent - vom einfachen Chatbot zur RAG-basierten Anwendung mit UI"
has_toc: true
---

# RAG Workshop
{: .no_toc }

> **Tech-Doku Assistent bauen**
> Schrittweise Entwicklung vom einfachen Chatbot zur intelligenten RAG-Anwendung mit UI (Module M04-M11)

---

# Inhaltsverzeichnis
{: .no_toc .text-delta }

1. TOC
{:toc}

---

## ProjektÃ¼bersicht

In dieser Ãœbungsaufgabe bauen Sie schrittweise einen **Tech-Doku Assistenten**, der technische Fragen beantwortet und dabei immer intelligenter wird.

**Lernziele:**
- Aufbau einer GenAI-Anwendung von Grund auf
- Schrittweise Integration von LangChain-Features
- Praktische Anwendung der Module M04-M11
- Best Practices fÃ¼r strukturierten Notebook-Code

**Zeitaufwand:** ca. 4-6 Stunden (je nach Vorkenntnissen)

**Arbeitsumgebung:** Google Colab oder Jupyter Notebook

---

## Notebook-Struktur

Sie erstellen **ein Notebook** mit **7 aufbauenden Kapiteln** (oder 7 separate Notebooks):

```
ğŸ““ Tech_Doku_Assistent.ipynb
   â”œâ”€â”€ ğŸ¯ Kapitel 1: Basis-Chatbot (M04)
   â”œâ”€â”€ ğŸ“Š Kapitel 2: Token-Optimierung (M05)
   â”œâ”€â”€ ğŸ’¬ Kapitel 3: Chat-History & Memory (M06)
   â”œâ”€â”€ ğŸ”§ Kapitel 4: Strukturierte Ausgaben (M07)
   â”œâ”€â”€ ğŸ“š Kapitel 5: RAG-Integration (M08)
   â”œâ”€â”€ ğŸ¤– Kapitel 6: Agent mit Tools (M10)
   â””â”€â”€ ğŸŒ Kapitel 7: Gradio-UI (M11)
```

**Empfehlung:** Beginnen Sie mit einem Notebook und fÃ¼gen Sie nach jedem Kapitel eine Markdown-Zelle mit "---" zur Trennung hinzu.

---

## Vorbereitung: Google Colab Setup

Bevor Sie starten, richten Sie Ihre Colab-Umgebung ein:

### API-Key in Colab Secrets speichern

1. Klicken Sie in Colab auf das SchlÃ¼ssel-Symbol ğŸ”‘ (linke Sidebar)
2. FÃ¼gen Sie `OPENAI_API_KEY` hinzu
3. Aktivieren Sie "Notebook access"

### Basis-Pakete installieren

FÃ¼hren Sie zu Beginn des Notebooks aus:

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“¦ INSTALLATION (Einmalig ausfÃ¼hren)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

!pip install -q langchain>=1.0.0 langchain-openai>=0.2.0 langchain-community
!pip install -q chromadb tiktoken gradio
```

### API-Key laden

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”‘ API-KEY SETUP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import os
from google.colab import userdata

# API-Key aus Colab Secrets laden
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')
```

---

## Kapitel 1: Basis-Chatbot (Modul M04)

**Lernziel:** LangChain-Grundlagen, Prompt-Templates, einfache LLM-Interaktion

### Aufgabe 1.1: LLM initialisieren

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ KAPITEL 1: BASIS-CHATBOT (M04)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from langchain.chat_models import init_chat_model
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# LLM initialisieren
llm = init_chat_model("openai:gpt-4o-mini", temperature=0.3)
...
```

### Aufgabe 1.2: Interaktive Chat-Schleife

```python
# Chat-Funktion fÃ¼r Notebook
def tech_chat():
    """Einfache Chat-Schleife fÃ¼r Jupyter/Colab"""
    print("ğŸ¤– Tech-Doku Assistent gestartet!")
    ...
```

**Erfolgskriterium:**
- âœ… Der Bot beantwortet technische Fragen korrekt
- âœ… Der Chat lÃ¤uft in einer Schleife (bis "exit")
- âœ… LCEL-Syntax (`|`) wird verwendet

---

## Kapitel 2: Token-Optimierung (Modul M05)

**Lernziel:** Transformer-Konzepte verstehen, Token-ZÃ¤hlung, Kontext-Management

### Aufgabe 2.1: Token-ZÃ¤hlung implementieren

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Š KAPITEL 2: TOKEN-OPTIMIERUNG (M05)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import tiktoken

# Token-Counter Funktion
def count_tokens(text: str, model: str = "gpt-4o-mini") -> int:
    """ZÃ¤hlt Tokens fÃ¼r ein gegebenes Modell"""
    encoding = tiktoken.encoding_for_model(model)
    ...
```

### Aufgabe 2.2: Chat mit Token-Tracking

```python
def tech_chat_mit_tokens():
    """Chat mit Token-Statistiken"""
    print("ğŸ¤– Tech-Doku Assistent (mit Token-Tracking)")
    print("   (Schreibe 'exit' zum Beenden)\n")
    ...
```

**Erfolgskriterium:**
- âœ… Token-ZÃ¤hlung funktioniert korrekt
- âœ… Statistiken werden nach jeder Frage angezeigt
- âœ… Warnung bei langen Antworten (>500 Tokens)
- âœ… Session-Gesamtstatistik am Ende

---

## Kapitel 3: Chat-History & Memory (Modul M06)

**Lernziel:** Konversationskontext verwalten, Chat-History nutzen

### Aufgabe 3.1: Memory implementieren

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ’¬ KAPITEL 3: CHAT-HISTORY & MEMORY (M06)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

# Memory-Store (speichert alle Sessions)
store = {}

def get_session_history(session_id: str) -> InMemoryChatMessageHistory:
    """Holt oder erstellt Chat-History fÃ¼r Session"""
    if session_id not in store:
        ...
```

### Aufgabe 3.2: Chat mit Kontext-Bewusstsein

```python
def tech_chat_mit_memory():
    """Chat mit KonversationsgedÃ¤chtnis"""
    session_id = "user_session_1"
    print("ğŸ¤– Tech-Doku Assistent (mit Memory)")
    ...
```

**Erfolgskriterium:**
- âœ… Bot erinnert sich an vorherige Fragen
- âœ… Antworten beziehen sich auf Kontext
- âœ… 'reset' Befehl lÃ¶scht History
- âœ… History-LÃ¤nge wird angezeigt

---

## Kapitel 4: Strukturierte Ausgaben (Modul M07)

**Lernziel:** Pydantic-Modelle, `with_structured_output()`, JSON-Schema

### Aufgabe 4.1: Pydantic-Modell definieren

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”§ KAPITEL 4: STRUKTURIERTE AUSGABEN (M07)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from pydantic import BaseModel, Field
from typing import Literal

class FAQEntry(BaseModel):
    """Strukturierte FAQ-Eingabe"""
    frage: str = Field(description="Die ursprÃ¼ngliche Frage")
    antwort: str = Field(description="Die Antwort (max 200 Zeichen)")
    ...
```

### Aufgabe 4.2: FAQ-Datenbank aufbauen

```python
import json

def create_faq_database():
    """Interaktive FAQ-Erstellung"""
    faq_list = []
    print("ğŸ”§ FAQ-Generator")
    ...
```

**Erfolgskriterium:**
- âœ… Strukturierte JSON-Ausgabe
- âœ… Schema-Validierung funktioniert
- âœ… Alle Felder korrekt befÃ¼llt
- âœ… Export in JSON-Datei

---

## Kapitel 5: RAG-Integration (Modul M08)

**Lernziel:** Retrieval-Augmented Generation, Vektordatenbank, Embeddings

### Aufgabe 5.1: Dokumente hochladen (Colab File Upload)

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“š KAPITEL 5: RAG-INTEGRATION (M08)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from google.colab import files
import os

# Verzeichnis fÃ¼r Dokumente erstellen
os.makedirs('docs', exist_ok=True)
...
```

### Aufgabe 5.2: Vektordatenbank erstellen

```python
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

# Dokumente laden
loader = DirectoryLoader('docs/', glob="**/*.md", loader_cls=TextLoader)
documents = loader.load()
...
```

### Aufgabe 5.3: RAG-Chain implementieren

```python
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# Hilfsfunktion: Dokumente formatieren
def format_docs(docs):
    """Formatiert Retrieved Docs fÃ¼r Prompt"""
    return "\n\n".join([f"Quelle: {doc.metadata.get('source', 'Unbekannt')}\n{doc.page_content}" for doc in docs])
...
```

### Aufgabe 5.4: RAG-Chat mit Quellenangaben

```python
def rag_chat():
    """RAG-Chat mit Quellenangaben"""
    print("ğŸ¤– Tech-Doku Assistent (RAG-Modus)")
    print("   (Schreibe 'exit' zum Beenden)\n")
    ...
```

**Erfolgskriterium:**
- âœ… Dokumente werden hochgeladen und indiziert
- âœ… Retrieval findet relevante Chunks
- âœ… Antworten basieren auf Dokumenten
- âœ… Quellenangaben werden angezeigt

---

## Kapitel 6: Agent mit Tools (Modul M10)

**Lernziel:** LangChain Agents, Tool-Definition, Function Calling

### Aufgabe 6.1: Tools definieren

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¤– KAPITEL 6: AGENT MIT TOOLS (M10)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from langchain_core.tools import tool
import ast

@tool
def search_documentation(query: str) -> str:
    """Durchsucht die technische Dokumentation nach relevanten Informationen."""
    # Nutze Retriever aus Kapitel 5
    docs = retriever.get_relevant_documents(query)
    ...

@tool
def calculate_token_cost(text: str, model: str = "gpt-4o-mini") -> str:
    """Berechnet Token-Anzahl und geschÃ¤tzte Kosten fÃ¼r einen Text."""
    # Token zÃ¤hlen
    tokens = count_tokens(text, model)
    ...

@tool
def validate_python_code(code: str) -> str:
    """Validiert Python-Code auf Syntax-Fehler."""
    try:
        ast.parse(code)
        ...
```

### Aufgabe 6.2: Agent erstellen

```python
from langchain.agents import create_agent

# Agent erstellen (benÃ¶tigt gpt-4o oder gpt-4o-mini fÃ¼r Function Calling)
agent = create_agent(
    model="openai:gpt-4o-mini",  # Wichtig: Function Calling Support!
    tools=tools,
    ...
)
```

### Aufgabe 6.3: Agent-Chat

```python
def agent_chat():
    """Interactive Agent Chat"""
    print("ğŸ¤– Tech-Doku Assistent (Agent-Modus)")
    print("   Tools: ğŸ“š Doku-Suche | ğŸ“Š Token-Rechner | âœ… Code-Validator")
    ...
```

**Erfolgskriterium:**
- âœ… Alle 3 Tools funktionieren einzeln
- âœ… Agent nutzt Tools korrekt
- âœ… Entscheidungslogik ist nachvollziehbar
- âœ… Debug-Modus zeigt Tool-Aufrufe

---

## Kapitel 7: Gradio-UI (Modul M11)

**Lernziel:** Web-Interface mit Gradio, State-Management, Event-Handling

### Aufgabe 7.1: Basis-UI erstellen

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸŒ KAPITEL 7: GRADIO-UI (M11)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import gradio as gr

# Chat-Handler-Funktionen
def chat_handler(message, history):
    """Verarbeitet normale Chat-Anfragen"""
    antwort = chain.invoke({"frage": message})
    ...

def rag_handler(message, history):
    """Verarbeitet RAG-basierte Anfragen"""
    antwort = rag_chain.invoke(message)
    ...

def agent_handler(message, history):
    """Verarbeitet Agent-Anfragen"""
    response = agent.invoke({
        "messages": [{"role": "user", "content": message}]
    })
    ...
```

### Aufgabe 7.2: Gradio-App implementieren

```python
# Gradio Interface
with gr.Blocks(title="Tech-Doku Assistent") as demo:
    gr.Markdown("# ğŸ¤– Tech-Doku Assistent")
    gr.Markdown("*Powered by LangChain & OpenAI*")
    ...
```

**Colab-spezifische Hinweise:**
- `share=True` erstellt einen Ã¶ffentlichen Link (fÃ¼r 72h gÃ¼ltig)
- Der Link kann mit anderen geteilt werden
- Gradio lÃ¤uft direkt in Colab ohne separaten Server

**Erfolgskriterium:**
- âœ… UI lÃ¤uft in Colab mit Ã¶ffentlichem Link
- âœ… Alle 3 Tabs funktionieren
- âœ… Token-Tracking wird live aktualisiert
- âœ… "Chat lÃ¶schen" Button funktioniert

---

## Bonusaufgaben (Optional)

### Bonus 1: Persistenz
- Speichern Sie Chat-History in JSON
- Laden Sie vorherige Sessions beim Start
- Implementieren Sie Session-Management

### Bonus 2: Erweiterte RAG-Features
- Hybrid-Search (Keyword + Semantic)
- Re-Ranking der Retrieval-Ergebnisse
- Chunk-Overlap-Visualisierung

### Bonus 3: Multi-Agenten-System mit LangGraph
- Spezialisierte Agents (Docker-Expert, Kubernetes-Expert)
- Supervisor-Agent zur Koordination
- State Machine fÃ¼r komplexe Workflows

### Bonus 4: Notebook dokumentieren
- Erstellen Sie ein Inhaltsverzeichnis mit Markdown-Zellen
- FÃ¼gen Sie Emoji-Header fÃ¼r jedes Kapitel hinzu
- Dokumentieren Sie Lernziele und Erfolgskriterien

---

## Bewertungskriterien

| Phase | Punkte | Kriterien |
|-------|--------|-----------|
| 1: Basis-Chatbot | 10 | FunktionalitÃ¤t, Code-QualitÃ¤t, LCEL-Nutzung |
| 2: Token-Optimierung | 10 | Korrekte ZÃ¤hlung, Statistiken, Warnungen |
| 3: Chat-Memory | 15 | Context-Awareness, Memory-Management |
| 4: Strukturierte Ausgaben | 15 | Pydantic-Modelle, Validierung |
| 5: RAG-Integration | 20 | Retrieval-QualitÃ¤t, Quellenangaben |
| 6: Agent mit Tools | 20 | Tool-Implementation, Agent-Logik |
| 7: Gradio-UI | 10 | Usability, Features, Design |
| **Gesamt** | **100** | |

**Bestanden:** â‰¥ 60 Punkte

---

## Hilfreiche Ressourcen

**LangChain Dokumentation:**
- [init_chat_model()](https://python.langchain.com/docs/concepts/chat_models/)
- [RAG Tutorial](https://python.langchain.com/docs/tutorials/rag/)
- [Agents](https://python.langchain.com/docs/concepts/agents/)

**Code-Vorlagen:**
- [LangChain 1.0 Must-Haves](/GenAI/LangChain_1.0_Must_Haves.html)
- [Notebook Template Guide](/GenAI/Notebook_Template_Guide.html)

**Projekt-Beispiele:**
- `01_notebook/M04_LangChain101.ipynb`
- `01_notebook/M08_RAG_LangChain.ipynb`
- `01_notebook/M10_Agenten_LangChain.ipynb`

---

## Abgabe

**Format:**
- **Jupyter Notebook** (`Tech_Doku_Assistent.ipynb`)
  - Mit allen 7 Kapiteln ausfÃ¼hrbar
  - Saubere Markdown-Strukturierung
  - Code-Zellen mit Kommentaren
- **Dokumentations-Dateien** (3-5 .md Dateien fÃ¼r RAG)
- **README.md** mit:
  - Kurzbeschreibung des Projekts
  - Setup-Anleitung (API-Keys, Colab-Link)
  - Screenshot des Gradio-UI
- Optional: **Demo-Video** (max. 5 Min.) oder **Colab-Link**

**Deadline:** [Wird vom Dozenten festgelegt]

**Einreichung:**
- Als **Colab-Link** (Ã¶ffentlich freigegeben)
- ODER als **ZIP-Archiv** mit .ipynb + docs/
- ODER als **Git-Repository-Link**

### Checkliste vor Abgabe
- [ ] Notebook lÃ¤uft von oben bis unten fehlerfrei durch
- [ ] Alle API-Keys sind Ã¼ber Colab Secrets eingebunden (nicht hardcodiert!)
- [ ] Alle 7 Kapitel sind implementiert
- [ ] Mindestens 3 Markdown-Dateien fÃ¼r RAG vorhanden
- [ ] Gradio-UI lÃ¤uft und erstellt share-Link
- [ ] Erfolgskriterien aus allen Kapiteln erfÃ¼llt
- [ ] README.md erklÃ¤rt das Projekt

---

## FAQ

**Q: Muss ich alle Kapitel implementieren?**
A: Kapitel 1-5 sind Pflicht (70 Punkte). Kapitel 6-7 sind optional fÃ¼r Bonuspunkte (30 Punkte).

**Q: Kann ich separate Notebooks erstellen statt einem groÃŸen?**
A: Ja! Sie kÃ¶nnen 7 separate Notebooks erstellen (z.B. `Kapitel_1_Chat.ipynb` bis `Kapitel_7_Gradio.ipynb`). Achten Sie dann darauf, dass Kapitel 6-7 auf vorherige Kapitelergebnisse zugreifen kÃ¶nnen.

**Q: Welches LLM-Modell soll ich verwenden?**
A: `gpt-4o-mini` ist ausreichend und kosteneffizient. FÃ¼r Kapitel 6 (Agent) funktioniert `gpt-4o-mini` ebenfalls, da es Function Calling unterstÃ¼tzt.

**Q: Kann ich andere Vektordatenbanken nutzen?**
A: Ja, FAISS ist in Colab sogar etwas schneller als ChromaDB. Qdrant ist ebenfalls mÃ¶glich.

**Q: Wo bekomme ich Markdown-Dateien fÃ¼r RAG?**
A: Optionen:
  - Erstellen Sie eigene .md Dateien mit technischen Infos
  - Laden Sie offizielle Docs herunter (z.B. Docker, Kubernetes)
  - Nutzen Sie `markitdown` fÃ¼r PDFâ†’Markdown Konvertierung
  - Kopieren Sie Wikipedia-Artikel als Markdown

**Q: Mein Colab-Notebook stÃ¼rzt beim Gradio-Launch ab**
A: HÃ¤ufigste Ursachen:
  - RAM-Limit erreicht â†’ Runtime â†’ Factory reset runtime
  - Firewall blockiert share-Link â†’ Versuchen Sie `share=False` fÃ¼r lokalen Zugriff
  - Alte Gradio-Version â†’ `!pip install --upgrade gradio`

**Q: Kann ich die Ãœbung auch lokal (ohne Colab) machen?**
A: Ja! Verwenden Sie dann Jupyter Notebook/Lab lokal und ersetzen Sie:
  - `from google.colab import userdata` â†’ `from dotenv import load_dotenv`
  - `files.upload()` â†’ Lokale File-Pfade
  - `share=True` â†’ Optional fÃ¼r Gradio

---

**Version:** 1.2 (Ohne LÃ¶sungen - nur Aufgabenstellung)
**Letzte Aktualisierung:** Januar 2026
**Kurs:** Generative KI. Verstehen. Anwenden. Gestalten.
**Module:** M04, M05, M06, M07, M08, M10, M11
**Arbeitsumgebung:** Google Colab oder Jupyter Notebook
